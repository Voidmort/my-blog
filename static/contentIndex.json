{"Kaggle/AerialCactusIdentification":{"slug":"Kaggle/AerialCactusIdentification","filePath":"Kaggle/AerialCactusIdentification.md","title":"Aerial Cactus Identification","links":[],"tags":["Kaggle","keras"],"content":"仙人掌识别\nTo assess the impact of climate change on Earth’s flora and fauna, it is vital to quantify how human activities such as logging, mining, and agriculture are impacting our protected natural areas. Researchers in Mexico have created the VIGIA project, which aims to build a system for autonomous surveillance of protected areas. A first step in such an effort is the ability to recognize the vegetation inside the protected areas. In this competition, you are tasked with creation of an algorithm that can identify a specific type of cactus in aerial imagery.\nwww.kaggle.com/c/aerial-cactus-identification\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: github.com/kaggle/docker-python\n# For example, here&#039;s several helpful packages to load in \n \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n \n# Input data files are available in the &quot;../input/&quot; directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n \nimport os\nprint(os.listdir(&quot;../input&quot;))\n \n# Any results you write to the current directory are saved as output.\n[&#039;aerial-cactus-identification&#039;]\n\nfrom tqdm import tqdm, tqdm_notebook\nimport cv2\nimport seaborn as sns\nfrom keras import layers\nfrom keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\nfrom keras.models import Model, load_model\nfrom keras.utils import layer_utils\nfrom keras.utils.data_utils import get_file\nfrom keras.applications.imagenet_utils import preprocess_input\nfrom keras.initializers import glorot_uniform\nfrom keras import optimizers\nimge 预处理\n加载数据集\n首先解压图片数据\nimport zipfile\n \nDataset = &quot;aerial-cactus-identification&quot;\nwith zipfile.ZipFile(&quot;../input/&quot;+Dataset+&quot;/train.zip&quot;,&quot;r&quot;) as z:\n    z.extractall(&quot;.&quot;)\nwith zipfile.ZipFile(&quot;../input/&quot;+Dataset+&quot;/test.zip&quot;,&quot;r&quot;) as z:\n    z.extractall(&quot;.&quot;)\nprint(os.listdir(&quot;./&quot;))\n[&#039;train&#039;, &#039;test&#039;, &#039;__notebook_source__.ipynb&#039;]\n\ntrain_path = &#039;./train/&#039;\ntest_path = &quot;./test/&quot;\ntrain_csv = pd.read_csv(&#039;../input/aerial-cactus-identification/train.csv&#039;)\n把图片转换为矩阵\nlabels = []\nimages = []\nimage_name = train_csv[&#039;id&#039;].values\nfor img_id in tqdm_notebook(image_name):\n    imdata = cv2.imread(train_path + img_id)\n    images.append(imdata)\n    has_cactus = train_csv[train_csv[&#039;id&#039;] == img_id][&#039;has_cactus&#039;].values\n    labels.append(has_cactus)\nHBox(children=(IntProgress(value=0, max=17500), HTML(value=&#039;&#039;)))\n\nlabels[0:10]\n[array([1]),\n array([1]),\n array([1]),\n array([1]),\n array([1]),\n array([1]),\n array([0]),\n array([0]),\n array([1]),\n array([1])]\n\n画出图片观察一下，第1，2个图片有仙人掌，第7个图片没有。\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(1, 3)\nax[0].imshow(images[0])\nax[1].imshow(images[1])\nax[2].imshow(images[6])\n\n数值归一化\nimages = np.asarray(images)\nimages = images.astype(&#039;float32&#039;)\nimages /= 255\nlabels = np.asarray(labels)\nimages.shape, labels.shape\n((17500, 32, 32, 3), (17500, 1))\n\n画出有无仙人掌图片条形图\nsns.countplot(np.squeeze(labels))\n\n构建模型\n# GRADED FUNCTION: identity_block\n \ndef identity_block(X, f, filters, stage, block):\n    &quot;&quot;&quot;\n    Implementation of the identity block as defined in Figure 3\n    \n    Arguments:\n    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n    f -- integer, specifying the shape of the middle CONV&#039;s window for the main path\n    filters -- python list of integers, defining the number of filters in the CONV layers of the main path\n    stage -- integer, used to name the layers, depending on their position in the network\n    block -- string/character, used to name the layers, depending on their position in the network\n    \n    Returns:\n    X -- output of the identity block, tensor of shape (n_H, n_W, n_C)\n    &quot;&quot;&quot;\n    \n    # defining name basis\n    conv_name_base = &#039;res&#039; + str(stage) + block + &#039;_branch&#039;\n    bn_name_base = &#039;bn&#039; + str(stage) + block + &#039;_branch&#039;\n    \n    # Retrieve Filters\n    F1, F2, F3 = filters\n    \n    # Save the input value. You&#039;ll need this later to add back to the main path. \n    X_shortcut = X\n    \n    # First component of main path\n    X = Conv2D(filters = F1, kernel_size = (1, 1), strides = (1,1), padding = &#039;valid&#039;, name = conv_name_base + &#039;2a&#039;, kernel_initializer = glorot_uniform(seed=0))(X)\n    X = BatchNormalization(axis = 3, name = bn_name_base + &#039;2a&#039;)(X)\n    X = Activation(&#039;relu&#039;)(X)\n    \n    ### START CODE HERE ###\n    \n    # Second component of main path (≈3 lines)\n    X = Conv2D(filters = F2, kernel_size = (f, f), strides = (1, 1), padding = &quot;same&quot;, name = conv_name_base + &quot;2b&quot;, kernel_initializer = glorot_uniform(seed=0))(X)\n    X = BatchNormalization(axis = 3, name = bn_name_base + &quot;2b&quot;)(X)\n    X = Activation(&quot;relu&quot;)(X)\n \n    # Third component of main path (≈2 lines)\n    X = Conv2D(filters = F3, kernel_size = (1, 1), strides = (1, 1), padding = &quot;valid&quot;, name = conv_name_base + &quot;2c&quot;, kernel_initializer = glorot_uniform(seed=0))(X)\n    X = BatchNormalization(axis = 3, name = bn_name_base + &quot;2c&quot;)(X)\n \n    # Final step: Add shortcut value to main path, and pass it through a RELU activation (≈2 lines)\n    X = Add()([X, X_shortcut])\n    X = Activation(&quot;relu&quot;)(X)\n    \n    ### END CODE HERE ###\n    \n    return X\n# GRADED FUNCTION: convolutional_block\n \ndef convolutional_block(X, f, filters, stage, block, s = 2):\n    &quot;&quot;&quot;\n    Implementation of the convolutional block as defined in Figure 4\n    \n    Arguments:\n    X -- input tensor of shape (m, n_H_prev, n_W_prev, n_C_prev)\n    f -- integer, specifying the shape of the middle CONV&#039;s window for the main path\n    filters -- python list of integers, defining the number of filters in the CONV layers of the main path\n    stage -- integer, used to name the layers, depending on their position in the network\n    block -- string/character, used to name the layers, depending on their position in the network\n    s -- Integer, specifying the stride to be used\n    \n    Returns:\n    X -- output of the convolutional block, tensor of shape (n_H, n_W, n_C)\n    &quot;&quot;&quot;\n    \n    # defining name basis\n    conv_name_base = &#039;res&#039; + str(stage) + block + &#039;_branch&#039;\n    bn_name_base = &#039;bn&#039; + str(stage) + block + &#039;_branch&#039;\n    \n    # Retrieve Filters\n    F1, F2, F3 = filters\n    \n    # Save the input value\n    X_shortcut = X\n \n \n    ##### MAIN PATH #####\n    # First component of main path \n    X = Conv2D(F1, (1, 1), strides = (s,s), name = conv_name_base + &#039;2a&#039;, kernel_initializer = glorot_uniform(seed=0))(X)\n    X = BatchNormalization(axis = 3, name = bn_name_base + &#039;2a&#039;)(X)\n    X = Activation(&#039;relu&#039;)(X)\n    \n    ### START CODE HERE ###\n \n    # Second component of main path (≈3 lines)\n    X = Conv2D(F2, (f, f), strides = (1, 1), name = conv_name_base + &quot;2b&quot;, kernel_initializer = glorot_uniform(seed=0), padding = &quot;same&quot;)(X)\n    X = BatchNormalization(axis = 3, name = bn_name_base + &quot;2b&quot;)(X)\n    X = Activation(&quot;relu&quot;)(X)\n \n    # Third component of main path (≈2 lines)\n    X = Conv2D(F3, (1, 1), strides = (1, 1), name = conv_name_base + &quot;2c&quot;, kernel_initializer = glorot_uniform(seed=0))(X)\n    X = BatchNormalization(axis = 3, name = bn_name_base + &quot;2c&quot;)(X)\n \n    ##### SHORTCUT PATH #### (≈2 lines)\n    X_shortcut = Conv2D(F3, (1, 1), strides = (s, s), name = conv_name_base + &quot;1&quot;, kernel_initializer = glorot_uniform(seed=0))(X_shortcut)\n    X_shortcut = BatchNormalization(axis = 3, name = bn_name_base + &quot;1&quot;)(X_shortcut)\n \n    # Final step: Add shortcut value to main path, and pass it through a RELU activation (≈2 lines)\n    X = Add()([X, X_shortcut])\n    X = Activation(&quot;relu&quot;)(X)\n    \n    ### END CODE HERE ###\n    \n    return X\n# GRADED FUNCTION: ResNet50\n \ndef ResNet50(input_shape = (32, 32, 3), classes = 1):\n    &quot;&quot;&quot;\n    Implementation of the popular ResNet50 the following architecture:\n    CONV2D -&gt; BATCHNORM -&gt; RELU -&gt; MAXPOOL -&gt; CONVBLOCK -&gt; IDBLOCK*2 -&gt; CONVBLOCK -&gt; IDBLOCK*3\n    -&gt; CONVBLOCK -&gt; IDBLOCK*5 -&gt; CONVBLOCK -&gt; IDBLOCK*2 -&gt; AVGPOOL -&gt; TOPLAYER\n \n    Arguments:\n    input_shape -- shape of the images of the dataset\n    classes -- integer, number of classes\n \n    Returns:\n    model -- a Model() instance in Keras\n    &quot;&quot;&quot;\n    \n    # Define the input as a tensor with shape input_shape\n    X_input = Input(input_shape)\n \n    \n    # Zero-Padding\n    X = ZeroPadding2D((3, 3))(X_input)\n    \n    # Stage 1\n    X = Conv2D(32, (7, 7), strides = (2, 2), name = &#039;conv1&#039;, kernel_initializer = glorot_uniform(seed=0))(X)\n    X = BatchNormalization(axis = 3, name = &#039;bn_conv1&#039;)(X)\n    X = Activation(&#039;relu&#039;)(X)\n    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n \n    # Stage 2\n    X = convolutional_block(X, f = 3, filters = [32, 32, 256], stage = 2, block=&#039;a&#039;, s = 1)\n    X = identity_block(X, 3, [32, 32, 256], stage=2, block=&#039;b&#039;)\n    ### START CODE HERE ###\n \n    # Stage 3 (≈4 lines)\n    X = convolutional_block(X, f = 3, filters = [64, 64, 512], stage = 3, block = &quot;a&quot;, s = 2)\n    X = identity_block(X, f = 3, filters = [64, 64, 512], stage = 3, block = &quot;b&quot;)\n \n    # Stage 4 (≈6 lines)\n    X = convolutional_block(X, f = 3, filters = [128, 128, 1024], stage = 4, block = &quot;a&quot;, s = 2)\n    X = identity_block(X, f = 3, filters = [128, 128, 1024], stage = 4, block = &quot;b&quot;)\n \n    # Stage 5 (≈3 lines)\n    X = convolutional_block(X, f = 3, filters = [256, 256, 2048], stage = 5, block = &quot;a&quot;, s = 2)\n    # AVGPOOL (≈1 line). Use &quot;X = AveragePooling2D(...)(X)&quot;\n    #X = AveragePooling2D((2, 2), name = &quot;ave_pool&quot;)(X)\n    \n    ### END CODE HERE ###\n \n    # output layer\n    X = Flatten()(X)\n    X = Dense(classes, activation=&#039;softmax&#039;, name=&#039;fc&#039; + str(classes), kernel_initializer = glorot_uniform(seed=0))(X)\n    \n    \n    # Create model\n    model = Model(inputs = X_input, outputs = X, name=&#039;ResNet50&#039;)\n \n    return model\nmodel = ResNet50()\nmodel.summary()\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_1 (Conv2D)            (None, 30, 30, 32)        896       \n_________________________________________________________________\nconv2d_2 (Conv2D)            (None, 28, 28, 32)        9248      \n_________________________________________________________________\nmax_pooling2d_2 (MaxPooling2 (None, 14, 14, 32)        0         \n_________________________________________________________________\nconv2d_3 (Conv2D)            (None, 12, 12, 64)        18496     \n_________________________________________________________________\nconv2d_4 (Conv2D)            (None, 10, 10, 64)        36928     \n_________________________________________________________________\nmax_pooling2d_3 (MaxPooling2 (None, 5, 5, 64)          0         \n_________________________________________________________________\nflatten_2 (Flatten)          (None, 1600)              0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 512)               819712    \n_________________________________________________________________\ndense_2 (Dense)              (None, 1)                 513       \n=================================================================\nTotal params: 885,793\nTrainable params: 885,793\nNon-trainable params: 0\n_________________________________________________________________\n\n开始预测\nmodel.compile(loss=&#039;binary_crossentropy&#039;,      \n              optimizer=optimizers.RMSprop(lr=1e-4),         \n              metrics=[&#039;acc&#039;])\nhist = model.fit(images, labels,\n                validation_split=0.2,\n                batch_size=100,\n                epochs = 20,\n                )\nTrain on 14000 samples, validate on 3500 samples\nEpoch 1/20\n14000/14000 [==============================] - 4s 316us/step - loss: 0.3205 - acc: 0.8562 - val_loss: 0.2419 - val_acc: 0.9023\nEpoch 2/20\n14000/14000 [==============================] - 1s 74us/step - loss: 0.1864 - acc: 0.9275 - val_loss: 0.1531 - val_acc: 0.9431\nEpoch 3/20\n14000/14000 [==============================] - 1s 74us/step - loss: 0.1672 - acc: 0.9336 - val_loss: 0.1459 - val_acc: 0.9429\nEpoch 4/20\n14000/14000 [==============================] - 1s 74us/step - loss: 0.1520 - acc: 0.9414 - val_loss: 0.1198 - val_acc: 0.9563\nEpoch 5/20\n14000/14000 [==============================] - 1s 74us/step - loss: 0.1383 - acc: 0.9474 - val_loss: 0.1515 - val_acc: 0.9437\nEpoch 6/20\n14000/14000 [==============================] - 1s 75us/step - loss: 0.1271 - acc: 0.9528 - val_loss: 0.1026 - val_acc: 0.9649\nEpoch 7/20\n14000/14000 [==============================] - 1s 74us/step - loss: 0.1176 - acc: 0.9560 - val_loss: 0.1102 - val_acc: 0.9611\nEpoch 8/20\n14000/14000 [==============================] - 1s 76us/step - loss: 0.1113 - acc: 0.9574 - val_loss: 0.0971 - val_acc: 0.9663\nEpoch 9/20\n14000/14000 [==============================] - 1s 75us/step - loss: 0.1046 - acc: 0.9626 - val_loss: 0.0826 - val_acc: 0.9729\nEpoch 10/20\n14000/14000 [==============================] - 1s 75us/step - loss: 0.1002 - acc: 0.9626 - val_loss: 0.0854 - val_acc: 0.9700\nEpoch 11/20\n14000/14000 [==============================] - 1s 75us/step - loss: 0.0928 - acc: 0.9646 - val_loss: 0.0860 - val_acc: 0.9700\nEpoch 12/20\n14000/14000 [==============================] - 1s 75us/step - loss: 0.0869 - acc: 0.9683 - val_loss: 0.2181 - val_acc: 0.9163\nEpoch 13/20\n14000/14000 [==============================] - 1s 75us/step - loss: 0.0829 - acc: 0.9694 - val_loss: 0.0929 - val_acc: 0.9663\nEpoch 14/20\n14000/14000 [==============================] - 1s 75us/step - loss: 0.0793 - acc: 0.9705 - val_loss: 0.0618 - val_acc: 0.9814\nEpoch 15/20\n14000/14000 [==============================] - 1s 74us/step - loss: 0.0737 - acc: 0.9731 - val_loss: 0.0687 - val_acc: 0.9777\nEpoch 16/20\n14000/14000 [==============================] - 1s 74us/step - loss: 0.0718 - acc: 0.9738 - val_loss: 0.0578 - val_acc: 0.9823\nEpoch 17/20\n14000/14000 [==============================] - 1s 74us/step - loss: 0.0671 - acc: 0.9750 - val_loss: 0.0616 - val_acc: 0.9786\nEpoch 18/20\n14000/14000 [==============================] - 1s 74us/step - loss: 0.0620 - acc: 0.9784 - val_loss: 0.1049 - val_acc: 0.9626\nEpoch 19/20\n14000/14000 [==============================] - 1s 75us/step - loss: 0.0612 - acc: 0.9771 - val_loss: 0.0627 - val_acc: 0.9791\nEpoch 20/20\n14000/14000 [==============================] - 1s 74us/step - loss: 0.0592 - acc: 0.9780 - val_loss: 0.0547 - val_acc: 0.9834\n\nhist.history.keys()\ndict_keys([&#039;val_loss&#039;, &#039;val_acc&#039;, &#039;loss&#039;, &#039;acc&#039;])\n\nimport matplotlib.pyplot as plt\n# summarize history for accuracy\nplt.plot(hist.history[&#039;acc&#039;])\nplt.plot(hist.history[&#039;val_acc&#039;])\nplt.title(&#039;model accuracy&#039;)\nplt.ylabel(&#039;accuracy&#039;)\nplt.xlabel(&#039;epoch&#039;)\nplt.legend([&#039;train&#039;, &#039;test&#039;], loc=&#039;upper left&#039;)\nplt.show()\n# summarize history for loss\nplt.plot(hist.history[&#039;loss&#039;])\nplt.plot(hist.history[&#039;val_loss&#039;])\nplt.title(&#039;model loss&#039;)\nplt.ylabel(&#039;loss&#039;)\nplt.xlabel(&#039;epoch&#039;)\nplt.legend([&#039;train&#039;, &#039;test&#039;], loc=&#039;upper left&#039;)\nplt.show()\n\n\n预测\ntest = []\ntest_id = []\nfor img_id in tqdm_notebook(os.listdir(test_path)):\n    test.append(cv2.imread(test_path + img_id))     \n    test_id.append(img_id)\ntest = np.asarray(test)\ntest = test.astype(&#039;float32&#039;)\ntest /= 255\ntest_predictions = model.predict(test)\nsub_df = pd.DataFrame(test_predictions, columns=[&#039;has_cactus&#039;])\nsub_df[&#039;has_cactus&#039;] = sub_df[&#039;has_cactus&#039;].apply(lambda x: 1 if x &gt; 0.75 else 0)\nsub_df[&#039;id&#039;] = &#039;&#039;\ncols = sub_df.columns.tolist()\ncols = cols[-1:] + cols[:-1]\nsub_df=sub_df[cols]\nfor i, img in enumerate(test_id):\n    sub_df.set_value(i,&#039;id&#039;,img)\nsns.countplot(sub_df[&#039;has_cactus&#039;])\n\nsub_df.to_csv(&#039;submission.csv&#039;,index=False)\n得分\nPublic Score\n0.9583\n简单的模型\n#model\nfrom keras import models\nmodel = models.Sequential() \n \nmodel.add(layers.Conv2D(32, (3, 3), activation=&#039;relu&#039;,                     \n                                    input_shape=(32, 32, 3))) \nmodel.add(layers.Conv2D(32, (3, 3), activation=&#039;relu&#039;))\nmodel.add(layers.MaxPooling2D((2, 2))) \n \n \nmodel.add(layers.Conv2D(64, (3, 3), activation=&#039;relu&#039;))\nmodel.add(layers.Conv2D(64, (3, 3), activation=&#039;relu&#039;))\nmodel.add(layers.MaxPooling2D((2, 2)))\n \nmodel.add(layers.Flatten()) \nmodel.add(layers.Dense(512, activation=&#039;relu&#039;)) \nmodel.add(layers.Dense(1, activation=&#039;sigmoid&#039;))"},"Kaggle/House_Prices_Advanced_Regression_Techniques":{"slug":"Kaggle/House_Prices_Advanced_Regression_Techniques","filePath":"Kaggle/House_Prices_Advanced_Regression_Techniques.md","title":"House Prices Advanced Regression Techniques","links":[],"tags":["Kaggle"],"content":"Kaggle Competition 的练习\n房价预测\n# 数据分析库\nimport pandas as pd\nimport numpy as np\nimport random\n \n# 数据可视化\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n \n# 机器学习库\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n \npd.options.display.max_rows = 10  # 最大显示行数\npd.options.display.float_format = &#039;{:.5f}&#039;.format  # 精确度 保留一位小数\ntrain_df = pd.read_csv(&#039;/train.csv&#039;)\ntest_df = pd.read_csv(&#039;/test.csv&#039;)\ntrain_df.shape, test_df.shape\n((1460, 81), (1459, 80))\n\ntrain_df.head()\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      Id\n      MSSubClass\n      MSZoning\n      LotFrontage\n      LotArea\n      Street\n      Alley\n      LotShape\n      LandContour\n      Utilities\n      LotConfig\n      LandSlope\n      Neighborhood\n      Condition1\n      Condition2\n      BldgType\n      HouseStyle\n      OverallQual\n      OverallCond\n      YearBuilt\n      YearRemodAdd\n      RoofStyle\n      RoofMatl\n      Exterior1st\n      Exterior2nd\n      MasVnrType\n      MasVnrArea\n      ExterQual\n      ExterCond\n      Foundation\n      BsmtQual\n      BsmtCond\n      BsmtExposure\n      BsmtFinType1\n      BsmtFinSF1\n      BsmtFinType2\n      BsmtFinSF2\n      BsmtUnfSF\n      TotalBsmtSF\n      Heating\n      ...\n      CentralAir\n      Electrical\n      1stFlrSF\n      2ndFlrSF\n      LowQualFinSF\n      GrLivArea\n      BsmtFullBath\n      BsmtHalfBath\n      FullBath\n      HalfBath\n      BedroomAbvGr\n      KitchenAbvGr\n      KitchenQual\n      TotRmsAbvGrd\n      Functional\n      Fireplaces\n      FireplaceQu\n      GarageType\n      GarageYrBlt\n      GarageFinish\n      GarageCars\n      GarageArea\n      GarageQual\n      GarageCond\n      PavedDrive\n      WoodDeckSF\n      OpenPorchSF\n      EnclosedPorch\n      3SsnPorch\n      ScreenPorch\n      PoolArea\n      PoolQC\n      Fence\n      MiscFeature\n      MiscVal\n      MoSold\n      YrSold\n      SaleType\n      SaleCondition\n      SalePrice\n    \n  \n  \n    \n      0\n      1\n      60\n      RL\n      65.00000\n      8450\n      Pave\n      NaN\n      Reg\n      Lvl\n      AllPub\n      Inside\n      Gtl\n      CollgCr\n      Norm\n      Norm\n      1Fam\n      2Story\n      7\n      5\n      2003\n      2003\n      Gable\n      CompShg\n      VinylSd\n      VinylSd\n      BrkFace\n      196.00000\n      Gd\n      TA\n      PConc\n      Gd\n      TA\n      No\n      GLQ\n      706\n      Unf\n      0\n      150\n      856\n      GasA\n      ...\n      Y\n      SBrkr\n      856\n      854\n      0\n      1710\n      1\n      0\n      2\n      1\n      3\n      1\n      Gd\n      8\n      Typ\n      0\n      NaN\n      Attchd\n      2003.00000\n      RFn\n      2\n      548\n      TA\n      TA\n      Y\n      0\n      61\n      0\n      0\n      0\n      0\n      NaN\n      NaN\n      NaN\n      0\n      2\n      2008\n      WD\n      Normal\n      208500\n    \n    \n      1\n      2\n      20\n      RL\n      80.00000\n      9600\n      Pave\n      NaN\n      Reg\n      Lvl\n      AllPub\n      FR2\n      Gtl\n      Veenker\n      Feedr\n      Norm\n      1Fam\n      1Story\n      6\n      8\n      1976\n      1976\n      Gable\n      CompShg\n      MetalSd\n      MetalSd\n      None\n      0.00000\n      TA\n      TA\n      CBlock\n      Gd\n      TA\n      Gd\n      ALQ\n      978\n      Unf\n      0\n      284\n      1262\n      GasA\n      ...\n      Y\n      SBrkr\n      1262\n      0\n      0\n      1262\n      0\n      1\n      2\n      0\n      3\n      1\n      TA\n      6\n      Typ\n      1\n      TA\n      Attchd\n      1976.00000\n      RFn\n      2\n      460\n      TA\n      TA\n      Y\n      298\n      0\n      0\n      0\n      0\n      0\n      NaN\n      NaN\n      NaN\n      0\n      5\n      2007\n      WD\n      Normal\n      181500\n    \n    \n      2\n      3\n      60\n      RL\n      68.00000\n      11250\n      Pave\n      NaN\n      IR1\n      Lvl\n      AllPub\n      Inside\n      Gtl\n      CollgCr\n      Norm\n      Norm\n      1Fam\n      2Story\n      7\n      5\n      2001\n      2002\n      Gable\n      CompShg\n      VinylSd\n      VinylSd\n      BrkFace\n      162.00000\n      Gd\n      TA\n      PConc\n      Gd\n      TA\n      Mn\n      GLQ\n      486\n      Unf\n      0\n      434\n      920\n      GasA\n      ...\n      Y\n      SBrkr\n      920\n      866\n      0\n      1786\n      1\n      0\n      2\n      1\n      3\n      1\n      Gd\n      6\n      Typ\n      1\n      TA\n      Attchd\n      2001.00000\n      RFn\n      2\n      608\n      TA\n      TA\n      Y\n      0\n      42\n      0\n      0\n      0\n      0\n      NaN\n      NaN\n      NaN\n      0\n      9\n      2008\n      WD\n      Normal\n      223500\n    \n    \n      3\n      4\n      70\n      RL\n      60.00000\n      9550\n      Pave\n      NaN\n      IR1\n      Lvl\n      AllPub\n      Corner\n      Gtl\n      Crawfor\n      Norm\n      Norm\n      1Fam\n      2Story\n      7\n      5\n      1915\n      1970\n      Gable\n      CompShg\n      Wd Sdng\n      Wd Shng\n      None\n      0.00000\n      TA\n      TA\n      BrkTil\n      TA\n      Gd\n      No\n      ALQ\n      216\n      Unf\n      0\n      540\n      756\n      GasA\n      ...\n      Y\n      SBrkr\n      961\n      756\n      0\n      1717\n      1\n      0\n      1\n      0\n      3\n      1\n      Gd\n      7\n      Typ\n      1\n      Gd\n      Detchd\n      1998.00000\n      Unf\n      3\n      642\n      TA\n      TA\n      Y\n      0\n      35\n      272\n      0\n      0\n      0\n      NaN\n      NaN\n      NaN\n      0\n      2\n      2006\n      WD\n      Abnorml\n      140000\n    \n    \n      4\n      5\n      60\n      RL\n      84.00000\n      14260\n      Pave\n      NaN\n      IR1\n      Lvl\n      AllPub\n      FR2\n      Gtl\n      NoRidge\n      Norm\n      Norm\n      1Fam\n      2Story\n      8\n      5\n      2000\n      2000\n      Gable\n      CompShg\n      VinylSd\n      VinylSd\n      BrkFace\n      350.00000\n      Gd\n      TA\n      PConc\n      Gd\n      TA\n      Av\n      GLQ\n      655\n      Unf\n      0\n      490\n      1145\n      GasA\n      ...\n      Y\n      SBrkr\n      1145\n      1053\n      0\n      2198\n      1\n      0\n      2\n      1\n      4\n      1\n      Gd\n      9\n      Typ\n      1\n      TA\n      Attchd\n      2000.00000\n      RFn\n      3\n      836\n      TA\n      TA\n      Y\n      192\n      84\n      0\n      0\n      0\n      0\n      NaN\n      NaN\n      NaN\n      0\n      12\n      2008\n      WD\n      Normal\n      250000\n    \n  \n\n5 rows × 81 columns\n\n# test_df.head()\ntrain_df.describe()\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      Id\n      MSSubClass\n      LotFrontage\n      LotArea\n      OverallQual\n      OverallCond\n      YearBuilt\n      YearRemodAdd\n      MasVnrArea\n      BsmtFinSF1\n      BsmtFinSF2\n      BsmtUnfSF\n      TotalBsmtSF\n      1stFlrSF\n      2ndFlrSF\n      LowQualFinSF\n      GrLivArea\n      BsmtFullBath\n      BsmtHalfBath\n      FullBath\n      HalfBath\n      BedroomAbvGr\n      KitchenAbvGr\n      TotRmsAbvGrd\n      Fireplaces\n      GarageYrBlt\n      GarageCars\n      GarageArea\n      WoodDeckSF\n      OpenPorchSF\n      EnclosedPorch\n      3SsnPorch\n      ScreenPorch\n      PoolArea\n      MiscVal\n      MoSold\n      YrSold\n      SalePrice\n    \n  \n  \n    \n      count\n      1460.00000\n      1460.00000\n      1201.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1452.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1379.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n    \n    \n      mean\n      730.50000\n      56.89726\n      70.04996\n      10516.82808\n      6.09932\n      5.57534\n      1971.26781\n      1984.86575\n      103.68526\n      443.63973\n      46.54932\n      567.24041\n      1057.42945\n      1162.62671\n      346.99247\n      5.84452\n      1515.46370\n      0.42534\n      0.05753\n      1.56507\n      0.38288\n      2.86644\n      1.04658\n      6.51781\n      0.61301\n      1978.50616\n      1.76712\n      472.98014\n      94.24452\n      46.66027\n      21.95411\n      3.40959\n      15.06096\n      2.75890\n      43.48904\n      6.32192\n      2007.81575\n      180921.19589\n    \n    \n      std\n      421.61001\n      42.30057\n      24.28475\n      9981.26493\n      1.38300\n      1.11280\n      30.20290\n      20.64541\n      181.06621\n      456.09809\n      161.31927\n      441.86696\n      438.70532\n      386.58774\n      436.52844\n      48.62308\n      525.48038\n      0.51891\n      0.23875\n      0.55092\n      0.50289\n      0.81578\n      0.22034\n      1.62539\n      0.64467\n      24.68972\n      0.74732\n      213.80484\n      125.33879\n      66.25603\n      61.11915\n      29.31733\n      55.75742\n      40.17731\n      496.12302\n      2.70363\n      1.32810\n      79442.50288\n    \n    \n      min\n      1.00000\n      20.00000\n      21.00000\n      1300.00000\n      1.00000\n      1.00000\n      1872.00000\n      1950.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      334.00000\n      0.00000\n      0.00000\n      334.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      2.00000\n      0.00000\n      1900.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      1.00000\n      2006.00000\n      34900.00000\n    \n    \n      25%\n      365.75000\n      20.00000\n      59.00000\n      7553.50000\n      5.00000\n      5.00000\n      1954.00000\n      1967.00000\n      0.00000\n      0.00000\n      0.00000\n      223.00000\n      795.75000\n      882.00000\n      0.00000\n      0.00000\n      1129.50000\n      0.00000\n      0.00000\n      1.00000\n      0.00000\n      2.00000\n      1.00000\n      5.00000\n      0.00000\n      1961.00000\n      1.00000\n      334.50000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      5.00000\n      2007.00000\n      129975.00000\n    \n    \n      50%\n      730.50000\n      50.00000\n      69.00000\n      9478.50000\n      6.00000\n      5.00000\n      1973.00000\n      1994.00000\n      0.00000\n      383.50000\n      0.00000\n      477.50000\n      991.50000\n      1087.00000\n      0.00000\n      0.00000\n      1464.00000\n      0.00000\n      0.00000\n      2.00000\n      0.00000\n      3.00000\n      1.00000\n      6.00000\n      1.00000\n      1980.00000\n      2.00000\n      480.00000\n      0.00000\n      25.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      6.00000\n      2008.00000\n      163000.00000\n    \n    \n      75%\n      1095.25000\n      70.00000\n      80.00000\n      11601.50000\n      7.00000\n      6.00000\n      2000.00000\n      2004.00000\n      166.00000\n      712.25000\n      0.00000\n      808.00000\n      1298.25000\n      1391.25000\n      728.00000\n      0.00000\n      1776.75000\n      1.00000\n      0.00000\n      2.00000\n      1.00000\n      3.00000\n      1.00000\n      7.00000\n      1.00000\n      2002.00000\n      2.00000\n      576.00000\n      168.00000\n      68.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      8.00000\n      2009.00000\n      214000.00000\n    \n    \n      max\n      1460.00000\n      190.00000\n      313.00000\n      215245.00000\n      10.00000\n      9.00000\n      2010.00000\n      2010.00000\n      1600.00000\n      5644.00000\n      1474.00000\n      2336.00000\n      6110.00000\n      4692.00000\n      2065.00000\n      572.00000\n      5642.00000\n      3.00000\n      2.00000\n      3.00000\n      2.00000\n      8.00000\n      3.00000\n      14.00000\n      3.00000\n      2010.00000\n      4.00000\n      1418.00000\n      857.00000\n      547.00000\n      552.00000\n      508.00000\n      480.00000\n      738.00000\n      15500.00000\n      12.00000\n      2010.00000\n      755000.00000\n    \n  \n\n\n# test_df.describe()\ntrain_df.info()\nprint(&#039;_&#039; * 50)\ntest_df.info()\n&lt;class &#039;pandas.core.frame.DataFrame&#039;&gt;\nRangeIndex: 1460 entries, 0 to 1459\nData columns (total 81 columns):\nId               1460 non-null int64\nMSSubClass       1460 non-null int64\nMSZoning         1460 non-null object\nLotFrontage      1201 non-null float64\nLotArea          1460 non-null int64\nStreet           1460 non-null object\nAlley            91 non-null object\nLotShape         1460 non-null object\nLandContour      1460 non-null object\nUtilities        1460 non-null object\nLotConfig        1460 non-null object\nLandSlope        1460 non-null object\nNeighborhood     1460 non-null object\nCondition1       1460 non-null object\nCondition2       1460 non-null object\nBldgType         1460 non-null object\nHouseStyle       1460 non-null object\nOverallQual      1460 non-null int64\nOverallCond      1460 non-null int64\nYearBuilt        1460 non-null int64\nYearRemodAdd     1460 non-null int64\nRoofStyle        1460 non-null object\nRoofMatl         1460 non-null object\nExterior1st      1460 non-null object\nExterior2nd      1460 non-null object\nMasVnrType       1452 non-null object\nMasVnrArea       1452 non-null float64\nExterQual        1460 non-null object\nExterCond        1460 non-null object\nFoundation       1460 non-null object\nBsmtQual         1423 non-null object\nBsmtCond         1423 non-null object\nBsmtExposure     1422 non-null object\nBsmtFinType1     1423 non-null object\nBsmtFinSF1       1460 non-null int64\nBsmtFinType2     1422 non-null object\nBsmtFinSF2       1460 non-null int64\nBsmtUnfSF        1460 non-null int64\nTotalBsmtSF      1460 non-null int64\nHeating          1460 non-null object\nHeatingQC        1460 non-null object\nCentralAir       1460 non-null object\nElectrical       1459 non-null object\n1stFlrSF         1460 non-null int64\n2ndFlrSF         1460 non-null int64\nLowQualFinSF     1460 non-null int64\nGrLivArea        1460 non-null int64\nBsmtFullBath     1460 non-null int64\nBsmtHalfBath     1460 non-null int64\nFullBath         1460 non-null int64\nHalfBath         1460 non-null int64\nBedroomAbvGr     1460 non-null int64\nKitchenAbvGr     1460 non-null int64\nKitchenQual      1460 non-null object\nTotRmsAbvGrd     1460 non-null int64\nFunctional       1460 non-null object\nFireplaces       1460 non-null int64\nFireplaceQu      770 non-null object\nGarageType       1379 non-null object\nGarageYrBlt      1379 non-null float64\nGarageFinish     1379 non-null object\nGarageCars       1460 non-null int64\nGarageArea       1460 non-null int64\nGarageQual       1379 non-null object\nGarageCond       1379 non-null object\nPavedDrive       1460 non-null object\nWoodDeckSF       1460 non-null int64\nOpenPorchSF      1460 non-null int64\nEnclosedPorch    1460 non-null int64\n3SsnPorch        1460 non-null int64\nScreenPorch      1460 non-null int64\nPoolArea         1460 non-null int64\nPoolQC           7 non-null object\nFence            281 non-null object\nMiscFeature      54 non-null object\nMiscVal          1460 non-null int64\nMoSold           1460 non-null int64\nYrSold           1460 non-null int64\nSaleType         1460 non-null object\nSaleCondition    1460 non-null object\nSalePrice        1460 non-null int64\ndtypes: float64(3), int64(35), object(43)\nmemory usage: 924.0+ KB\n__________________________________________________\n&lt;class &#039;pandas.core.frame.DataFrame&#039;&gt;\nRangeIndex: 1459 entries, 0 to 1458\nData columns (total 80 columns):\nId               1459 non-null int64\nMSSubClass       1459 non-null int64\nMSZoning         1455 non-null object\nLotFrontage      1232 non-null float64\nLotArea          1459 non-null int64\nStreet           1459 non-null object\nAlley            107 non-null object\nLotShape         1459 non-null object\nLandContour      1459 non-null object\nUtilities        1457 non-null object\nLotConfig        1459 non-null object\nLandSlope        1459 non-null object\nNeighborhood     1459 non-null object\nCondition1       1459 non-null object\nCondition2       1459 non-null object\nBldgType         1459 non-null object\nHouseStyle       1459 non-null object\nOverallQual      1459 non-null int64\nOverallCond      1459 non-null int64\nYearBuilt        1459 non-null int64\nYearRemodAdd     1459 non-null int64\nRoofStyle        1459 non-null object\nRoofMatl         1459 non-null object\nExterior1st      1458 non-null object\nExterior2nd      1458 non-null object\nMasVnrType       1443 non-null object\nMasVnrArea       1444 non-null float64\nExterQual        1459 non-null object\nExterCond        1459 non-null object\nFoundation       1459 non-null object\nBsmtQual         1415 non-null object\nBsmtCond         1414 non-null object\nBsmtExposure     1415 non-null object\nBsmtFinType1     1417 non-null object\nBsmtFinSF1       1458 non-null float64\nBsmtFinType2     1417 non-null object\nBsmtFinSF2       1458 non-null float64\nBsmtUnfSF        1458 non-null float64\nTotalBsmtSF      1458 non-null float64\nHeating          1459 non-null object\nHeatingQC        1459 non-null object\nCentralAir       1459 non-null object\nElectrical       1459 non-null object\n1stFlrSF         1459 non-null int64\n2ndFlrSF         1459 non-null int64\nLowQualFinSF     1459 non-null int64\nGrLivArea        1459 non-null int64\nBsmtFullBath     1457 non-null float64\nBsmtHalfBath     1457 non-null float64\nFullBath         1459 non-null int64\nHalfBath         1459 non-null int64\nBedroomAbvGr     1459 non-null int64\nKitchenAbvGr     1459 non-null int64\nKitchenQual      1458 non-null object\nTotRmsAbvGrd     1459 non-null int64\nFunctional       1457 non-null object\nFireplaces       1459 non-null int64\nFireplaceQu      729 non-null object\nGarageType       1383 non-null object\nGarageYrBlt      1381 non-null float64\nGarageFinish     1381 non-null object\nGarageCars       1458 non-null float64\nGarageArea       1458 non-null float64\nGarageQual       1381 non-null object\nGarageCond       1381 non-null object\nPavedDrive       1459 non-null object\nWoodDeckSF       1459 non-null int64\nOpenPorchSF      1459 non-null int64\nEnclosedPorch    1459 non-null int64\n3SsnPorch        1459 non-null int64\nScreenPorch      1459 non-null int64\nPoolArea         1459 non-null int64\nPoolQC           3 non-null object\nFence            290 non-null object\nMiscFeature      51 non-null object\nMiscVal          1459 non-null int64\nMoSold           1459 non-null int64\nYrSold           1459 non-null int64\nSaleType         1458 non-null object\nSaleCondition    1459 non-null object\ndtypes: float64(11), int64(26), object(43)\nmemory usage: 912.0+ KB\n\ntrain_df.describe(include=&quot;O&quot;)\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      MSZoning\n      Street\n      Alley\n      LotShape\n      LandContour\n      Utilities\n      LotConfig\n      LandSlope\n      Neighborhood\n      Condition1\n      Condition2\n      BldgType\n      HouseStyle\n      RoofStyle\n      RoofMatl\n      Exterior1st\n      Exterior2nd\n      MasVnrType\n      ExterQual\n      ExterCond\n      Foundation\n      BsmtQual\n      BsmtCond\n      BsmtExposure\n      BsmtFinType1\n      BsmtFinType2\n      Heating\n      HeatingQC\n      CentralAir\n      Electrical\n      KitchenQual\n      Functional\n      FireplaceQu\n      GarageType\n      GarageFinish\n      GarageQual\n      GarageCond\n      PavedDrive\n      PoolQC\n      Fence\n      MiscFeature\n      SaleType\n      SaleCondition\n    \n  \n  \n    \n      count\n      1460\n      1460\n      91\n      1460\n      1460\n      1460\n      1460\n      1460\n      1460\n      1460\n      1460\n      1460\n      1460\n      1460\n      1460\n      1460\n      1460\n      1452\n      1460\n      1460\n      1460\n      1423\n      1423\n      1422\n      1423\n      1422\n      1460\n      1460\n      1460\n      1459\n      1460\n      1460\n      770\n      1379\n      1379\n      1379\n      1379\n      1460\n      7\n      281\n      54\n      1460\n      1460\n    \n    \n      unique\n      5\n      2\n      2\n      4\n      4\n      2\n      5\n      3\n      25\n      9\n      8\n      5\n      8\n      6\n      8\n      15\n      16\n      4\n      4\n      5\n      6\n      4\n      4\n      4\n      6\n      6\n      6\n      5\n      2\n      5\n      4\n      7\n      5\n      6\n      3\n      5\n      5\n      3\n      3\n      4\n      4\n      9\n      6\n    \n    \n      top\n      RL\n      Pave\n      Grvl\n      Reg\n      Lvl\n      AllPub\n      Inside\n      Gtl\n      NAmes\n      Norm\n      Norm\n      1Fam\n      1Story\n      Gable\n      CompShg\n      VinylSd\n      VinylSd\n      None\n      TA\n      TA\n      PConc\n      TA\n      TA\n      No\n      Unf\n      Unf\n      GasA\n      Ex\n      Y\n      SBrkr\n      TA\n      Typ\n      Gd\n      Attchd\n      Unf\n      TA\n      TA\n      Y\n      Gd\n      MnPrv\n      Shed\n      WD\n      Normal\n    \n    \n      freq\n      1151\n      1454\n      50\n      925\n      1311\n      1459\n      1052\n      1382\n      225\n      1260\n      1445\n      1220\n      726\n      1141\n      1434\n      515\n      504\n      864\n      906\n      1282\n      647\n      649\n      1311\n      953\n      430\n      1256\n      1428\n      741\n      1365\n      1334\n      735\n      1360\n      380\n      870\n      605\n      1311\n      1326\n      1340\n      3\n      157\n      49\n      1267\n      1198\n    \n  \n\n\n \n分析概要\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeatureStatusDisposeAlley缺失比较多删除PoolQC只有七家有游泳池并且和 PoolArea 相关先不填充 删除Fence栏杆质量只有20%的有缺失的填充为没有MiscFeature其他项目也只有好少的房子有先不填充 删除FireplaceQu有一半家没有壁炉填 0Garagetype空代表没有填 0Garagefinish空代表没有填 0Garagequal空代表没有填 0Garagecond空代表没有填 0LotFrontage和物业相连的街道有1/3缺失没想到太好的填充方法 删除\n整理 description 文件\n数据描述文件记录了所有特征所代表的含义，其中许多特征是字符串，现在我们要整理为个字典，便于我们查询。\ndescription_dict = {}\nwith open(&#039;/data_description.txt&#039;,&#039;r&#039;) as description:\n    description_data = description.read()\n    description.close()\n    \ndescription_data = description_data.split(&#039;\\n&#039;)\nfor i in description_data:\n    if &#039;:&#039; in i:\n        key = i.split(&#039;:&#039;)[0]\n        description_dict[key] = []\n    elif i.split() and &#039;       &#039; in i:\n        value = i.split()[0]\n        description_dict[key].append(value)\n \nprint(description_dict)\n{&#039;MSSubClass&#039;: [&#039;20&#039;, &#039;30&#039;, &#039;40&#039;, &#039;45&#039;, &#039;50&#039;, &#039;60&#039;, &#039;70&#039;, &#039;75&#039;, &#039;80&#039;, &#039;85&#039;, &#039;90&#039;, &#039;120&#039;, &#039;150&#039;, &#039;160&#039;, &#039;180&#039;, &#039;190&#039;], &#039;MSZoning&#039;: [&#039;A&#039;, &#039;C&#039;, &#039;FV&#039;, &#039;I&#039;, &#039;RH&#039;, &#039;RL&#039;, &#039;RP&#039;, &#039;RM&#039;], &#039;LotFrontage&#039;: [], &#039;LotArea&#039;: [], &#039;Street&#039;: [&#039;Grvl&#039;, &#039;Pave&#039;], &#039;Alley&#039;: [&#039;Grvl&#039;, &#039;Pave&#039;, &#039;NA&#039;], &#039;LotShape&#039;: [&#039;Reg&#039;, &#039;IR1&#039;, &#039;IR2&#039;, &#039;IR3&#039;], &#039;LandContour&#039;: [&#039;Lvl&#039;, &#039;Bnk&#039;, &#039;HLS&#039;, &#039;Low&#039;], &#039;Utilities&#039;: [&#039;AllPub&#039;, &#039;NoSewr&#039;, &#039;NoSeWa&#039;, &#039;ELO&#039;], &#039;LotConfig&#039;: [&#039;Inside&#039;, &#039;Corner&#039;, &#039;CulDSac&#039;, &#039;FR2&#039;, &#039;FR3&#039;], &#039;LandSlope&#039;: [&#039;Gtl&#039;, &#039;Mod&#039;, &#039;Sev&#039;], &#039;Neighborhood&#039;: [&#039;Blmngtn&#039;, &#039;Blueste&#039;, &#039;BrDale&#039;, &#039;BrkSide&#039;, &#039;ClearCr&#039;, &#039;CollgCr&#039;, &#039;Crawfor&#039;, &#039;Edwards&#039;, &#039;Gilbert&#039;, &#039;IDOTRR&#039;, &#039;MeadowV&#039;, &#039;Mitchel&#039;, &#039;Names&#039;, &#039;NoRidge&#039;, &#039;NPkVill&#039;, &#039;NridgHt&#039;, &#039;NWAmes&#039;, &#039;OldTown&#039;, &#039;SWISU&#039;, &#039;Sawyer&#039;, &#039;SawyerW&#039;, &#039;Somerst&#039;, &#039;StoneBr&#039;, &#039;Timber&#039;, &#039;Veenker&#039;], &#039;Condition1&#039;: [&#039;Artery&#039;, &#039;Feedr&#039;, &#039;Norm&#039;, &#039;RRNn&#039;, &#039;RRAn&#039;, &#039;PosN&#039;, &#039;PosA&#039;, &#039;RRNe&#039;, &#039;RRAe&#039;], &#039;Condition2&#039;: [&#039;Artery&#039;, &#039;Feedr&#039;, &#039;Norm&#039;, &#039;RRNn&#039;, &#039;RRAn&#039;, &#039;PosN&#039;, &#039;PosA&#039;, &#039;RRNe&#039;, &#039;RRAe&#039;], &#039;BldgType&#039;: [&#039;1Fam&#039;, &#039;2FmCon&#039;, &#039;Duplx&#039;, &#039;TwnhsE&#039;, &#039;TwnhsI&#039;], &#039;HouseStyle&#039;: [&#039;1Story&#039;], &#039;       1.5Fin\\tOne and one-half story&#039;: [], &#039;       1.5Unf\\tOne and one-half story&#039;: [&#039;2Story&#039;], &#039;       2.5Fin\\tTwo and one-half story&#039;: [], &#039;       2.5Unf\\tTwo and one-half story&#039;: [&#039;SFoyer&#039;, &#039;SLvl&#039;], &#039;OverallQual&#039;: [&#039;10&#039;, &#039;9&#039;, &#039;8&#039;, &#039;7&#039;, &#039;6&#039;, &#039;5&#039;, &#039;4&#039;, &#039;3&#039;, &#039;2&#039;, &#039;1&#039;], &#039;OverallCond&#039;: [&#039;10&#039;, &#039;9&#039;, &#039;8&#039;, &#039;7&#039;, &#039;6&#039;, &#039;5&#039;, &#039;4&#039;, &#039;3&#039;, &#039;2&#039;, &#039;1&#039;], &#039;YearBuilt&#039;: [], &#039;YearRemodAdd&#039;: [], &#039;RoofStyle&#039;: [&#039;Flat&#039;, &#039;Gable&#039;, &#039;Gambrel&#039;, &#039;Hip&#039;, &#039;Mansard&#039;, &#039;Shed&#039;], &#039;RoofMatl&#039;: [&#039;ClyTile&#039;, &#039;CompShg&#039;, &#039;Membran&#039;, &#039;Metal&#039;, &#039;Roll&#039;, &#039;Tar&amp;Grv&#039;, &#039;WdShake&#039;, &#039;WdShngl&#039;], &#039;Exterior1st&#039;: [&#039;AsbShng&#039;, &#039;AsphShn&#039;, &#039;BrkComm&#039;, &#039;BrkFace&#039;, &#039;CBlock&#039;, &#039;CemntBd&#039;, &#039;HdBoard&#039;, &#039;ImStucc&#039;, &#039;MetalSd&#039;, &#039;Other&#039;, &#039;Plywood&#039;, &#039;PreCast&#039;, &#039;Stone&#039;, &#039;Stucco&#039;, &#039;VinylSd&#039;, &#039;Wd&#039;, &#039;WdShing&#039;], &#039;Exterior2nd&#039;: [&#039;AsbShng&#039;, &#039;AsphShn&#039;, &#039;BrkComm&#039;, &#039;BrkFace&#039;, &#039;CBlock&#039;, &#039;CemntBd&#039;, &#039;HdBoard&#039;, &#039;ImStucc&#039;, &#039;MetalSd&#039;, &#039;Other&#039;, &#039;Plywood&#039;, &#039;PreCast&#039;, &#039;Stone&#039;, &#039;Stucco&#039;, &#039;VinylSd&#039;, &#039;Wd&#039;, &#039;WdShing&#039;], &#039;MasVnrType&#039;: [&#039;BrkCmn&#039;, &#039;BrkFace&#039;, &#039;CBlock&#039;, &#039;None&#039;, &#039;Stone&#039;], &#039;MasVnrArea&#039;: [], &#039;ExterQual&#039;: [&#039;Ex&#039;, &#039;Gd&#039;, &#039;TA&#039;, &#039;Fa&#039;, &#039;Po&#039;], &#039;ExterCond&#039;: [&#039;Ex&#039;, &#039;Gd&#039;, &#039;TA&#039;, &#039;Fa&#039;, &#039;Po&#039;], &#039;Foundation&#039;: [&#039;BrkTil&#039;, &#039;CBlock&#039;, &#039;PConc&#039;, &#039;Slab&#039;, &#039;Stone&#039;, &#039;Wood&#039;], &#039;BsmtQual&#039;: [&#039;Ex&#039;, &#039;Gd&#039;, &#039;TA&#039;, &#039;Fa&#039;, &#039;Po&#039;, &#039;NA&#039;], &#039;BsmtCond&#039;: [&#039;Ex&#039;, &#039;Gd&#039;, &#039;TA&#039;, &#039;Fa&#039;, &#039;Po&#039;, &#039;NA&#039;], &#039;BsmtExposure&#039;: [&#039;Gd&#039;, &#039;Av&#039;, &#039;Mn&#039;, &#039;No&#039;, &#039;NA&#039;], &#039;BsmtFinType1&#039;: [&#039;GLQ&#039;, &#039;ALQ&#039;, &#039;BLQ&#039;, &#039;Rec&#039;, &#039;LwQ&#039;, &#039;Unf&#039;, &#039;NA&#039;], &#039;BsmtFinSF1&#039;: [], &#039;BsmtFinType2&#039;: [&#039;GLQ&#039;, &#039;ALQ&#039;, &#039;BLQ&#039;, &#039;Rec&#039;, &#039;LwQ&#039;, &#039;Unf&#039;, &#039;NA&#039;], &#039;BsmtFinSF2&#039;: [], &#039;BsmtUnfSF&#039;: [], &#039;TotalBsmtSF&#039;: [], &#039;Heating&#039;: [&#039;Floor&#039;, &#039;GasA&#039;, &#039;GasW&#039;, &#039;Grav&#039;, &#039;OthW&#039;, &#039;Wall&#039;], &#039;HeatingQC&#039;: [&#039;Ex&#039;, &#039;Gd&#039;, &#039;TA&#039;, &#039;Fa&#039;, &#039;Po&#039;], &#039;CentralAir&#039;: [&#039;N&#039;, &#039;Y&#039;], &#039;Electrical&#039;: [&#039;SBrkr&#039;, &#039;FuseA&#039;, &#039;FuseF&#039;, &#039;FuseP&#039;, &#039;Mix&#039;], &#039;1stFlrSF&#039;: [], &#039;2ndFlrSF&#039;: [], &#039;LowQualFinSF&#039;: [], &#039;GrLivArea&#039;: [], &#039;BsmtFullBath&#039;: [], &#039;BsmtHalfBath&#039;: [], &#039;FullBath&#039;: [], &#039;HalfBath&#039;: [], &#039;Bedroom&#039;: [], &#039;Kitchen&#039;: [], &#039;KitchenQual&#039;: [&#039;Ex&#039;, &#039;Gd&#039;, &#039;TA&#039;, &#039;Fa&#039;, &#039;Po&#039;], &#039;TotRmsAbvGrd&#039;: [], &#039;Functional&#039;: [&#039;Typ&#039;, &#039;Min1&#039;, &#039;Min2&#039;, &#039;Mod&#039;, &#039;Maj1&#039;, &#039;Maj2&#039;, &#039;Sev&#039;, &#039;Sal&#039;], &#039;Fireplaces&#039;: [], &#039;FireplaceQu&#039;: [&#039;Ex&#039;, &#039;Gd&#039;, &#039;TA&#039;, &#039;Fa&#039;, &#039;Po&#039;, &#039;NA&#039;], &#039;GarageType&#039;: [&#039;2Types&#039;, &#039;Attchd&#039;, &#039;Basment&#039;, &#039;BuiltIn&#039;, &#039;CarPort&#039;, &#039;Detchd&#039;, &#039;NA&#039;], &#039;GarageYrBlt&#039;: [], &#039;GarageFinish&#039;: [&#039;Fin&#039;, &#039;RFn&#039;, &#039;Unf&#039;, &#039;NA&#039;], &#039;GarageCars&#039;: [], &#039;GarageArea&#039;: [], &#039;GarageQual&#039;: [&#039;Ex&#039;, &#039;Gd&#039;, &#039;TA&#039;, &#039;Fa&#039;, &#039;Po&#039;, &#039;NA&#039;], &#039;GarageCond&#039;: [&#039;Ex&#039;, &#039;Gd&#039;, &#039;TA&#039;, &#039;Fa&#039;, &#039;Po&#039;, &#039;NA&#039;], &#039;PavedDrive&#039;: [&#039;Y&#039;, &#039;P&#039;, &#039;N&#039;], &#039;WoodDeckSF&#039;: [], &#039;OpenPorchSF&#039;: [], &#039;EnclosedPorch&#039;: [], &#039;3SsnPorch&#039;: [], &#039;ScreenPorch&#039;: [], &#039;PoolArea&#039;: [], &#039;PoolQC&#039;: [&#039;Ex&#039;, &#039;Gd&#039;, &#039;TA&#039;, &#039;Fa&#039;, &#039;NA&#039;], &#039;Fence&#039;: [&#039;GdPrv&#039;, &#039;MnPrv&#039;, &#039;GdWo&#039;, &#039;MnWw&#039;, &#039;NA&#039;], &#039;MiscFeature&#039;: [&#039;Elev&#039;, &#039;Gar2&#039;, &#039;Othr&#039;, &#039;Shed&#039;, &#039;TenC&#039;, &#039;NA&#039;], &#039;MiscVal&#039;: [], &#039;MoSold&#039;: [], &#039;YrSold&#039;: [], &#039;SaleType&#039;: [&#039;WD&#039;, &#039;CWD&#039;, &#039;VWD&#039;, &#039;New&#039;, &#039;COD&#039;, &#039;Con&#039;, &#039;ConLw&#039;, &#039;ConLI&#039;, &#039;ConLD&#039;, &#039;Oth&#039;], &#039;SaleCondition&#039;: [&#039;Normal&#039;, &#039;Abnorml&#039;, &#039;AdjLand&#039;, &#039;Alloca&#039;, &#039;Family&#039;, &#039;Partial&#039;]}\n\ndescription_dict[&#039;FireplaceQu&#039;]\n[&#039;Ex&#039;, &#039;Gd&#039;, &#039;TA&#039;, &#039;Fa&#039;, &#039;Po&#039;, &#039;NA&#039;]\n\n预处理\n首先先删除一些确实较多和不太好填充的feature。\n# 删除\ntrain_df = train_df.drop([&#039;Alley&#039;, &#039;PoolQC&#039;, &#039;MiscFeature&#039;, &#039;LotFrontage&#039;], axis=1)\ntest_df = test_df.drop([&#039;Alley&#039;, &#039;PoolQC&#039;, &#039;MiscFeature&#039;, &#039;LotFrontage&#039;], axis=1)\n处理 GarageYrBlt: Year garage was built\n车库的年代，没有填充 0，改为 1900 年开始。\ndef preprocess_garage_year(dataset):\n    dataset = dataset.fillna(1900)\n    dataset -= 1900\n    return dataset\n    \ntrain_df[&#039;GarageYrBlt&#039;] = preprocess_garage_year(train_df[&#039;GarageYrBlt&#039;])\ntest_df[&#039;GarageYrBlt&#039;] = preprocess_garage_year(test_df[&#039;GarageYrBlt&#039;])\n处理 Electrical\nElectrical: Electrical system\nSBrkr    Standard Circuit Breakers &amp; Romex\nFuseA    Fuse Box over 60 AMP and all Romex wiring (Average) \nFuseF    60 AMP Fuse Box and mostly Romex wiring (Fair)\nFuseP    60 AMP Fuse Box and mostly knob &amp; tube wiring (poor)\nMix  Mixed\n\nfreq_port = train_df.Electrical.dropna().mode()[0] # 返回出现次数最多的值（众数）\nfreq_port\n&#039;SBrkr&#039;\n\ndef preprocess_garage_year(dataset):\n    dataset = dataset.fillna(freq_port)\n    return dataset\n    \ntrain_df[&#039;Electrical&#039;] = preprocess_garage_year(train_df[&#039;Electrical&#039;])\ntest_df[&#039;Electrical&#039;] = preprocess_garage_year(test_df[&#039;Electrical&#039;])\n处理 MasVnrArea: Masonry veneer area in square feet\n砖石饰面面积:砖石饰面面积(平方英尺)\n缺失的不是太多（148），mean 103，众数（75%以上）为 0，还没想到太好的填充，先填个0试试吧。\ntrain_df.MasVnrArea.describe()\ncount   1452.00000\nmean     103.68526\nstd      181.06621\nmin        0.00000\n25%        0.00000\n50%        0.00000\n75%      166.00000\nmax     1600.00000\nName: MasVnrArea, dtype: float64\n\ndef preprocess_masvararea(dataset):\n    dataset = dataset.fillna(0)\n    return dataset\n    \ntrain_df[&#039;MasVnrArea&#039;] = preprocess_masvararea(train_df[&#039;MasVnrArea&#039;])\ntest_df[&#039;MasVnrArea&#039;] = preprocess_masvararea(test_df[&#039;MasVnrArea&#039;])\n处理 MasVnrType\nMasVnrType: Masonry veneer type\nBrkCmn   Brick Common\nBrkFace  Brick Face\nCBlock   Cinder Block\nNone None\nStone    Stone\n\n这个值很奇怪，不太明白这是什么，是没有好呢还是 Stone 好呢？\ntrain_df.MasVnrType.dropna().mode()[0] # 返回出现次数最多的值（众数）\n&#039;None&#039;\n\n大多数都没有，那就把缺失值填为没有吧。\ndef preprocess_masvnrtype(dataset):\n    dataset = dataset.fillna(&#039;None&#039;)\n    return dataset\n    \ntrain_df[&#039;MasVnrType&#039;] = preprocess_masvnrtype(train_df[&#039;MasVnrType&#039;])\ntest_df[&#039;MasVnrType&#039;] = preprocess_masvnrtype(test_df[&#039;MasVnrType&#039;])\n处理其他缺失 feature\n需要填充缺失值和重编码。\n根据 data description 把 字符串类型的 feature 重编码。\n构造 feature 对应的 map\n观察发现以下这些缺失我们可以填充，顺便重编码。\nmissing_value = [&#039;Fence&#039;,\n    &#039;FireplaceQu&#039;,\n    &#039;GarageType&#039;,\n    &#039;GarageFinish&#039;,\n    &#039;GarageQual&#039;,\n    &#039;GarageCond&#039;,\n    &#039;BsmtQual&#039;,\n    &#039;BsmtCond&#039;,\n    &#039;BsmtExposure&#039;,\n    &#039;BsmtFinType1&#039;,\n    &#039;BsmtFinType2&#039;,]\ndef generate_map(map_list, end_index=1):\n    d = {}\n    j = len(map_list) - end_index\n    for i in map_list:\n        d[i] = j\n        j -= 1\n    return d\nmissing_map_dict = {}\nfor feature in missing_value:\n    missing_map_dict[feature] = generate_map(description_dict[feature])\n \nmissing_map_dict\n{&#039;BsmtCond&#039;: {&#039;Ex&#039;: 5, &#039;Fa&#039;: 2, &#039;Gd&#039;: 4, &#039;NA&#039;: 0, &#039;Po&#039;: 1, &#039;TA&#039;: 3},\n &#039;BsmtExposure&#039;: {&#039;Av&#039;: 3, &#039;Gd&#039;: 4, &#039;Mn&#039;: 2, &#039;NA&#039;: 0, &#039;No&#039;: 1},\n &#039;BsmtFinType1&#039;: {&#039;ALQ&#039;: 5,\n  &#039;BLQ&#039;: 4,\n  &#039;GLQ&#039;: 6,\n  &#039;LwQ&#039;: 2,\n  &#039;NA&#039;: 0,\n  &#039;Rec&#039;: 3,\n  &#039;Unf&#039;: 1},\n &#039;BsmtFinType2&#039;: {&#039;ALQ&#039;: 5,\n  &#039;BLQ&#039;: 4,\n  &#039;GLQ&#039;: 6,\n  &#039;LwQ&#039;: 2,\n  &#039;NA&#039;: 0,\n  &#039;Rec&#039;: 3,\n  &#039;Unf&#039;: 1},\n &#039;BsmtQual&#039;: {&#039;Ex&#039;: 5, &#039;Fa&#039;: 2, &#039;Gd&#039;: 4, &#039;NA&#039;: 0, &#039;Po&#039;: 1, &#039;TA&#039;: 3},\n &#039;Fence&#039;: {&#039;GdPrv&#039;: 4, &#039;GdWo&#039;: 2, &#039;MnPrv&#039;: 3, &#039;MnWw&#039;: 1, &#039;NA&#039;: 0},\n &#039;FireplaceQu&#039;: {&#039;Ex&#039;: 5, &#039;Fa&#039;: 2, &#039;Gd&#039;: 4, &#039;NA&#039;: 0, &#039;Po&#039;: 1, &#039;TA&#039;: 3},\n &#039;GarageCond&#039;: {&#039;Ex&#039;: 5, &#039;Fa&#039;: 2, &#039;Gd&#039;: 4, &#039;NA&#039;: 0, &#039;Po&#039;: 1, &#039;TA&#039;: 3},\n &#039;GarageFinish&#039;: {&#039;Fin&#039;: 3, &#039;NA&#039;: 0, &#039;RFn&#039;: 2, &#039;Unf&#039;: 1},\n &#039;GarageQual&#039;: {&#039;Ex&#039;: 5, &#039;Fa&#039;: 2, &#039;Gd&#039;: 4, &#039;NA&#039;: 0, &#039;Po&#039;: 1, &#039;TA&#039;: 3},\n &#039;GarageType&#039;: {&#039;2Types&#039;: 6,\n  &#039;Attchd&#039;: 5,\n  &#039;Basment&#039;: 4,\n  &#039;BuiltIn&#039;: 3,\n  &#039;CarPort&#039;: 2,\n  &#039;Detchd&#039;: 1,\n  &#039;NA&#039;: 0}}\n\n# 预处理 feature 把 str 转换为序列 \ndef preprocess_feature_strtoint(feature_df, feature_mapping, default=0):\n    feature_df = feature_df.map(feature_mapping)\n    feature_df = feature_df.fillna(default)\n    return feature_df\ndef preprocess_feature(dataset):\n    for feature in missing_value:\n        dataset[feature] = preprocess_feature_strtoint(dataset[feature], missing_map_dict[feature])\n    return dataset\ntrain_df = preprocess_feature(train_df)\ntest_df = preprocess_feature(test_df)\n检查训练集缺失值，已经没有了。\ntrain_df[train_df.isnull().values==True]\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      Id\n      MSSubClass\n      MSZoning\n      LotArea\n      Street\n      LotShape\n      LandContour\n      Utilities\n      LotConfig\n      LandSlope\n      Neighborhood\n      Condition1\n      Condition2\n      BldgType\n      HouseStyle\n      OverallQual\n      OverallCond\n      YearBuilt\n      YearRemodAdd\n      RoofStyle\n      RoofMatl\n      Exterior1st\n      Exterior2nd\n      MasVnrType\n      MasVnrArea\n      ExterQual\n      ExterCond\n      Foundation\n      BsmtQual\n      BsmtCond\n      BsmtExposure\n      BsmtFinType1\n      BsmtFinSF1\n      BsmtFinType2\n      BsmtFinSF2\n      BsmtUnfSF\n      TotalBsmtSF\n      Heating\n      HeatingQC\n      CentralAir\n      Electrical\n      1stFlrSF\n      2ndFlrSF\n      LowQualFinSF\n      GrLivArea\n      BsmtFullBath\n      BsmtHalfBath\n      FullBath\n      HalfBath\n      BedroomAbvGr\n      KitchenAbvGr\n      KitchenQual\n      TotRmsAbvGrd\n      Functional\n      Fireplaces\n      FireplaceQu\n      GarageType\n      GarageYrBlt\n      GarageFinish\n      GarageCars\n      GarageArea\n      GarageQual\n      GarageCond\n      PavedDrive\n      WoodDeckSF\n      OpenPorchSF\n      EnclosedPorch\n      3SsnPorch\n      ScreenPorch\n      PoolArea\n      Fence\n      MiscVal\n      MoSold\n      YrSold\n      SaleType\n      SaleCondition\n      SalePrice\n    \n  \n  \n  \n\n\n填充测试集\n测试集还有许多缺失，先决定用众数填充。\ntest_df[test_df.isnull().values==True]\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      Id\n      MSSubClass\n      MSZoning\n      LotArea\n      Street\n      LotShape\n      LandContour\n      Utilities\n      LotConfig\n      LandSlope\n      Neighborhood\n      Condition1\n      Condition2\n      BldgType\n      HouseStyle\n      OverallQual\n      OverallCond\n      YearBuilt\n      YearRemodAdd\n      RoofStyle\n      RoofMatl\n      Exterior1st\n      Exterior2nd\n      MasVnrType\n      MasVnrArea\n      ExterQual\n      ExterCond\n      Foundation\n      BsmtQual\n      BsmtCond\n      BsmtExposure\n      BsmtFinType1\n      BsmtFinSF1\n      BsmtFinType2\n      BsmtFinSF2\n      BsmtUnfSF\n      TotalBsmtSF\n      Heating\n      HeatingQC\n      CentralAir\n      Electrical\n      1stFlrSF\n      2ndFlrSF\n      LowQualFinSF\n      GrLivArea\n      BsmtFullBath\n      BsmtHalfBath\n      FullBath\n      HalfBath\n      BedroomAbvGr\n      KitchenAbvGr\n      KitchenQual\n      TotRmsAbvGrd\n      Functional\n      Fireplaces\n      FireplaceQu\n      GarageType\n      GarageYrBlt\n      GarageFinish\n      GarageCars\n      GarageArea\n      GarageQual\n      GarageCond\n      PavedDrive\n      WoodDeckSF\n      OpenPorchSF\n      EnclosedPorch\n      3SsnPorch\n      ScreenPorch\n      PoolArea\n      Fence\n      MiscVal\n      MoSold\n      YrSold\n      SaleType\n      SaleCondition\n    \n  \n  \n    \n      95\n      1556\n      50\n      RL\n      10632\n      Pave\n      IR1\n      Lvl\n      AllPub\n      Inside\n      Gtl\n      ClearCr\n      Norm\n      Norm\n      1Fam\n      1.5Fin\n      5\n      3\n      1917\n      1950\n      Gable\n      CompShg\n      Wd Sdng\n      Wd Sdng\n      None\n      0.00000\n      TA\n      TA\n      BrkTil\n      4.00000\n      2.00000\n      1.00000\n      1.00000\n      0.00000\n      1.00000\n      0.00000\n      689.00000\n      689.00000\n      GasA\n      Gd\n      N\n      SBrkr\n      725\n      499\n      0\n      1224\n      0.00000\n      0.00000\n      1\n      1\n      3\n      1\n      NaN\n      6\n      Mod\n      0\n      0.00000\n      1.00000\n      17.00000\n      1.00000\n      1.00000\n      180.00000\n      2.00000\n      2.00000\n      N\n      0\n      0\n      248\n      0\n      0\n      0\n      0.00000\n      0\n      1\n      2010\n      COD\n      Normal\n    \n    \n      455\n      1916\n      30\n      NaN\n      21780\n      Grvl\n      Reg\n      Lvl\n      NaN\n      Inside\n      Gtl\n      IDOTRR\n      Norm\n      Norm\n      1Fam\n      1Story\n      2\n      4\n      1910\n      1950\n      Gable\n      CompShg\n      Wd Sdng\n      Wd Sdng\n      None\n      0.00000\n      Fa\n      Fa\n      CBlock\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      GasA\n      TA\n      N\n      FuseA\n      810\n      0\n      0\n      810\n      0.00000\n      0.00000\n      1\n      0\n      1\n      1\n      TA\n      4\n      Min1\n      0\n      0.00000\n      1.00000\n      75.00000\n      1.00000\n      1.00000\n      280.00000\n      3.00000\n      3.00000\n      N\n      119\n      24\n      0\n      0\n      0\n      0\n      0.00000\n      0\n      3\n      2009\n      ConLD\n      Normal\n    \n    \n      455\n      1916\n      30\n      NaN\n      21780\n      Grvl\n      Reg\n      Lvl\n      NaN\n      Inside\n      Gtl\n      IDOTRR\n      Norm\n      Norm\n      1Fam\n      1Story\n      2\n      4\n      1910\n      1950\n      Gable\n      CompShg\n      Wd Sdng\n      Wd Sdng\n      None\n      0.00000\n      Fa\n      Fa\n      CBlock\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      GasA\n      TA\n      N\n      FuseA\n      810\n      0\n      0\n      810\n      0.00000\n      0.00000\n      1\n      0\n      1\n      1\n      TA\n      4\n      Min1\n      0\n      0.00000\n      1.00000\n      75.00000\n      1.00000\n      1.00000\n      280.00000\n      3.00000\n      3.00000\n      N\n      119\n      24\n      0\n      0\n      0\n      0\n      0.00000\n      0\n      3\n      2009\n      ConLD\n      Normal\n    \n    \n      485\n      1946\n      20\n      RL\n      31220\n      Pave\n      IR1\n      Bnk\n      NaN\n      FR2\n      Gtl\n      Gilbert\n      Feedr\n      Norm\n      1Fam\n      1Story\n      6\n      2\n      1952\n      1952\n      Hip\n      CompShg\n      BrkFace\n      BrkFace\n      None\n      0.00000\n      TA\n      TA\n      CBlock\n      3.00000\n      3.00000\n      1.00000\n      1.00000\n      0.00000\n      1.00000\n      0.00000\n      1632.00000\n      1632.00000\n      GasA\n      TA\n      Y\n      FuseA\n      1474\n      0\n      0\n      1474\n      0.00000\n      0.00000\n      1\n      0\n      3\n      1\n      TA\n      7\n      Min2\n      2\n      4.00000\n      5.00000\n      52.00000\n      1.00000\n      2.00000\n      495.00000\n      3.00000\n      3.00000\n      Y\n      0\n      0\n      144\n      0\n      0\n      0\n      0.00000\n      750\n      5\n      2008\n      WD\n      Normal\n    \n    \n      660\n      2121\n      20\n      RM\n      5940\n      Pave\n      IR1\n      Lvl\n      AllPub\n      FR3\n      Gtl\n      BrkSide\n      Feedr\n      Norm\n      1Fam\n      1Story\n      4\n      7\n      1946\n      1950\n      Gable\n      CompShg\n      MetalSd\n      CBlock\n      None\n      0.00000\n      TA\n      TA\n      PConc\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      nan\n      0.00000\n      nan\n      nan\n      nan\n      GasA\n      TA\n      Y\n      FuseA\n      896\n      0\n      0\n      896\n      nan\n      nan\n      1\n      0\n      2\n      1\n      TA\n      4\n      Typ\n      0\n      0.00000\n      1.00000\n      46.00000\n      1.00000\n      1.00000\n      280.00000\n      3.00000\n      3.00000\n      Y\n      0\n      0\n      0\n      0\n      0\n      0\n      3.00000\n      0\n      4\n      2008\n      ConLD\n      Abnorml\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      1013\n      2474\n      50\n      RM\n      10320\n      Pave\n      Reg\n      Lvl\n      AllPub\n      Corner\n      Gtl\n      IDOTRR\n      Artery\n      Norm\n      1Fam\n      1.5Fin\n      4\n      1\n      1910\n      1950\n      Gable\n      CompShg\n      Wd Sdng\n      Wd Sdng\n      None\n      0.00000\n      Fa\n      Fa\n      CBlock\n      3.00000\n      2.00000\n      1.00000\n      1.00000\n      0.00000\n      1.00000\n      0.00000\n      771.00000\n      771.00000\n      GasA\n      Fa\n      Y\n      SBrkr\n      866\n      504\n      114\n      1484\n      0.00000\n      0.00000\n      2\n      0\n      3\n      1\n      TA\n      6\n      NaN\n      0\n      0.00000\n      1.00000\n      10.00000\n      1.00000\n      1.00000\n      264.00000\n      3.00000\n      2.00000\n      N\n      14\n      211\n      0\n      0\n      84\n      0\n      0.00000\n      0\n      9\n      2007\n      COD\n      Abnorml\n    \n    \n      1029\n      2490\n      20\n      RL\n      13770\n      Pave\n      Reg\n      Lvl\n      AllPub\n      Corner\n      Gtl\n      Sawyer\n      Feedr\n      Norm\n      1Fam\n      1Story\n      5\n      6\n      1958\n      1998\n      Gable\n      CompShg\n      Plywood\n      Plywood\n      BrkFace\n      340.00000\n      TA\n      TA\n      CBlock\n      3.00000\n      3.00000\n      2.00000\n      3.00000\n      190.00000\n      4.00000\n      873.00000\n      95.00000\n      1158.00000\n      GasA\n      TA\n      Y\n      SBrkr\n      1176\n      0\n      0\n      1176\n      1.00000\n      0.00000\n      1\n      0\n      3\n      1\n      TA\n      6\n      Typ\n      2\n      4.00000\n      5.00000\n      58.00000\n      1.00000\n      1.00000\n      303.00000\n      3.00000\n      3.00000\n      Y\n      0\n      0\n      0\n      0\n      0\n      0\n      0.00000\n      0\n      10\n      2007\n      NaN\n      Normal\n    \n    \n      1116\n      2577\n      70\n      RM\n      9060\n      Pave\n      Reg\n      Lvl\n      AllPub\n      Inside\n      Gtl\n      IDOTRR\n      Norm\n      Norm\n      1Fam\n      2Story\n      5\n      6\n      1923\n      1999\n      Gable\n      CompShg\n      Wd Sdng\n      Plywood\n      None\n      0.00000\n      TA\n      TA\n      BrkTil\n      4.00000\n      3.00000\n      1.00000\n      5.00000\n      548.00000\n      1.00000\n      0.00000\n      311.00000\n      859.00000\n      GasA\n      Ex\n      Y\n      SBrkr\n      942\n      886\n      0\n      1828\n      0.00000\n      0.00000\n      2\n      0\n      3\n      1\n      Gd\n      6\n      Typ\n      0\n      0.00000\n      1.00000\n      0.00000\n      0.00000\n      nan\n      nan\n      0.00000\n      0.00000\n      Y\n      174\n      0\n      212\n      0\n      0\n      0\n      3.00000\n      0\n      3\n      2007\n      WD\n      Alloca\n    \n    \n      1116\n      2577\n      70\n      RM\n      9060\n      Pave\n      Reg\n      Lvl\n      AllPub\n      Inside\n      Gtl\n      IDOTRR\n      Norm\n      Norm\n      1Fam\n      2Story\n      5\n      6\n      1923\n      1999\n      Gable\n      CompShg\n      Wd Sdng\n      Plywood\n      None\n      0.00000\n      TA\n      TA\n      BrkTil\n      4.00000\n      3.00000\n      1.00000\n      5.00000\n      548.00000\n      1.00000\n      0.00000\n      311.00000\n      859.00000\n      GasA\n      Ex\n      Y\n      SBrkr\n      942\n      886\n      0\n      1828\n      0.00000\n      0.00000\n      2\n      0\n      3\n      1\n      Gd\n      6\n      Typ\n      0\n      0.00000\n      1.00000\n      0.00000\n      0.00000\n      nan\n      nan\n      0.00000\n      0.00000\n      Y\n      174\n      0\n      212\n      0\n      0\n      0\n      3.00000\n      0\n      3\n      2007\n      WD\n      Alloca\n    \n    \n      1444\n      2905\n      20\n      NaN\n      31250\n      Pave\n      Reg\n      Lvl\n      AllPub\n      Inside\n      Gtl\n      Mitchel\n      Artery\n      Norm\n      1Fam\n      1Story\n      1\n      3\n      1951\n      1951\n      Gable\n      CompShg\n      CBlock\n      VinylSd\n      None\n      0.00000\n      TA\n      Fa\n      CBlock\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      GasA\n      TA\n      Y\n      FuseA\n      1600\n      0\n      0\n      1600\n      0.00000\n      0.00000\n      1\n      1\n      3\n      1\n      TA\n      6\n      Mod\n      0\n      0.00000\n      5.00000\n      51.00000\n      1.00000\n      1.00000\n      270.00000\n      2.00000\n      3.00000\n      N\n      0\n      0\n      135\n      0\n      0\n      0\n      0.00000\n      0\n      5\n      2006\n      WD\n      Normal\n    \n  \n\n22 rows × 76 columns\n\nfor i in test_df.columns.values.tolist():\n    freq_port = test_df[i].dropna().mode()[0] # 返回出现次数最多的值（众数)\n    test_df[i] = test_df[i].fillna(freq_port)\n字符串类型 feature 重编码\n处理完缺失值后观察下还有那些feature是字符串形式的。\ntrain_df.describe(include=&quot;O&quot;)\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      MSZoning\n      Street\n      LotShape\n      LandContour\n      Utilities\n      LotConfig\n      LandSlope\n      Neighborhood\n      Condition1\n      Condition2\n      BldgType\n      HouseStyle\n      RoofStyle\n      RoofMatl\n      Exterior1st\n      Exterior2nd\n      MasVnrType\n      ExterQual\n      ExterCond\n      Foundation\n      Heating\n      HeatingQC\n      CentralAir\n      Electrical\n      KitchenQual\n      Functional\n      PavedDrive\n      SaleType\n      SaleCondition\n    \n  \n  \n    \n      count\n      1460\n      1460\n      1460\n      1460\n      1460\n      1460\n      1460\n      1460\n      1460\n      1460\n      1460\n      1460\n      1460\n      1460\n      1460\n      1460\n      1460\n      1460\n      1460\n      1460\n      1460\n      1460\n      1460\n      1460\n      1460\n      1460\n      1460\n      1460\n      1460\n    \n    \n      unique\n      5\n      2\n      4\n      4\n      2\n      5\n      3\n      25\n      9\n      8\n      5\n      8\n      6\n      8\n      15\n      16\n      4\n      4\n      5\n      6\n      6\n      5\n      2\n      5\n      4\n      7\n      3\n      9\n      6\n    \n    \n      top\n      RL\n      Pave\n      Reg\n      Lvl\n      AllPub\n      Inside\n      Gtl\n      NAmes\n      Norm\n      Norm\n      1Fam\n      1Story\n      Gable\n      CompShg\n      VinylSd\n      VinylSd\n      None\n      TA\n      TA\n      PConc\n      GasA\n      Ex\n      Y\n      SBrkr\n      TA\n      Typ\n      Y\n      WD\n      Normal\n    \n    \n      freq\n      1151\n      1454\n      925\n      1311\n      1459\n      1052\n      1382\n      225\n      1260\n      1445\n      1220\n      726\n      1141\n      1434\n      515\n      504\n      872\n      906\n      1282\n      647\n      1428\n      741\n      1365\n      1335\n      735\n      1360\n      1340\n      1267\n      1198\n    \n  \n\n\n目前还有以下的 feature 需要编码\n[‘MSZoning’, ‘Street’, ‘LotShape’, ‘LandContour’, ‘Utilities’, ‘LotConfig’, ‘LandSlope’, ‘Neighborhood’, ‘Condition1’, ‘Condition2’, ‘BldgType’, ‘HouseStyle’, ‘RoofStyle’, ‘RoofMatl’, ‘Exterior1st’, ‘Exterior2nd’, ‘MasVnrType’, ‘ExterQual’, ‘ExterCond’, ‘Foundation’, ‘Heating’, ‘HeatingQC’, ‘CentralAir’, ‘Electrical’, ‘KitchenQual’, ‘Functional’, ‘PavedDrive’, ‘SaleType’, ‘SaleCondition’]\nfeature = [&#039;MSZoning&#039;, &#039;Street&#039;, &#039;LotShape&#039;, &#039;LandContour&#039;, &#039;Utilities&#039;, &#039;LotConfig&#039;, &#039;LandSlope&#039;, &#039;Neighborhood&#039;, &#039;Condition1&#039;, &#039;Condition2&#039;, &#039;BldgType&#039;, &#039;HouseStyle&#039;, &#039;RoofStyle&#039;, &#039;RoofMatl&#039;, &#039;Exterior1st&#039;, &#039;Exterior2nd&#039;, &#039;MasVnrType&#039;, &#039;ExterQual&#039;, &#039;ExterCond&#039;, &#039;Foundation&#039;, &#039;Heating&#039;, &#039;HeatingQC&#039;, &#039;CentralAir&#039;, &#039;Electrical&#039;, &#039;KitchenQual&#039;, &#039;Functional&#039;, &#039;PavedDrive&#039;, &#039;SaleType&#039;, &#039;SaleCondition&#039;]\nfor i in feature:\n    print(set(train_df[i]))\n{&#039;RL&#039;, &#039;FV&#039;, &#039;C (all)&#039;, &#039;RM&#039;, &#039;RH&#039;}\n{&#039;Grvl&#039;, &#039;Pave&#039;}\n{&#039;IR2&#039;, &#039;Reg&#039;, &#039;IR3&#039;, &#039;IR1&#039;}\n{&#039;Lvl&#039;, &#039;Low&#039;, &#039;HLS&#039;, &#039;Bnk&#039;}\n{&#039;AllPub&#039;, &#039;NoSeWa&#039;}\n{&#039;CulDSac&#039;, &#039;FR2&#039;, &#039;Corner&#039;, &#039;FR3&#039;, &#039;Inside&#039;}\n{&#039;Gtl&#039;, &#039;Mod&#039;, &#039;Sev&#039;}\n{&#039;BrDale&#039;, &#039;NoRidge&#039;, &#039;Blmngtn&#039;, &#039;Sawyer&#039;, &#039;NPkVill&#039;, &#039;NridgHt&#039;, &#039;SawyerW&#039;, &#039;Mitchel&#039;, &#039;OldTown&#039;, &#039;NWAmes&#039;, &#039;NAmes&#039;, &#039;Somerst&#039;, &#039;Veenker&#039;, &#039;SWISU&#039;, &#039;CollgCr&#039;, &#039;BrkSide&#039;, &#039;ClearCr&#039;, &#039;IDOTRR&#039;, &#039;Crawfor&#039;, &#039;StoneBr&#039;, &#039;Timber&#039;, &#039;Gilbert&#039;, &#039;Blueste&#039;, &#039;MeadowV&#039;, &#039;Edwards&#039;}\n{&#039;Artery&#039;, &#039;PosN&#039;, &#039;PosA&#039;, &#039;Norm&#039;, &#039;RRNn&#039;, &#039;RRAe&#039;, &#039;RRNe&#039;, &#039;Feedr&#039;, &#039;RRAn&#039;}\n{&#039;Artery&#039;, &#039;PosN&#039;, &#039;PosA&#039;, &#039;Norm&#039;, &#039;RRNn&#039;, &#039;RRAe&#039;, &#039;Feedr&#039;, &#039;RRAn&#039;}\n{&#039;2fmCon&#039;, &#039;Duplex&#039;, &#039;Twnhs&#039;, &#039;TwnhsE&#039;, &#039;1Fam&#039;}\n{&#039;SFoyer&#039;, &#039;1.5Fin&#039;, &#039;2Story&#039;, &#039;1.5Unf&#039;, &#039;2.5Fin&#039;, &#039;2.5Unf&#039;, &#039;1Story&#039;, &#039;SLvl&#039;}\n{&#039;Mansard&#039;, &#039;Shed&#039;, &#039;Gable&#039;, &#039;Flat&#039;, &#039;Gambrel&#039;, &#039;Hip&#039;}\n{&#039;Roll&#039;, &#039;Metal&#039;, &#039;ClyTile&#039;, &#039;WdShngl&#039;, &#039;CompShg&#039;, &#039;Tar&amp;Grv&#039;, &#039;Membran&#039;, &#039;WdShake&#039;}\n{&#039;BrkFace&#039;, &#039;Plywood&#039;, &#039;MetalSd&#039;, &#039;Stucco&#039;, &#039;WdShing&#039;, &#039;CBlock&#039;, &#039;AsphShn&#039;, &#039;Stone&#039;, &#039;ImStucc&#039;, &#039;CemntBd&#039;, &#039;Wd Sdng&#039;, &#039;VinylSd&#039;, &#039;BrkComm&#039;, &#039;AsbShng&#039;, &#039;HdBoard&#039;}\n{&#039;BrkFace&#039;, &#039;Plywood&#039;, &#039;Wd Shng&#039;, &#039;MetalSd&#039;, &#039;CmentBd&#039;, &#039;Stucco&#039;, &#039;Other&#039;, &#039;CBlock&#039;, &#039;AsphShn&#039;, &#039;ImStucc&#039;, &#039;Stone&#039;, &#039;Wd Sdng&#039;, &#039;VinylSd&#039;, &#039;AsbShng&#039;, &#039;Brk Cmn&#039;, &#039;HdBoard&#039;}\n{&#039;BrkFace&#039;, &#039;None&#039;, &#039;BrkCmn&#039;, &#039;Stone&#039;}\n{&#039;Ex&#039;, &#039;Gd&#039;, &#039;Fa&#039;, &#039;TA&#039;}\n{&#039;Fa&#039;, &#039;Gd&#039;, &#039;Po&#039;, &#039;TA&#039;, &#039;Ex&#039;}\n{&#039;BrkTil&#039;, &#039;PConc&#039;, &#039;CBlock&#039;, &#039;Stone&#039;, &#039;Slab&#039;, &#039;Wood&#039;}\n{&#039;GasA&#039;, &#039;GasW&#039;, &#039;Wall&#039;, &#039;Floor&#039;, &#039;Grav&#039;, &#039;OthW&#039;}\n{&#039;Fa&#039;, &#039;Gd&#039;, &#039;Po&#039;, &#039;TA&#039;, &#039;Ex&#039;}\n{&#039;Y&#039;, &#039;N&#039;}\n{&#039;SBrkr&#039;, &#039;Mix&#039;, &#039;FuseA&#039;, &#039;FuseP&#039;, &#039;FuseF&#039;}\n{&#039;Ex&#039;, &#039;Gd&#039;, &#039;Fa&#039;, &#039;TA&#039;}\n{&#039;Mod&#039;, &#039;Min1&#039;, &#039;Maj1&#039;, &#039;Min2&#039;, &#039;Maj2&#039;, &#039;Typ&#039;, &#039;Sev&#039;}\n{&#039;Y&#039;, &#039;N&#039;, &#039;P&#039;}\n{&#039;Con&#039;, &#039;ConLD&#039;, &#039;ConLw&#039;, &#039;CWD&#039;, &#039;WD&#039;, &#039;New&#039;, &#039;ConLI&#039;, &#039;Oth&#039;, &#039;COD&#039;}\n{&#039;Abnorml&#039;, &#039;Alloca&#039;, &#039;Normal&#039;, &#039;AdjLand&#039;, &#039;Partial&#039;, &#039;Family&#039;}\n\n观察到这些 feature 即有优劣等级划分的，也有没有等级的，既然如此，把有等级区分的编码为序列，没有等级的使用独热码。\n有等级的：\nMSZoning\nExterQual\nExterCond\nHeatingQC\nKitchenQual\n\n其余的使用onehot编码。\norder_feature_set ={&#039;MSZoning&#039;,\n&#039;ExterQual&#039;,\n&#039;ExterCond&#039;,\n&#039;HeatingQC&#039;,\n&#039;KitchenQual&#039;,}\n \nonehot_feature_set = set(feature) - order_feature_set\nOrder 编码\ndef preprocess_order_feature(dataset):\n    for i in order_feature_set:\n        order_map = generate_map(description_dict[i], 0)\n        dataset[i] = dataset[i].map(order_map)\n        dataset[i] = dataset[i].fillna(0)\n    return dataset\ntrain_df = preprocess_order_feature(train_df)\ntest_df = preprocess_order_feature(test_df)\ntrain_df.describe()\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      Id\n      MSSubClass\n      MSZoning\n      LotArea\n      OverallQual\n      OverallCond\n      YearBuilt\n      YearRemodAdd\n      MasVnrArea\n      ExterQual\n      ExterCond\n      BsmtQual\n      BsmtCond\n      BsmtExposure\n      BsmtFinType1\n      BsmtFinSF1\n      BsmtFinType2\n      BsmtFinSF2\n      BsmtUnfSF\n      TotalBsmtSF\n      HeatingQC\n      1stFlrSF\n      2ndFlrSF\n      LowQualFinSF\n      GrLivArea\n      BsmtFullBath\n      BsmtHalfBath\n      FullBath\n      HalfBath\n      BedroomAbvGr\n      KitchenAbvGr\n      KitchenQual\n      TotRmsAbvGrd\n      Fireplaces\n      FireplaceQu\n      GarageType\n      GarageYrBlt\n      GarageFinish\n      GarageCars\n      GarageArea\n      GarageQual\n      GarageCond\n      WoodDeckSF\n      OpenPorchSF\n      EnclosedPorch\n      3SsnPorch\n      ScreenPorch\n      PoolArea\n      Fence\n      MiscVal\n      MoSold\n      YrSold\n      SalePrice\n    \n  \n  \n    \n      count\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n    \n    \n      mean\n      730.50000\n      56.89726\n      2.82534\n      10516.82808\n      6.09932\n      5.57534\n      1971.26781\n      1984.86575\n      103.11712\n      3.39589\n      3.08356\n      3.48904\n      2.93493\n      1.63014\n      3.54589\n      443.63973\n      1.24726\n      46.54932\n      567.24041\n      1057.42945\n      4.14521\n      1162.62671\n      346.99247\n      5.84452\n      1515.46370\n      0.42534\n      0.05753\n      1.56507\n      0.38288\n      2.86644\n      1.04658\n      3.51164\n      6.51781\n      0.61301\n      1.82534\n      3.51438\n      74.15068\n      1.71575\n      1.76712\n      472.98014\n      2.81027\n      2.80890\n      94.24452\n      46.66027\n      21.95411\n      3.40959\n      15.06096\n      2.75890\n      0.56575\n      43.48904\n      6.32192\n      2007.81575\n      180921.19589\n    \n    \n      std\n      421.61001\n      42.30057\n      1.02017\n      9981.26493\n      1.38300\n      1.11280\n      30.20290\n      20.64541\n      180.73137\n      0.57428\n      0.35105\n      0.87648\n      0.55216\n      1.06739\n      2.10778\n      456.09809\n      0.89233\n      161.31927\n      441.86696\n      438.70532\n      0.95950\n      386.58774\n      436.52844\n      48.62308\n      525.48038\n      0.51891\n      0.23875\n      0.55092\n      0.50289\n      0.81578\n      0.22034\n      0.66376\n      1.62539\n      0.64467\n      1.81088\n      1.93321\n      29.98205\n      0.89283\n      0.74732\n      213.80484\n      0.72290\n      0.71969\n      125.33879\n      66.25603\n      61.11915\n      29.31733\n      55.75742\n      40.17731\n      1.20448\n      496.12302\n      2.70363\n      1.32810\n      79442.50288\n    \n    \n      min\n      1.00000\n      20.00000\n      0.00000\n      1300.00000\n      1.00000\n      1.00000\n      1872.00000\n      1950.00000\n      0.00000\n      2.00000\n      1.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      1.00000\n      334.00000\n      0.00000\n      0.00000\n      334.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      2.00000\n      2.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      1.00000\n      2006.00000\n      34900.00000\n    \n    \n      25%\n      365.75000\n      20.00000\n      3.00000\n      7553.50000\n      5.00000\n      5.00000\n      1954.00000\n      1967.00000\n      0.00000\n      3.00000\n      3.00000\n      3.00000\n      3.00000\n      1.00000\n      1.00000\n      0.00000\n      1.00000\n      0.00000\n      223.00000\n      795.75000\n      3.00000\n      882.00000\n      0.00000\n      0.00000\n      1129.50000\n      0.00000\n      0.00000\n      1.00000\n      0.00000\n      2.00000\n      1.00000\n      3.00000\n      5.00000\n      0.00000\n      0.00000\n      1.00000\n      58.00000\n      1.00000\n      1.00000\n      334.50000\n      3.00000\n      3.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      5.00000\n      2007.00000\n      129975.00000\n    \n    \n      50%\n      730.50000\n      50.00000\n      3.00000\n      9478.50000\n      6.00000\n      5.00000\n      1973.00000\n      1994.00000\n      0.00000\n      3.00000\n      3.00000\n      4.00000\n      3.00000\n      1.00000\n      4.00000\n      383.50000\n      1.00000\n      0.00000\n      477.50000\n      991.50000\n      5.00000\n      1087.00000\n      0.00000\n      0.00000\n      1464.00000\n      0.00000\n      0.00000\n      2.00000\n      0.00000\n      3.00000\n      1.00000\n      3.00000\n      6.00000\n      1.00000\n      2.00000\n      5.00000\n      77.00000\n      2.00000\n      2.00000\n      480.00000\n      3.00000\n      3.00000\n      0.00000\n      25.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      6.00000\n      2008.00000\n      163000.00000\n    \n    \n      75%\n      1095.25000\n      70.00000\n      3.00000\n      11601.50000\n      7.00000\n      6.00000\n      2000.00000\n      2004.00000\n      164.25000\n      4.00000\n      3.00000\n      4.00000\n      3.00000\n      2.00000\n      6.00000\n      712.25000\n      1.00000\n      0.00000\n      808.00000\n      1298.25000\n      5.00000\n      1391.25000\n      728.00000\n      0.00000\n      1776.75000\n      1.00000\n      0.00000\n      2.00000\n      1.00000\n      3.00000\n      1.00000\n      4.00000\n      7.00000\n      1.00000\n      4.00000\n      5.00000\n      101.00000\n      2.00000\n      2.00000\n      576.00000\n      3.00000\n      3.00000\n      168.00000\n      68.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      8.00000\n      2009.00000\n      214000.00000\n    \n    \n      max\n      1460.00000\n      190.00000\n      6.00000\n      215245.00000\n      10.00000\n      9.00000\n      2010.00000\n      2010.00000\n      1600.00000\n      5.00000\n      5.00000\n      5.00000\n      4.00000\n      4.00000\n      6.00000\n      5644.00000\n      6.00000\n      1474.00000\n      2336.00000\n      6110.00000\n      5.00000\n      4692.00000\n      2065.00000\n      572.00000\n      5642.00000\n      3.00000\n      2.00000\n      3.00000\n      2.00000\n      8.00000\n      3.00000\n      5.00000\n      14.00000\n      3.00000\n      5.00000\n      6.00000\n      110.00000\n      3.00000\n      4.00000\n      1418.00000\n      5.00000\n      5.00000\n      857.00000\n      547.00000\n      552.00000\n      508.00000\n      480.00000\n      738.00000\n      4.00000\n      15500.00000\n      12.00000\n      2010.00000\n      755000.00000\n    \n  \n\n\nOne hot 编码\ntrain_df.shape, test_df.shape\n((1460, 77), (1459, 76))\n\ndef preprocess_onehot_feature(dataset):\n    dataset_len = len(dataset)\n    result = pd.DataFrame()\n    for i in onehot_feature_set:\n        tmp_pd = pd.get_dummies(dataset[i], prefix=i, dtype=float)\n        result = pd.concat([result, tmp_pd], axis=1)\n    return result\ntrain_onehot_df = preprocess_onehot_feature(train_df)\ntest_onehot_df = preprocess_onehot_feature(test_df)\nprint(train_onehot_df.shape, test_onehot_df.shape)\ntest_missing = set(train_onehot_df.columns.values.tolist()) - set(test_onehot_df.columns.values.tolist())\ntest_missing\n(1460, 168) (1459, 153)\n\n\n\n\n\n{&#039;Condition2_RRAe&#039;,\n &#039;Condition2_RRAn&#039;,\n &#039;Condition2_RRNn&#039;,\n &#039;Electrical_Mix&#039;,\n &#039;Exterior1st_ImStucc&#039;,\n &#039;Exterior1st_Stone&#039;,\n &#039;Exterior2nd_Other&#039;,\n &#039;Heating_Floor&#039;,\n &#039;Heating_OthW&#039;,\n &#039;HouseStyle_2.5Fin&#039;,\n &#039;RoofMatl_ClyTile&#039;,\n &#039;RoofMatl_Membran&#039;,\n &#039;RoofMatl_Metal&#039;,\n &#039;RoofMatl_Roll&#039;,\n &#039;Utilities_NoSeWa&#039;}\n\n又遇到个坑，测试集比训练集的情况少。\n# 生成一个缺失的 dagafarm\ntest_missing_zeros = pd.Series(np.zeros(1459))\ntest_missing_fd = pd.DataFrame({k: test_missing_zeros for k in test_missing} )\n \n# 把缺失的填进去\ntest_onehot_df = pd.concat([test_onehot_df, test_missing_fd], axis=1)\nprint(train_onehot_df.shape, test_onehot_df.shape)\n(1460, 168) (1459, 168)\n\n把 新 one hot 编码的 feature 添加到 原集合中，并删除原来的 feature\n# 把one hot的feature添加到训练集和测试集\ntrain_df = pd.concat([train_df, train_onehot_df], axis=1)\ntest_df = pd.concat([test_df, test_onehot_df], axis=1)\n \n# 同步删除训练集和测试集中的原feature\ntrain_df = train_df.drop(onehot_feature_set, axis=1)\ntest_df = test_df.drop(onehot_feature_set, axis=1)\ntrain_df.describe()\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      Id\n      MSSubClass\n      MSZoning\n      LotArea\n      OverallQual\n      OverallCond\n      YearBuilt\n      YearRemodAdd\n      MasVnrArea\n      ExterQual\n      ExterCond\n      BsmtQual\n      BsmtCond\n      BsmtExposure\n      BsmtFinType1\n      BsmtFinSF1\n      BsmtFinType2\n      BsmtFinSF2\n      BsmtUnfSF\n      TotalBsmtSF\n      HeatingQC\n      1stFlrSF\n      2ndFlrSF\n      LowQualFinSF\n      GrLivArea\n      BsmtFullBath\n      BsmtHalfBath\n      FullBath\n      HalfBath\n      BedroomAbvGr\n      KitchenAbvGr\n      KitchenQual\n      TotRmsAbvGrd\n      Fireplaces\n      FireplaceQu\n      GarageType\n      GarageYrBlt\n      GarageFinish\n      GarageCars\n      GarageArea\n      ...\n      Exterior1st_BrkFace\n      Exterior1st_CBlock\n      Exterior1st_CemntBd\n      Exterior1st_HdBoard\n      Exterior1st_ImStucc\n      Exterior1st_MetalSd\n      Exterior1st_Plywood\n      Exterior1st_Stone\n      Exterior1st_Stucco\n      Exterior1st_VinylSd\n      Exterior1st_Wd Sdng\n      Exterior1st_WdShing\n      RoofMatl_ClyTile\n      RoofMatl_CompShg\n      RoofMatl_Membran\n      RoofMatl_Metal\n      RoofMatl_Roll\n      RoofMatl_Tar&amp;Grv\n      RoofMatl_WdShake\n      RoofMatl_WdShngl\n      Electrical_FuseA\n      Electrical_FuseF\n      Electrical_FuseP\n      Electrical_Mix\n      Electrical_SBrkr\n      SaleCondition_Abnorml\n      SaleCondition_AdjLand\n      SaleCondition_Alloca\n      SaleCondition_Family\n      SaleCondition_Normal\n      SaleCondition_Partial\n      Condition1_Artery\n      Condition1_Feedr\n      Condition1_Norm\n      Condition1_PosA\n      Condition1_PosN\n      Condition1_RRAe\n      Condition1_RRAn\n      Condition1_RRNe\n      Condition1_RRNn\n    \n  \n  \n    \n      count\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      ...\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n      1460.00000\n    \n    \n      mean\n      730.50000\n      56.89726\n      2.82534\n      10516.82808\n      6.09932\n      5.57534\n      1971.26781\n      1984.86575\n      103.11712\n      3.39589\n      3.08356\n      3.48904\n      2.93493\n      1.63014\n      3.54589\n      443.63973\n      1.24726\n      46.54932\n      567.24041\n      1057.42945\n      4.14521\n      1162.62671\n      346.99247\n      5.84452\n      1515.46370\n      0.42534\n      0.05753\n      1.56507\n      0.38288\n      2.86644\n      1.04658\n      3.51164\n      6.51781\n      0.61301\n      1.82534\n      3.51438\n      74.15068\n      1.71575\n      1.76712\n      472.98014\n      ...\n      0.03425\n      0.00068\n      0.04178\n      0.15205\n      0.00068\n      0.15068\n      0.07397\n      0.00137\n      0.01712\n      0.35274\n      0.14110\n      0.01781\n      0.00068\n      0.98219\n      0.00068\n      0.00068\n      0.00068\n      0.00753\n      0.00342\n      0.00411\n      0.06438\n      0.01849\n      0.00205\n      0.00068\n      0.91438\n      0.06918\n      0.00274\n      0.00822\n      0.01370\n      0.82055\n      0.08562\n      0.03288\n      0.05548\n      0.86301\n      0.00548\n      0.01301\n      0.00753\n      0.01781\n      0.00137\n      0.00342\n    \n    \n      std\n      421.61001\n      42.30057\n      1.02017\n      9981.26493\n      1.38300\n      1.11280\n      30.20290\n      20.64541\n      180.73137\n      0.57428\n      0.35105\n      0.87648\n      0.55216\n      1.06739\n      2.10778\n      456.09809\n      0.89233\n      161.31927\n      441.86696\n      438.70532\n      0.95950\n      386.58774\n      436.52844\n      48.62308\n      525.48038\n      0.51891\n      0.23875\n      0.55092\n      0.50289\n      0.81578\n      0.22034\n      0.66376\n      1.62539\n      0.64467\n      1.81088\n      1.93321\n      29.98205\n      0.89283\n      0.74732\n      213.80484\n      ...\n      0.18192\n      0.02617\n      0.20016\n      0.35920\n      0.02617\n      0.35786\n      0.26182\n      0.03700\n      0.12978\n      0.47799\n      0.34824\n      0.13230\n      0.02617\n      0.13230\n      0.02617\n      0.02617\n      0.02617\n      0.08650\n      0.05844\n      0.06400\n      0.24552\n      0.13477\n      0.04530\n      0.02617\n      0.27989\n      0.25384\n      0.05229\n      0.09032\n      0.11628\n      0.38386\n      0.27989\n      0.17837\n      0.22899\n      0.34395\n      0.07385\n      0.11337\n      0.08650\n      0.13230\n      0.03700\n      0.05844\n    \n    \n      min\n      1.00000\n      20.00000\n      0.00000\n      1300.00000\n      1.00000\n      1.00000\n      1872.00000\n      1950.00000\n      0.00000\n      2.00000\n      1.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      1.00000\n      334.00000\n      0.00000\n      0.00000\n      334.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      2.00000\n      2.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      ...\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n    \n    \n      25%\n      365.75000\n      20.00000\n      3.00000\n      7553.50000\n      5.00000\n      5.00000\n      1954.00000\n      1967.00000\n      0.00000\n      3.00000\n      3.00000\n      3.00000\n      3.00000\n      1.00000\n      1.00000\n      0.00000\n      1.00000\n      0.00000\n      223.00000\n      795.75000\n      3.00000\n      882.00000\n      0.00000\n      0.00000\n      1129.50000\n      0.00000\n      0.00000\n      1.00000\n      0.00000\n      2.00000\n      1.00000\n      3.00000\n      5.00000\n      0.00000\n      0.00000\n      1.00000\n      58.00000\n      1.00000\n      1.00000\n      334.50000\n      ...\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      1.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      1.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      1.00000\n      0.00000\n      0.00000\n      0.00000\n      1.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n    \n    \n      50%\n      730.50000\n      50.00000\n      3.00000\n      9478.50000\n      6.00000\n      5.00000\n      1973.00000\n      1994.00000\n      0.00000\n      3.00000\n      3.00000\n      4.00000\n      3.00000\n      1.00000\n      4.00000\n      383.50000\n      1.00000\n      0.00000\n      477.50000\n      991.50000\n      5.00000\n      1087.00000\n      0.00000\n      0.00000\n      1464.00000\n      0.00000\n      0.00000\n      2.00000\n      0.00000\n      3.00000\n      1.00000\n      3.00000\n      6.00000\n      1.00000\n      2.00000\n      5.00000\n      77.00000\n      2.00000\n      2.00000\n      480.00000\n      ...\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      1.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      1.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      1.00000\n      0.00000\n      0.00000\n      0.00000\n      1.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n    \n    \n      75%\n      1095.25000\n      70.00000\n      3.00000\n      11601.50000\n      7.00000\n      6.00000\n      2000.00000\n      2004.00000\n      164.25000\n      4.00000\n      3.00000\n      4.00000\n      3.00000\n      2.00000\n      6.00000\n      712.25000\n      1.00000\n      0.00000\n      808.00000\n      1298.25000\n      5.00000\n      1391.25000\n      728.00000\n      0.00000\n      1776.75000\n      1.00000\n      0.00000\n      2.00000\n      1.00000\n      3.00000\n      1.00000\n      4.00000\n      7.00000\n      1.00000\n      4.00000\n      5.00000\n      101.00000\n      2.00000\n      2.00000\n      576.00000\n      ...\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      1.00000\n      0.00000\n      0.00000\n      0.00000\n      1.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      1.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      1.00000\n      0.00000\n      0.00000\n      0.00000\n      1.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n    \n    \n      max\n      1460.00000\n      190.00000\n      6.00000\n      215245.00000\n      10.00000\n      9.00000\n      2010.00000\n      2010.00000\n      1600.00000\n      5.00000\n      5.00000\n      5.00000\n      4.00000\n      4.00000\n      6.00000\n      5644.00000\n      6.00000\n      1474.00000\n      2336.00000\n      6110.00000\n      5.00000\n      4692.00000\n      2065.00000\n      572.00000\n      5642.00000\n      3.00000\n      2.00000\n      3.00000\n      2.00000\n      8.00000\n      3.00000\n      5.00000\n      14.00000\n      3.00000\n      5.00000\n      6.00000\n      110.00000\n      3.00000\n      4.00000\n      1418.00000\n      ...\n      1.00000\n      1.00000\n      1.00000\n      1.00000\n      1.00000\n      1.00000\n      1.00000\n      1.00000\n      1.00000\n      1.00000\n      1.00000\n      1.00000\n      1.00000\n      1.00000\n      1.00000\n      1.00000\n      1.00000\n      1.00000\n      1.00000\n      1.00000\n      1.00000\n      1.00000\n      1.00000\n      1.00000\n      1.00000\n      1.00000\n      1.00000\n      1.00000\n      1.00000\n      1.00000\n      1.00000\n      1.00000\n      1.00000\n      1.00000\n      1.00000\n      1.00000\n      1.00000\n      1.00000\n      1.00000\n      1.00000\n    \n  \n\n8 rows × 221 columns\n\ntrain_df.info()\n&lt;class &#039;pandas.core.frame.DataFrame&#039;&gt;\nRangeIndex: 1460 entries, 0 to 1459\nColumns: 221 entries, Id to Condition1_RRNn\ndtypes: float64(182), int64(39)\nmemory usage: 2.5 MB\n\n终于处理完了，已经没有 object 类型的数据了。接下来就能建模了。\n模型预测\ntrain_df.sample(frac=1)\nX_train = train_df.drop([&#039;SalePrice&#039;, &#039;Id&#039;], axis=1)\nY_train = train_df[&#039;SalePrice&#039;]\nX_test = test_df.drop(&#039;Id&#039;, axis=1).copy()\nX_train.shape, Y_train.shape, X_test.shape\n((1460, 219), (1460,), (1459, 219))\n\n先用逻辑回归看看\nX_train\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      Id\n      MSSubClass\n      MSZoning\n      LotArea\n      OverallQual\n      OverallCond\n      YearBuilt\n      YearRemodAdd\n      MasVnrArea\n      ExterQual\n      ExterCond\n      BsmtQual\n      BsmtCond\n      BsmtExposure\n      BsmtFinType1\n      BsmtFinSF1\n      BsmtFinType2\n      BsmtFinSF2\n      BsmtUnfSF\n      TotalBsmtSF\n      HeatingQC\n      1stFlrSF\n      2ndFlrSF\n      LowQualFinSF\n      GrLivArea\n      BsmtFullBath\n      BsmtHalfBath\n      FullBath\n      HalfBath\n      BedroomAbvGr\n      KitchenAbvGr\n      KitchenQual\n      TotRmsAbvGrd\n      Fireplaces\n      FireplaceQu\n      GarageType\n      GarageYrBlt\n      GarageFinish\n      GarageCars\n      GarageArea\n      ...\n      Exterior1st_BrkFace\n      Exterior1st_CBlock\n      Exterior1st_CemntBd\n      Exterior1st_HdBoard\n      Exterior1st_ImStucc\n      Exterior1st_MetalSd\n      Exterior1st_Plywood\n      Exterior1st_Stone\n      Exterior1st_Stucco\n      Exterior1st_VinylSd\n      Exterior1st_Wd Sdng\n      Exterior1st_WdShing\n      RoofMatl_ClyTile\n      RoofMatl_CompShg\n      RoofMatl_Membran\n      RoofMatl_Metal\n      RoofMatl_Roll\n      RoofMatl_Tar&amp;Grv\n      RoofMatl_WdShake\n      RoofMatl_WdShngl\n      Electrical_FuseA\n      Electrical_FuseF\n      Electrical_FuseP\n      Electrical_Mix\n      Electrical_SBrkr\n      SaleCondition_Abnorml\n      SaleCondition_AdjLand\n      SaleCondition_Alloca\n      SaleCondition_Family\n      SaleCondition_Normal\n      SaleCondition_Partial\n      Condition1_Artery\n      Condition1_Feedr\n      Condition1_Norm\n      Condition1_PosA\n      Condition1_PosN\n      Condition1_RRAe\n      Condition1_RRAn\n      Condition1_RRNe\n      Condition1_RRNn\n    \n  \n  \n    \n      0\n      1\n      60\n      3.00000\n      8450\n      7\n      5\n      2003\n      2003\n      196.00000\n      4\n      3\n      4.00000\n      3.00000\n      1.00000\n      6.00000\n      706\n      1.00000\n      0\n      150\n      856\n      5\n      856\n      854\n      0\n      1710\n      1\n      0\n      2\n      1\n      3\n      1\n      4\n      8\n      0\n      0.00000\n      5.00000\n      103.00000\n      2.00000\n      2\n      548\n      ...\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      1.00000\n      0.00000\n      0.00000\n      0.00000\n      1.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      1.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      1.00000\n      0.00000\n      0.00000\n      0.00000\n      1.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n    \n    \n      1\n      2\n      20\n      3.00000\n      9600\n      6\n      8\n      1976\n      1976\n      0.00000\n      3\n      3\n      4.00000\n      3.00000\n      4.00000\n      5.00000\n      978\n      1.00000\n      0\n      284\n      1262\n      5\n      1262\n      0\n      0\n      1262\n      0\n      1\n      2\n      0\n      3\n      1\n      3\n      6\n      1\n      3.00000\n      5.00000\n      76.00000\n      2.00000\n      2\n      460\n      ...\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      1.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      1.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      1.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      1.00000\n      0.00000\n      0.00000\n      1.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n    \n    \n      2\n      3\n      60\n      3.00000\n      11250\n      7\n      5\n      2001\n      2002\n      162.00000\n      4\n      3\n      4.00000\n      3.00000\n      2.00000\n      6.00000\n      486\n      1.00000\n      0\n      434\n      920\n      5\n      920\n      866\n      0\n      1786\n      1\n      0\n      2\n      1\n      3\n      1\n      4\n      6\n      1\n      3.00000\n      5.00000\n      101.00000\n      2.00000\n      2\n      608\n      ...\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      1.00000\n      0.00000\n      0.00000\n      0.00000\n      1.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      1.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      1.00000\n      0.00000\n      0.00000\n      0.00000\n      1.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n    \n    \n      3\n      4\n      70\n      3.00000\n      9550\n      7\n      5\n      1915\n      1970\n      0.00000\n      3\n      3\n      3.00000\n      4.00000\n      1.00000\n      5.00000\n      216\n      1.00000\n      0\n      540\n      756\n      4\n      961\n      756\n      0\n      1717\n      1\n      0\n      1\n      0\n      3\n      1\n      4\n      7\n      1\n      4.00000\n      1.00000\n      98.00000\n      1.00000\n      3\n      642\n      ...\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      1.00000\n      0.00000\n      0.00000\n      1.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      1.00000\n      1.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      1.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n    \n    \n      4\n      5\n      60\n      3.00000\n      14260\n      8\n      5\n      2000\n      2000\n      350.00000\n      4\n      3\n      4.00000\n      3.00000\n      3.00000\n      6.00000\n      655\n      1.00000\n      0\n      490\n      1145\n      5\n      1145\n      1053\n      0\n      2198\n      1\n      0\n      2\n      1\n      4\n      1\n      4\n      9\n      1\n      3.00000\n      5.00000\n      100.00000\n      2.00000\n      3\n      836\n      ...\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      1.00000\n      0.00000\n      0.00000\n      0.00000\n      1.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      1.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      1.00000\n      0.00000\n      0.00000\n      0.00000\n      1.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      1455\n      1456\n      60\n      3.00000\n      7917\n      6\n      5\n      1999\n      2000\n      0.00000\n      3\n      3\n      4.00000\n      3.00000\n      1.00000\n      1.00000\n      0\n      1.00000\n      0\n      953\n      953\n      5\n      953\n      694\n      0\n      1647\n      0\n      0\n      2\n      1\n      3\n      1\n      3\n      7\n      1\n      3.00000\n      5.00000\n      99.00000\n      2.00000\n      2\n      460\n      ...\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      1.00000\n      0.00000\n      0.00000\n      0.00000\n      1.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      1.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      1.00000\n      0.00000\n      0.00000\n      0.00000\n      1.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n    \n    \n      1456\n      1457\n      20\n      3.00000\n      13175\n      6\n      6\n      1978\n      1988\n      119.00000\n      3\n      3\n      4.00000\n      3.00000\n      1.00000\n      5.00000\n      790\n      3.00000\n      163\n      589\n      1542\n      3\n      2073\n      0\n      0\n      2073\n      1\n      0\n      2\n      0\n      3\n      1\n      3\n      7\n      2\n      3.00000\n      5.00000\n      78.00000\n      1.00000\n      2\n      500\n      ...\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      1.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      1.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      1.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      1.00000\n      0.00000\n      0.00000\n      0.00000\n      1.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n    \n    \n      1457\n      1458\n      70\n      3.00000\n      9042\n      7\n      9\n      1941\n      2006\n      0.00000\n      5\n      4\n      3.00000\n      4.00000\n      1.00000\n      6.00000\n      275\n      1.00000\n      0\n      877\n      1152\n      5\n      1188\n      1152\n      0\n      2340\n      0\n      0\n      2\n      0\n      4\n      1\n      4\n      9\n      2\n      4.00000\n      5.00000\n      41.00000\n      2.00000\n      1\n      252\n      ...\n      0.00000\n      0.00000\n      1.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      1.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      1.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      1.00000\n      0.00000\n      0.00000\n      0.00000\n      1.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n    \n    \n      1458\n      1459\n      20\n      3.00000\n      9717\n      5\n      6\n      1950\n      1996\n      0.00000\n      3\n      3\n      3.00000\n      3.00000\n      2.00000\n      6.00000\n      49\n      3.00000\n      1029\n      0\n      1078\n      4\n      1078\n      0\n      0\n      1078\n      1\n      0\n      1\n      0\n      2\n      1\n      4\n      5\n      0\n      0.00000\n      5.00000\n      50.00000\n      1.00000\n      1\n      240\n      ...\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      1.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      1.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      1.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      1.00000\n      0.00000\n      0.00000\n      0.00000\n      1.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n    \n    \n      1459\n      1460\n      20\n      3.00000\n      9937\n      5\n      6\n      1965\n      1965\n      0.00000\n      4\n      3\n      3.00000\n      3.00000\n      1.00000\n      4.00000\n      830\n      2.00000\n      290\n      136\n      1256\n      4\n      1256\n      0\n      0\n      1256\n      1\n      0\n      1\n      1\n      3\n      1\n      3\n      6\n      0\n      0.00000\n      5.00000\n      65.00000\n      3.00000\n      1\n      276\n      ...\n      0.00000\n      0.00000\n      0.00000\n      1.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      1.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      1.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      1.00000\n      0.00000\n      0.00000\n      0.00000\n      1.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n      0.00000\n    \n  \n\n1460 rows × 220 columns\n\n# 逻辑回归\n \nlogreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\nY_pred = logreg.predict(X_test)\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)\nacc_log\n/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to &#039;lbfgs&#039; in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to &#039;auto&#039; in 0.22. Specify the multi_class option to silence this warning.\n  &quot;this warning.&quot;, FutureWarning)\n/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  &quot;the number of iterations.&quot;, ConvergenceWarning)\n\n\n\n\n\n90.55\n\n# Random Forest\n \nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\nY_pred = random_forest.predict(X_test)\nrandom_forest.score(X_train, Y_train)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nacc_random_forest\n100.0\n\n \nX = X_train.loc[: 1000]\nY = Y_train.loc[: 1000]\n \nX_t = X_train.loc[1000: ]\nY_t = Y_train.loc[1000: ]\n \nlogreg = RandomForestClassifier(n_estimators=100)\nlogreg.fit(X, Y)\npred = logreg.predict(X_t)\n \nplt.figure(figsize=(30, 5))\nx = np.array(range(len(pred)))\nplt.scatter(x, pred)\nplt.scatter(x, Y_t)\nplt.show()\n\nplt.figure(figsize=(30, 5))\nplt.plot(pred - Y_t)\nplt.show()\nnp.var(pred - Y_t)\n\n2231563230.032489\n\nnp.array([range(len(pred))]*10).T\narray([[  0,   0,   0, ...,   0,   0,   0],\n       [  1,   1,   1, ...,   1,   1,   1],\n       [  2,   2,   2, ...,   2,   2,   2],\n       ...,\n       [457, 457, 457, ..., 457, 457, 457],\n       [458, 458, 458, ..., 458, 458, 458],\n       [459, 459, 459, ..., 459, 459, 459]])\n\n# Support Vector Machines\n \nsvm = SVC()\nsvm.fit(X_train, Y_train)\nY_pred = svm.predict(X_test)\nacc_svm = round(svm.score(X_train, Y_train) * 100, 2)\nacc_svm\n/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from &#039;auto&#039; to &#039;scale&#039; in version 0.22 to account better for unscaled features. Set gamma explicitly to &#039;auto&#039; or &#039;scale&#039; to avoid this warning.\n  &quot;avoid this warning.&quot;, FutureWarning)\n\n\n\n\n\n99.52\n\n# 保存结果\n \nsubmission = pd.DataFrame({\n    &quot;Id&quot;: test_df[&quot;Id&quot;],\n    &quot;SalePrice&quot;: Y_pred\n    })\nsubmission.to_csv(&quot;/submission.csv&quot;, index=False)\n    "},"Kaggle/House_Prices_Advanced_Regression_Techniques_V2":{"slug":"Kaggle/House_Prices_Advanced_Regression_Techniques_V2","filePath":"Kaggle/House_Prices_Advanced_Regression_Techniques_V2.md","title":"House Prices Advanced Regression Techniques V2","links":[],"tags":["Kaggle"],"content":"Kaggle Competition 的练习\n房价预测\n# 安装 vecstack\n!pip install vecstack\n# 数据分析库\nimport pandas as pd\nimport numpy as np\nimport random\n \n# 数据可视化\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.style.use(&#039;ggplot&#039;)\n \n# 机器学习库\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor, BaggingRegressor\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom sklearn.model_selection import RandomizedSearchCV, cross_val_score, StratifiedKFold, learning_curve, KFold, train_test_split\n \n# Ensemble Models\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n \n# Package for stacking models\nfrom vecstack import stacking\n导入数据\ntrain = pd.read_csv(&#039;/train.csv&#039;, index_col=&#039;Id&#039;)\ntest = pd.read_csv(&#039;/test.csv&#039;, index_col=&#039;Id&#039;)\n \ntrain.head()\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      MSSubClass\n      MSZoning\n      LotFrontage\n      LotArea\n      Street\n      Alley\n      LotShape\n      LandContour\n      Utilities\n      LotConfig\n      LandSlope\n      Neighborhood\n      Condition1\n      Condition2\n      BldgType\n      HouseStyle\n      OverallQual\n      OverallCond\n      YearBuilt\n      YearRemodAdd\n      RoofStyle\n      RoofMatl\n      Exterior1st\n      Exterior2nd\n      MasVnrType\n      MasVnrArea\n      ExterQual\n      ExterCond\n      Foundation\n      BsmtQual\n      BsmtCond\n      BsmtExposure\n      BsmtFinType1\n      BsmtFinSF1\n      BsmtFinType2\n      BsmtFinSF2\n      BsmtUnfSF\n      TotalBsmtSF\n      Heating\n      HeatingQC\n      CentralAir\n      Electrical\n      1stFlrSF\n      2ndFlrSF\n      LowQualFinSF\n      GrLivArea\n      BsmtFullBath\n      BsmtHalfBath\n      FullBath\n      HalfBath\n      BedroomAbvGr\n      KitchenAbvGr\n      KitchenQual\n      TotRmsAbvGrd\n      Functional\n      Fireplaces\n      FireplaceQu\n      GarageType\n      GarageYrBlt\n      GarageFinish\n      GarageCars\n      GarageArea\n      GarageQual\n      GarageCond\n      PavedDrive\n      WoodDeckSF\n      OpenPorchSF\n      EnclosedPorch\n      3SsnPorch\n      ScreenPorch\n      PoolArea\n      PoolQC\n      Fence\n      MiscFeature\n      MiscVal\n      MoSold\n      YrSold\n      SaleType\n      SaleCondition\n      SalePrice\n    \n    \n      Id\n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n      \n    \n  \n  \n    \n      1\n      60\n      RL\n      65.0\n      8450\n      Pave\n      NaN\n      Reg\n      Lvl\n      AllPub\n      Inside\n      Gtl\n      CollgCr\n      Norm\n      Norm\n      1Fam\n      2Story\n      7\n      5\n      2003\n      2003\n      Gable\n      CompShg\n      VinylSd\n      VinylSd\n      BrkFace\n      196.0\n      Gd\n      TA\n      PConc\n      Gd\n      TA\n      No\n      GLQ\n      706\n      Unf\n      0\n      150\n      856\n      GasA\n      Ex\n      Y\n      SBrkr\n      856\n      854\n      0\n      1710\n      1\n      0\n      2\n      1\n      3\n      1\n      Gd\n      8\n      Typ\n      0\n      NaN\n      Attchd\n      2003.0\n      RFn\n      2\n      548\n      TA\n      TA\n      Y\n      0\n      61\n      0\n      0\n      0\n      0\n      NaN\n      NaN\n      NaN\n      0\n      2\n      2008\n      WD\n      Normal\n      208500\n    \n    \n      2\n      20\n      RL\n      80.0\n      9600\n      Pave\n      NaN\n      Reg\n      Lvl\n      AllPub\n      FR2\n      Gtl\n      Veenker\n      Feedr\n      Norm\n      1Fam\n      1Story\n      6\n      8\n      1976\n      1976\n      Gable\n      CompShg\n      MetalSd\n      MetalSd\n      None\n      0.0\n      TA\n      TA\n      CBlock\n      Gd\n      TA\n      Gd\n      ALQ\n      978\n      Unf\n      0\n      284\n      1262\n      GasA\n      Ex\n      Y\n      SBrkr\n      1262\n      0\n      0\n      1262\n      0\n      1\n      2\n      0\n      3\n      1\n      TA\n      6\n      Typ\n      1\n      TA\n      Attchd\n      1976.0\n      RFn\n      2\n      460\n      TA\n      TA\n      Y\n      298\n      0\n      0\n      0\n      0\n      0\n      NaN\n      NaN\n      NaN\n      0\n      5\n      2007\n      WD\n      Normal\n      181500\n    \n    \n      3\n      60\n      RL\n      68.0\n      11250\n      Pave\n      NaN\n      IR1\n      Lvl\n      AllPub\n      Inside\n      Gtl\n      CollgCr\n      Norm\n      Norm\n      1Fam\n      2Story\n      7\n      5\n      2001\n      2002\n      Gable\n      CompShg\n      VinylSd\n      VinylSd\n      BrkFace\n      162.0\n      Gd\n      TA\n      PConc\n      Gd\n      TA\n      Mn\n      GLQ\n      486\n      Unf\n      0\n      434\n      920\n      GasA\n      Ex\n      Y\n      SBrkr\n      920\n      866\n      0\n      1786\n      1\n      0\n      2\n      1\n      3\n      1\n      Gd\n      6\n      Typ\n      1\n      TA\n      Attchd\n      2001.0\n      RFn\n      2\n      608\n      TA\n      TA\n      Y\n      0\n      42\n      0\n      0\n      0\n      0\n      NaN\n      NaN\n      NaN\n      0\n      9\n      2008\n      WD\n      Normal\n      223500\n    \n    \n      4\n      70\n      RL\n      60.0\n      9550\n      Pave\n      NaN\n      IR1\n      Lvl\n      AllPub\n      Corner\n      Gtl\n      Crawfor\n      Norm\n      Norm\n      1Fam\n      2Story\n      7\n      5\n      1915\n      1970\n      Gable\n      CompShg\n      Wd Sdng\n      Wd Shng\n      None\n      0.0\n      TA\n      TA\n      BrkTil\n      TA\n      Gd\n      No\n      ALQ\n      216\n      Unf\n      0\n      540\n      756\n      GasA\n      Gd\n      Y\n      SBrkr\n      961\n      756\n      0\n      1717\n      1\n      0\n      1\n      0\n      3\n      1\n      Gd\n      7\n      Typ\n      1\n      Gd\n      Detchd\n      1998.0\n      Unf\n      3\n      642\n      TA\n      TA\n      Y\n      0\n      35\n      272\n      0\n      0\n      0\n      NaN\n      NaN\n      NaN\n      0\n      2\n      2006\n      WD\n      Abnorml\n      140000\n    \n    \n      5\n      60\n      RL\n      84.0\n      14260\n      Pave\n      NaN\n      IR1\n      Lvl\n      AllPub\n      FR2\n      Gtl\n      NoRidge\n      Norm\n      Norm\n      1Fam\n      2Story\n      8\n      5\n      2000\n      2000\n      Gable\n      CompShg\n      VinylSd\n      VinylSd\n      BrkFace\n      350.0\n      Gd\n      TA\n      PConc\n      Gd\n      TA\n      Av\n      GLQ\n      655\n      Unf\n      0\n      490\n      1145\n      GasA\n      Ex\n      Y\n      SBrkr\n      1145\n      1053\n      0\n      2198\n      1\n      0\n      2\n      1\n      4\n      1\n      Gd\n      9\n      Typ\n      1\n      TA\n      Attchd\n      2000.0\n      RFn\n      3\n      836\n      TA\n      TA\n      Y\n      192\n      84\n      0\n      0\n      0\n      0\n      NaN\n      NaN\n      NaN\n      0\n      12\n      2008\n      WD\n      Normal\n      250000\n    \n  \n\n\n浏览数据\ntrain.info()\n&lt;class &#039;pandas.core.frame.DataFrame&#039;&gt;\nInt64Index: 1460 entries, 1 to 1460\nData columns (total 80 columns):\nMSSubClass       1460 non-null int64\nMSZoning         1460 non-null object\nLotFrontage      1201 non-null float64\nLotArea          1460 non-null int64\nStreet           1460 non-null object\nAlley            91 non-null object\nLotShape         1460 non-null object\nLandContour      1460 non-null object\nUtilities        1460 non-null object\nLotConfig        1460 non-null object\nLandSlope        1460 non-null object\nNeighborhood     1460 non-null object\nCondition1       1460 non-null object\nCondition2       1460 non-null object\nBldgType         1460 non-null object\nHouseStyle       1460 non-null object\nOverallQual      1460 non-null int64\nOverallCond      1460 non-null int64\nYearBuilt        1460 non-null int64\nYearRemodAdd     1460 non-null int64\nRoofStyle        1460 non-null object\nRoofMatl         1460 non-null object\nExterior1st      1460 non-null object\nExterior2nd      1460 non-null object\nMasVnrType       1452 non-null object\nMasVnrArea       1452 non-null float64\nExterQual        1460 non-null object\nExterCond        1460 non-null object\nFoundation       1460 non-null object\nBsmtQual         1423 non-null object\nBsmtCond         1423 non-null object\nBsmtExposure     1422 non-null object\nBsmtFinType1     1423 non-null object\nBsmtFinSF1       1460 non-null int64\nBsmtFinType2     1422 non-null object\nBsmtFinSF2       1460 non-null int64\nBsmtUnfSF        1460 non-null int64\nTotalBsmtSF      1460 non-null int64\nHeating          1460 non-null object\nHeatingQC        1460 non-null object\nCentralAir       1460 non-null object\nElectrical       1459 non-null object\n1stFlrSF         1460 non-null int64\n2ndFlrSF         1460 non-null int64\nLowQualFinSF     1460 non-null int64\nGrLivArea        1460 non-null int64\nBsmtFullBath     1460 non-null int64\nBsmtHalfBath     1460 non-null int64\nFullBath         1460 non-null int64\nHalfBath         1460 non-null int64\nBedroomAbvGr     1460 non-null int64\nKitchenAbvGr     1460 non-null int64\nKitchenQual      1460 non-null object\nTotRmsAbvGrd     1460 non-null int64\nFunctional       1460 non-null object\nFireplaces       1460 non-null int64\nFireplaceQu      770 non-null object\nGarageType       1379 non-null object\nGarageYrBlt      1379 non-null float64\nGarageFinish     1379 non-null object\nGarageCars       1460 non-null int64\nGarageArea       1460 non-null int64\nGarageQual       1379 non-null object\nGarageCond       1379 non-null object\nPavedDrive       1460 non-null object\nWoodDeckSF       1460 non-null int64\nOpenPorchSF      1460 non-null int64\nEnclosedPorch    1460 non-null int64\n3SsnPorch        1460 non-null int64\nScreenPorch      1460 non-null int64\nPoolArea         1460 non-null int64\nPoolQC           7 non-null object\nFence            281 non-null object\nMiscFeature      54 non-null object\nMiscVal          1460 non-null int64\nMoSold           1460 non-null int64\nYrSold           1460 non-null int64\nSaleType         1460 non-null object\nSaleCondition    1460 non-null object\nSalePrice        1460 non-null int64\ndtypes: float64(3), int64(34), object(43)\nmemory usage: 923.9+ KB\n\ntrain.describe(include=&quot;O&quot;)\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      MSZoning\n      Street\n      Alley\n      LotShape\n      LandContour\n      Utilities\n      LotConfig\n      LandSlope\n      Neighborhood\n      Condition1\n      Condition2\n      BldgType\n      HouseStyle\n      RoofStyle\n      RoofMatl\n      Exterior1st\n      Exterior2nd\n      MasVnrType\n      ExterQual\n      ExterCond\n      Foundation\n      BsmtQual\n      BsmtCond\n      BsmtExposure\n      BsmtFinType1\n      BsmtFinType2\n      Heating\n      HeatingQC\n      CentralAir\n      Electrical\n      KitchenQual\n      Functional\n      FireplaceQu\n      GarageType\n      GarageFinish\n      GarageQual\n      GarageCond\n      PavedDrive\n      PoolQC\n      Fence\n      MiscFeature\n      SaleType\n      SaleCondition\n    \n  \n  \n    \n      count\n      1460\n      1460\n      91\n      1460\n      1460\n      1460\n      1460\n      1460\n      1460\n      1460\n      1460\n      1460\n      1460\n      1460\n      1460\n      1460\n      1460\n      1452\n      1460\n      1460\n      1460\n      1423\n      1423\n      1422\n      1423\n      1422\n      1460\n      1460\n      1460\n      1459\n      1460\n      1460\n      770\n      1379\n      1379\n      1379\n      1379\n      1460\n      7\n      281\n      54\n      1460\n      1460\n    \n    \n      unique\n      5\n      2\n      2\n      4\n      4\n      2\n      5\n      3\n      25\n      9\n      8\n      5\n      8\n      6\n      8\n      15\n      16\n      4\n      4\n      5\n      6\n      4\n      4\n      4\n      6\n      6\n      6\n      5\n      2\n      5\n      4\n      7\n      5\n      6\n      3\n      5\n      5\n      3\n      3\n      4\n      4\n      9\n      6\n    \n    \n      top\n      RL\n      Pave\n      Grvl\n      Reg\n      Lvl\n      AllPub\n      Inside\n      Gtl\n      NAmes\n      Norm\n      Norm\n      1Fam\n      1Story\n      Gable\n      CompShg\n      VinylSd\n      VinylSd\n      None\n      TA\n      TA\n      PConc\n      TA\n      TA\n      No\n      Unf\n      Unf\n      GasA\n      Ex\n      Y\n      SBrkr\n      TA\n      Typ\n      Gd\n      Attchd\n      Unf\n      TA\n      TA\n      Y\n      Gd\n      MnPrv\n      Shed\n      WD\n      Normal\n    \n    \n      freq\n      1151\n      1454\n      50\n      925\n      1311\n      1459\n      1052\n      1382\n      225\n      1260\n      1445\n      1220\n      726\n      1141\n      1434\n      515\n      504\n      864\n      906\n      1282\n      647\n      649\n      1311\n      953\n      430\n      1256\n      1428\n      741\n      1365\n      1334\n      735\n      1360\n      380\n      870\n      605\n      1311\n      1326\n      1340\n      3\n      157\n      49\n      1267\n      1198\n    \n  \n\n\n检查缺失数据\ntrain_missing = train.isnull().sum()\ntrain_missing = train_missing[train_missing &gt; 0]\ntrain_missing\nLotFrontage      259\nAlley           1369\nMasVnrType         8\nMasVnrArea         8\nBsmtQual          37\nBsmtCond          37\nBsmtExposure      38\nBsmtFinType1      37\nBsmtFinType2      38\nElectrical         1\nFireplaceQu      690\nGarageType        81\nGarageYrBlt       81\nGarageFinish      81\nGarageQual        81\nGarageCond        81\nPoolQC          1453\nFence           1179\nMiscFeature     1406\ndtype: int64\n\ntest_missing = test.isnull().sum()\ntest_missing = test_missing[test_missing &gt; 0]\ntest_missing\nMSZoning           4\nLotFrontage      227\nAlley           1352\nUtilities          2\nExterior1st        1\nExterior2nd        1\nMasVnrType        16\nMasVnrArea        15\nBsmtQual          44\nBsmtCond          45\nBsmtExposure      44\nBsmtFinType1      42\nBsmtFinSF1         1\nBsmtFinType2      42\nBsmtFinSF2         1\nBsmtUnfSF          1\nTotalBsmtSF        1\nBsmtFullBath       2\nBsmtHalfBath       2\nKitchenQual        1\nFunctional         2\nFireplaceQu      730\nGarageType        76\nGarageYrBlt       78\nGarageFinish      78\nGarageCars         1\nGarageArea         1\nGarageQual        78\nGarageCond        78\nPoolQC          1456\nFence           1169\nMiscFeature     1408\nSaleType           1\ndtype: int64\n\n# 可视化缺失数据\ndef plot_missing(df):\n    # 寻找缺失的列\n    missing = df.isnull().sum()\n    missing = missing[missing &gt; 0]\n    missing.sort_values(inplace=True)\n    \n    # 画出缺失值的柱状图。\n    missing.plot.bar(figsize=(10,8))\n    plt.xlabel(&#039;Columns with missing values&#039;)\n    plt.ylabel(&#039;Count&#039;)\n    \n    # 搜索缺失值\n    import missingno as msno\n    msno.matrix(df=df, figsize=(10,8))\n    # 查看相关性\n    #msno.heatmap(df=df,figsize=(10,8))\nplot_missing(train)\n\n\nplot_missing(test)\n\n\n分析概要\n以下是缺失比较多的feature\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeatureTrain missTest missDisposLotFrontage259227填个中位数吧Alley13691352删除FireplaceQu690730fireplaceQU 和 fireplaces 有关 缺失项貌似都是没有fireplace的PoolQC14531456删除Fence11791169删除MiscFeature14061408删除\n处理缺失值\n# 删除缺失过多的feature\ntrain = train.drop([&#039;Alley&#039;, &#039;PoolQC&#039;, &#039;MiscFeature&#039;, &#039;Fence&#039;], axis=1)\ntest = test.drop([&#039;Alley&#039;, &#039;PoolQC&#039;, &#039;MiscFeature&#039;, &#039;Fence&#039;], axis=1)\n# FireplaceQu 缺失的都认为是没有的\ntrain[&#039;FireplaceQu&#039;] = train[&#039;FireplaceQu&#039;].fillna(&#039;NA&#039;)\ntest[&#039;FireplaceQu&#039;] = test[&#039;FireplaceQu&#039;].fillna(&#039;NA&#039;)\n# 填充其他缺失值\ndef fill_missing_values(df):\n    # 查找缺失值\n    missing = df.isnull().sum()\n    missing = missing[missing &gt; 0]\n    \n    for column in missing.index:\n        # 类型为 object 的填众数\n        if df[column].dtype == &#039;object&#039;:\n            df[column].fillna(df[column].value_counts().index[0], inplace=True)\n        # 其他类型填中位数\n        else:\n            df[column].fillna(df[column].median(), inplace=True)\nfill_missing_values(train)\ntrain.isnull().sum().max()\n0\n\nfill_missing_values(test)\ntest.isnull().sum().max()\n0\n\n好的，已经没有缺失数据了。\n整理 description 文件\n数据描述文件记录了所有特征所代表的含义，其中许多特征是字符串，现在我们要整理为个字典，便于我们查询。\ndescription_dict = {}\nwith open(&#039;/data_description.txt&#039;,&#039;r&#039;) as description:\n    description_data = description.read()\n    description.close()\n    \ndescription_data = description_data.split(&#039;\\n&#039;)\nfor i in description_data:\n    if &#039;:&#039; in i:\n        key = i.split(&#039;:&#039;)[0]\n        description_dict[key] = []\n    elif i.split() and &#039;       &#039; in i:\n        value = i.split()[0]\n        description_dict[key].append(value)\n \nprint(description_dict)\n{&#039;MSSubClass&#039;: [&#039;20&#039;, &#039;30&#039;, &#039;40&#039;, &#039;45&#039;, &#039;50&#039;, &#039;60&#039;, &#039;70&#039;, &#039;75&#039;, &#039;80&#039;, &#039;85&#039;, &#039;90&#039;, &#039;120&#039;, &#039;150&#039;, &#039;160&#039;, &#039;180&#039;, &#039;190&#039;], &#039;MSZoning&#039;: [&#039;A&#039;, &#039;C&#039;, &#039;FV&#039;, &#039;I&#039;, &#039;RH&#039;, &#039;RL&#039;, &#039;RP&#039;, &#039;RM&#039;], &#039;LotFrontage&#039;: [], &#039;LotArea&#039;: [], &#039;Street&#039;: [&#039;Grvl&#039;, &#039;Pave&#039;], &#039;Alley&#039;: [&#039;Grvl&#039;, &#039;Pave&#039;, &#039;NA&#039;], &#039;LotShape&#039;: [&#039;Reg&#039;, &#039;IR1&#039;, &#039;IR2&#039;, &#039;IR3&#039;], &#039;LandContour&#039;: [&#039;Lvl&#039;, &#039;Bnk&#039;, &#039;HLS&#039;, &#039;Low&#039;], &#039;Utilities&#039;: [&#039;AllPub&#039;, &#039;NoSewr&#039;, &#039;NoSeWa&#039;, &#039;ELO&#039;], &#039;LotConfig&#039;: [&#039;Inside&#039;, &#039;Corner&#039;, &#039;CulDSac&#039;, &#039;FR2&#039;, &#039;FR3&#039;], &#039;LandSlope&#039;: [&#039;Gtl&#039;, &#039;Mod&#039;, &#039;Sev&#039;], &#039;Neighborhood&#039;: [&#039;Blmngtn&#039;, &#039;Blueste&#039;, &#039;BrDale&#039;, &#039;BrkSide&#039;, &#039;ClearCr&#039;, &#039;CollgCr&#039;, &#039;Crawfor&#039;, &#039;Edwards&#039;, &#039;Gilbert&#039;, &#039;IDOTRR&#039;, &#039;MeadowV&#039;, &#039;Mitchel&#039;, &#039;Names&#039;, &#039;NoRidge&#039;, &#039;NPkVill&#039;, &#039;NridgHt&#039;, &#039;NWAmes&#039;, &#039;OldTown&#039;, &#039;SWISU&#039;, &#039;Sawyer&#039;, &#039;SawyerW&#039;, &#039;Somerst&#039;, &#039;StoneBr&#039;, &#039;Timber&#039;, &#039;Veenker&#039;], &#039;Condition1&#039;: [&#039;Artery&#039;, &#039;Feedr&#039;, &#039;Norm&#039;, &#039;RRNn&#039;, &#039;RRAn&#039;, &#039;PosN&#039;, &#039;PosA&#039;, &#039;RRNe&#039;, &#039;RRAe&#039;], &#039;Condition2&#039;: [&#039;Artery&#039;, &#039;Feedr&#039;, &#039;Norm&#039;, &#039;RRNn&#039;, &#039;RRAn&#039;, &#039;PosN&#039;, &#039;PosA&#039;, &#039;RRNe&#039;, &#039;RRAe&#039;], &#039;BldgType&#039;: [&#039;1Fam&#039;, &#039;2FmCon&#039;, &#039;Duplx&#039;, &#039;TwnhsE&#039;, &#039;TwnhsI&#039;], &#039;HouseStyle&#039;: [&#039;1Story&#039;], &#039;       1.5Fin\\tOne and one-half story&#039;: [], &#039;       1.5Unf\\tOne and one-half story&#039;: [&#039;2Story&#039;], &#039;       2.5Fin\\tTwo and one-half story&#039;: [], &#039;       2.5Unf\\tTwo and one-half story&#039;: [&#039;SFoyer&#039;, &#039;SLvl&#039;], &#039;OverallQual&#039;: [&#039;10&#039;, &#039;9&#039;, &#039;8&#039;, &#039;7&#039;, &#039;6&#039;, &#039;5&#039;, &#039;4&#039;, &#039;3&#039;, &#039;2&#039;, &#039;1&#039;], &#039;OverallCond&#039;: [&#039;10&#039;, &#039;9&#039;, &#039;8&#039;, &#039;7&#039;, &#039;6&#039;, &#039;5&#039;, &#039;4&#039;, &#039;3&#039;, &#039;2&#039;, &#039;1&#039;], &#039;YearBuilt&#039;: [], &#039;YearRemodAdd&#039;: [], &#039;RoofStyle&#039;: [&#039;Flat&#039;, &#039;Gable&#039;, &#039;Gambrel&#039;, &#039;Hip&#039;, &#039;Mansard&#039;, &#039;Shed&#039;], &#039;RoofMatl&#039;: [&#039;ClyTile&#039;, &#039;CompShg&#039;, &#039;Membran&#039;, &#039;Metal&#039;, &#039;Roll&#039;, &#039;Tar&amp;Grv&#039;, &#039;WdShake&#039;, &#039;WdShngl&#039;], &#039;Exterior1st&#039;: [&#039;AsbShng&#039;, &#039;AsphShn&#039;, &#039;BrkComm&#039;, &#039;BrkFace&#039;, &#039;CBlock&#039;, &#039;CemntBd&#039;, &#039;HdBoard&#039;, &#039;ImStucc&#039;, &#039;MetalSd&#039;, &#039;Other&#039;, &#039;Plywood&#039;, &#039;PreCast&#039;, &#039;Stone&#039;, &#039;Stucco&#039;, &#039;VinylSd&#039;, &#039;Wd&#039;, &#039;WdShing&#039;], &#039;Exterior2nd&#039;: [&#039;AsbShng&#039;, &#039;AsphShn&#039;, &#039;BrkComm&#039;, &#039;BrkFace&#039;, &#039;CBlock&#039;, &#039;CemntBd&#039;, &#039;HdBoard&#039;, &#039;ImStucc&#039;, &#039;MetalSd&#039;, &#039;Other&#039;, &#039;Plywood&#039;, &#039;PreCast&#039;, &#039;Stone&#039;, &#039;Stucco&#039;, &#039;VinylSd&#039;, &#039;Wd&#039;, &#039;WdShing&#039;], &#039;MasVnrType&#039;: [&#039;BrkCmn&#039;, &#039;BrkFace&#039;, &#039;CBlock&#039;, &#039;None&#039;, &#039;Stone&#039;], &#039;MasVnrArea&#039;: [], &#039;ExterQual&#039;: [&#039;Ex&#039;, &#039;Gd&#039;, &#039;TA&#039;, &#039;Fa&#039;, &#039;Po&#039;], &#039;ExterCond&#039;: [&#039;Ex&#039;, &#039;Gd&#039;, &#039;TA&#039;, &#039;Fa&#039;, &#039;Po&#039;], &#039;Foundation&#039;: [&#039;BrkTil&#039;, &#039;CBlock&#039;, &#039;PConc&#039;, &#039;Slab&#039;, &#039;Stone&#039;, &#039;Wood&#039;], &#039;BsmtQual&#039;: [&#039;Ex&#039;, &#039;Gd&#039;, &#039;TA&#039;, &#039;Fa&#039;, &#039;Po&#039;, &#039;NA&#039;], &#039;BsmtCond&#039;: [&#039;Ex&#039;, &#039;Gd&#039;, &#039;TA&#039;, &#039;Fa&#039;, &#039;Po&#039;, &#039;NA&#039;], &#039;BsmtExposure&#039;: [&#039;Gd&#039;, &#039;Av&#039;, &#039;Mn&#039;, &#039;No&#039;, &#039;NA&#039;], &#039;BsmtFinType1&#039;: [&#039;GLQ&#039;, &#039;ALQ&#039;, &#039;BLQ&#039;, &#039;Rec&#039;, &#039;LwQ&#039;, &#039;Unf&#039;, &#039;NA&#039;], &#039;BsmtFinSF1&#039;: [], &#039;BsmtFinType2&#039;: [&#039;GLQ&#039;, &#039;ALQ&#039;, &#039;BLQ&#039;, &#039;Rec&#039;, &#039;LwQ&#039;, &#039;Unf&#039;, &#039;NA&#039;], &#039;BsmtFinSF2&#039;: [], &#039;BsmtUnfSF&#039;: [], &#039;TotalBsmtSF&#039;: [], &#039;Heating&#039;: [&#039;Floor&#039;, &#039;GasA&#039;, &#039;GasW&#039;, &#039;Grav&#039;, &#039;OthW&#039;, &#039;Wall&#039;], &#039;HeatingQC&#039;: [&#039;Ex&#039;, &#039;Gd&#039;, &#039;TA&#039;, &#039;Fa&#039;, &#039;Po&#039;], &#039;CentralAir&#039;: [&#039;N&#039;, &#039;Y&#039;], &#039;Electrical&#039;: [&#039;SBrkr&#039;, &#039;FuseA&#039;, &#039;FuseF&#039;, &#039;FuseP&#039;, &#039;Mix&#039;], &#039;1stFlrSF&#039;: [], &#039;2ndFlrSF&#039;: [], &#039;LowQualFinSF&#039;: [], &#039;GrLivArea&#039;: [], &#039;BsmtFullBath&#039;: [], &#039;BsmtHalfBath&#039;: [], &#039;FullBath&#039;: [], &#039;HalfBath&#039;: [], &#039;Bedroom&#039;: [], &#039;Kitchen&#039;: [], &#039;KitchenQual&#039;: [&#039;Ex&#039;, &#039;Gd&#039;, &#039;TA&#039;, &#039;Fa&#039;, &#039;Po&#039;], &#039;TotRmsAbvGrd&#039;: [], &#039;Functional&#039;: [&#039;Typ&#039;, &#039;Min1&#039;, &#039;Min2&#039;, &#039;Mod&#039;, &#039;Maj1&#039;, &#039;Maj2&#039;, &#039;Sev&#039;, &#039;Sal&#039;], &#039;Fireplaces&#039;: [], &#039;FireplaceQu&#039;: [&#039;Ex&#039;, &#039;Gd&#039;, &#039;TA&#039;, &#039;Fa&#039;, &#039;Po&#039;, &#039;NA&#039;], &#039;GarageType&#039;: [&#039;2Types&#039;, &#039;Attchd&#039;, &#039;Basment&#039;, &#039;BuiltIn&#039;, &#039;CarPort&#039;, &#039;Detchd&#039;, &#039;NA&#039;], &#039;GarageYrBlt&#039;: [], &#039;GarageFinish&#039;: [&#039;Fin&#039;, &#039;RFn&#039;, &#039;Unf&#039;, &#039;NA&#039;], &#039;GarageCars&#039;: [], &#039;GarageArea&#039;: [], &#039;GarageQual&#039;: [&#039;Ex&#039;, &#039;Gd&#039;, &#039;TA&#039;, &#039;Fa&#039;, &#039;Po&#039;, &#039;NA&#039;], &#039;GarageCond&#039;: [&#039;Ex&#039;, &#039;Gd&#039;, &#039;TA&#039;, &#039;Fa&#039;, &#039;Po&#039;, &#039;NA&#039;], &#039;PavedDrive&#039;: [&#039;Y&#039;, &#039;P&#039;, &#039;N&#039;], &#039;WoodDeckSF&#039;: [], &#039;OpenPorchSF&#039;: [], &#039;EnclosedPorch&#039;: [], &#039;3SsnPorch&#039;: [], &#039;ScreenPorch&#039;: [], &#039;PoolArea&#039;: [], &#039;PoolQC&#039;: [&#039;Ex&#039;, &#039;Gd&#039;, &#039;TA&#039;, &#039;Fa&#039;, &#039;NA&#039;], &#039;Fence&#039;: [&#039;GdPrv&#039;, &#039;MnPrv&#039;, &#039;GdWo&#039;, &#039;MnWw&#039;, &#039;NA&#039;], &#039;MiscFeature&#039;: [&#039;Elev&#039;, &#039;Gar2&#039;, &#039;Othr&#039;, &#039;Shed&#039;, &#039;TenC&#039;, &#039;NA&#039;], &#039;MiscVal&#039;: [], &#039;MoSold&#039;: [], &#039;YrSold&#039;: [], &#039;SaleType&#039;: [&#039;WD&#039;, &#039;CWD&#039;, &#039;VWD&#039;, &#039;New&#039;, &#039;COD&#039;, &#039;Con&#039;, &#039;ConLw&#039;, &#039;ConLI&#039;, &#039;ConLD&#039;, &#039;Oth&#039;], &#039;SaleCondition&#039;: [&#039;Normal&#039;, &#039;Abnorml&#039;, &#039;AdjLand&#039;, &#039;Alloca&#039;, &#039;Family&#039;, &#039;Partial&#039;]}\n\n字符串类型的 feature 重编码\n# 这个函数的作用是得到数据集中非数字的feature column\ndef get_object_column(df):\n    object_column = []\n    for column in df.columns:\n        if df[column].dtype == &#039;object&#039;:\n            object_column.append(column)\n    return object_column\n#object_column = get_object_column(train)\n# 这个函数的作用是把 description_dict 中的 value 转换为对应数字的字典\ndef generate_map(map_list, end_index=1):\n    d = {}\n    j = len(map_list) - end_index\n    for i in map_list:\n        d[i] = j\n        j -= 1\n    return d\ndef preprocess_order_feature(df):\n    for i in get_object_column(df):\n        order_map = generate_map(description_dict[i], 0)\n        df[i] = df[i].map(order_map)\n        df[i] = df[i].fillna(0)\n    return df\ntrain = preprocess_order_feature(train)\ntest = preprocess_order_feature(test)\n观察数据\ntrain.describe()\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      MSSubClass\n      MSZoning\n      LotFrontage\n      LotArea\n      Street\n      LotShape\n      LandContour\n      Utilities\n      LotConfig\n      LandSlope\n      Neighborhood\n      Condition1\n      Condition2\n      BldgType\n      HouseStyle\n      OverallQual\n      OverallCond\n      YearBuilt\n      YearRemodAdd\n      RoofStyle\n      RoofMatl\n      Exterior1st\n      Exterior2nd\n      MasVnrType\n      MasVnrArea\n      ExterQual\n      ExterCond\n      Foundation\n      BsmtQual\n      BsmtCond\n      BsmtExposure\n      BsmtFinType1\n      BsmtFinSF1\n      BsmtFinType2\n      BsmtFinSF2\n      BsmtUnfSF\n      TotalBsmtSF\n      Heating\n      HeatingQC\n      CentralAir\n      Electrical\n      1stFlrSF\n      2ndFlrSF\n      LowQualFinSF\n      GrLivArea\n      BsmtFullBath\n      BsmtHalfBath\n      FullBath\n      HalfBath\n      BedroomAbvGr\n      KitchenAbvGr\n      KitchenQual\n      TotRmsAbvGrd\n      Functional\n      Fireplaces\n      FireplaceQu\n      GarageType\n      GarageYrBlt\n      GarageFinish\n      GarageCars\n      GarageArea\n      GarageQual\n      GarageCond\n      PavedDrive\n      WoodDeckSF\n      OpenPorchSF\n      EnclosedPorch\n      3SsnPorch\n      ScreenPorch\n      PoolArea\n      MiscVal\n      MoSold\n      YrSold\n      SaleType\n      SaleCondition\n      SalePrice\n    \n  \n  \n    \n      count\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.00000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n    \n    \n      mean\n      56.897260\n      2.825342\n      69.863699\n      10516.828082\n      1.004110\n      3.591781\n      3.814384\n      3.998630\n      4.583562\n      2.937671\n      10.747945\n      6.969178\n      6.993151\n      4.334247\n      0.497260\n      6.099315\n      5.575342\n      1971.267808\n      1984.865753\n      4.589726\n      6.924658\n      5.958904\n      5.271918\n      2.552740\n      103.117123\n      3.39589\n      3.083562\n      4.603425\n      4.565068\n      4.010959\n      2.656164\n      4.571233\n      443.639726\n      2.273288\n      46.549315\n      567.240411\n      1057.429452\n      4.963699\n      4.145205\n      1.065068\n      4.889726\n      1162.626712\n      346.992466\n      5.844521\n      1515.463699\n      0.425342\n      0.057534\n      1.565068\n      0.382877\n      2.866438\n      1.046575\n      3.511644\n      6.517808\n      7.841781\n      0.613014\n      2.825342\n      4.791781\n      1978.589041\n      2.771233\n      1.767123\n      472.980137\n      3.976712\n      3.975342\n      2.856164\n      94.244521\n      46.660274\n      21.954110\n      3.409589\n      15.060959\n      2.758904\n      43.489041\n      6.321918\n      2007.815753\n      9.509589\n      5.417808\n      180921.195890\n    \n    \n      std\n      42.300571\n      1.020174\n      22.027677\n      9981.264932\n      0.063996\n      0.582296\n      0.606509\n      0.052342\n      0.773448\n      0.276232\n      7.565716\n      0.878349\n      0.248272\n      1.555218\n      0.500164\n      1.382997\n      1.112799\n      30.202904\n      20.645407\n      0.834998\n      0.599127\n      4.426038\n      4.263353\n      1.046204\n      180.731373\n      0.57428\n      0.351054\n      0.722394\n      0.678071\n      0.284178\n      1.039123\n      2.070649\n      456.098091\n      0.869859\n      161.319273\n      441.866955\n      438.705324\n      0.295124\n      0.959501\n      0.246731\n      0.394658\n      386.587738\n      436.528436\n      48.623081\n      525.480383\n      0.518911\n      0.238753\n      0.550916\n      0.502885\n      0.815778\n      0.220338\n      0.663760\n      1.625393\n      0.667698\n      0.644666\n      1.810877\n      1.759864\n      23.997022\n      0.811835\n      0.747315\n      213.804841\n      0.241665\n      0.232860\n      0.496592\n      125.338794\n      66.256028\n      61.119149\n      29.317331\n      55.757415\n      40.177307\n      496.123024\n      2.703626\n      1.328095\n      1.368616\n      1.475209\n      79442.502883\n    \n    \n      min\n      20.000000\n      0.000000\n      21.000000\n      1300.000000\n      1.000000\n      1.000000\n      1.000000\n      2.000000\n      1.000000\n      1.000000\n      0.000000\n      1.000000\n      1.000000\n      0.000000\n      0.000000\n      1.000000\n      1.000000\n      1872.000000\n      1950.000000\n      1.000000\n      1.000000\n      0.000000\n      0.000000\n      1.000000\n      0.000000\n      2.00000\n      1.000000\n      1.000000\n      3.000000\n      2.000000\n      2.000000\n      2.000000\n      0.000000\n      2.000000\n      0.000000\n      0.000000\n      0.000000\n      1.000000\n      1.000000\n      1.000000\n      1.000000\n      334.000000\n      0.000000\n      0.000000\n      334.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      2.000000\n      2.000000\n      2.000000\n      0.000000\n      1.000000\n      2.000000\n      1900.000000\n      2.000000\n      0.000000\n      0.000000\n      2.000000\n      2.000000\n      1.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      1.000000\n      2006.000000\n      1.000000\n      1.000000\n      34900.000000\n    \n    \n      25%\n      20.000000\n      3.000000\n      60.000000\n      7553.500000\n      1.000000\n      3.000000\n      4.000000\n      4.000000\n      4.000000\n      3.000000\n      4.000000\n      7.000000\n      7.000000\n      5.000000\n      0.000000\n      5.000000\n      5.000000\n      1954.000000\n      1967.000000\n      5.000000\n      7.000000\n      3.000000\n      3.000000\n      2.000000\n      0.000000\n      3.00000\n      3.000000\n      4.000000\n      4.000000\n      4.000000\n      2.000000\n      2.000000\n      0.000000\n      2.000000\n      0.000000\n      223.000000\n      795.750000\n      5.000000\n      3.000000\n      1.000000\n      5.000000\n      882.000000\n      0.000000\n      0.000000\n      1129.500000\n      0.000000\n      0.000000\n      1.000000\n      0.000000\n      2.000000\n      1.000000\n      3.000000\n      5.000000\n      8.000000\n      0.000000\n      1.000000\n      2.000000\n      1962.000000\n      2.000000\n      1.000000\n      334.500000\n      4.000000\n      4.000000\n      3.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      5.000000\n      2007.000000\n      10.000000\n      6.000000\n      129975.000000\n    \n    \n      50%\n      50.000000\n      3.000000\n      69.000000\n      9478.500000\n      1.000000\n      4.000000\n      4.000000\n      4.000000\n      5.000000\n      3.000000\n      10.000000\n      7.000000\n      7.000000\n      5.000000\n      0.000000\n      6.000000\n      5.000000\n      1973.000000\n      1994.000000\n      5.000000\n      7.000000\n      3.000000\n      3.000000\n      2.000000\n      0.000000\n      3.00000\n      3.000000\n      5.000000\n      5.000000\n      4.000000\n      2.000000\n      5.000000\n      383.500000\n      2.000000\n      0.000000\n      477.500000\n      991.500000\n      5.000000\n      5.000000\n      1.000000\n      5.000000\n      1087.000000\n      0.000000\n      0.000000\n      1464.000000\n      0.000000\n      0.000000\n      2.000000\n      0.000000\n      3.000000\n      1.000000\n      3.000000\n      6.000000\n      8.000000\n      1.000000\n      3.000000\n      6.000000\n      1980.000000\n      3.000000\n      2.000000\n      480.000000\n      4.000000\n      4.000000\n      3.000000\n      0.000000\n      25.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      6.000000\n      2008.000000\n      10.000000\n      6.000000\n      163000.000000\n    \n    \n      75%\n      70.000000\n      3.000000\n      79.000000\n      11601.500000\n      1.000000\n      4.000000\n      4.000000\n      4.000000\n      5.000000\n      3.000000\n      18.000000\n      7.000000\n      7.000000\n      5.000000\n      1.000000\n      7.000000\n      6.000000\n      2000.000000\n      2004.000000\n      5.000000\n      7.000000\n      9.000000\n      9.000000\n      4.000000\n      164.250000\n      4.00000\n      3.000000\n      5.000000\n      5.000000\n      4.000000\n      3.000000\n      7.000000\n      712.250000\n      2.000000\n      0.000000\n      808.000000\n      1298.250000\n      5.000000\n      5.000000\n      1.000000\n      5.000000\n      1391.250000\n      728.000000\n      0.000000\n      1776.750000\n      1.000000\n      0.000000\n      2.000000\n      1.000000\n      3.000000\n      1.000000\n      4.000000\n      7.000000\n      8.000000\n      1.000000\n      5.000000\n      6.000000\n      2001.000000\n      3.000000\n      2.000000\n      576.000000\n      4.000000\n      4.000000\n      3.000000\n      168.000000\n      68.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      8.000000\n      2009.000000\n      10.000000\n      6.000000\n      214000.000000\n    \n    \n      max\n      190.000000\n      6.000000\n      313.000000\n      215245.000000\n      2.000000\n      4.000000\n      4.000000\n      4.000000\n      5.000000\n      3.000000\n      25.000000\n      9.000000\n      9.000000\n      5.000000\n      1.000000\n      10.000000\n      9.000000\n      2010.000000\n      2010.000000\n      6.000000\n      8.000000\n      17.000000\n      17.000000\n      5.000000\n      1600.000000\n      5.00000\n      5.000000\n      6.000000\n      6.000000\n      5.000000\n      5.000000\n      7.000000\n      5644.000000\n      7.000000\n      1474.000000\n      2336.000000\n      6110.000000\n      6.000000\n      5.000000\n      2.000000\n      5.000000\n      4692.000000\n      2065.000000\n      572.000000\n      5642.000000\n      3.000000\n      2.000000\n      3.000000\n      2.000000\n      8.000000\n      3.000000\n      5.000000\n      14.000000\n      8.000000\n      3.000000\n      6.000000\n      7.000000\n      2010.000000\n      4.000000\n      4.000000\n      1418.000000\n      6.000000\n      6.000000\n      3.000000\n      857.000000\n      547.000000\n      552.000000\n      508.000000\n      480.000000\n      738.000000\n      15500.000000\n      12.000000\n      2010.000000\n      10.000000\n      6.000000\n      755000.000000\n    \n  \n\n\n观察 feature 之间的相关性\ncorr_mat = train[[&quot;SalePrice&quot;,&quot;MSSubClass&quot;,&quot;MSZoning&quot;,&quot;LotFrontage&quot;,&quot;LotArea&quot;, &quot;BldgType&quot;,\n                       &quot;OverallQual&quot;, &quot;OverallCond&quot;,&quot;YearBuilt&quot;, &quot;BedroomAbvGr&quot;, &quot;PoolArea&quot;, &quot;GarageArea&quot;,\n                       &quot;SaleType&quot;, &quot;MoSold&quot;]].corr()\n# corr_mat = train.corr()\nf, ax = plt.subplots(figsize=(16, 8))\nsns.heatmap(corr_mat, vmax=1 , square=True)\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7fb6c086fa20&gt;\n\n\n观察热力图，颜色越浅相关性越大。关于热力图→ this video.\n观察年限和售价的规律\nf, ax = plt.subplots(figsize=(16, 8))\nsns.lineplot(x=&#039;YearBuilt&#039;, y=&#039;SalePrice&#039;, data=train)\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7fb6a5211320&gt;\n\n\n在二十世纪的时候价格增长迅速。\n综合质量和售价有明显的相关性\nf, ax = plt.subplots(figsize=(12, 8))\nsns.lineplot(x=&#039;OverallQual&#039;, y=&#039;SalePrice&#039;, color=&#039;green&#039;,data=train)\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7fb6a3e6d8d0&gt;\n\n\n我们可以看到，随着房屋整体质量的提高，销售价格快速上涨，这是非常合理的。\n观察售价\nf, ax = plt.subplots(figsize=(10, 6))\nsns.distplot(train[&#039;SalePrice&#039;])\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7fb6a25cdba8&gt;\n\n\n大部分的房屋售价在 100000 到  200000 之间。\n构建预测模型\nX = train.drop(&#039;SalePrice&#039;, axis=1)\ny = np.ravel(np.array(train[[&#039;SalePrice&#039;]]))\nprint(y.shape)\ny\n(1460,)\n\n\n\n\n\narray([208500, 181500, 223500, ..., 266500, 142125, 147500])\n\n# Use train_test_split from sci-kit learn to segment our data into train and a local testset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n定义评估函数\n评估函数基于预测值的对数与观察到的销售价格的对数之间的均方根误差(RMSE)。\ndef rmse(y, y_pred):\n    return np.sqrt(mean_squared_error(np.log(y), np.log(y_pred)))\n随机森林\nrandom_forest = RandomForestRegressor(n_estimators=1200,\n                                      max_depth=15,\n                                      min_samples_split=5,\n                                      min_samples_leaf=5,\n                                      max_features=None,\n                                      random_state=42,\n                                      oob_score=True)\nkf = KFold(n_splits=5)\ny_pred = cross_val_score(random_forest, X, y, cv=kf)\ny_pred.mean()\n0.8500001566166802\n\nrandom_forest.fit(X, y)\nRandomForestRegressor(bootstrap=True, criterion=&#039;mse&#039;, max_depth=15,\n                      max_features=None, max_leaf_nodes=None,\n                      min_impurity_decrease=0.0, min_impurity_split=None,\n                      min_samples_leaf=5, min_samples_split=5,\n                      min_weight_fraction_leaf=0.0, n_estimators=1200,\n                      n_jobs=None, oob_score=True, random_state=42, verbose=0,\n                      warm_start=False)\n\nrf_pred = random_forest.predict(test)\nrf_pred\narray([126945.71699684, 153924.56961003, 182182.80294353, ...,\n       156066.28489667, 117296.65091637, 224995.13115853])\n\nXG Boost\nxg_boost = XGBRegressor(learning_rate=0.01,\n                        n_estimators=6000,\n                        max_depth=4, \n                        min_child_weight=1,\n                        gamma=0.6,\n                        subsample=0.7,\n                        colsample_bytree=0.2,\n                        objective=&#039;reg:linear&#039;,\n                        nthread=-1,\n                        scale_pos_weight=1,\n                        seed=27,\n                        reg_alpha=0.00006)\n \nkf = KFold(n_splits=5)\ny_pred = cross_val_score(xg_boost, X, y, cv=kf)\ny_pred.mean()\n[03:11:43] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[03:11:51] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[03:11:59] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[03:12:06] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n[03:12:13] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n\n\n\n\n\n0.8959027545454475\n\nxg_boost.fit(X, y)\n[03:12:21] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n\n\n\n\n\nXGBRegressor(base_score=0.5, booster=&#039;gbtree&#039;, colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=0.2, gamma=0.6,\n             importance_type=&#039;gain&#039;, learning_rate=0.01, max_delta_step=0,\n             max_depth=4, min_child_weight=1, missing=None, n_estimators=6000,\n             n_jobs=1, nthread=-1, objective=&#039;reg:linear&#039;, random_state=0,\n             reg_alpha=6e-05, reg_lambda=1, scale_pos_weight=1, seed=27,\n             silent=None, subsample=0.7, verbosity=1)\n\nxgb_pred = xg_boost.predict(test)\nxgb_pred\narray([127263.77 , 163056.02 , 193208.25 , ..., 173218.75 , 115712.914,\n       211616.88 ], dtype=float32)\n\nGradient Boost Regressor(GBM)\ng_boost = GradientBoostingRegressor(n_estimators=6000,\n                                    learning_rate=0.01,\n                                    max_depth=5,\n                                    max_features=&#039;sqrt&#039;,\n                                    min_samples_leaf=15,\n                                    min_samples_split=10,\n                                    loss=&#039;ls&#039;,\n                                    random_state=42\n                                    )\n \nkf = KFold(n_splits=5)\ny_pred = cross_val_score(g_boost, X, y, cv=kf)\ny_pred.mean()\n0.8905525972502479\n\ng_boost.fit(X, y)\nGradientBoostingRegressor(alpha=0.9, criterion=&#039;friedman_mse&#039;, init=None,\n                          learning_rate=0.01, loss=&#039;ls&#039;, max_depth=5,\n                          max_features=&#039;sqrt&#039;, max_leaf_nodes=None,\n                          min_impurity_decrease=0.0, min_impurity_split=None,\n                          min_samples_leaf=15, min_samples_split=10,\n                          min_weight_fraction_leaf=0.0, n_estimators=6000,\n                          n_iter_no_change=None, presort=&#039;auto&#039;,\n                          random_state=42, subsample=1.0, tol=0.0001,\n                          validation_fraction=0.1, verbose=0, warm_start=False)\n\ngbm_pred = g_boost.predict(test)\ngbm_pred\narray([125909.87154943, 163184.8652622 , 187872.72372976, ...,\n       176429.22616544, 119620.23912638, 211953.00041747])\n\nLight GBM\nlightgbm = LGBMRegressor(objective=&#039;regression&#039;,\n                         num_leaves=6,\n                         learning_rate=0.01,\n                         n_estimators=6400,\n                         verbose=-1,\n                         bagging_fraction=0.8,\n                         bagging_freq=4,\n                         bagging_seed=6,\n                         feature_fraction=0.2,\n                         feature_fraction_seed=7\n                         )\nkf = KFold(n_splits=5)\ny_pred = cross_val_score(lightgbm, X, y, cv=kf)\ny_pred.mean()\n0.8881867437856128\n\nlightgbm.fit(X,y)\nLGBMRegressor(bagging_fraction=0.8, bagging_freq=4, bagging_seed=6,\n              boosting_type=&#039;gbdt&#039;, class_weight=None, colsample_bytree=1.0,\n              feature_fraction=0.2, feature_fraction_seed=7,\n              importance_type=&#039;split&#039;, learning_rate=0.01, max_depth=-1,\n              min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n              n_estimators=6400, n_jobs=-1, num_leaves=6,\n              objective=&#039;regression&#039;, random_state=None, reg_alpha=0.0,\n              reg_lambda=0.0, silent=True, subsample=1.0,\n              subsample_for_bin=200000, subsample_freq=0, verbose=-1)\n\nlgb_pred = lightgbm.predict(test)\nlgb_pred\narray([124759.4703253 , 161206.70919126, 187680.444818  , ...,\n       168310.83365532, 123698.90457326, 206480.92047866])\n\nLogistic Regression\nlogreg = LogisticRegression()\nkf = KFold(n_splits=5)\ny_pred = cross_val_score(logreg, X, y, cv=kf)\ny_pred.mean()\nlogreg.fit(X, y)\n/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to &#039;lbfgs&#039; in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to &#039;auto&#039; in 0.22. Specify the multi_class option to silence this warning.\n  &quot;this warning.&quot;, FutureWarning)\n/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  &quot;the number of iterations.&quot;, ConvergenceWarning)\n\n\n\n\n\nLogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n                   multi_class=&#039;warn&#039;, n_jobs=None, penalty=&#039;l2&#039;,\n                   random_state=None, solver=&#039;warn&#039;, tol=0.0001, verbose=0,\n                   warm_start=False)\n\nround(logreg.score(X, y) * 100, 2)\n88.9\n\nlog_pred = logreg.predict(test)\nlog_pred\narray([135500, 128950, 175000, ..., 133900, 190000, 187500])\n\n模型的叠加\n叠加(也称为元集成)是一种模型集成技术，用于组合来自多个预测模型的信息，生成性能更好的新模型。在这个项目中，我们使用名为vecstack的python包，它可以帮助我们对前面导入的模型进行堆栈。它实际上非常容易使用，可以查看文档了解更多信息。vecstack\nmodels = [g_boost, xg_boost, lightgbm, random_forest]\nStrain, S_test = stacking(models,\n                          X_train,\n                          y_train,\n                          X_test,\n                          regression=True,\n                          mode=&#039;oof_pred_bag&#039;,\n                          metric=rmse,\n                          n_folds=5,\n                          random_state=25,\n                          verbose=2)\ntask:         [regression]\nmetric:       [rmse]\nmode:         [oof_pred_bag]\nn_models:     [4]\n\nmodel  0:     [GradientBoostingRegressor]\n    fold  0:  [0.12653004]\n    fold  1:  [0.13818165]\n    fold  2:  [0.10747644]\n    fold  3:  [0.14980732]\n    fold  4:  [0.11127270]\n    ----\n    MEAN:     [0.12665363] + [0.01595833]\n    FULL:     [0.12764756]\n\nmodel  1:     [XGBRegressor]\n[03:23:54] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n    fold  0:  [0.11631560]\n[03:24:00] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n    fold  1:  [0.14701253]\n[03:24:06] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n    fold  2:  [0.10450330]\n[03:24:12] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n    fold  3:  [0.14328067]\n[03:24:18] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n    fold  4:  [0.10632026]\n    ----\n    MEAN:     [0.12348647] + [0.01817552]\n    FULL:     [0.12481458]\n\nmodel  2:     [LGBMRegressor]\n    fold  0:  [0.12668239]\n    fold  1:  [0.14251415]\n    fold  2:  [0.11409004]\n    fold  3:  [0.15394461]\n    fold  4:  [0.11576550]\n    ----\n    MEAN:     [0.13059934] + [0.01545902]\n    FULL:     [0.13150292]\n\nmodel  3:     [RandomForestRegressor]\n    fold  0:  [0.13803357]\n    fold  1:  [0.16746496]\n    fold  2:  [0.13370269]\n    fold  3:  [0.17907099]\n    fold  4:  [0.13625091]\n    ----\n    MEAN:     [0.15090463] + [0.01867560]\n    FULL:     [0.15204350]\n\nStrain, S_test\n(array([[145154.57609501, 140247.640625  , 144708.92814448,\n         136304.80002144],\n        [441586.84575012, 453786.875     , 476049.8262998 ,\n         433615.5690085 ],\n        [205559.38156983, 199459.953125  , 204548.62741617,\n         189012.87710637],\n        ...,\n        [229773.83814053, 245324.03125   , 222988.34529258,\n         233726.54189435],\n        [ 78529.68615301,  81706.46875   ,  74919.86211206,\n          94651.91862458],\n        [126564.42955093, 118016.921875  , 131591.97464745,\n         134449.34870648]]),\n array([[156946.11019358, 162235.903125  , 156274.58204718,\n         174271.19166786],\n        [168719.74644755, 171368.26875   , 172660.15892698,\n         168988.4858204 ],\n        [165875.73697659, 167827.703125  , 166511.09786993,\n         144720.7621456 ],\n        ...,\n        [235105.18179731, 240780.803125  , 236012.18528746,\n         224310.44197611],\n        [311340.99357469, 306275.29375   , 305036.50098821,\n         319953.95931372],\n        [100400.26285948,  97430.8671875 ,  97576.05463032,\n         109458.70614352]]))\n\n# Initialize 2nd level model\nxgb_lev2 = XGBRegressor(learning_rate=0.1, \n                        n_estimators=500,\n                        max_depth=3,\n                        n_jobs=-1,\n                        random_state=17\n                       )\n \n# Fit the 2nd level model on the output of level 1\nxgb_lev2.fit(Strain, y_train)\n[03:25:26] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n\n\n\n\n\nXGBRegressor(base_score=0.5, booster=&#039;gbtree&#039;, colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=1, gamma=0,\n             importance_type=&#039;gain&#039;, learning_rate=0.1, max_delta_step=0,\n             max_depth=3, min_child_weight=1, missing=None, n_estimators=500,\n             n_jobs=-1, nthread=None, objective=&#039;reg:linear&#039;, random_state=17,\n             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n             silent=None, subsample=1, verbosity=1)\n\n# Make predictions on the localized test set\nstacked_pred = xgb_lev2.predict(S_test)\nprint(&quot;RMSE of Stacked Model: {}&quot;.format(rmse(y_test,stacked_pred)))\nRMSE of Stacked Model: 0.12290530277450722\n\ny1_pred_L1 = models[0].predict(test)\ny2_pred_L1 = models[1].predict(test)\ny3_pred_L1 = models[2].predict(test)\ny4_pred_L1 = models[3].predict(test)\nS_test_L1 = np.c_[y1_pred_L1, y2_pred_L1, y3_pred_L1, y4_pred_L1]\ntest_stacked_pred = xgb_lev2.predict(S_test_L1)\n# Save the predictions in form of a dataframe\nsubmission = pd.DataFrame()\n \nsubmission[&#039;Id&#039;] = np.array(test.index)\nsubmission[&#039;SalePrice&#039;] = test_stacked_pred\nsubmission.to_csv(&#039;/submissionV2.csv&#039;, index=False)\n混合较好得分的 submission\n因为不知道最终的测试集合的正真数据是什么，只能一遍一遍提交去蒙，看到别人的方法是混合他人较好的提交去验证，尝试下看看。\nsubmission_v1 = pd.read_csv(&#039;/House_price_submission_v44.csv&#039;)\nsubmission_v2 = pd.read_csv(&#039;/submissionV19.csv&#039;)\nsubmission_v3 = pd.read_csv(&#039;/blended_submission.csv&#039;)\nfinal_blend = 0.5*submission_v1.SalePrice.values + 0.2*submission_v2.SalePrice.values + 0.3*submission_v3.SalePrice.values\n \nblended_submission = pd.DataFrame()\n \nblended_submission[&#039;Id&#039;] = submission_v1.Id.values\nblended_submission[&#039;SalePrice&#039;] = final_blend\nblended_submission.to_csv(&#039;/submissionV20.csv&#039;, index=False)\nblended_submission\n\n呃，就这样吧，最终和前十名差不到0.004。\n用 Tensorfolw 试试\nimport math\nimport tensorflow as tf\nfrom tensorflow.python.data import Dataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\ntf.logging.set_verbosity(tf.logging.ERROR)\ncorrelation_dataframe = train.copy()\nsaleprice_corr = correlation_dataframe.corr()[&#039;SalePrice&#039;]\nsaleprice_corr = saleprice_corr[saleprice_corr &gt; 0]\ncorr_feature = saleprice_corr.index\ncorr_feature\nIndex([&#039;MSZoning&#039;, &#039;LotFrontage&#039;, &#039;LotArea&#039;, &#039;Utilities&#039;, &#039;BldgType&#039;,\n       &#039;OverallQual&#039;, &#039;YearBuilt&#039;, &#039;YearRemodAdd&#039;, &#039;MasVnrType&#039;, &#039;MasVnrArea&#039;,\n       &#039;ExterQual&#039;, &#039;ExterCond&#039;, &#039;BsmtQual&#039;, &#039;BsmtCond&#039;, &#039;BsmtExposure&#039;,\n       &#039;BsmtFinType1&#039;, &#039;BsmtFinSF1&#039;, &#039;BsmtUnfSF&#039;, &#039;TotalBsmtSF&#039;, &#039;Heating&#039;,\n       &#039;HeatingQC&#039;, &#039;Electrical&#039;, &#039;1stFlrSF&#039;, &#039;2ndFlrSF&#039;, &#039;GrLivArea&#039;,\n       &#039;BsmtFullBath&#039;, &#039;FullBath&#039;, &#039;HalfBath&#039;, &#039;BedroomAbvGr&#039;, &#039;KitchenQual&#039;,\n       &#039;TotRmsAbvGrd&#039;, &#039;Functional&#039;, &#039;Fireplaces&#039;, &#039;FireplaceQu&#039;, &#039;GarageType&#039;,\n       &#039;GarageYrBlt&#039;, &#039;GarageFinish&#039;, &#039;GarageCars&#039;, &#039;GarageArea&#039;, &#039;GarageQual&#039;,\n       &#039;GarageCond&#039;, &#039;PavedDrive&#039;, &#039;WoodDeckSF&#039;, &#039;OpenPorchSF&#039;, &#039;3SsnPorch&#039;,\n       &#039;ScreenPorch&#039;, &#039;PoolArea&#039;, &#039;MoSold&#039;, &#039;SalePrice&#039;],\n      dtype=&#039;object&#039;)\n\ndef preprocess_feature(df, corr_feature):\n    newdf = pd.DataFrame()\n    for feature in corr_feature:\n        newdf[feature] = df[feature]\n    return newdf\nX = preprocess_feature(train, corr_feature).drop(&#039;SalePrice&#039;, axis=1)\ny = np.ravel(np.array(train[[&#039;SalePrice&#039;]]))\nX.describe()\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      MSZoning\n      LotFrontage\n      LotArea\n      Utilities\n      BldgType\n      OverallQual\n      YearBuilt\n      YearRemodAdd\n      MasVnrType\n      MasVnrArea\n      ExterQual\n      ExterCond\n      BsmtQual\n      BsmtCond\n      BsmtExposure\n      BsmtFinType1\n      BsmtFinSF1\n      BsmtUnfSF\n      TotalBsmtSF\n      Heating\n      HeatingQC\n      Electrical\n      1stFlrSF\n      2ndFlrSF\n      GrLivArea\n      BsmtFullBath\n      FullBath\n      HalfBath\n      BedroomAbvGr\n      KitchenQual\n      TotRmsAbvGrd\n      Functional\n      Fireplaces\n      FireplaceQu\n      GarageType\n      GarageYrBlt\n      GarageFinish\n      GarageCars\n      GarageArea\n      GarageQual\n      GarageCond\n      PavedDrive\n      WoodDeckSF\n      OpenPorchSF\n      3SsnPorch\n      ScreenPorch\n      PoolArea\n      MoSold\n    \n  \n  \n    \n      count\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.00000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n      1460.000000\n    \n    \n      mean\n      2.825342\n      69.863699\n      10516.828082\n      3.998630\n      4.334247\n      6.099315\n      1971.267808\n      1984.865753\n      2.552740\n      103.117123\n      3.39589\n      3.083562\n      4.565068\n      4.010959\n      2.656164\n      4.571233\n      443.639726\n      567.240411\n      1057.429452\n      4.963699\n      4.145205\n      4.889726\n      1162.626712\n      346.992466\n      1515.463699\n      0.425342\n      1.565068\n      0.382877\n      2.866438\n      3.511644\n      6.517808\n      7.841781\n      0.613014\n      2.825342\n      4.791781\n      1978.589041\n      2.771233\n      1.767123\n      472.980137\n      3.976712\n      3.975342\n      2.856164\n      94.244521\n      46.660274\n      3.409589\n      15.060959\n      2.758904\n      6.321918\n    \n    \n      std\n      1.020174\n      22.027677\n      9981.264932\n      0.052342\n      1.555218\n      1.382997\n      30.202904\n      20.645407\n      1.046204\n      180.731373\n      0.57428\n      0.351054\n      0.678071\n      0.284178\n      1.039123\n      2.070649\n      456.098091\n      441.866955\n      438.705324\n      0.295124\n      0.959501\n      0.394658\n      386.587738\n      436.528436\n      525.480383\n      0.518911\n      0.550916\n      0.502885\n      0.815778\n      0.663760\n      1.625393\n      0.667698\n      0.644666\n      1.810877\n      1.759864\n      23.997022\n      0.811835\n      0.747315\n      213.804841\n      0.241665\n      0.232860\n      0.496592\n      125.338794\n      66.256028\n      29.317331\n      55.757415\n      40.177307\n      2.703626\n    \n    \n      min\n      0.000000\n      21.000000\n      1300.000000\n      2.000000\n      0.000000\n      1.000000\n      1872.000000\n      1950.000000\n      1.000000\n      0.000000\n      2.00000\n      1.000000\n      3.000000\n      2.000000\n      2.000000\n      2.000000\n      0.000000\n      0.000000\n      0.000000\n      1.000000\n      1.000000\n      1.000000\n      334.000000\n      0.000000\n      334.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      2.000000\n      2.000000\n      2.000000\n      0.000000\n      1.000000\n      2.000000\n      1900.000000\n      2.000000\n      0.000000\n      0.000000\n      2.000000\n      2.000000\n      1.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      1.000000\n    \n    \n      25%\n      3.000000\n      60.000000\n      7553.500000\n      4.000000\n      5.000000\n      5.000000\n      1954.000000\n      1967.000000\n      2.000000\n      0.000000\n      3.00000\n      3.000000\n      4.000000\n      4.000000\n      2.000000\n      2.000000\n      0.000000\n      223.000000\n      795.750000\n      5.000000\n      3.000000\n      5.000000\n      882.000000\n      0.000000\n      1129.500000\n      0.000000\n      1.000000\n      0.000000\n      2.000000\n      3.000000\n      5.000000\n      8.000000\n      0.000000\n      1.000000\n      2.000000\n      1962.000000\n      2.000000\n      1.000000\n      334.500000\n      4.000000\n      4.000000\n      3.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      0.000000\n      5.000000\n    \n    \n      50%\n      3.000000\n      69.000000\n      9478.500000\n      4.000000\n      5.000000\n      6.000000\n      1973.000000\n      1994.000000\n      2.000000\n      0.000000\n      3.00000\n      3.000000\n      5.000000\n      4.000000\n      2.000000\n      5.000000\n      383.500000\n      477.500000\n      991.500000\n      5.000000\n      5.000000\n      5.000000\n      1087.000000\n      0.000000\n      1464.000000\n      0.000000\n      2.000000\n      0.000000\n      3.000000\n      3.000000\n      6.000000\n      8.000000\n      1.000000\n      3.000000\n      6.000000\n      1980.000000\n      3.000000\n      2.000000\n      480.000000\n      4.000000\n      4.000000\n      3.000000\n      0.000000\n      25.000000\n      0.000000\n      0.000000\n      0.000000\n      6.000000\n    \n    \n      75%\n      3.000000\n      79.000000\n      11601.500000\n      4.000000\n      5.000000\n      7.000000\n      2000.000000\n      2004.000000\n      4.000000\n      164.250000\n      4.00000\n      3.000000\n      5.000000\n      4.000000\n      3.000000\n      7.000000\n      712.250000\n      808.000000\n      1298.250000\n      5.000000\n      5.000000\n      5.000000\n      1391.250000\n      728.000000\n      1776.750000\n      1.000000\n      2.000000\n      1.000000\n      3.000000\n      4.000000\n      7.000000\n      8.000000\n      1.000000\n      5.000000\n      6.000000\n      2001.000000\n      3.000000\n      2.000000\n      576.000000\n      4.000000\n      4.000000\n      3.000000\n      168.000000\n      68.000000\n      0.000000\n      0.000000\n      0.000000\n      8.000000\n    \n    \n      max\n      6.000000\n      313.000000\n      215245.000000\n      4.000000\n      5.000000\n      10.000000\n      2010.000000\n      2010.000000\n      5.000000\n      1600.000000\n      5.00000\n      5.000000\n      6.000000\n      5.000000\n      5.000000\n      7.000000\n      5644.000000\n      2336.000000\n      6110.000000\n      6.000000\n      5.000000\n      5.000000\n      4692.000000\n      2065.000000\n      5642.000000\n      3.000000\n      3.000000\n      2.000000\n      8.000000\n      5.000000\n      14.000000\n      8.000000\n      3.000000\n      6.000000\n      7.000000\n      2010.000000\n      4.000000\n      4.000000\n      1418.000000\n      6.000000\n      6.000000\n      3.000000\n      857.000000\n      547.000000\n      508.000000\n      480.000000\n      738.000000\n      12.000000\n    \n  \n\n\ndef my_input_fn(features, targets, batch_size=1, shuffle=True, num_epochs=None):\n    features = {key: np.array(value) for key, value in dict(features).items()}\n    \n    ds = Dataset.from_tensor_slices((features, targets))\n    ds = ds.batch(batch_size).repeat(num_epochs)\n    \n    if shuffle:\n        ds = ds.shuffle(10000)\n    \n    features, labels = ds.make_one_shot_iterator().get_next()\n    return features, labels\ndef construct_feature_columns(input_features):\n    &quot;&quot;&quot;构造TensorFlow特征列\n        参数:\n            input_features:要使用的数字输入特性的名称。\n        返回:\n            一个 feature columns 集合\n    &quot;&quot;&quot;\n    return set([tf.feature_column.numeric_column(my_feature) \n                for my_feature in input_features])\ndef train_model(\n    learning_rate,\n    strps,\n    batch_size,\n    training_examples,\n    training_targets,\n    validation_examples,\n    validation_targets):\n    &quot;&quot;&quot;训练多元特征的线性回归模型\n        除训练外，此功能还打印训练进度信息，\n        以及随着时间的推移而失去的训练和验证。\n    参数:\n        learning_rate:一个float，表示学习率\n        steps:一个非零的int，训练步骤的总数。训练步骤\n            由使用单个批处理的向前和向后传递组成。\n        batch_size:一个非零的int\n        training_example: DataFrame 包含一个或多个列\n        &#039;  california_housing_dataframe &#039;作为训练的输入feature\n        training_targets:一个&#039; DataFrame &#039;，它只包含一列\n        &#039; california_housing_dataframe &#039;作为训练的目标。\n        validation_example: &#039; DataFrame &#039;包含一个或多个列\n        &#039; california_housing_dataframe &#039;作为验证的输入feature\n        validation_targets: &#039; DataFrame &#039;，仅包含来自其中的一列\n        &#039; california_housing_dataframe &#039;作为验证的目标。\n    返回:\n        在训练数据上训练的“线性回归器”对象\n    &quot;&quot;&quot;\n    periods = 10\n    steps_per_period = strps / periods\n    \n    # 创建一个线性回归对象\n    my_optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n    my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)\n    linear_regressor = tf.estimator.LinearRegressor(\n        feature_columns=construct_feature_columns(training_examples),\n        optimizer=my_optimizer\n    )\n    \n    # 创建输入函数\n    training_input_fn = lambda: my_input_fn(\n        training_examples,\n        training_targets,\n        batch_size=batch_size)\n    \n    predict_training_input_fn = lambda: my_input_fn(\n      training_examples, \n      training_targets, \n      num_epochs=1, \n      shuffle=False)\n    \n    predict_validation_input_fn = lambda: my_input_fn(\n        validation_examples, \n        validation_targets,\n        num_epochs=1,\n        shuffle=False)\n    \n    #训练模型，但要在循环中进行，这样我们才能定期评估\n    #损失指标\n    print(&quot;Training model...&quot;)\n    print(&quot;RMSE (on training data):&quot;)\n    training_rmse = []\n    validation_rmse = []\n    for period in range (0, periods):\n      # Train the model, starting from the prior state.\n      linear_regressor.train(\n          input_fn=training_input_fn,\n          steps=steps_per_period,\n      )\n      # Take a break and compute predictions.\n      training_predictions = linear_regressor.predict(input_fn=predict_training_input_fn)\n      training_predictions = np.array([item[&#039;predictions&#039;][0] for item in training_predictions])\n      \n      validation_predictions = linear_regressor.predict(input_fn=predict_validation_input_fn)\n      validation_predictions = np.array([item[&#039;predictions&#039;][0] for item in validation_predictions])\n      \n \n      # Compute training and validation loss.\n      training_root_mean_squared_error = math.sqrt(\n          metrics.mean_squared_error(training_predictions, training_targets))\n      validation_root_mean_squared_error = math.sqrt(\n          metrics.mean_squared_error(validation_predictions, validation_targets))\n      # Occasionally print the current loss.\n      print(&quot;  period %02d : %0.2f&quot; % (period, training_root_mean_squared_error))\n      # Add the loss metrics from this period to our list.\n      training_rmse.append(training_root_mean_squared_error)\n      validation_rmse.append(validation_root_mean_squared_error)\n    print(&quot;Model training finished.&quot;)\n \n    # Output a graph of loss metrics over periods.\n    plt.ylabel(&quot;RMSE&quot;)\n    plt.xlabel(&quot;Periods&quot;)\n    plt.title(&quot;Root Mean Squared Error vs. Periods&quot;)\n    plt.tight_layout()\n    plt.plot(training_rmse, label=&quot;training&quot;)\n    plt.plot(validation_rmse, label=&quot;validation&quot;)\n    plt.legend()\n \n    return linear_regressor\nlinear_regressor = train_model(\n    learning_rate=0.5,\n    strps=200,\n    batch_size=5,\n    training_examples=X_train,\n    training_targets=y_train,\n    validation_examples=X_test,\n    validation_targets=y_test)\n \nTraining model...\nRMSE (on training data):\n  period 00 : 115236.06\n  period 01 : 135729.29\n  period 02 : 94007.13\n  period 03 : 94940.56\n  period 04 : 78776.40\n  period 05 : 73407.86\n  period 06 : 79100.02\n  period 07 : 76995.32\n  period 08 : 94657.55\n  period 09 : 55721.83\nModel training finished.\n\n\nlinear_regressor2 = train_model(\n    learning_rate=0.25,\n    strps=500,\n    batch_size=5,\n    training_examples=X_train,\n    training_targets=y_train,\n    validation_examples=X_test,\n    validation_targets=y_test)\nTraining model...\nRMSE (on training data):\n  period 00 : 126663.18\n  period 01 : 89900.76\n  period 02 : 67300.53\n  period 03 : 73597.13\n  period 04 : 59429.95\n  period 05 : 60645.34\n  period 06 : 57056.42\n  period 07 : 55974.51\n  period 08 : 59490.64\n  period 09 : 59963.44\nModel training finished.\n\n\npredict_test_input_fn = lambda: my_input_fn(\n      test, \n      test, \n      num_epochs=1, \n      shuffle=False)\n \ntest_predictions = linear_regressor.predict(input_fn=predict_test_input_fn)\ntest_predictions = np.array([item[&#039;predictions&#039;][0] for item in test_predictions])\ntest_predictions\narray([152403.8 , 178793.02, 186845.23, ..., 203018.39, 142496.02,\n       197809.12], dtype=float32)\n\n就先这样吧。"},"Kaggle/Titanic_Machine_Learning_from_Disaster.2":{"slug":"Kaggle/Titanic_Machine_Learning_from_Disaster.2","filePath":"Kaggle/Titanic_Machine_Learning_from_Disaster.2.md","title":"Titanic Machine Learning from Disaster","links":[],"tags":["Kaggle"],"content":"Kaggle Competition 的练习\n泰坦尼克号：从灾难中学习机器\n# 数据分析库\nimport pandas as pd\nimport numpy as np\nimport random\n \n# 数据可视化\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n \n# 机器学习库\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n \npd.options.display.max_rows = 10  # 最大显示行数\npd.options.display.float_format = &#039;{:.5f}&#039;.format  # 精确度 保留一位小数\n加载数据\n首先加载浏览数据\ntrain_df = pd.read_csv(&#039;/train.csv&#039;)\ntest_df = pd.read_csv(&#039;/test.csv&#039;)\ntrain_df.head()\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      PassengerId\n      Survived\n      Pclass\n      Name\n      Sex\n      Age\n      SibSp\n      Parch\n      Ticket\n      Fare\n      Cabin\n      Embarked\n    \n  \n  \n    \n      0\n      1\n      0\n      3\n      Braund, Mr. Owen Harris\n      male\n      22.00000\n      1\n      0\n      A/5 21171\n      7.25000\n      NaN\n      S\n    \n    \n      1\n      2\n      1\n      1\n      Cumings, Mrs. John Bradley (Florence Briggs Th...\n      female\n      38.00000\n      1\n      0\n      PC 17599\n      71.28330\n      C85\n      C\n    \n    \n      2\n      3\n      1\n      3\n      Heikkinen, Miss. Laina\n      female\n      26.00000\n      0\n      0\n      STON/O2. 3101282\n      7.92500\n      NaN\n      S\n    \n    \n      3\n      4\n      1\n      1\n      Futrelle, Mrs. Jacques Heath (Lily May Peel)\n      female\n      35.00000\n      1\n      0\n      113803\n      53.10000\n      C123\n      S\n    \n    \n      4\n      5\n      0\n      3\n      Allen, Mr. William Henry\n      male\n      35.00000\n      0\n      0\n      373450\n      8.05000\n      NaN\n      S\n    \n  \n\n\ntest_df.head()\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      PassengerId\n      Pclass\n      Name\n      Sex\n      Age\n      SibSp\n      Parch\n      Ticket\n      Fare\n      Cabin\n      Embarked\n    \n  \n  \n    \n      0\n      892\n      3\n      Kelly, Mr. James\n      male\n      34.50000\n      0\n      0\n      330911\n      7.82920\n      NaN\n      Q\n    \n    \n      1\n      893\n      3\n      Wilkes, Mrs. James (Ellen Needs)\n      female\n      47.00000\n      1\n      0\n      363272\n      7.00000\n      NaN\n      S\n    \n    \n      2\n      894\n      2\n      Myles, Mr. Thomas Francis\n      male\n      62.00000\n      0\n      0\n      240276\n      9.68750\n      NaN\n      Q\n    \n    \n      3\n      895\n      3\n      Wirz, Mr. Albert\n      male\n      27.00000\n      0\n      0\n      315154\n      8.66250\n      NaN\n      S\n    \n    \n      4\n      896\n      3\n      Hirvonen, Mrs. Alexander (Helga E Lindqvist)\n      female\n      22.00000\n      1\n      1\n      3101298\n      12.28750\n      NaN\n      S\n    \n  \n\n\ntrain_df.describe()\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      PassengerId\n      Survived\n      Pclass\n      Age\n      SibSp\n      Parch\n      Fare\n    \n  \n  \n    \n      count\n      891.00000\n      891.00000\n      891.00000\n      714.00000\n      891.00000\n      891.00000\n      891.00000\n    \n    \n      mean\n      446.00000\n      0.38384\n      2.30864\n      29.69912\n      0.52301\n      0.38159\n      32.20421\n    \n    \n      std\n      257.35384\n      0.48659\n      0.83607\n      14.52650\n      1.10274\n      0.80606\n      49.69343\n    \n    \n      min\n      1.00000\n      0.00000\n      1.00000\n      0.42000\n      0.00000\n      0.00000\n      0.00000\n    \n    \n      25%\n      223.50000\n      0.00000\n      2.00000\n      20.12500\n      0.00000\n      0.00000\n      7.91040\n    \n    \n      50%\n      446.00000\n      0.00000\n      3.00000\n      28.00000\n      0.00000\n      0.00000\n      14.45420\n    \n    \n      75%\n      668.50000\n      1.00000\n      3.00000\n      38.00000\n      1.00000\n      0.00000\n      31.00000\n    \n    \n      max\n      891.00000\n      1.00000\n      3.00000\n      80.00000\n      8.00000\n      6.00000\n      512.32920\n    \n  \n\n\ntest_df.describe()\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      PassengerId\n      Pclass\n      Age\n      SibSp\n      Parch\n      Fare\n    \n  \n  \n    \n      count\n      418.00000\n      418.00000\n      332.00000\n      418.00000\n      418.00000\n      417.00000\n    \n    \n      mean\n      1100.50000\n      2.26555\n      30.27259\n      0.44737\n      0.39234\n      35.62719\n    \n    \n      std\n      120.81046\n      0.84184\n      14.18121\n      0.89676\n      0.98143\n      55.90758\n    \n    \n      min\n      892.00000\n      1.00000\n      0.17000\n      0.00000\n      0.00000\n      0.00000\n    \n    \n      25%\n      996.25000\n      1.00000\n      21.00000\n      0.00000\n      0.00000\n      7.89580\n    \n    \n      50%\n      1100.50000\n      3.00000\n      27.00000\n      0.00000\n      0.00000\n      14.45420\n    \n    \n      75%\n      1204.75000\n      3.00000\n      39.00000\n      1.00000\n      0.00000\n      31.50000\n    \n    \n      max\n      1309.00000\n      3.00000\n      76.00000\n      8.00000\n      9.00000\n      512.32920\n    \n  \n\n\n训练集有 PassengerId\tSurvived\tPclass\tName\tSex\tAge\tSibSp\tParch\tTicket\tFare\tCabin\tEmbarked 共十二列数据，测试集中没有的Survived就是我们要预测的值。\n检查数据\n检查数据类型和缺失情况。\ntrain_df.info()\nprint(&#039;_&#039;*40)\ntest_df.info()\n&lt;class &#039;pandas.core.frame.DataFrame&#039;&gt;\nRangeIndex: 891 entries, 0 to 890\nData columns (total 12 columns):\nPassengerId    891 non-null int64\nSurvived       891 non-null int64\nPclass         891 non-null int64\nName           891 non-null object\nSex            891 non-null object\nAge            714 non-null float64\nSibSp          891 non-null int64\nParch          891 non-null int64\nTicket         891 non-null object\nFare           891 non-null float64\nCabin          204 non-null object\nEmbarked       889 non-null object\ndtypes: float64(2), int64(5), object(5)\nmemory usage: 83.6+ KB\n________________________________________\n&lt;class &#039;pandas.core.frame.DataFrame&#039;&gt;\nRangeIndex: 418 entries, 0 to 417\nData columns (total 11 columns):\nPassengerId    418 non-null int64\nPclass         418 non-null int64\nName           418 non-null object\nSex            418 non-null object\nAge            332 non-null float64\nSibSp          418 non-null int64\nParch          418 non-null int64\nTicket         418 non-null object\nFare           417 non-null float64\nCabin          91 non-null object\nEmbarked       418 non-null object\ndtypes: float64(2), int64(4), object(5)\nmemory usage: 36.0+ KB\n\n观察发现，Age Cabin Embarked 存在缺失，并且数据类型既有数字也有字符串。测试集中 Fare 缺失了一个。\n观察特征的分布\nName 是唯一的共 891\nSex 有两种，male 占 64.7%（top=male, freq/count=64.7%）\nTicket 不同的种类比较多\nCabin 有许多乘客在同一个 cabin\nEmbarked 有三种大多数是 S\n\ntrain_df.describe(include=[&quot;O&quot;])\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      Name\n      Sex\n      Ticket\n      Cabin\n      Embarked\n    \n  \n  \n    \n      count\n      891\n      891\n      891\n      204\n      889\n    \n    \n      unique\n      891\n      2\n      681\n      147\n      3\n    \n    \n      top\n      Renouf, Mr. Peter Henry\n      male\n      1601\n      C23 C25 C27\n      S\n    \n    \n      freq\n      1\n      577\n      7\n      4\n      644\n    \n  \n\n\n数据预处理\n在训练集中存在着缺失值和错误值，所以要筛选出有价值的特征，填充缺失的数据。\n删除\n首先删除没有价值或价值比较低的特征。\n根据我们的假设和决定，我们先放弃 Cabin 和 Ticket 。\n我们对训练和测试数据集执行相同的操作以保持其一致。\nprint(&quot;Before&quot;, train_df.shape, test_df.shape)\n \ntrain_df = train_df.drop([&quot;Ticket&quot;, &quot;Cabin&quot;], axis=1)\ntest_df = test_df.drop([&quot;Ticket&quot;, &quot;Cabin&quot;], axis=1)\ncombine = [train_df, test_df]\n \n&quot;After&quot;, train_df.shape, test_df.shape, combine[0].shape, combine[1].shape\nBefore (891, 12) (418, 11)\n\n\n\n\n\n(&#039;After&#039;, (891, 10), (418, 9), (891, 10), (418, 9))\n\nName\n首先观察到 Name 都是唯一的并且在 Name 中间存在称谓，提取出名字中间的称谓。\nfor dataset in combine:\n    dataset[&quot;Title&quot;] = dataset.Name.str.extract(&#039; ([A-Za-z]+)\\.&#039;, expand=False)\n    \npd.crosstab(train_df[&quot;Title&quot;], train_df[&quot;Sex&quot;])\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      Sex\n      female\n      male\n    \n    \n      Title\n      \n      \n    \n  \n  \n    \n      Capt\n      0\n      1\n    \n    \n      Col\n      0\n      2\n    \n    \n      Countess\n      1\n      0\n    \n    \n      Don\n      0\n      1\n    \n    \n      Dr\n      1\n      6\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      Mr\n      0\n      517\n    \n    \n      Mrs\n      125\n      0\n    \n    \n      Ms\n      1\n      0\n    \n    \n      Rev\n      0\n      6\n    \n    \n      Sir\n      0\n      1\n    \n  \n\n17 rows × 2 columns\n\n把称呼替换为更为常见的，不常见的定义为 Rare\nfor dataset in combine:\n    dataset[&quot;Title&quot;] = dataset[&quot;Title&quot;].replace([&quot;Lady&quot;, &quot;Countess&quot;, &quot;Capt&quot;, &quot;Col&quot;, \\\n                                                 &quot;Don&quot;, &quot;Dr&quot;, &quot;Major&quot;, &quot;Rev&quot;, &quot;Sir&quot;, \\\n                                                 &quot;Jonkheer&quot;, &quot;Dona&quot;], &quot;Rare&quot;)\n    dataset[&quot;Title&quot;] = dataset[&quot;Title&quot;].replace(&quot;Mlle&quot;, &quot;Miss&quot;)\n    dataset[&quot;Title&quot;] = dataset[&quot;Title&quot;].replace(&quot;Ms&quot;, &quot;Miss&quot;)\n    dataset[&quot;Title&quot;] = dataset[&quot;Title&quot;].replace(&quot;Mme&quot;, &quot;Mrs&quot;)\n \ntrain_df[[&quot;Title&quot;, &quot;Survived&quot;]].groupby([&quot;Title&quot;], as_index=False).mean()\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      Title\n      Survived\n    \n  \n  \n    \n      0\n      Master\n      0.57500\n    \n    \n      1\n      Miss\n      0.70270\n    \n    \n      2\n      Mr\n      0.15667\n    \n    \n      3\n      Mrs\n      0.79365\n    \n    \n      4\n      Rare\n      0.34783\n    \n  \n\n\n把 Titles 转换为数字\ntitle_mapping = {&quot;Mr&quot;: 1, &quot;Miss&quot;: 2, &quot;Mrs&quot;: 3, &quot;Master&quot;: 4, &quot;Rare&quot;: 5}\nfor dataset in combine:\n    dataset[&quot;Title&quot;] = dataset[&quot;Title&quot;].map(title_mapping)\n    dataset[&quot;Title&quot;] = dataset[&quot;Title&quot;].fillna(0) # 缺失值补0\n \ntrain_df.head()\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      PassengerId\n      Survived\n      Pclass\n      Name\n      Sex\n      Age\n      SibSp\n      Parch\n      Fare\n      Embarked\n      Title\n    \n  \n  \n    \n      0\n      1\n      0\n      3\n      Braund, Mr. Owen Harris\n      male\n      22.00000\n      1\n      0\n      7.25000\n      S\n      1\n    \n    \n      1\n      2\n      1\n      1\n      Cumings, Mrs. John Bradley (Florence Briggs Th...\n      female\n      38.00000\n      1\n      0\n      71.28330\n      C\n      3\n    \n    \n      2\n      3\n      1\n      3\n      Heikkinen, Miss. Laina\n      female\n      26.00000\n      0\n      0\n      7.92500\n      S\n      2\n    \n    \n      3\n      4\n      1\n      1\n      Futrelle, Mrs. Jacques Heath (Lily May Peel)\n      female\n      35.00000\n      1\n      0\n      53.10000\n      S\n      3\n    \n    \n      4\n      5\n      0\n      3\n      Allen, Mr. William Henry\n      male\n      35.00000\n      0\n      0\n      8.05000\n      S\n      1\n    \n  \n\n\n现在可以删除 Name 和 PassengerId了\ntrain_df = train_df.drop([&quot;Name&quot;, &quot;PassengerId&quot;], axis=1)\ntest_df = test_df.drop([&quot;Name&quot;], axis=1)\ncombine = [train_df, test_df]\ntrain_df.shape, test_df.shape\n((891, 9), (418, 9))\n\nSex\n将包含字符串的特征转换为数值。这是大多数模型算法所必需的。这样做也将有助于我们实现功能完成目标。\n让我们首先将 Sex 特征转换为一个新的 feature，其中 female = 1，male = 0。\nfor dataset in combine:\n    dataset[&quot;Sex&quot;] = dataset[&quot;Sex&quot;].map({&quot;female&quot;: 1, &quot;male&quot;: 0}).astype(int)\n \ntrain_df.head()\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      Survived\n      Pclass\n      Sex\n      Age\n      SibSp\n      Parch\n      Fare\n      Embarked\n      Title\n    \n  \n  \n    \n      0\n      0\n      3\n      0\n      22.00000\n      1\n      0\n      7.25000\n      S\n      1\n    \n    \n      1\n      1\n      1\n      1\n      38.00000\n      1\n      0\n      71.28330\n      C\n      3\n    \n    \n      2\n      1\n      3\n      1\n      26.00000\n      0\n      0\n      7.92500\n      S\n      2\n    \n    \n      3\n      1\n      1\n      1\n      35.00000\n      1\n      0\n      53.10000\n      S\n      3\n    \n    \n      4\n      0\n      3\n      0\n      35.00000\n      0\n      0\n      8.05000\n      S\n      1\n    \n  \n\n\nAge\n现在处理缺少值或空值的问题。\n我们首先处理 Age 。\n可以考虑三种方法来完成特征的填充。\n\n\n一种简单的方法是在均值的标准差之间生成随机数。\n\n\n使用其他相关特征猜测缺失值。在这个例子中，我们注意到 Age，Gender 和 Pclass 之间的相关性。用猜年龄值中位值跨越套 Pclass 和性别特征组合年龄。因此，Pclass 的中位数年龄 = 1且性别 = 0，Pclass = 1 且性别 = 1，依此类推…\n\n\n结合方法1和2因此，不是基于中位数来猜测年龄值，而是根据Pclass和Gender组合的集合使用均值和标准差之间的随机数。\n\n\n方法1和3将随机噪声引入我们的模型。多次执行的结果可能会有所不同。我们选择方法2。\ngrid = sns.FacetGrid(train_df, row=&quot;Pclass&quot;, col=&quot;Sex&quot;, size=2.2, aspect=1.6)\ngrid.map(plt.hist, &quot;Age&quot;, alpha=0.5, bins=20)\ngrid.add_legend()\n/usr/local/lib/python3.6/dist-packages/seaborn/axisgrid.py:230: UserWarning: The `size` paramter has been renamed to `height`; please update your code.\n  warnings.warn(msg, UserWarning)\n\n\n\n\n\n&lt;seaborn.axisgrid.FacetGrid at 0x7f3b28420a90&gt;\n\n\n首先准备一个空数组，猜测 Age 和 Pclass × Geender 有关系\nguess_ages = np.zeros((2, 3))\nguess_ages\narray([[0., 0., 0.],\n       [0., 0., 0.]])\n\n现在我们迭代 Sex(0,1) 和 Pclass(1,2,3) 来猜测这六种组合的 Age。\nfor dataset in combine:\n    for i in range(0, 2):\n        for j in range(0, 3):\n            guess_df = dataset[(dataset[&quot;Sex&quot;] == i) &amp; (dataset[&quot;Pclass&quot;] == j+1)][&quot;Age&quot;].dropna()\n            age_guess = guess_df.median()\n            guess_ages[i, j] = int(age_guess/0.5 + 0.5) * 0.5\n            \n    for i in range(0, 2):\n        for j in range(0, 3):\n            dataset.loc[(dataset.Age.isnull()) &amp; (dataset.Sex == i) &amp; (dataset.Pclass == j+1), &quot;Age&quot;] = guess_ages[i, j]\n    \n    dataset[&quot;Age&quot;] = dataset[&quot;Age&quot;].astype(int)\n    \ntrain_df.head()\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      Survived\n      Pclass\n      Sex\n      Age\n      SibSp\n      Parch\n      Fare\n      Embarked\n      Title\n    \n  \n  \n    \n      0\n      0\n      3\n      0\n      22\n      1\n      0\n      7.25000\n      S\n      1\n    \n    \n      1\n      1\n      1\n      1\n      38\n      1\n      0\n      71.28330\n      C\n      3\n    \n    \n      2\n      1\n      3\n      1\n      26\n      0\n      0\n      7.92500\n      S\n      2\n    \n    \n      3\n      1\n      1\n      1\n      35\n      1\n      0\n      53.10000\n      S\n      3\n    \n    \n      4\n      0\n      3\n      0\n      35\n      0\n      0\n      8.05000\n      S\n      1\n    \n  \n\n\n让我们创建 AgeBand，并确定与存活的相关性。\n# 把Age分为5箱\ntrain_df[&quot;AgeBand&quot;] = pd.cut(train_df[&quot;Age&quot;], 5)\ntrain_df[[&quot;AgeBand&quot;, &quot;Survived&quot;]].groupby([&quot;AgeBand&quot;], as_index=False).mean().sort_values(by=&quot;AgeBand&quot;, ascending=True)\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      AgeBand\n      Survived\n    \n  \n  \n    \n      0\n      (-0.08, 16.0]\n      0.55000\n    \n    \n      1\n      (16.0, 32.0]\n      0.33737\n    \n    \n      2\n      (32.0, 48.0]\n      0.41204\n    \n    \n      3\n      (48.0, 64.0]\n      0.43478\n    \n    \n      4\n      (64.0, 80.0]\n      0.09091\n    \n  \n\n\n用这个频率来把 Age 分为五箱来替代原 Age。\nfor dataset in combine:\n    dataset.loc[dataset[&quot;Age&quot;] &lt;= 16, &quot;Age&quot;] = 0\n    dataset.loc[(dataset[&quot;Age&quot;] &gt; 16) &amp; (dataset[&quot;Age&quot;] &lt;= 32), &quot;Age&quot;] = 1\n    dataset.loc[(dataset[&quot;Age&quot;] &gt; 32) &amp; (dataset[&quot;Age&quot;] &lt;= 48), &quot;Age&quot;] = 2\n    dataset.loc[(dataset[&quot;Age&quot;] &gt; 48) &amp; (dataset[&quot;Age&quot;] &lt;= 64), &quot;Age&quot;] = 3\n    dataset.loc[dataset[&quot;Age&quot;] &gt; 64, &quot;Age&quot;] = 4\n \ntrain_df.head()\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      Survived\n      Pclass\n      Sex\n      Age\n      SibSp\n      Parch\n      Fare\n      Embarked\n      Title\n      AgeBand\n    \n  \n  \n    \n      0\n      0\n      3\n      0\n      1\n      1\n      0\n      7.25000\n      S\n      1\n      (16.0, 32.0]\n    \n    \n      1\n      1\n      1\n      1\n      2\n      1\n      0\n      71.28330\n      C\n      3\n      (32.0, 48.0]\n    \n    \n      2\n      1\n      3\n      1\n      1\n      0\n      0\n      7.92500\n      S\n      2\n      (16.0, 32.0]\n    \n    \n      3\n      1\n      1\n      1\n      2\n      1\n      0\n      53.10000\n      S\n      3\n      (32.0, 48.0]\n    \n    \n      4\n      0\n      3\n      0\n      2\n      0\n      0\n      8.05000\n      S\n      1\n      (32.0, 48.0]\n    \n  \n\n\n移除 AgeBand feature\ntrain_df = train_df.drop([&quot;AgeBand&quot;], axis=1)\ncombine = [train_df, test_df]\ntrain_df.head()\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      Survived\n      Pclass\n      Sex\n      Age\n      SibSp\n      Parch\n      Fare\n      Embarked\n      Title\n    \n  \n  \n    \n      0\n      0\n      3\n      0\n      1\n      1\n      0\n      7.25000\n      S\n      1\n    \n    \n      1\n      1\n      1\n      1\n      2\n      1\n      0\n      71.28330\n      C\n      3\n    \n    \n      2\n      1\n      3\n      1\n      1\n      0\n      0\n      7.92500\n      S\n      2\n    \n    \n      3\n      1\n      1\n      1\n      2\n      1\n      0\n      53.10000\n      S\n      3\n    \n    \n      4\n      0\n      3\n      0\n      2\n      0\n      0\n      8.05000\n      S\n      1\n    \n  \n\n\nEmbarked\nEmbarked 特征取值为 S、Q、C。我们的训练数据集有两个缺失的值。用最常见的 Embarked 来填充（众数填充）。\nfreq_port = train_df.Embarked.dropna().mode()[0] # 返回出现次数最多的值（众数）\nfreq_port\n&#039;S&#039;\n\nfor dataset in combine:\n    dataset[&quot;Embarked&quot;] = dataset[&quot;Embarked&quot;].fillna(freq_port)\n    \ntrain_df[[&quot;Embarked&quot;, &quot;Survived&quot;]].groupby([&quot;Embarked&quot;], as_index=False).mean().sort_values(by=&quot;Survived&quot;, ascending=False)\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      Embarked\n      Survived\n    \n  \n  \n    \n      0\n      C\n      0.55357\n    \n    \n      1\n      Q\n      0.38961\n    \n    \n      2\n      S\n      0.33901\n    \n  \n\n\n现在，我们可以把 Embarked 转换为一个新的数字序列。\nfor dataset in combine:\n    dataset[&quot;Embarked&quot;] = dataset[&quot;Embarked&quot;].map( {&quot;S&quot;: 0, &quot;C&quot;: 1, &quot;Q&quot;: 2} ).astype(int)\n \ntrain_df.head()\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      Survived\n      Pclass\n      Sex\n      Age\n      SibSp\n      Parch\n      Fare\n      Embarked\n      Title\n    \n  \n  \n    \n      0\n      0\n      3\n      0\n      1\n      1\n      0\n      7.25000\n      0\n      1\n    \n    \n      1\n      1\n      1\n      1\n      2\n      1\n      0\n      71.28330\n      1\n      3\n    \n    \n      2\n      1\n      3\n      1\n      1\n      0\n      0\n      7.92500\n      0\n      2\n    \n    \n      3\n      1\n      1\n      1\n      2\n      1\n      0\n      53.10000\n      0\n      3\n    \n    \n      4\n      0\n      3\n      0\n      2\n      0\n      0\n      8.05000\n      0\n      1\n    \n  \n\n\nFare\n现在，我们可以使用 df.fillna 填充 test dataset 中 Fare 的单个缺失值，使用 median （中位数）来填充。我们只需要一行代码就可以做到这一点。\n注意，我们并没有创建一个中间的新特性，也没有对相关性进行任何进一步的分析来猜测缺失的特性，因为我们只是替换了一个值。以达到了模型算法对非空值运算的要求。\ntest_df[&quot;Fare&quot;].fillna(test_df[&quot;Fare&quot;].dropna().median(), inplace=True)\ntest_df.head()\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      PassengerId\n      Pclass\n      Sex\n      Age\n      SibSp\n      Parch\n      Fare\n      Embarked\n      Title\n    \n  \n  \n    \n      0\n      892\n      3\n      0\n      2\n      0\n      0\n      7.82920\n      2\n      1\n    \n    \n      1\n      893\n      3\n      1\n      2\n      1\n      0\n      7.00000\n      0\n      3\n    \n    \n      2\n      894\n      2\n      0\n      3\n      0\n      0\n      9.68750\n      2\n      1\n    \n    \n      3\n      895\n      3\n      0\n      1\n      0\n      0\n      8.66250\n      0\n      1\n    \n    \n      4\n      896\n      3\n      1\n      1\n      1\n      1\n      12.28750\n      0\n      3\n    \n  \n\n\n创建一个 FareBand\ntrain_df[&quot;FareBand&quot;] = pd.qcut(train_df[&quot;Fare&quot;], 4)\ntrain_df[[&quot;FareBand&quot;, &quot;Survived&quot;]].groupby([&quot;FareBand&quot;], as_index=False).mean().sort_values(by=&quot;FareBand&quot;, ascending=True)\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      FareBand\n      Survived\n    \n  \n  \n    \n      0\n      (-0.001, 7.91]\n      0.19731\n    \n    \n      1\n      (7.91, 14.454]\n      0.30357\n    \n    \n      2\n      (14.454, 31.0]\n      0.45495\n    \n    \n      3\n      (31.0, 512.329]\n      0.58108\n    \n  \n\n\n基于 FareBand 将 Fare 转换为序列值。\nfor dataset in combine:\n    dataset.loc[dataset[&quot;Fare&quot;] &lt;= 7.91, &quot;Fare&quot;] = 0\n    dataset.loc[(dataset[&quot;Fare&quot;] &gt; 7.91) &amp; (dataset[&quot;Fare&quot;] &lt;= 14.454), &quot;Fare&quot;] = 1\n    dataset.loc[(dataset[&quot;Fare&quot;] &gt; 14.454) &amp; (dataset[&quot;Fare&quot;] &lt;= 31), &quot;Fare&quot;] = 2\n    dataset.loc[dataset[&quot;Fare&quot;] &gt; 31, &quot;Fare&quot;] = 3\n    dataset[&quot;Fare&quot;] = dataset[&quot;Fare&quot;].astype(int)\n \ntrain_df = train_df.drop([&quot;FareBand&quot;], axis=1)\ncombine = [train_df, test_df]\ntrain_df.head()\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      Survived\n      Pclass\n      Sex\n      Age\n      SibSp\n      Parch\n      Fare\n      Embarked\n      Title\n    \n  \n  \n    \n      0\n      0\n      3\n      0\n      1\n      1\n      0\n      0\n      0\n      1\n    \n    \n      1\n      1\n      1\n      1\n      2\n      1\n      0\n      3\n      1\n      3\n    \n    \n      2\n      1\n      3\n      1\n      1\n      0\n      0\n      1\n      0\n      2\n    \n    \n      3\n      1\n      1\n      1\n      2\n      1\n      0\n      3\n      0\n      3\n    \n    \n      4\n      0\n      3\n      0\n      2\n      0\n      0\n      1\n      0\n      1\n    \n  \n\n\n模型预测\nX_train = train_df.drop(&quot;Survived&quot;, axis=1)\nY_train = train_df[&quot;Survived&quot;]\nX_test = test_df.drop(&quot;PassengerId&quot;, axis=1).copy()\nX_train.shape, Y_train.shape, X_test.shape\n((891, 8), (891,), (418, 8))\n\nX_train.describe()\nX_test\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      Pclass\n      Sex\n      Age\n      SibSp\n      Parch\n      Fare\n      Embarked\n      Title\n    \n  \n  \n    \n      0\n      3\n      0\n      2\n      0\n      0\n      0\n      2\n      1\n    \n    \n      1\n      3\n      1\n      2\n      1\n      0\n      0\n      0\n      3\n    \n    \n      2\n      2\n      0\n      3\n      0\n      0\n      1\n      2\n      1\n    \n    \n      3\n      3\n      0\n      1\n      0\n      0\n      1\n      0\n      1\n    \n    \n      4\n      3\n      1\n      1\n      1\n      1\n      1\n      0\n      3\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      413\n      3\n      0\n      1\n      0\n      0\n      1\n      0\n      1\n    \n    \n      414\n      1\n      1\n      2\n      0\n      0\n      3\n      1\n      5\n    \n    \n      415\n      3\n      0\n      2\n      0\n      0\n      0\n      0\n      1\n    \n    \n      416\n      3\n      0\n      1\n      0\n      0\n      1\n      0\n      1\n    \n    \n      417\n      3\n      0\n      1\n      1\n      1\n      2\n      1\n      4\n    \n  \n\n418 rows × 8 columns\n\n现在，我们准备训练一个模型并预测。有60多种预测建模算法可供选择。我们必须了解问题的类型和解决方案的需求，以便将范围缩小到我们可以评估的几个选定的模型。\n我们的问题是一个分类和回归问题。我们想要确定输出(Survived or not)与其他变量或特性(Gender, Age, Port……)之间的关系。这是一个监督学习。有了这两个标准 —— 监督学习加上分类和回归，我们可以把模型的选择范围缩小到几个。\n这些包括:\n\n逻辑回归\nKNN或k近邻\n支持向量机\n朴素贝叶斯分类器\n决策树\n随机森林\n感知器\n随机梯度下降\nRVM 相关向量机\n\n# 逻辑回归\n \nlogreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\nY_pred = logreg.predict(X_test)\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)\nacc_log\n/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to &#039;lbfgs&#039; in 0.22. Specify a solver to silence this warning.\n  FutureWarning)\n\n\n\n\n\n81.37\n\n我们可以使用逻辑回归来验证和检查我们的预测。可以通过计算决策函数中特征的系数来实现。\n正系数增加了响应的 log-odds (从而增加了概率)，负系数减少了响应的 log-odds (从而减少了概率)。\n性别是正系数最高的，说明随着性别值的增加(男性: 0 女性:1)，存活的概率增加最多。\n相反，随着 Pclass 的增加，生存概率下降。\n这样，Age 是一个很好的人工特征，因为它与存活有第二高的负相关。\nTitle 也是第二高的正相关。\ncoeff_df = pd.DataFrame(train_df.columns.delete(0))\ncoeff_df.columns = [&#039;Feature&#039;]\ncoeff_df[&quot;Correlation&quot;] = pd.Series(logreg.coef_[0])\n \ncoeff_df.sort_values(by=&#039;Correlation&#039;, ascending=False)\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      Feature\n      Correlation\n    \n  \n  \n    \n      1\n      Sex\n      2.19360\n    \n    \n      7\n      Title\n      0.49431\n    \n    \n      5\n      Fare\n      0.31180\n    \n    \n      6\n      Embarked\n      0.24051\n    \n    \n      4\n      Parch\n      -0.25322\n    \n    \n      3\n      SibSp\n      -0.50649\n    \n    \n      2\n      Age\n      -0.65716\n    \n    \n      0\n      Pclass\n      -0.91037\n    \n  \n\n\n其次，我们使用支持向量机建模，这是监督学习的算法，用于数据分类和回归分析。给定一组训练样本，每个样本都被标记为属于两个类别中的一个或另一个类别，SVM训练算法建立一个模型，将新的测试样本分配给其中一个类别或另一个类别，使其成为一个非概率二元线性分类器。\n该模型生成的评分高于逻辑回归模型。\n# Support Vector Machines\n \nsvm = SVC()\nsvm.fit(X_train, Y_train)\nY_pred = svm.predict(X_test)\nacc_svm = round(svm.score(X_train, Y_train) * 100, 2)\nacc_svm\n/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from &#039;auto&#039; to &#039;scale&#039; in version 0.22 to account better for unscaled features. Set gamma explicitly to &#039;auto&#039; or &#039;scale&#039; to avoid this warning.\n  &quot;avoid this warning.&quot;, FutureWarning)\n\n\n\n\n\n83.73\n\n在模式识别中，k近邻算法(简称k-NN)是一种用于分类和回归的非参数算法。一个样本由它的邻居的多数投票来分类，这个样本被分配到它的k个最近邻居中最常见的类(k是一个正整数，通常很小)。如果 k = 1，那么对象就被简单地分配给那个最近邻居的类。\nKNN得分优于logistic回归，但低于SVM。\n# KNN\n \nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, Y_train)\nY_pred = knn.predict(X_test)\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\nacc_knn\n84.4\n\n在机器学习中，朴素贝叶斯分类器是一组基于贝叶斯定理的简单概率分类器，特征之间具有强(朴素)独立性假设。朴素贝叶斯分类器是高度可伸缩的，在一个学习问题中需要一些参数在变量(特征)的数量上是线性的。\n模型生成的置信度评分是目前评价模型中最低的。\n# Gaussian Naive Bayes\n \ngaussian = GaussianNB()\ngaussian.fit(X_train, Y_train)\nY_pred = gaussian.predict(X_test)\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\nacc_gaussian\n80.13\n\n感知器是一种二进制分类器的监督学习算法(函数，它可以决定一个输入是否属于某个特定的类，由一个数字向量表示)。它是一种线性分类器，即基于一组权值与特征向量相结合的线性预测函数进行预测的分类算法。该算法允许在线学习，每次处理训练集中的一个元素。\n# Perceptron\n \nperceptron = Perceptron()\nperceptron.fit(X_train, Y_train)\nY_pred = perceptron.predict(X_test)\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\nacc_perceptron\n80.58\n\n# Linear SVC\n \nlinear_svc = LinearSVC()\nlinear_svc.fit(X_train, Y_train)\nY_pred = linear_svc.predict(X_test)\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\nacc_linear_svc\n/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  &quot;the number of iterations.&quot;, ConvergenceWarning)\n\n\n\n\n\n81.14\n\n# Stochastic Gradient Descent\n \nsgd = SGDClassifier()\nsgd.fit(X_train, Y_train)\nY_pred = sgd.predict(X_test)\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\nacc_sgd\n80.92\n\n使用决策树作为预测模型，是一种十分常用的分类方法。他是一种监管学习，所谓监管学习就是给定一堆样本，每个样本都有一组属性和一个类别，这些类别是事先确定的，那么通过学习得到一个分类器，这个分类器能够对新出现的对象给出正确的分类。\n目前模型中最高的评分。\n# Decision Tree\n \ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\nY_pred = decision_tree.predict(X_test)\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\nacc_decision_tree\n89.0\n\n下一个模型随机森林是最受欢迎的之一。一个包含多个决策树的分类器， 并且其输出的类别是由个别树输出的类别的众数而定。\n模型目前评分中最高的。我们决定使用这个模型的输出(Y_pred)提交结果。\n# Random Forest\n \nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\nY_pred = random_forest.predict(X_test)\nrandom_forest.score(X_train, Y_train)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nacc_random_forest\n89.0\n\n模型评价\n现在我们可以对所有模型的评估进行排序，以选择最适合我们问题的模型。在决策树和随机森林得分相同的情况下，我们选择随机森林来纠正决策树对训练集过度拟合的习惯。\nmodels = pd.DataFrame({\n    &quot;Model&quot;: [&quot;Support Vector Machines&quot;, &quot;KNN&quot;, &quot;Logistic Regression&quot;,\n              &quot;Random Forest&quot;, &quot;Naive Bayes&quot;, &quot;Percep tron&quot;,\n              &quot;Stochastic Gradient Decent&quot;, &quot;Linear SVC&quot;, &quot;Decision Tree&quot;],\n    &quot;Score&quot;: [acc_svm, acc_knn, acc_log, acc_random_forest, acc_gaussian,\n              acc_perceptron, acc_sgd, acc_linear_svc, acc_decision_tree]\n    })\nmodels.sort_values(by=&quot;Score&quot;, ascending=False)\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      Model\n      Score\n    \n  \n  \n    \n      3\n      Random Forest\n      89.00000\n    \n    \n      8\n      Decision Tree\n      89.00000\n    \n    \n      1\n      KNN\n      84.40000\n    \n    \n      0\n      Support Vector Machines\n      83.73000\n    \n    \n      2\n      Logistic Regression\n      81.37000\n    \n    \n      7\n      Linear SVC\n      81.14000\n    \n    \n      6\n      Stochastic Gradient Decent\n      80.92000\n    \n    \n      5\n      Percep tron\n      80.58000\n    \n    \n      4\n      Naive Bayes\n      80.13000\n    \n  \n\n\n# 保存结果\n \nsubmission = pd.DataFrame({\n    &quot;PassengerId&quot;: test_df[&quot;PassengerId&quot;],\n    &quot;Survived&quot;: Y_pred\n    })\nsubmission.to_csv(&quot;/submission.csv&quot;, index=False)"},"Linux/Django-部署(Apache)":{"slug":"Linux/Django-部署(Apache)","filePath":"Linux/Django 部署(Apache).md","title":"Django 部署(Apache) 趟过的坑","links":[],"tags":["Django"],"content":"Django是一个，由Python写成的开放源代码的Web应用框架，在使用apache部署的时候走了好多坑这里记录下。\n参考：\nDjango教程\napache部署\n前提条件\n\n一个服务器，我使用的是阿里云服务器。\n推荐使用ubuntu镜像，因为软件集成度高（就是简单，傻瓜也会玩）。\n已经使用Django搭建好web服务，如何搭建看Django教程。\n\n这里只记录部署apache的坑了，其他上面都有详细讲解，就略了。\n安装apache2和mod_wsgi\nsudo apt-get install apache2\n \n# Python 2\nsudo apt-get install libapache2-mod-wsgi\n \n# Python 3\nsudo apt-get install libapache2-mod-wsgi-py3\n看版本！！！（非常重要）\n版本不同在配置上有区别，推荐使用比较新的版本，也就是2.4以上，如果是1，下面的配置是不一样的！！！\napachectl -v\n \nServer version: Apache/2.4.18 (Ubuntu)\nServer built:   2018-06-07T19:43:03\n先别急着配置，看看能不能正常启动\nsudo service apache2 restart\n\n这时候正常情况会启动默认配置，使用浏览器访问你服务器的外网IP，如果正常会显示下图：\n\n无法访问请检查阿里云的防火墙设置，看端口是否允许通过，浏览器默认是80，这里顺便加一个8080，供下面测试。\n\nApache2 Ubuntu Default Page 页面可以正常访问代表我们的apache安装成功，下面开始修改配置文件。\n设置目录和文件的权限\n一般目录权限设置为 755，文件权限设置为 644\n假如项目位置在 /home/user/WebService （在WebService 下面有一个 manage.py，WebService 是项目名称）\ncd /home/user/\nsudo chmod -R 644 WebService\nsudo find WebService -type d | xargs chmod 755\n\nDjango 的 settings.py 要设置清楚\nmedia 文件夹一般用来存放用户上传文件，static 一般用来放自己网站的js，css，图片等，在settings.py中的相关设置\nSTATIC_URL 为静态文件的网址 STATIC_ROOT 为静态文件的根目录，\nMEDIA_URL 为用户上传文件夹的根目录，MEDIA_URL为对应的访问网址\n需要media的 要给media目录单独设置写的权限\nALLOWED_HOSTS是为了限定请求中的host值，以防止黑客构造包来发送请求。只有在列表中的host才能访问。\n注意：在这里本人强烈建议不要使用通配符去配置，另外当DEBUG设置为False的时候必须配置这个配置。否则会抛出异常。*\nALLOWED_HOSTS = [&#039;*&#039;]\n\n这里先写个*等全部调通了再改。。。\napache的配置文件\ncd /etc/apache2/sites-available\nsudo vim mysite.conf\n\n在这里我们自己写个配置\n&lt;VirtualHost *:8080&gt;\n    ServerName www.yourdomain.com\n\tServerAdmin youremail@mail.com\n\tErrorLog ${APACHE_LOG_DIR}/error.log\n\tCustomLog ${APACHE_LOG_DIR}/access.log combined\n\tAlias /static/ /home/user//WebService/static/\n\t&lt;Directory /home/user/WebService/static&gt;\n\t   Options Indexes FollowSymLinks\n\t   AllowOverride None\n\t   Require all granted\n\t&lt;/Directory&gt;\n\tWSGIScriptAlias / /home/user/WebService/WebService/wsgi.py\n\t&lt;Directory /home/user/WebService/WebService&gt;\n\t&lt;Files wsgi.py&gt;\n\t   Options Indexes FollowSymLinks\n\t   AllowOverride None\n\t   Require all granted\n\t&lt;/Files&gt;\n\t&lt;/Directory&gt;\n&lt;/VirtualHost&gt;\n\n根据自己的情况改改，要注意目录要对，并且也写下面这个配置，apache版本不同配置是不同的！！！\nOptions Indexes FollowSymLinks\nAllowOverride None\nRequire all granted\n因为我们刚配置里写的是8080的端口，所以要把它加到监听列表里\nsudo vim /etc/apache2/ports.conf\n\nListen 80\n\n加一句\n\nListen 80\nListen 8080\n\nwsgi.py文件修改\n修改这个文件的目的就是把apache2和你的网站project联系起来\nimport os\nfrom os.path import join,dirname,abspath # +\nfrom django.core.wsgi import get_wsgi_application\n \nPROJECT_DIR = dirname(dirname(abspath(__file__))) # +\nimport sys # +\nsys.path.insert(0,PROJECT_DIR) # +\n \nos.environ.setdefault(&quot;DJANGO_SETTINGS_MODULE&quot;, &quot;WebService.settings&quot;)\nfrom django.core.wsgi import get_wsgi_application\napplication = get_wsgi_application()\n激活新配\n这里不用写路径\nsudo a2ensite mysite 或 sudo a2ensite mysite.conf\n\n重启apach\nsudo service apache2 restart\n\n访问 你的网站，记得加端口号 0.0.0.0:8080\n出错看log\ncat /var/log/apache2/error.log\n\n虚拟环境部署\n通常我们的系统中会有多个python环境，使用virtualenv管理\n源代码安装python3.7\n上Python官网下载最新版本的source包，解压后进入安装目录，配置makefile，编译，安装。\n# ./configure --prefix=/usr/local --enable-shared --with-ssl\n# make\n# make install\n\n—prefix=/usr/local —enable-shared 的意思是创建共享链接，以便其他软件编译时调用\n—with-ssl 的意思是允许ssl，pip安装的时候会用到\n安装虚拟环境\nPython 虚拟环境用于将软件包安装与系统隔离开来。\n创建一个新的虚拟环境，方法是选择 Python 解释器并创建一个 ./venv 目录来存放它：\n \n$ virtualenv --system-site-packages -p python3.7 ./venv\n \n使用特定于 shell 的命令激活该虚拟环境：\n \n$ source ./venv/bin/activate  # sh, bash, ksh, or zsh\n \n当 virtualenv 处于有效状态时，shell 提示符带有 (venv) 前缀。\n \n在不影响主机系统设置的情况下，在虚拟环境中安装软件包。首先升级 pip：\n \n$ pip install --upgrade pip\n \n$ pip list  # show packages installed within the virtual environment\n \n之后要退出 virtualenv，请使用以下命令：\n \n$ deactivate  # don&#039;t exit until you&#039;re done using TensorFlow\n编译mod_wsgi\nmod_wsgi官网\nmod_wsgi是一个apache的模块，用来把python web和apache连接起来，说实话，不咋好用，一定要下载最新版本，老版本会有不少问题\n下载地址\n解压后进入安装目录，配置makefile，编译，安装。\n./configure --with-apxs=/usr/bin/apxs2 --with-python=py3.7env/venv/bin/python3.7\nmake\nmake install\n\napxs2没有的话以防万一就装一下，—with-python指的是我们想要链接的python目标\n配置Apache\n在上面的配置基础上加两行\n\tWSGIScriptAlias / /home/user/WebService/WebService/wsgi.py # 在这行下加俩\n\tWSGIDaemonProcess yourdomain.com python-path=/home/user/WebService:/home/user/py3.7env/venv/lib/python3.7/site-packages\n\tWSGIProcessGroup yourdomain.com\n\nWSGIDaemonProcess 你的域名 python-path=刚才用virtualenv创建的python包的路径\n重启apache\n$ service apache2 restart\n看log我们的apache成功的链接到python3.7：\nApache/2.4.18 (Ubuntu) mod_wsgi/4.6.5 Python/3.7 configured — resuming normal operations\n如果出错\n看错误代码慢慢查，我研究了一天才成功，首先就是编译的时候configure后一定要带对参数，缺少的库也要全手动安装，还有靠一部分运气才能成功。"},"Linux/Django":{"slug":"Linux/Django","filePath":"Linux/Django.md","title":"Django","links":[],"tags":["Django"],"content":"Python下有许多款不同的 Web 框架。Django是重量级选手中最有代表性的一位。许多成功的网站和APP都基于Django。\nDjango是一个开放源代码的Web应用框架，由Python写成。\nDjango采用了MVC的软件设计模式，即模型M，视图V和控制器C。\nDjango的安装\npip install Django\n \n&gt;&gt;&gt; import django\n&gt;&gt;&gt; django.VERSION\n(2, 1, 7, &#039;final&#039;, 0)\n创建工程\n安装完django后我们会有一个 django-admin的管理工具，我们使用此工具来创建工程。\ndjango-admin startproject projectname\n$ cd projectname/\n$ tree\n.\n|-- projectname\n|   |-- __init__.py\n|   |-- settings.py\n|   |-- urls.py\n|   `-- wsgi.py\n`-- manage.py\n\n目录说明：\nprojectname: 项目的容器。\nmanage.py: 一个实用的命令行工具，可让你以各种方式与该 Django 项目进行交互。\n__init__.py: 一个空文件，告诉 Python 该目录是一个 Python 包。\nsettings.py: 该 Django 项目的设置/配置。\nurls.py: 该 Django 项目的 URL 声明; 一份由 Django 驱动的网站&quot;目录&quot;。\nwsgi.py: 一个 WSGI 兼容的 Web 服务器的入口，以便运行你的项目。\n\n接下来我们进入 projectname 目录输入以下命令，启动服务器：\npython manage.py runserver 0.0.0.0:8000\n0.0.0.0 让其它电脑可连接到开发服务器，8000 为端口号。如果不说明，那么端口号默认为 8000。\n在浏览器输入你服务器的 ip（这里我们输入本机 IP 地址： 127.0.0.1:8000） 及端口号，如果正常启动，会打开django默认页面。\n创建 APP\npython manage.py startapp 应用名\n新建一个应用文件appname，它的里面也创建了一些py文件和包：\npython manage.py startapp appname\n.\n|-- appname\n|   |-- __init__.py\n|   |-- admin.py\n|   |-- apps.py\n|   |-- migrations\n|   |-- models.py\n|   |-- test.py\n|   |-- views.py\n|   |-- urls.py\n\n下面介绍这些文件都是什么：\nadmin.py：管理站点模型的声明文件，默认为空。\napps.py：应用信息定义文件。在其中生成了类Appconfig，类用于定义应用名等Meta数据。\nmigrations: 用于在之后定义引用迁移功能。\nmodels.py: 添加模型层数据类的文件。\ntest.py：测试代码文件。\nviews.py：定义URL响应函数。\nurls.py：需要自己创建，作为子路由。\n\n创建好app后在settings.py中的INSTALLED_APPS添加appname\nINSTALLED_APPS = [\n    &#039;django.contrib.admin&#039;,\n    &#039;django.contrib.auth&#039;,\n    &#039;django.contrib.contenttypes&#039;,\n    &#039;django.contrib.sessions&#039;,\n    &#039;django.contrib.messages&#039;,\n    &#039;django.contrib.staticfiles&#039;,\n    &#039;appname&#039;,\n]\n\n网页和静态文件\n在appname中新建一个templates文件夹存放html\n新建static文件夹存放js，image，css等\n并且在settings.py中添加静态文件目录\nSTATIC_URL = &#039;/static/&#039;\nSTATICFILES_DIRS = [\n    os.path.join(BASE_DIR, &quot;static&quot;),\n]\n\n注意：每一个app中的static访问路径都是相同的。\n在html中的使用：\n首先创建了一个css文件\n \nappname/static/js/style.css\n \n如何使用，appname/templates/index.html里添加下面代码：\n \n{% load staticfiles %}\n    &lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;{% static &#039;js/style.css&#039; %}&quot; /&gt;\n添加脚本目录\n我们有一些脚本写在另一个文件夹，我们希望引入它们，在settings.py中添加\nimport sys\nsys.path.insert(0, os.path.join(BASE_DIR, &#039;/home/Scripts&#039;))\n\n数据库相关\n部署静态文件\n收集静态文件\npython manage.py collectstatic\n这一句话就会把以前放在app下static中的静态文件全部拷贝到 settings.py 中设置的 STATIC_ROOT 文件夹中\n用UWSGI部署\nuWSGI是一个Web服务器，它实现了WSGI协议、uwsgi、http等协议。Nginx中HttpUwsgiModule的作用是与uWSGI服务器进行交换。\n要注意 WSGI / uwsgi / uWSGI 这三个概念的区分。\nWSGI是一种通信协议。\nuwsgi是一种线路协议而不是通信协议，在此常用于在uWSGI服务器与其他网络服务器的数据通信。\n而uWSGI是实现了uwsgi和WSGI两种协议的Web服务器。\nuwsgi协议是一个uWSGI服务器自有的协议，它用于定义传输信息的类型（type of information），每一个uwsgi packet前4byte为传输信息类型描述，它与WSGI相比是两样东西。\nuwsgi安装命令\nsudo pip install uwsgi --upgrade\n\nuwsgi.ini配置文件编写\n[uwsgi]\nhttp-socket = 0.0.0.0:80  # IP地址与端口号\nchdir = /home/user/myProject/  # Django工程的目录\nvirtualenv = /home/user/pyvenv/env3.7 # python虚拟环境目录\nwsgi-file = myProject/wsgi.py # wsgi.py的位置，接在chdir后面\nstatic-map = /static=/home/user/myProject/static # 静态文件的位置\nenable-threads = true   # 允许在django创建线程\nauto-procname = true    # 自动给进程起名字\ndaemonize = /home/user/log/uwsgi.log    # log地址\npidfile = /home/user/myProject/uwsgi.pid    # 保存主进程pid的文件\ndisable-logging = true  # log只记录错误信息\nbuffer-size = 51200 # 允许传输的字节数\nprocesses = 4   # 进程数\nthreads = 2 # 线程数\nvacuum = true   # 允许主进程\n\n关闭只能关闭pidfile里记录的pid号，不能kill其它三个进程\nuwsgi --ini uwsgi.ini   # 启动\nuwsgi --reload uwsgi.pid    # 重启\nuwsgi --stop uwsgi.pid  # 关闭\n\n部署到服务器\n用 apache2 或 nginx 示例代码\napache2配置文件\nAlias /static/ /path/to/collected_static/\n \n&lt;Directory /path/to/collected_static&gt;\n    Require all granted\n&lt;/Directory&gt;\n\nnginx 示例代码：\nlocation /media  {\n    alias /path/to/project/media;\n}\n \nlocation /static {\n    alias /path/to/project/collected_static;\n}\n"},"Linux/Git学习笔记":{"slug":"Linux/Git学习笔记","filePath":"Linux/Git学习笔记.md","title":"Git学习笔记","links":[],"tags":["git"],"content":"Git是一个开源的分布式版本控制系统，用于敏捷高效地处理任何或小或大的项目。\r\nGit 是 Linus Torvalds 为了帮助管理 Linux 内核开发而开发的一个开放源码的版本控制软件。\r\nGit 与常用的版本控制工具 CVS, Subversion 等不同，它采用了分布式版本库的方式，不必服务器端软件支持。\r\nGit不同于SVN之前写过 SVN使用手册 ，搭建过SVN服务，现在复习一下Git\n配置Git\n首先创建SSH key：\nssh-keygen -t rsa -C &quot;email@email.com&quot;\n\n在~/目录下生成.ssh文件夹，打开id_rsa.pub，复制里面的key。\r\n在github上，进入 Account Settings（账户配置），左边选择SSH Keys，Add SSH\r\nKey,title随便填，粘贴在电脑上生成的key。\r\n验证是否成功\nssh -T git@github.com\n\n如果是第一次的会提示是否continue，输入yes就会看到：You’ve successfully authenticated, but GitHub does not provide shell access 。这就表示已成功连上github。\r\n接下来我们要做的就是把本地仓库传到github上去，在此之前还需要设置username和email，因为github每次commit都会记录他们。\ngit config --global user.name &quot;your name&quot;\r\ngit config --global user.eamil &quot;your_eamail@email.com&quot;\n\n上传远程仓库，需要添加远程地址，仓库需要在github上先建立好\ngit remote add origin git@github.com:yourName/yourRepo.git\n\n检出仓库\n克隆本地仓库：\ngit clone /path/to/repository \n\n克隆远程仓库：\ngit clone username@host:/path/to/repository\n\n推送流程\n$mkdir test  #创建一个测试目录\r\n$cd test/    #进入test目录\r\n$echo &quot;#git test file&quot; &gt;&gt; README.md #给readme文件写入内容\r\n$ls\r\nREADME.md\r\n$git init   #初始化init\r\n$git add README.md  #添加文件\r\n[master (root-commit) 0205aab] 添加 README.md 文件\r\n 1 file changed, 1 insertion(+)\r\n create mode 100644 README.md\r\n$git commit -m &quot;add readme.md file&quot;     #提交备注信息\r\n\r\n#提交到github\r\n$ git remote add origin git@github.com:usename/Repositoryname.git\r\n$ git push -u origin master\n\n常用命令\n#初始化git\r\n$git init\r\n\r\n#拷贝一个仓库到本地\r\n$git clone [url]\r\n\r\n#添加文件到缓存\r\n$git add\r\n\r\n#查看当前项目状态\r\n$git status -s\r\n#A 加入缓存 M有改动 \r\n\r\n#查看修改\r\n执行 git diff 来查看执行 git status 的结果的详细信息。\r\ngit diff 命令显示已写入缓存与已修改但尚未写入缓存的改动的区别。git diff 有两个主要的应用场景。\r\n尚未缓存的改动：git diff\r\n查看已缓存的改动： git diff --cached\r\n查看已缓存的与未缓存的所有改动：git diff HEAD\r\n显示摘要而非整个 diff：git diff --stat\r\n\r\n#添加到仓库\r\n$git commit -m &quot;描述&quot;\r\n\r\n#取消缓存内容\r\n$git reset HEAD\r\n\r\n#删除文件\r\n如果只是简单地从工作目录中手工删除文件，运行 git status 时就会在 Changes not staged for commit 的提示。\r\n要从 Git 中移除某个文件，就必须要从已跟踪文件清\r\n单中移除，然后提交。可以用以下命令完成此项工作\r\n\r\n$git rm &lt;file&gt;\r\n\r\n如果删除之前修改过并且已经放到暂存区域的话，则必须要用强制删除选项 -f\r\n\r\n$git rm -f &lt;file&gt;\r\n\r\n如果把文件从暂存区域移除，但仍然希望保留在当前工作目录中，换句话说，仅是从跟踪清单中删除，使用 --cached 选项即可\r\n\r\n$git rm --cached &lt;file&gt;\r\n\r\n#重命名\r\n$git mv oldname newname\r\n\r\n#push到远程分支\r\ngit push origin 本地分支名字:远程分支名\r\n\r\n#查看所有分支\r\ngit branch -a\r\n\r\n    * master\r\n      remotes/origin/HEAD -&gt; origin/master\r\n      \r\n#创建并切换分支 \r\ngit checkout -b 本地分支名 origin/远程分支名\n\n查看当前远程仓库：\n$ git remote\r\norigin\r\n$ git remote -v\r\norigin  git@github.com:Voidmort/blogs.git (fetch)\r\norigin  git@github.com:Voidmort/blogs.git (push)\n\nGit 有两个命令用来提取远程仓库的更新。\r\n1、从远程仓库下载新分支与数据：\ngit fetch\n\n该命令执行完后需要执行git merge 远程分支到你所在的分支。\r\n2、从远端仓库提取数据并尝试合并到当前分支：\ngit merge\n\n该命令就是在执行 git fetch 之后紧接着执行 git merge 远程分支到你所在的任意分支。\n删除\n$ git remote -v\r\norigin  git@github.com:Voidmort/blogs.git (fetch)\r\norigin  git@github.com:Voidmort/blogs.git (push)\r\n\r\n#添加仓库2\r\n$ git remote add origin2 git@github.com:Voidmort/blogs.git\r\n\r\n\r\n$ git remote -v\r\norigin  git@github.com:Voidmort/blogs.git (fetch)\r\norigin  git@github.com:Voidmort/blogs.git (push)\r\norigin2 git@github.com:Voidmort/blogs.git (fetch)\r\norigin2 git@github.com:Voidmort/blogs.git (push)\r\n\r\n#删除仓库2\r\n$ git remote rm origin2\r\n\r\n$ git remote -v\r\norigin  git@github.com:Voidmort/blogs.git (fetch)\r\norigin  git@github.com:Voidmort/blogs.git (push)\r\n\n\ngit在终端不能识别中文\n$ git status\r\nOn branch master\r\nYour branch is up to date with &#039;origin/master&#039;.\r\n\r\nChanges not staged for commit:\r\n  (use &quot;git add &lt;file&gt;...&quot; to update what will be committed)\r\n  (use &quot;git checkout -- &lt;file&gt;...&quot; to discard changes in working directory)\r\n\r\n        modified:   &quot;\\346\\234\\272\\345\\231\\250\\345\\255\\246\\344\\271\\240\\345\\256\\236\\346\\210\\230\\357\\274\\210\\345\\215\\201\\357\\274\\211.ipynb&quot;\n\ncore.quotepath设为false的话，就不会对0x80以上的字符进行quote。中文显示正常。\ngit config --global core.quotepath false\n"},"Linux/Markdown学习笔记":{"slug":"Linux/Markdown学习笔记","filePath":"Linux/Markdown学习笔记.md","title":"Markdown学习笔记","links":["path/to/img.jpg"],"tags":["markdown"],"content":"初次写博客，从搭建github hexo走了不少坑，稍后记录搭建博客过程，首先学习使用markdown\n教程来源：www.markdown.cn/\nmarkdown是一个HTML的转换工具，是HTML的书写格式\n标题语法\n类Atx形式在首行插入1个到6个#，对应到标题1到6\n一定要# + 空格 + 标题\n加空格！！！！！！\n\t# 一级标题\n\t## 二级标题\n\t###### 六级标题\n\n区块引用\n段首使用 &gt; 作为引用，引用部分也支持markdown语法\n\t&gt;hello word!\r\n\t&gt;# 标题一\r\n\t&gt;代码提示：\r\n\t&gt;    return Null;\n\n\nhello word!\n标题一\n代码提示：\nreturn Null;\n\n列表\nmarkdown支持有序和无序列表\n无序列表标记符号可以是“* + -”，有序符号是数字加英文点“1. ”\n\t* red\r\n\t* green\r\n\t* blue\r\n\t+ red\r\n\t+ green\r\n\t- red\r\n\t1.good\r\n\t2.name\r\n\t3.learn \n\n\nred\ngreen\nblue\n\n\nred\ngreen\n\n\nred\n\n\ngood\nname\nlearn\n\n代码区块\n要在 Markdown 中建立代码区块很简单，只要简单地缩进 4 个空格或是 1 个制表符就可以，例如，下面的输入：\r\n这是一个普通段落：\n    //这是一个代码区块。\r\n\t#include &lt;stdio.h&gt;\r\n\t#include &lt;stdlib.h&gt;\r\n\t\r\n\tint main()\r\n\t{\r\n\t\tprint(&quot;Hello word!\\n&quot;);\r\n\t\treturn 0;\r\n\t}\n\n分割线\n三个连续的* - _\n*********************\n\n链接\n链接文字用[方括号]标记\n要建立一个内行式链接，只要在方括号后面紧接着元括号并插入网址链接\nThis is [an example](example.com/ &quot;Title&quot;) inline link.\n[This link](example.net/) has no title attribute.\n\n百度一下\nThis is an example inline link.\nThis link has no title attribute.\n代码\n标记段行内代码，用反引号包起来（’）\nuse the &#039;printf()&#039; function\n\nUse the printf() function.\n图片\n和链接一样\n\t![Alt text](/path/to/img.jpg)\n\t![Alt text](/path/to/img.jpg &quot;Optional title&quot;)\n\nAlt text\n详细叙述如下：\n一个惊叹号 !\n接着一个方括号，里面放上图片的替代文字\n接着一个普通括号，里面放上图片的网址，最后还可以用引号包住并加上 选择性的 ‘title’ 文字。\n其他\n反斜杠\n在一下符号前面加入反斜杠来插入普通符号：\n\t\\   反斜线\n\t`   反引号\n\t*   星号\n\t_   底线\n\t{}  花括号\n\t[]  方括号\n\t()  括弧\n\t#   井字号\n\t+   加号\n\t-   减号\n\t.   英文句点\n\t!   惊叹号\n\n自动链接\n处理短链接用&lt;&gt;括住能够自动转换成链接\nwww.baidu.com\nwxj5658@hotmai.com\n编辑软件\nwindows平台\n\nMarkdownpad\n\n激活：\n\t注册信息\n\n\t邮箱地址：\n\n\tSoar360@live.com\n\n\t授权密钥：\n\tGBPduHjWfJU1mZqcPM3BikjYKF6xKhlKIys3i1MU2eJHqWGImDHzWdD6xhMNLGVpbP2M5SN6bnxn2kSE8qHqNY5QaaRxmO3YSMHxlv2EYpjdwLcPwfeTG7kUdnhKE0vVy4RidP6Y2wZ0q74f47fzsZo45JE2hfQBFi2O9Jldjp1mW8HUpTtLA2a5/sQytXJUQl/QKO0jUQY4pa5CCx20sV1ClOTZtAGngSOJtIOFXK599sBr5aIEFyH0K7H4BoNMiiDMnxt1rD8Vb/ikJdhGMMQr0R4B+L3nWU97eaVPTRKfWGDE8/eAgKzpGwrQQoDh+nzX1xoVQ8NAuH+s4UcSeQ==\n\n在windows 10 系统下，windows10 MarkdownPad html会产生一个 渲染错误 awesomium（ This view has crashed ），此时就需要下载一个 HTML UI ENGINE（awesomium_v1.6.6_sdk_win）去解决该错误，该组件的下载地址：\nmarkdownpad.com/download/awesomium_v1.6.6_sdk_win.exe"},"Linux/docker-+-TensorFlow2.0-+-jupyter-lab":{"slug":"Linux/docker-+-TensorFlow2.0-+-jupyter-lab","filePath":"Linux/docker + TensorFlow2.0 + jupyter lab.md","title":"docker + TensorFlow2.0 + jupyter lab","links":[],"tags":["docker","TensorFlow2","jupyter"],"content":"服务器为 Ubuntu\n首先当然需要有Docker\n安装docker tensorflow jupyter\n参考tensorflow安装教程\n输入一下命令：\n$ sudo docker run -it -p 8888:8888 tensorflow/tensorflow:nightly-py3-jupyter\r\nUnable to find image &#039;tensorflow/tensorflow:nightly-py3-jupyter&#039; locally\r\nnightly-py3-jupyter: Pulling from tensorflow/tensorflow\r\n5bed26d33875: Pull complete\r\nf11b29a9c730: Pull complete\r\n930bda195c84: Pull complete\r\n78bf9a5ad49e: Pull complete\r\n84227541b6bb: Pull complete\r\n4f31c9672ae8: Pull complete\r\n3ab00ff69975: Pull complete\r\nba1f9a4960a4: Pull complete\r\n1e2e9ebd327e: Extracting [==================================================&gt;]  590.2MB/590.2MB\r\n1e2e9ebd327e: Pull complete\r\n5dbeace46811: Pull complete\r\n236b4193378a: Pull complete\r\n94c8012aaf41: Pull complete\r\n8978cb54431f: Pull complete\r\na32047d10082: Pull complete\r\n4bfab90cc021: Pull complete\r\ndb51178e67ae: Pull complete\r\nc582e0693d6e: Pull complete\r\nd9275e9db168: Pull complete\r\n48835526d3e2: Pull complete\r\n5287adc8a8f2: Pull complete\r\n525c81ec54db: Pull complete\r\n0161c3804581: Pull complete\r\nebfa1948cb3e: Pull complete\r\n933aa5af1920: Pull complete\r\na600cede3739: Pull complete\r\n1d8aeacd0c4f: Pull complete\r\n683a7567eeea: Pull complete\r\nDigest: sha256:0b59d7826a07049f013c171bb96c26c0d4a6222f856e6520a6cd70d7be5ecdec\r\nStatus: Downloaded newer image for tensorflow/tensorflow:nightly-py3-jupyter\r\n\r\n[I 07:24:55.849 NotebookApp] Writing notebook server cookie secret to /root/.local/share/jupyter/runtime/notebook_cookie_secret\r\njupyter_http_over_ws extension initialized. Listening on /http_over_websocket\r\n[I 07:24:56.134 NotebookApp] Serving notebooks from local directory: /tf\r\n[I 07:24:56.134 NotebookApp] The Jupyter Notebook is running at:\r\n[I 07:24:56.134 NotebookApp] http://1d6d334faf94:8888/?token=ec98c98ec99956afa3a078ce9ae5e34f11a88b6bf92f0ac8\r\n[I 07:24:56.134 NotebookApp]  or http://127.0.0.1:8888/?token=ec98c98ec99956afa3a078ce9ae5e34f11a88b6bf92f0ac8\r\n[I 07:24:56.134 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\r\n[C 07:24:56.138 NotebookApp]\r\n\r\n    To access the notebook, open this file in a browser:\r\n        file:///root/.local/share/jupyter/runtime/nbserver-1-open.html\r\n    Or copy and paste one of these URLs:\r\n        http://1d6d334faf94:8888/?token=ec98c98ec99956afa3a078ce9ae5e34f11a88b6bf92f0ac8\r\n     or http://127.0.0.1:8888/?token=ec98c98ec99956afa3a078ce9ae5e34f11a88b6bf92f0ac8\n\n等待一会\n安装并成功启动了jupyter notebook，在浏览器中打开提示的网址：http://127.0.0.1:8888/?token=…\n按住 ctrl + c 退出\n接下来是重点\n安装jupyter lab\n查看docker镜像\n$ sudo docker images\r\nREPOSITORY                                         TAG                   IMAGE ID            CREATED             SIZE\r\ntensorflow/tensorflow                              nightly-py3-jupyter   37b1af999efc        4 months ago        2.42GB\n\n先删除刚启动的容器\nsudo docker stop 37b1af999efc  #关闭\r\nsudo docker container rm 37b1af999efc  #删除\n\n在本机上新建一个文件夹，和一个json文件\n$ mkdir jupyter_config\r\n$ cd jupyter_config/\r\n~/jupyter_config$ vim config.json\n\n这个文件内容是jupyter-lab的配置信息\n{\r\n    &quot;NotebookApp&quot;:{\r\n            &quot;ip&quot;:&quot;*&quot;,\r\n            &quot;port&quot;:8888,\r\n            &quot;password&quot;:&quot;&quot;,\r\n            &quot;open_browser&quot;:false,\r\n            &quot;token&quot;:&quot;&quot;,\r\n            &quot;allow_root&quot;:true\r\n    }\r\n}\n\n在docker中创建挂载jupyter_config文件夹的容器\n$ sudo docker run -itd -p 8888:8888 -v ~/【主机的目录】:/home/user【容器的目录】 tensorflow/tenso\r\nrflow:nightly-py3-jupyter bash\r\n$ sudo docker ps\r\nCONTAINER ID        IMAGE                                       COMMAND                  CREATED              STATUS\r\n          PORTS                         NAMES\r\ne72acb760f91        tensorflow/tensorflow:nightly-py3-jupyter   &quot;bash&quot;                   About a minute ago   Up About a\n\n进入该容器\n$ sudo docker exec -it e72acb760f91 bash\r\n\r\n________                               _______________\r\n___  __/__________________________________  ____/__  /________      __\r\n__  /  _  _ \\_  __ \\_  ___/  __ \\_  ___/_  /_   __  /_  __ \\_ | /| / /\r\n_  /   /  __/  / / /(__  )/ /_/ /  /   _  __/   _  / / /_/ /_ |/ |/ /\r\n/_/    \\___//_/ /_//____/ \\____//_/    /_/      /_/  \\____/____/|__/\n\n在容器中安装jupyter lab\nroot@e72acb760f91:/tf# pip3 install jupyterlab\n\n切换到我们挂载的jupyter_config文件夹去\nroot@e72acb760f91:/tf# cd /home/user/\r\nroot@e72acb760f91:/home/user# ls\r\njupyter_config\n\n启动jupyter lab\n# jupyter-lab --config ./jupyter_config/config.json\r\n[I 08:44:59.650 LabApp] Writing notebook server cookie secret to /root/.local/share/jupyter/runtime/notebook_cookie_secret\r\n[W 08:44:59.906 LabApp] WARNING: The notebook server is listening on all IP addresses and not using encryption. This is not recommended.\r\n[W 08:44:59.906 LabApp] WARNING: The notebook server is listening on all IP addresses and not using authentication. This is highly insecure and not recommended.\r\njupyter_http_over_ws extension initialized. Listening on /http_over_websocket\r\n[I 08:44:59.917 LabApp] JupyterLab extension loaded from /usr/local/lib/python3.6/dist-packages/jupyterlab\r\n[I 08:44:59.917 LabApp] JupyterLab application directory is /usr/local/share/jupyter/lab\r\n[I 08:44:59.920 LabApp] Serving notebooks from local directory: /home/xujie\r\n[I 08:44:59.920 LabApp] The Jupyter Notebook is running at:\r\n[I 08:44:59.920 LabApp] http://e72acb760f91:8888/\r\n[I 08:44:59.920 LabApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\n\n用浏览器打开\nhttp:// docker IP :8888/\n\nip查看方式：\n$ ifconfig\r\n\r\ndocker0   Link encap:Ethernet  HWaddr 02:42:52:46:56:2f\r\n          inet addr:172.17.0.1  Bcast:172.17.255.255  Mask:255.255.0.0\r\n          inet6 addr: fe80::42:52ff:fe46:562f/64 Scope:Link\r\n          UP BROADCAST RUNNING MULTICAST  MTU:1500  Metric:1\r\n          RX packets:13461 errors:0 dropped:0 overruns:0 frame:0\r\n          TX packets:17994 errors:0 dropped:0 overruns:0 carrier:0\r\n          collisions:0 txqueuelen:0\r\n          RX bytes:20897178 (20.8 MB)  TX bytes:114945170 (114.9 MB)\n\n完结，开始工作。"},"Linux/gRPC":{"slug":"Linux/gRPC","filePath":"Linux/gRPC.md","title":"gRPC","links":[],"tags":[],"content":"什么是RPC\n参考知乎 谁能用通俗的语言解释一下什么是 RPC 框架？\n之前只知道IPC，是指进程间通信(Inter Process Communication)，至少两个进程或线程间传送数据或信号的一些技术或方法，RPC和IPC类似，百科解释RPC（Remote Procedure Call）为远程过程调用，简单理解为，就是像调用本地函数一样调用远程函数，例如，有两台服务器A,B，一个应用部署在A服务器上，想要调用B服务器上的函数和方法，由于不在一个内存空间，不能直接调用，需要通过网络来表达调用的语义和传达调用的数据。\nRPC原理\n首先客户端需要告诉服务器，需要调用的函数，这里函数和进程ID存在一个映射，客户端远程调用时，需要查一下函数，找到对应的ID，然后执行函数的代码。\n客户端需要把本地参数传给远程函数，本地调用的过程中，直接压栈即可，但是在远程调用过程中不再同一个内存里，无法直接传递函数的参数，因此需要客户端把参数转换成字节流，传给服务端，然后服务端将字节流转换成自身能读取的格式，是一个序列化和反序列化的过程。\n数据准备好了之后，网络传输层需要把调用的ID和序列化后的参数传给服务端，然后把计算好的结果序列化传给客户端，因此TCP层即可完成上述过程，gRPC中采用的是HTTP2协议。\n\nRPC框架对比\nDubbo 是阿里巴巴公司开源的一个Java高性能优秀的服务框架，使得应用可通过高性能的RPC 实现服务的输出和输入功能，可以和Spring框架无缝集成。不过，略有遗憾的是，据说在淘宝内部，dubbo由于跟淘宝另一个类似的框架HSF（非开源）有竞争关系，导致dubbo团队已经解散，反到是当当网的扩展版本Dubbox仍在持续发展，墙内开花墙外香。Dubbox和Dubbo本质上没有区别，名字的含义扩展了Dubbo而已，以下扩展出来的功能，也是选择Dubbox很重要的考察点。\nMotan 是新浪微博开源的一个Java框架。它诞生的比较晚，起于2013年，2016年5月开源。Motan 在微博平台中已经广泛应用，每天为数百个服务完成近千亿次的调用。与Dubbo相比，Motan在功能方面并没有那么全面，也没有实现特别多的扩展。用的人比较少，功能和稳定性有待观望。对跨语言调用支持较差，主要支持java。\nHessian 采用的是二进制RPC协议，适用于发送二进制数据。但本身也是一个Web Service框架对RPC调用提供支持，功能简单，使用起来也方便。基于Http协议进行传输。通过Servlet提供远程服务。通过Hessain本身提供的API来发起请求。响应端根据Hessian提供的API来接受请求。\nrpcx 是Go语言生态圈的Dubbo， 比Dubbo更轻量，实现了Dubbo的许多特性，借助于Go语言优秀的并发特性和简洁语法，可以使用较少的代码实现分布式的RPC服务。\ngRPC 是Google开发的高性能、通用的开源RPC框架，其由Google主要面向移动应用开发并基于HTTP/2协议标准而设计，基于ProtoBuf(Protocol Buffers)序列化协议开发，且支持众多开发语言。本身它不是分布式的，所以要实现上面的框架的功能需要进一步的开发。\nthrift 是Apache的一个跨语言的高性能的服务框架，也得到了广泛的应用。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n功能HessianMontanrpcxgRPCThriftDubboDubboxSpring Cloud开发语言跨语言JavaGo跨语言跨语言JavaJavaJava分布式(服务治理)×√√××√√√多序列化框架支持hessian√(支持Hessian2、Json,可扩展)√× 只支持protobuf)×(thrift格式)√√√多种注册中心×√√××√√√管理中心×√√××√√√跨编程语言√×(支持php client和C server)×√√×××支持REST××××××√√关注度低中低中中中高中上手难度低低中中中低低中运维成本低中中中低中中中开源机构CauchoWeiboApacheGoogleApacheAlibabaDangdangApache\n参考\ngRPC简介\ngrpc.io/\ngRPC是可以在任何环境中运行的现代开源高性能RPC框架。它可以通过可插拔的支持来有效地连接数据中心内和跨数据中心的服务，以实现负载平衡，跟踪，运行状况检查和身份验证。它也适用于分布式计算的最后一英里，以将设备，移动应用程序和浏览器连接到后端服务。\n在gRPC中，客户端应用程序可以直接在其他计算机上的服务器应用程序上调用方法，就好像它是本地对象一样，从而使您更轻松地创建分布式应用程序和服务。与许多RPC系统一样，gRPC围绕定义服务的思想，指定可通过其参数和返回类型远程调用的方法。在服务器端，服务器实现此接口并运行gRPC服务器以处理客户端调用。在客户端，客户端具有一个存根（在某些语言中仅称为客户端），提供与服务器相同的方法。\n\n从Google内部的服务器到您自己的台式机，gRPC客户端和服务器都可以在各种环境中运行并相互通信，并且可以使用gRPC支持的任何语言编写。因此，例如，您可以使用Go，Python或Ruby的客户端轻松地用Java创建gRPC服务器。此外，最新的Google API的接口将具有gRPC版本，可让您轻松地在应用程序中内置Google功能。\nProtobuf\n在写gPRC前还需要知道Protobuf，Protocol Buffer的翻译为协议缓冲区， 是Google的与语言无关，与平台无关，可扩展的机制，用于对结构化数据进行序列化（例如XML），但更小，更快，更简单。您定义要一次构造数据的方式，然后可以使用生成的特殊源代码轻松地使用各种语言在各种数据流中写入和读取结构化数据。\n看一个非常简单的例子。假设要定义一个搜索请求消息格式，其中每个搜索请求都有一个查询字符串，您感兴趣的特定结果页面以及每页结果数量。这是.proto用于定义消息类型的文件。\nsyntax = &quot;proto3&quot;;\n\nmessage SearchRequest {\n  string query = 1;\n  int32 page_number = 2;\n  int32 result_per_page = 3;\n}\n\n\n文件的第一行指定正在使用proto3语法：如果不这样做，则协议缓冲区编译器将假定您正在使用proto2。这必须是文件的第一行，非空，非注释行。\n\n\n所述SearchRequest消息定义指定了三个字段（名称/值对），一个用于每条数据要在此类型的消息包括。每个字段都有一个名称和类型。\n\n.proto文件最终生成什么\n当你使用protoc来编译一个.proto文件的时候，编译器将利用你在文件中定义的类型生成你打算使用的语言的代码文件。生成的代码包括getting setting 接口和序列化，反序列化接口。\n\n对于C ++，编译器会从每个.proto文件生成一个.h和一个.cc文件，并为您文件中描述的每种消息类型提供一个类。\n对于Java，编译器生成一个.java文件，其中包含每种消息类型的类，以及Builder用于创建消息类实例的特殊类。\nPython有点不同 - Python编译器生成一个模块，其中包含每个消息类型的静态描述符，然后，用一个元类在运行时创建必要的Python数据访问类。\n对于Go，编译器会为.pb.go文件中的每种消息类型生成一个类型的文件。\n对于Ruby，编译器生成一个.rb包含消息类型的Ruby模块的文件。\n对于Objective-C，编译器从每个.proto文件生成一个pbobjc.h和一个pbobjc.m文件，其中包含文件中描述的每种消息类型的类。\n对于C＃，编译器会从每个.proto文件生成一个.cs文件，其中包含文件中描述的每种消息类型的类。\n\ngPRC的HelloWord\n记录使用python的gRPC\n安装gPRC和gRPC工具\n$ python -m pip install grpcio\n$ python -m pip install grpcio-tools\n\ngRPC工具包括协议缓冲区编译器protoc和用于根据.proto服务定义生成服务器和客户端代码的特殊插件。\n首先新建一个protos的文件夹编写helloworld.proto\n#protos\\helloworld.proto\nsyntax = &quot;proto3&quot;;\n\noption java_multiple_files = true;\noption java_package = &quot;io.grpc.examples.helloworld&quot;;\noption java_outer_classname = &quot;HelloWorldProto&quot;;\noption objc_class_prefix = &quot;HLW&quot;;\n\npackage helloworld;\n\n// The greeting service definition.\nservice Greeter {\n  // Sends a greeting\n  rpc SayHello (HelloRequest) returns (HelloReply) {}\n}\n\n// The request message containing the user&#039;s name.\nmessage HelloRequest {\n  string name = 1;\n}\n\n// The response message containing the greetings\nmessage HelloReply {\n  string message = 1;\n}\n\n接下来，我们需要生成应用程序使用的gRPC代码。\n$ python -m grpc_tools.protoc -I./protos --python_out=. --grpc_python_out=. ./protos/helloworld.proto\n\n目录下会生成两个文件\n\nhelloworld_pb2.py\n\n\nhelloworld_pb2_grpc.py\n\n然后我们就可以调用这两个文件的生成的接口了，\n在Server端定义接口的实现\n# greeter_server.py\n&quot;&quot;&quot;The Python implementation of the GRPC helloworld.Greeter server.&quot;&quot;&quot;\n\nfrom concurrent import futures\nimport logging\n\nimport grpc\n\nimport helloworld_pb2\nimport helloworld_pb2_grpc\n\n\nclass Greeter(helloworld_pb2_grpc.GreeterServicer):\n\n    def SayHello(self, request, context):\n        return helloworld_pb2.HelloReply(message=&#039;Hello, %s!&#039; % request.name)\n\n\ndef serve():\n    server = grpc.server(futures.ThreadPoolExecutor(max_workers=10))\n    helloworld_pb2_grpc.add_GreeterServicer_to_server(Greeter(), server)\n    server.add_insecure_port(&#039;[::]:50051&#039;)\n    server.start()\n    server.wait_for_termination()\n\n\nif __name__ == &#039;__main__&#039;:\n    logging.basicConfig()\n    serve()\n\nClient端调用远程方法\n# greeter_client.py\n&quot;&quot;&quot;The Python implementation of the GRPC helloworld.Greeter client.&quot;&quot;&quot;\n\nfrom __future__ import print_function\nimport logging\n\nimport grpc\n\nimport helloworld_pb2\nimport helloworld_pb2_grpc\n\n\ndef run():\n    # NOTE(gRPC Python Team): .close() is possible on a channel and should be\n    # used in circumstances in which the with statement does not fit the needs\n    # of the code.\n    with grpc.insecure_channel(&#039;localhost:50051&#039;) as channel:\n        stub = helloworld_pb2_grpc.GreeterStub(channel)\n        response = stub.SayHello(helloworld_pb2.HelloRequest(name=&#039;you&#039;))\n    print(&quot;Greeter client received: &quot; + response.message)\n\n\nif __name__ == &#039;__main__&#039;:\n    logging.basicConfig()\n    run()\n\n首先启动server\n$ python greeter_server.py\n\n运行client\n$ python greeter_client.py\n\n输出：\nGreeter client received: Hello, you!\n\n跨语言调用\ngRPC是支持不同语言调用的，C#写一个Server，用python的Client调用。\n首先用Visual Studio新建一个工程，添加Greeter和GreeterServer两个项目\n\nGreeter添加Grpc的NuGet包\n\nGoggle.Protobuf\n\n\nGrpc\n\n\nGrpc.Tools\n\n选择.proto的properties，build改为Protobuf\n\n现在选择Greeter项目，生成会build出 Helloworld.cs 和 HelloworldGrpc.cs两个文件。\n详情\n编写GreeterServer的Program.cs，记得要引用Greeter\n// GreeterServer/Program.cs\nusing System;\nusing System.Threading.Tasks;\nusing Grpc.Core;\nusing Helloworld;\n \nnamespace GreeterServer\n{\n    class GreeterImpl : Greeter.GreeterBase\n    {\n        // Server side handler of the SayHello RPC\n        public override Task&lt;HelloReply&gt; SayHello(HelloRequest request, ServerCallContext context)\n        {\n            return Task.FromResult(new HelloReply { Message = &quot;Hello &quot; + request.Name });\n        }\n}\n \n    class Program\n    {\n        const int Port = 50051;\n \n        public static void Main(string[] args)\n        {\n            Server server = new Server\n            {\n                Services = { Greeter.BindService(new GreeterImpl()) },\n                Ports = { new ServerPort(&quot;localhost&quot;, Port, ServerCredentials.Insecure) }\n            };\n            server.Start();\n \n            Console.WriteLine(&quot;Greeter server listening on port &quot; + Port);\n            Console.WriteLine(&quot;Press any key to stop the server...&quot;);\n            Console.ReadKey();\n \n            server.ShutdownAsync().Wait();\n        }\n    }\n}\n然后就可以构建并执行C#的Server了\n&gt; dotnet build Greeter.sln\n&gt; cd GreeterServer\\\n&gt; dotnet run\nGreeter server listening on port 51062\nPress any key to stop the server...\n\n再执行之前用python写的client\npython greeter_client.py\nGreeter client received: Hello you\n"},"Linux/site-packages文件夹位置添加到Anaconda":{"slug":"Linux/site-packages文件夹位置添加到Anaconda","filePath":"Linux/site-packages文件夹位置添加到Anaconda.md","title":"site-packages文件夹位置添加到Anaconda","links":[],"tags":[],"content":"要将新的site-packages文件夹位置添加到Anaconda Python中，可以按照以下步骤操作：\n\n打开你的终端或命令提示符（Windows）。\n输入conda info --envs以查看当前存在的环境及其安装路径。\n选择您想要添加新site-packages的环境并激活它。例如，使用以下命令激活名为myenv的环境：\n\n复制代码\nconda activate myenv\n\n在激活的环境中，输入以下命令，其中”path/to/new/site-packages”是您要添加的新目录的路径：\n\n复制代码\nconda develop path/to/new/site-packages\n\n确认是否成功添加了新的site-packages，在终端或命令提示符窗口中输入以下命令：\n\n复制代码\nconda list\n\n如果你想删除已经添加的site-packages，则可以使用以下命令：\n\n复制代码\nconda develop --uninstall path/to/new/site-packages\n注意：以上命令假设您已经安装了conda，并且已经在系统PATH中设置了conda。如果没有，请确保先安装conda并将其添加到PATH中，然后再尝试上述步骤。"},"Linux/ssh-映射":{"slug":"Linux/ssh-映射","filePath":"Linux/ssh 映射.md","title":"ssh 映射","links":[],"tags":[],"content":"把本地映射到远程\nssh -f -N -T -R 8017:127.0.0.1:8027 user@182.92.100.235\n把远程映射到本地\nssh -NfL 127.0.0.1:27017:127.0.0.1:27017 user@182.92.100.235\n利用SSH反向代理登录内网主机\n解决问题：内网主机没有公网IP，无法从外网登录\n\n1. 中转服务器设置\n\n\n关闭对应端口的防火墙\nufw allow [remote_port]\n\n\n设置ssh配置文件\nvim /etc/ssh/sshd_configGatewayPorts yes\n\n\n重启ssh服务\nsystemctl restart ssh\n\n\n2. 内网主机ssh连接中转服务器\nssh -f -N -T -R [remote_port]:localhost:22 [jump_server_user]@[jump_server_ip]\n这个 SSH 命令用于在内网主机和中转服务器之间创建反向代理隧道。下面是命令参数的解释：\n\n\n-f：此选项告诉 SSH 在建立连接后将自身放入后台运行。即使你关闭终端，SSH 仍会继续运行。\n\n\n-N：这个选项告诉 SSH 不要执行远程命令。因为我们只需要创建一个端口转发，而不需要在远程主机上执行命令，所以使用 -N。\n\n\n-T：禁用伪终端分配。因为我们只关心端口转发，所以不需要分配一个伪终端。\n\n\n-R [remote_port]:localhost:22：这是 SSH 反向代理的关键选项。-R 表示反向代理，[remote_port] 是中转服务器上用于监听请求的端口，localhost:22 表示内网主机的 SSH 服务地址。将流量从中转服务器的 [remote_port] 转发到内网主机的端口 22（SSH 服务默认端口）。\n\n\n[jump_server_user]@[jump_server_ip]：中转服务器的用户名和 IP 地址或域名。\n\n\n此命令将在内网主机和中转服务器之间建立一个 SSH 连接，并在连接中创建一个反向代理。外部客户端可以通过连接到中转服务器的 [remote_port] 来访问内网主机上的 SSH 服务。这种方法通常用于内网穿透，让外部客户端可以访问内网服务。\n3. 外网客户端连接内网主机\nssh -p [remote_port] [user_on_internal_host]@[jump_server_ip]\n4. 公钥登录（安全）\n中转服务器和内网主机都要设置\n\n\n客户端创建公钥（如果没有）(客户端)\nssh-keygen\n\n\n将公钥传到服务端主机的目标用户上\n\n\n手动复制 （服务端）\nchmod 700 ~/.ssh\ntouch ~/.ssh/authorized_keys\nchmod 600 ~/.ssh/authorized_keys\nvim ~/.ssh/authorized_keys\n将公钥内容粘贴到 authorized_keys 文件中并保存\n\n\nssh-copy-id （客户端）\nssh-copy-id -i ~/.ssh/id_rsa.pub [user]@[server_ip]mkdir -p ~/.ssh\n\n\n修改ssh配置文件 （服务端）\nvim /etc/ssh/sshd_configPubkeyAuthentication yes # 启用公钥认证\nPasswordAuthentication no # 关闭密码认证，谨慎操作！\n\n\n重启ssh服务 （服务端）\nsystemctl restart ssh\n\n\n5. 设置自动重连（稳定）\n\n\n查看ssh隧道进程是否运行（中转服务器）\nlsof -i :[remote_port]\n\n\n结束之前的进程（中转服务器）\nkill [pid_of_ssh_tunnel]\n\n\nautossh自动重连\nautossh -M 0 -f -N -T -q -o “ServerAliveInterval 60” -o “ServerAliveCountMax 3” -R [remote_port]:localhost:22 [jump_server_user]@[jump_server_ip]\n\n\nautossh命令的参数如下：\n\n\n-M 0：这个选项用于设置控制连接的监视端口。0 表示不启用控制连接。在这种配置下，autossh 仅依赖于 SSH 本身的 ServerAlive 机制（见后文）来检测连接状态。\n\n\n-f：此选项告诉 autossh 在建立连接后将自身放入后台运行。即使你关闭终端，autossh 仍会继续运行。\n\n\n-N：这个选项告诉 SSH 不要执行远程命令。因为我们只需要创建一个端口转发，而不需要在远程主机上执行命令，所以使用 -N。\n\n\n-T：禁用伪终端分配。因为我们只关心端口转发，所以不需要分配一个伪终端。\n\n\n-q：这个选项告诉 autossh 以安静模式运行。在这种模式下，autossh 不会输出任何信息，除非发生错误。\n\n\n-o &quot;ServerAliveInterval 60&quot;：这个选项设置了 SSH 客户端向服务器发送空闲数据包的频率，以保持连接活跃。在这里，我们设置了 60 秒发送一次。如果在指定时间内没有其他数据包通过连接，客户端会自动发送一个数据包。\n\n\n-o &quot;ServerAliveCountMax 3&quot;：这个选项设置了在断开连接前，允许连续丢失的空闲数据包的最大数量。在这里，我们设置了 3 次。如果连续 3 次空闲数据包没有收到服务器的响应，SSH 客户端将认为连接已断开，并自动重连。\n\n\n-R [remote_port]:localhost:22：这是 SSH 反向代理的关键选项。-R 表示反向代理，[remote_port] 是中转服务器上用于监听请求的端口，localhost:22 表示内网主机的 SSH 服务地址。将流量从中转服务器的 [remote_port] 转发到内网主机的端口 22（SSH 服务默认端口）。\n\n\n[jump_server_user]@[jump_server_ip]：中转服务器的用户名和 IP 地址或域名。\n\n"},"Linux/tmux-常用命令":{"slug":"Linux/tmux-常用命令","filePath":"Linux/tmux 常用命令.md","title":"tmux 常用命令","links":[],"tags":[],"content":"创建和管理会话\n# 创建一个新会话并命名\ntmux new -s name\n \n# 查看所有会话\ntmux ls\n \n# 进入一个已存在的会话\ntmux attach -t name\n \n# 从某个会话分离\nctrl+b d\n \n# 重命名当前会话\nctrl+b $\n \n# 杀死指定会话\ntmux kill-session -t name\n\n窗口管理\n# 创建一个新窗口\nctrl+b c\n \n# 水平分割窗口\nctrl+b %\n \n# 垂直分割窗口\nctrl+b &quot;\n \n# 切换窗口（按数字）\nctrl+b 0~9\n \n# 关闭当前窗口\nctrl+b &amp;\n \n# 切换到上/下一个窗口\nctrl+b n （下一个）\nctrl+b p （上一个）\n\n窗格管理\n# 在窗格之间切换\nctrl+b 方向键 ⬆️⬇️⬅️➡️\n \n# 调整窗格大小\nctrl+b : resize-pane -U  # 向上缩小\nctrl+b : resize-pane -D  # 向下缩小\nctrl+b : resize-pane -L  # 向左缩小\nctrl+b : resize-pane -R  # 向右缩小\n \n# 同步所有窗格\nctrl+b : setw synchronize-panes on\n \n# 取消同步所有窗格\nctrl+b : setw synchronize-panes off\n \n# 关闭当前窗格\nctrl+b x\n\n会话和窗口状态\n# 查看所有窗口和会话的布局\nctrl+b w\n \n# 刷新界面\nctrl+b r\n\n滚动和复制模式\n# 启用滚动模式\nctrl+b [\n \n# 退出滚动模式\nq\n \n# 启用复制模式\nctrl+b [  # 进入模式后可用上下键滚动\n \n# 开始复制\n空格键\n \n# 结束复制并保存到剪贴板\n回车键"},"Linux/windows-docker":{"slug":"Linux/windows-docker","filePath":"Linux/windows docker.md","title":"windows docker","links":[],"tags":[],"content":"services:\nwindows:\nimage: dockurr/windows\ncontainer_name: windows\nenvironment:\nVERSION: “win11”\nDISK_SIZE: “512G”\nRAM_SIZE: “32G”\nCPU_CORES: “8”\ndevices:\n- /dev/kvm\ncap_add:\n- NET_ADMIN\nports:\n- 8006:8006\n- 3389:3389/tcp\n- 3389:3389/udp\nstop_grace_period: 2m\nvolumes:\n\n~/win/data:/storage\n~:/data\n"},"Linux/博客搭建笔记":{"slug":"Linux/博客搭建笔记","filePath":"Linux/博客搭建笔记.md","title":"博客搭建笔记","links":[],"tags":["Ubuntu","hexo","github","node.js"],"content":"搭建环境\n这次尝试在Ubuntu环境下搭建github+hexo博客\n软件需要以下四个:\nUbuntu 16.04\nNode.js\nHexo\nGitHub\nUbuntu安装\n略\nNode环境安装\nHexo博客系统是静态网页的形似，依赖Node.js，简单的说 Node.js 就是运行在服务端的 JavaScript。Node.js 是一个基于Chrome JavaScript 运行时建立的一个平台。（其实我并不懂这个玩意）只是Hexo需要使用npm安装，npm是依托于node的安装软件管理系统\n方法一\nWindowns下直接下载安装，Ubuntu 我刚开始使用了apt-get install结果装完后版本过低使后面的搭建过程接连出错，直接使用编译好的文件安装，首先官网下载最新tar包然后链接为全局\ntar  xf node-v5.10.1-linux-x64.tar.gz -C /usr/local/\ncd /usr/local/\nmv node-v5.10.1-linux-x64/ nodejs\nln -s /usr/local/nodejs/bin/node /usr/local/bin\nln -s /usr/local/nodejs/bin/npm /usr/local/bin\n\n方法二\n源码安装\n参考了菜鸟教程\nNode.js 源码安装\n以下部分我们将介绍在Ubuntu Linux下安装 Node.js 。 其他的Linux系统，如Centos等类似如下安装步骤。\n在 Github 上获取 Node.js 源码：\n$ sudo git clone github.com/nodejs/node.git\nCloning into &#039;node&#039;...\n修改目录权限：\n$ sudo chmod -R 755 node\n使用 ./configure 创建编译文件，并按照：\n$ cd node\n$ sudo ./configure\n$ sudo make\n$ sudo make install\n查看 node 版本：\n \n$ node --version\nv10.0.0-pre\n$npm -v\n5.6.0\n\n源码编译的时间比我想象中的长啊\n方法三\nnodesource\n# Using Ubuntu\ncurl -sL deb.nodesource.com/setup_11.x | sudo -E bash -\nsudo apt-get install -y nodejs\n\n注册一个GitHub账号\n关于Git的学习使看了廖雪峰老师的博客，好久之前看的都忘了，哎\nwww.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000\nGithub账户注册和新建项目，项目必须要遵守格式：账户名.github.io，不然接下来会有很多麻烦。并且需要勾选Initialize this repository with a README\n在建好的项目右侧有个settings按钮，点击它，向下拉到GitHub Pages，你会看到那边有个网址，访问它，你将会惊奇的发现该项目已经被部署到网络上，能够通过外网来访问它。\nHexo安装\n找个合适的地方\nsudo npm npm install hexo-cli -g\n\n国内上npm很慢或失败，尝试淘宝源的cnpm\nnpm.taobao.org/\n\n输入hexo -v检查hexo是否安装成功\n输入hexo init,初始化项目，npm国外的源有点慢啊，可以修改使用淘宝源\n输入npm install， 安装所有组件\n输入hexo g，创建静态网页\n输入hexo s，开启服务器，访问http://localhost:4000\n中途出现了好多次error，除了要加sudo，还有一些奇怪的错误，但是重复了几遍就只剩warn了。。。\n\n总结 使用最新的软件和节点能减少出错几率\n\n将Hexo与Github page联系起来，设置Git的user name和email（如果是第一次的话）\n\ngit方面另写一篇博客\n\n测试：\n在终端 ssh -T git@github.com\n\nHi Voidmort! You&#039;ve successfully authenticated, but GitHub does not provide shell access.\n\n成功！\n\n配置Deployment，在其文件夹中，找到_config.yml文件，修改repo值（在末尾）\n# Deployment\n## Docs: hexo.io/docs/deployment.html\ndeploy:\n  type: git\n  repository: git@github.com:Voidmort/Voidmort.github.io.git\n  branch: master\n\nrepo值是github项目里的ssh（右下角）\n部署到git之前要装一个扩展：\n\nnpm install hexo-deployer-git —save\n\nhexo指令\nhexo n &quot;我的博客&quot; == hexo new &quot;我的博客&quot; #新建文章\nhexo p == hexo publish\nhexo g == hexo generate#生成\nhexo s == hexo server #启动服务预览\nhexo d == hexo deploy#部署\nhexo clean\n\n域名绑定\n我购买了阿里的域名，首先ping voidmort.github.io,查看IP地址然后直接在阿里域名管理里点新手引导写上IP地址，然后使用新域名登陆，发现上不去。。。\n在GitHub setting中找到Custom domain 写上刚购买的域名\n\nOK！ 博客搭建完成\n\nNext 主题晋级\n主题地址：\ntheme-next.iissnan.com\n搜索服务\n微搜索 由 lzlun129 贡献\nnpm install swig-templates\nTBD\nLocal Search 由 flashlab 贡献\n添加百度/谷歌/本地 自定义站点内容搜索\n安装 hexo-generator-searchdb，在站点的根目录下执行以下命令：\n$ npm install hexo-generator-searchdb —save\nProblem\n$ hexo d\nERROR Deployer not found: git\nnpm install —save hexo-deployer-git\nsearch:\npath: search.xml\nfield: post\nformat: html\nlimit: 10000\n编辑 主题配置文件，启用本地搜索功能：\nLocal search\nlocal_search:\nenable: true\nRSS：\n需要先安装 hexo-generator-feed 插件。\ngithub.com/hexojs/hexo-generator-feed\nLive2D：\nwww.npmjs.com/package/hexo-helper-live2d"},"Linux/虚拟机搭建Hadoop实验":{"slug":"Linux/虚拟机搭建Hadoop实验","filePath":"Linux/虚拟机搭建Hadoop实验.md","title":"虚拟机搭建Hadoop实验","links":[],"tags":["Hadoop","BigData"],"content":"在学习大数据的过程中搭建由三台虚拟机构成的Hadoop模型\n\n创建三台虚拟机\n使用软件：Vmware12\nCentos7最小镜像\n安装一台虚拟机，配置好网络与jdk，复制出两台同样的\n\n方法/步骤\n1.\n首先我们安装后centos7最小化系统后，并进入系统执行命令ifconfig，会发现系统提示命令未找到。具体展示效果如下图所示。\n\n2.\n然后输入命令查看本机是否分配IP,执行命令ip addr ，可以发现系统的网卡没有分配IP地址，在此我们需要记住本机网卡的名称，用于下一步使用，本篇中我们的网卡为：eno16777736。具体效果如下图所示。\n\n3.\n然后我们进入网卡配置文件的目录。执行命令 cd /etc/sysconfig/network-scripts/ 然后查看下面的网卡文件。具体效果如下图所示。\n\n4.\n然后我们找到对应的网卡文件执行命令 vi ifcfg-eno16777736。进行修改网卡文件，不同机器网卡不同，本篇以自己电脑为例展示。\n\n5.\n我们需要首先找到ONBOOT=no ，需要修改为ONBOOT=yes然后保存退出。\n\n6.\n然后执行命令 service network restart 重启网卡服务。具体操作如下图所示。\n\n7.\n执行完成后，我们再次执行命令 ip addr 查看是否分配到IP地址，可以看到已经分配到IP地址。具体操作如下图所示。\n\n8.\n然后我们执行命令yum provides ifconfig 查看哪个包提供了ifconfig命令，然后可以看到net-tools包提供ifconfig包， 具体操作如下图所示。\n\n9.\n然后我们执行命令安装net-tools包，执行命令：yum install net-tools。具体操作如下图所示。\n\n10.\n然后我们执行命令ifconfig，可以看到可以使用了，而且展示了系统的网卡信息。具体操作如下图所示。\n\n安装jdk：\n1.查看yum库中都有哪些jdk版本(暂时只发现了openjdk)\n[root@localhost ~]# yum search java|grep jdk\nldapjdk-javadoc.x86_64 : Javadoc for ldapjdk\njava-1.6.0-openjdk.x86_64 : OpenJDK Runtime Environment\njava-1.6.0-openjdk-demo.x86_64 : OpenJDK Demos\njava-1.6.0-openjdk-devel.x86_64 : OpenJDK Development Environment\njava-1.6.0-openjdk-javadoc.x86_64 : OpenJDK API Documentation\njava-1.6.0-openjdk-src.x86_64 : OpenJDK Source Bundle\njava-1.7.0-openjdk.x86_64 : OpenJDK Runtime Environment\njava-1.7.0-openjdk-demo.x86_64 : OpenJDK Demos\njava-1.7.0-openjdk-devel.x86_64 : OpenJDK Development Environment\njava-1.7.0-openjdk-javadoc.noarch : OpenJDK API Documentation\njava-1.7.0-openjdk-src.x86_64 : OpenJDK Source Bundle\njava-1.8.0-openjdk.x86_64 : OpenJDK Runtime Environment\njava-1.8.0-openjdk-demo.x86_64 : OpenJDK Demos\njava-1.8.0-openjdk-devel.x86_64 : OpenJDK Development Environment\njava-1.8.0-openjdk-headless.x86_64 : OpenJDK Runtime Environment\njava-1.8.0-openjdk-javadoc.noarch : OpenJDK API Documentation\njava-1.8.0-openjdk-src.x86_64 : OpenJDK Source Bundle\nldapjdk.x86_64 : The Mozilla LDAP Java SDK\n\n2.选择版本,进行安装\n//选择1.7版本进行安装\n[root@localhost ~]# yum install java-1.7.0-openjdk\n//安装完之后，默认的安装目录是在: /usr/lib/jvm/java-1.7.0-openjdk-1.7.0.75.x86_64\n\n3.设置环境变量 （如果已经有java命令不用设置）\n[root@localhost ~]# vi /etc/profile\n在profile文件中添加如下内容\n#set java environment\nJAVA_HOME=/usr/lib/jvm/java-1.7.0-openjdk-1.7.0.75.x86_64\nJRE_HOME=$JAVA_HOME/jre\nCLASS_PATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/lib\nPATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/bin\nexport JAVA_HOME JRE_HOME CLASS_PATH PATH\n\n4.让修改生效\n[root@localhost java]# source /etc/profile\n\n5.查看刚安装的Java版本信息。\n◆输入：java -version 可查看Java版本；\n\n配置hosts\nvi /etc/hosts\n\n说明：slaver217,slaver214作为datanode节点，master204作为namenode节点。另外，各datanode节点主机上只需配置如：172.16.51.214 slaver214。"},"Tensorflow/TensorFlow_1":{"slug":"Tensorflow/TensorFlow_1","filePath":"Tensorflow/TensorFlow_1.md","title":"TensorFlow 1","links":[],"tags":["Tensorflow"],"content":"Hello World\n当tensorflow的环境搭建好后我们就可以尝试run下了，先写个hello world看看吧。\nfrom __future__ import print_function\n \nimport tensorflow as tf\ntry:\n    tf.contrib.eager.enable_eager_execution()\nexcept ValueError:\n    pass\ntensor = tf.constant(&#039;Hello, world!&#039;)\ntensor_value = tensor.numpy()\nprint(tensor_value)\n \nb&#039;Hello, world!&#039;\n\nTensorFlow 编程概念\nTensorFlow的名称源自张量，张量是任意维度的数组。借助TensorFlow，可以操作具有很大维度的张量。\n标量是零维数组（零阶张量）。例如：&#039;hi&#039; 或 3\n矢量是一维数组（一阶张量）。例如：[2, 3, 5, 7, 11] 或 [3]\n矩阵是二维数组（二阶张量）。例如：[[3.1, 8.2, 5.9][4.3, -2.7, 6.5]]\n\nTensorFlow指令会创建，销毁和控制张量。典型TensorFlow程序中的大多数代码都是指令。\nTensorFlow图（也叫 计算图 或 数据流图）是一种图数据结构。很多TensorFlow程序由单个图构成，但是TensorFlow程序可以选择创建多个图。图的节点是指令；图的边是张量。张量流经图，在每个节点由一个指令操控。一个指令的输出张量通常会变成后续指令的输入张量。TensorFlow会实现延迟执行模型，意味着系统仅会根据相关节点的需求在需要时计算节点。\n张量可以作为常量或变量储存在图中。常量储存的是值是不会发生更改的张量，而变量储存的值是会发生更改的张量。常量和变量都只是图中的一种指令。常量是始终会返回同一张量值得指令。变量是会返回分配给得任何张量的指令。\n要定义常量，使用tf.constant指令，并传入它的值。例如：\nx = tf.constant([1.2])\n\n同样，可以创建变量：\ny = tf.Variable([3])\n\n改变值： y = y.assign([1])\n\n创建好变量或常量后，可以对它们使用其他指令（如tf.add）。\n图必须在TensorFlow会话中运行，会话储存了它所运行的图的状态：\n将 tf.Session()作为会话：\n    initialization = tf.global_variables_initializer()\n    print(y.eval())\n    \n\n在使用tf.Variable时可以调用tf.global_variables_initializer，以明确初始化这些变量。\n注意：会话可以将图分发到多个机器上执行（假设程序在某个分布式计算框架上运行）。\n总结\nTensorFlow编程有两个流程：\n1.将常量，变量和指令整合到一个图中。\n2.在一个会话中评估这些常量，变量和指令。\n\n创建一个简单的 TensorFlow 程序\n我们来看看如何编写一个将两个常量相加的简单 TensorFlow 程序。\n添加 import 语句\n想要运行tensorflow程序，必须添加这句：\nimport tensorflow as tf\n其他常见的import语句包括：\nimport matplotlib.pyplot as plt # 数据可视化\nimport numpy as np              # 较低级的数学python库\nimport pandas as pd             # 较高级的数学python库\n\nfrom __future__ import print_function\nimport tensorflow as tf\n \n# 创建一个图\ng = tf.Graph()\n \nwith g.as_default():\n    # 创建三个量，\n    x = tf.constant(8, name=&quot;x_const&quot;)\n    y = tf.constant(5, name=&quot;y_const&quot;)\n    sum = tf.add(x, y, name=&quot;x_y_sum&quot;)\n    python\n    # 创建一个会话，将会执行默认图\n    with tf.Session() as sess:\n        print(sum.eval())\nTF already imported with eager execution!\n13\n"},"Tensorflow/TensorFlow_10":{"slug":"Tensorflow/TensorFlow_10","filePath":"Tensorflow/TensorFlow_10.md","title":"TensorFlow 10","links":[],"tags":["Tensorflow"],"content":"稀疏数据和嵌入简介\n\n将影评字符串数据转换为稀疏特征矢量\n使用稀疏特征矢量实现情感分析线性模型\n通过将数据投射到二维空间的嵌入来实现情感分析 DNN 模型\n将嵌入可视化，以便查看模型学到的词语之间的关系\n\n在此练习中，我们将探讨稀疏数据，并使用影评文本数据（来自 ACL 2011 IMDB 数据集）进行嵌入。这些数据已被处理成 tf.Example 格式。\n设置\n我们导入依赖项并下载训练数据和测试数据。tf.keras 中包含一个文件下载和缓存工具，我们可以用它来检索数据集。\nfrom __future__ import print_function\n \nimport collections\nimport io\nimport math\n \nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom IPython import display\nfrom sklearn import metrics\n \ntf.logging.set_verbosity(tf.logging.ERROR)\ntrain_url = &#039;download.mlcc.google.cn/mledu-datasets/sparse-data-embedding/train.tfrecord&#039;\ntrain_path = tf.keras.utils.get_file(train_url.split(&#039;/&#039;)[-1], train_url)\ntest_url = &#039;download.mlcc.google.cn/mledu-datasets/sparse-data-embedding/test.tfrecord&#039;\ntest_path = tf.keras.utils.get_file(test_url.split(&#039;/&#039;)[-1], test_url)\nDownloading data from download.mlcc.google.cn/mledu-datasets/sparse-data-embedding/train.tfrecord\n41631744/41625533 [==============================] - 0s 0us/step\nDownloading data from download.mlcc.google.cn/mledu-datasets/sparse-data-embedding/test.tfrecord\n40689664/40688441 [==============================] - 0s 0us/step\n\n构建情感分析模型\n我们根据这些数据训练一个情感分析模型，以预测某条评价总体上是好评（标签为 1）还是差评（标签为 0）。\n为此，我们会使用词汇表（即我们预计将在数据中看到的每个术语的列表），将字符串值 terms 转换为特征矢量。在本练习中，我们创建了侧重于一组有限术语的小型词汇表。其中的大多数术语明确表示是好评或差评，但有些只是因为有趣而被添加进来。\n词汇表中的每个术语都与特征矢量中的一个坐标相对应。为了将样本的字符串值 terms 转换为这种矢量格式，我们按以下方式处理字符串值：如果该术语没有出现在样本字符串中，则坐标值将为 0；如果出现在样本字符串中，则值为 1。未出现在该词汇表中的样本中的术语将被弃用。\n注意：我们当然可以使用更大的词汇表，而且有创建此类词汇表的专用工具。此外，我们可以添加少量的 OOV（未收录词汇）分桶，您可以在其中对词汇表中未包含的术语进行哈希处理，而不仅仅是弃用这些术语。我们还可以使用__特征哈希__法对每个术语进行哈希处理，而不是创建显式词汇表。这在实践中很有效，但却不具备可解读性（这对本练习非常实用）。如需了解处理此类词汇表的工具，请参阅 tf.feature_column 模块。\n构建输入管道\n首先，我们来配置输入管道，以将数据导入 TensorFlow 模型中。我们可以使用以下函数来解析训练数据和测试数据（格式为 TFRecord），然后返回一个由特征和相应标签组成的字典。\ndef _parse_function(record):\n  &quot;&quot;&quot;Extracts features and labels.\n  \n  Args:\n    record: File path to a TFRecord file    \n  Returns:\n    A `tuple` `(labels, features)`:\n      features: A dict of tensors representing the features\n      labels: A tensor with the corresponding labels.\n  &quot;&quot;&quot;\n  features = {\n    &quot;terms&quot;: tf.VarLenFeature(dtype=tf.string), # terms are strings of varying lengths\n    &quot;labels&quot;: tf.FixedLenFeature(shape=[1], dtype=tf.float32) # labels are 0 or 1\n  }\n  \n  parsed_features = tf.parse_single_example(record, features)\n  \n  terms = parsed_features[&#039;terms&#039;].values\n  labels = parsed_features[&#039;labels&#039;]\n \n  return  {&#039;terms&#039;:terms}, labels\n为了确认函数是否能正常运行，我们为训练数据构建一个 TFRecordDataset，并使用上述函数将数据映射到特征和标签。\n# Create the Dataset object.\nds = tf.data.TFRecordDataset(train_path)\n# Map features and labels with the parse function.\nds = ds.map(_parse_function)\n \nds\n&lt;DatasetV1Adapter shapes: ({terms: (?,)}, (1,)), types: ({terms: tf.string}, tf.float32)&gt;\n\n运行以下单元，以从训练数据集中获取第一个样本。\nn = ds.make_one_shot_iterator().get_next()\nsess = tf.Session()\nsess.run(n)\n({&#039;terms&#039;: array([b&#039;but&#039;, b&#039;it&#039;, b&#039;does&#039;, b&#039;have&#039;, b&#039;some&#039;, b&#039;good&#039;, b&#039;action&#039;,\n         b&#039;and&#039;, b&#039;a&#039;, b&#039;plot&#039;, b&#039;that&#039;, b&#039;is&#039;, b&#039;somewhat&#039;, b&#039;interesting&#039;,\n         b&#039;.&#039;, b&#039;nevsky&#039;, b&#039;acts&#039;, b&#039;like&#039;, b&#039;a&#039;, b&#039;body&#039;, b&#039;builder&#039;,\n         b&#039;and&#039;, b&#039;he&#039;, b&#039;isn&#039;, b&quot;&#039;&quot;, b&#039;t&#039;, b&#039;all&#039;, b&#039;that&#039;, b&#039;attractive&#039;,\n         b&#039;,&#039;, b&#039;in&#039;, b&#039;fact&#039;, b&#039;,&#039;, b&#039;imo&#039;, b&#039;,&#039;, b&#039;he&#039;, b&#039;is&#039;, b&#039;ugly&#039;,\n         b&#039;.&#039;, b&#039;(&#039;, b&#039;his&#039;, b&#039;acting&#039;, b&#039;skills&#039;, b&#039;lack&#039;, b&#039;everything&#039;,\n         b&#039;!&#039;, b&#039;)&#039;, b&#039;sascha&#039;, b&#039;is&#039;, b&#039;played&#039;, b&#039;very&#039;, b&#039;well&#039;, b&#039;by&#039;,\n         b&#039;joanna&#039;, b&#039;pacula&#039;, b&#039;,&#039;, b&#039;but&#039;, b&#039;she&#039;, b&#039;needed&#039;, b&#039;more&#039;,\n         b&#039;lines&#039;, b&#039;than&#039;, b&#039;she&#039;, b&#039;was&#039;, b&#039;given&#039;, b&#039;,&#039;, b&#039;her&#039;,\n         b&#039;character&#039;, b&#039;needed&#039;, b&#039;to&#039;, b&#039;be&#039;, b&#039;developed&#039;, b&#039;.&#039;,\n         b&#039;there&#039;, b&#039;are&#039;, b&#039;way&#039;, b&#039;too&#039;, b&#039;many&#039;, b&#039;men&#039;, b&#039;in&#039;, b&#039;this&#039;,\n         b&#039;story&#039;, b&#039;,&#039;, b&#039;there&#039;, b&#039;is&#039;, b&#039;zero&#039;, b&#039;romance&#039;, b&#039;,&#039;, b&#039;too&#039;,\n         b&#039;much&#039;, b&#039;action&#039;, b&#039;,&#039;, b&#039;and&#039;, b&#039;way&#039;, b&#039;too&#039;, b&#039;dumb&#039;, b&#039;of&#039;,\n         b&#039;an&#039;, b&#039;ending&#039;, b&#039;.&#039;, b&#039;it&#039;, b&#039;is&#039;, b&#039;very&#039;, b&#039;violent&#039;, b&#039;.&#039;,\n         b&#039;i&#039;, b&#039;did&#039;, b&#039;however&#039;, b&#039;love&#039;, b&#039;the&#039;, b&#039;scenery&#039;, b&#039;,&#039;,\n         b&#039;this&#039;, b&#039;movie&#039;, b&#039;takes&#039;, b&#039;you&#039;, b&#039;all&#039;, b&#039;over&#039;, b&#039;the&#039;,\n         b&#039;world&#039;, b&#039;,&#039;, b&#039;and&#039;, b&#039;that&#039;, b&#039;is&#039;, b&#039;a&#039;, b&#039;bonus&#039;, b&#039;.&#039;, b&#039;i&#039;,\n         b&#039;also&#039;, b&#039;liked&#039;, b&#039;how&#039;, b&#039;it&#039;, b&#039;had&#039;, b&#039;some&#039;, b&#039;stuff&#039;,\n         b&#039;about&#039;, b&#039;the&#039;, b&#039;mafia&#039;, b&#039;in&#039;, b&#039;it&#039;, b&#039;,&#039;, b&#039;not&#039;, b&#039;too&#039;,\n         b&#039;much&#039;, b&#039;or&#039;, b&#039;too&#039;, b&#039;little&#039;, b&#039;,&#039;, b&#039;but&#039;, b&#039;enough&#039;,\n         b&#039;that&#039;, b&#039;it&#039;, b&#039;got&#039;, b&#039;my&#039;, b&#039;attention&#039;, b&#039;.&#039;, b&#039;the&#039;,\n         b&#039;actors&#039;, b&#039;needed&#039;, b&#039;to&#039;, b&#039;be&#039;, b&#039;more&#039;, b&#039;handsome&#039;, b&#039;.&#039;,\n         b&#039;.&#039;, b&#039;.&#039;, b&#039;the&#039;, b&#039;biggest&#039;, b&#039;problem&#039;, b&#039;i&#039;, b&#039;had&#039;, b&#039;was&#039;,\n         b&#039;that&#039;, b&#039;nevsky&#039;, b&#039;was&#039;, b&#039;just&#039;, b&#039;too&#039;, b&#039;normal&#039;, b&#039;,&#039;,\n         b&#039;not&#039;, b&#039;sexy&#039;, b&#039;enough&#039;, b&#039;.&#039;, b&#039;i&#039;, b&#039;think&#039;, b&#039;for&#039;, b&#039;most&#039;,\n         b&#039;guys&#039;, b&#039;,&#039;, b&#039;sascha&#039;, b&#039;will&#039;, b&#039;be&#039;, b&#039;hot&#039;, b&#039;enough&#039;, b&#039;,&#039;,\n         b&#039;but&#039;, b&#039;for&#039;, b&#039;us&#039;, b&#039;ladies&#039;, b&#039;that&#039;, b&#039;are&#039;, b&#039;fans&#039;, b&#039;of&#039;,\n         b&#039;action&#039;, b&#039;,&#039;, b&#039;nevsky&#039;, b&#039;just&#039;, b&#039;doesn&#039;, b&quot;&#039;&quot;, b&#039;t&#039;, b&#039;cut&#039;,\n         b&#039;it&#039;, b&#039;.&#039;, b&#039;overall&#039;, b&#039;,&#039;, b&#039;this&#039;, b&#039;movie&#039;, b&#039;was&#039;, b&#039;fine&#039;,\n         b&#039;,&#039;, b&#039;i&#039;, b&#039;didn&#039;, b&quot;&#039;&quot;, b&#039;t&#039;, b&#039;love&#039;, b&#039;it&#039;, b&#039;nor&#039;, b&#039;did&#039;,\n         b&#039;i&#039;, b&#039;hate&#039;, b&#039;it&#039;, b&#039;,&#039;, b&#039;just&#039;, b&#039;found&#039;, b&#039;it&#039;, b&#039;to&#039;, b&#039;be&#039;,\n         b&#039;another&#039;, b&#039;normal&#039;, b&#039;action&#039;, b&#039;flick&#039;, b&#039;.&#039;], dtype=object)},\n array([0.], dtype=float32))\n\n现在，我们构建一个正式的输入函数，可以将其传递给 TensorFlow Estimator 对象的 train() 方法。\n# Create an input_fn that parses the tf.Examples from the given files,\n# and split them into features and targets.\ndef _input_fn(input_filenames, num_epochs=None, shuffle=True):\n  \n  # Same code as above; create a dataset and map features and labels.\n  ds = tf.data.TFRecordDataset(input_filenames)\n  ds = ds.map(_parse_function)\n \n  if shuffle:\n    ds = ds.shuffle(10000)\n \n  # Our feature data is variable-length, so we pad and batch\n  # each field of the dataset structure to whatever size is necessary.     \n  ds = ds.padded_batch(25, ds.output_shapes)\n  \n  ds = ds.repeat(num_epochs)\n \n  \n  # Return the next batch of data.\n  features, labels = ds.make_one_shot_iterator().get_next()\n  return features, labels\n使用具有稀疏输入和显式词汇表的线性模型\n对于我们的第一个模型，我们将使用 50 个信息性术语来构建 LinearClassifier 模型；始终从简单入手！\n以下代码将为我们的术语构建特征列。categorical_column_with_vocabulary_list 函数可使用“字符串-特征矢量”映射来创建特征列。\n# 50 informative terms that compose our model vocabulary. \ninformative_terms = (&quot;bad&quot;, &quot;great&quot;, &quot;best&quot;, &quot;worst&quot;, &quot;fun&quot;, &quot;beautiful&quot;,\n                     &quot;excellent&quot;, &quot;poor&quot;, &quot;boring&quot;, &quot;awful&quot;, &quot;terrible&quot;,\n                     &quot;definitely&quot;, &quot;perfect&quot;, &quot;liked&quot;, &quot;worse&quot;, &quot;waste&quot;,\n                     &quot;entertaining&quot;, &quot;loved&quot;, &quot;unfortunately&quot;, &quot;amazing&quot;,\n                     &quot;enjoyed&quot;, &quot;favorite&quot;, &quot;horrible&quot;, &quot;brilliant&quot;, &quot;highly&quot;,\n                     &quot;simple&quot;, &quot;annoying&quot;, &quot;today&quot;, &quot;hilarious&quot;, &quot;enjoyable&quot;,\n                     &quot;dull&quot;, &quot;fantastic&quot;, &quot;poorly&quot;, &quot;fails&quot;, &quot;disappointing&quot;,\n                     &quot;disappointment&quot;, &quot;not&quot;, &quot;him&quot;, &quot;her&quot;, &quot;good&quot;, &quot;time&quot;,\n                     &quot;?&quot;, &quot;.&quot;, &quot;!&quot;, &quot;movie&quot;, &quot;film&quot;, &quot;action&quot;, &quot;comedy&quot;,\n                     &quot;drama&quot;, &quot;family&quot;)\n \nterms_feature_column = tf.feature_column.categorical_column_with_vocabulary_list(key=&quot;terms&quot;, vocabulary_list=informative_terms)\n接下来，我们将构建 LinearClassifier，在训练集中训练该模型，并在评估集中对其进行评估。阅读上述代码后，运行该模型以了解其效果。\nmy_optimizer = tf.train.AdagradOptimizer(learning_rate=0.1)\nmy_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)\n \nfeature_columns = [ terms_feature_column ]\n \n \nclassifier = tf.estimator.LinearClassifier(\n  feature_columns=feature_columns,\n  optimizer=my_optimizer,\n)\n \nclassifier.train(\n  input_fn=lambda: _input_fn([train_path]),\n  steps=1000)\n \nevaluation_metrics = classifier.evaluate(\n  input_fn=lambda: _input_fn([train_path]),\n  steps=1000)\nprint(&quot;Training set metrics:&quot;)\nfor m in evaluation_metrics:\n  print(m, evaluation_metrics[m])\nprint(&quot;---&quot;)\n \nevaluation_metrics = classifier.evaluate(\n  input_fn=lambda: _input_fn([test_path]),\n  steps=1000)\n \nprint(&quot;Test set metrics:&quot;)\nfor m in evaluation_metrics:\n  print(m, evaluation_metrics[m])\nprint(&quot;---&quot;)\nTraining set metrics:\naccuracy 0.78928\naccuracy_baseline 0.5\nauc 0.87206453\nauc_precision_recall 0.8640158\naverage_loss 0.45088252\nlabel/mean 0.5\nloss 11.272063\nprecision 0.77057767\nprediction/mean 0.4956976\nrecall 0.82384\nglobal_step 1000\n---\nTest set metrics:\naccuracy 0.78504\naccuracy_baseline 0.5\nauc 0.86939275\nauc_precision_recall 0.8610384\naverage_loss 0.4532239\nlabel/mean 0.5\nloss 11.330598\nprecision 0.7680963\nprediction/mean 0.49426404\nrecall 0.81664\nglobal_step 1000\n---\n\n使用深度神经网络 (DNN) 模型\n上述模型是一个线性模型，效果非常好。但是，我们可以使用 DNN 模型实现更好的效果吗？\n我们将 LinearClassifier 切换为 DNNClassifier。运行以下单元，看看您的模型效果如何。\n##################### Here&#039;s what we changed ##################################\nclassifier = tf.estimator.DNNClassifier(                                      #\n  feature_columns=[tf.feature_column.indicator_column(terms_feature_column)], #\n  hidden_units=[20,20],                                                       #\n  optimizer=my_optimizer,                                                     #\n)                                                                             #\n###############################################################################\n \ntry:\n  classifier.train(\n    input_fn=lambda: _input_fn([train_path]),\n    steps=1000)\n \n  evaluation_metrics = classifier.evaluate(\n    input_fn=lambda: _input_fn([train_path]),\n    steps=1)\n  print(&quot;Training set metrics:&quot;)\n  for m in evaluation_metrics:\n    print(m, evaluation_metrics[m])\n  print(&quot;---&quot;)\n \n  evaluation_metrics = classifier.evaluate(\n    input_fn=lambda: _input_fn([test_path]),\n    steps=1)\n \n  print(&quot;Test set metrics:&quot;)\n  for m in evaluation_metrics:\n    print(m, evaluation_metrics[m])\n  print(&quot;---&quot;)\nexcept ValueError as err:\n  print(err)\nTraining set metrics:\naccuracy 0.92\naccuracy_baseline 0.68\nauc 0.9705881\nauc_precision_recall 0.9885154\naverage_loss 0.33181748\nlabel/mean 0.68\nloss 8.295437\nprecision 1.0\nprediction/mean 0.52073544\nrecall 0.88235295\nglobal_step 1000\n---\nTest set metrics:\naccuracy 0.8\naccuracy_baseline 0.56\nauc 0.75974023\nauc_precision_recall 0.66889143\naverage_loss 0.7257034\nlabel/mean 0.56\nloss 18.142586\nprecision 0.84615386\nprediction/mean 0.4270074\nrecall 0.78571427\nglobal_step 1000\n---\n\n在 DNN 模型中使用嵌入\n在此任务中，我们将使用嵌入列来实现 DNN 模型。嵌入列会将稀疏数据作为输入，并返回一个低维度密集矢量作为输出。\n注意：从计算方面而言，embedding_column 通常是用于在稀疏数据中训练模型最有效的选项。在此练习末尾的可选部分，我们将更深入地讨论使用 embedding_column 与 indicator_column 之间的实现差异，以及如何在这两者之间做出权衡。\n在下面的代码中，执行以下操作：\n\n通过将数据投射到二维空间的 embedding_column 来为模型定义特征列（如需详细了解 embedding_column 的函数签名，请参阅相关 TF 文档）。\n定义符合以下规范的 DNNClassifier：\n\n具有两个隐藏层，每个包含 20 个单元\n采用学习速率为 0.1 的 AdaGrad 优化方法\ngradient_clip_norm 值为 5.0\n\n\n\n注意：在实践中，我们可能会将数据投射到 2 维以上（比如 50 或 100）的空间中。但就目前而言，2 维是比较容易可视化的维数。\n########################## SOLUTION CODE ########################################\nterms_embedding_column = tf.feature_column.embedding_column(terms_feature_column, dimension=2)\nfeature_columns = [ terms_embedding_column ]\n \nmy_optimizer = tf.train.AdagradOptimizer(learning_rate=0.1)\nmy_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)\n \nclassifier = tf.estimator.DNNClassifier(\n  feature_columns=feature_columns,\n  hidden_units=[20,20],\n  optimizer=my_optimizer\n)\n#################################################################################\n \nclassifier.train(\n  input_fn=lambda: _input_fn([train_path]),\n  steps=1000)\n \nevaluation_metrics = classifier.evaluate(\n  input_fn=lambda: _input_fn([train_path]),\n  steps=1000)\nprint(&quot;Training set metrics:&quot;)\nfor m in evaluation_metrics:\n  print(m, evaluation_metrics[m])\nprint(&quot;---&quot;)\n \nevaluation_metrics = classifier.evaluate(\n  input_fn=lambda: _input_fn([test_path]),\n  steps=1000)\n \nprint(&quot;Test set metrics:&quot;)\nfor m in evaluation_metrics:\n  print(m, evaluation_metrics[m])\nprint(&quot;---&quot;)\nTraining set metrics:\naccuracy 0.78516\naccuracy_baseline 0.5\nauc 0.8685013\nauc_precision_recall 0.8568284\naverage_loss 0.45557868\nlabel/mean 0.5\nloss 11.389467\nprecision 0.7566789\nprediction/mean 0.52443045\nrecall 0.84064\nglobal_step 1000\n---\nTest set metrics:\naccuracy 0.78168\naccuracy_baseline 0.5\nauc 0.8668425\nauc_precision_recall 0.85428405\naverage_loss 0.45733798\nlabel/mean 0.5\nloss 11.433449\nprecision 0.7556637\nprediction/mean 0.52328736\nrecall 0.83256\nglobal_step 1000\n---\n\n确信模型中确实存在嵌入\n上述模型使用了 embedding_column，而且似乎很有效，但这并没有让我们了解到内部发生的情形。我们如何检查该模型确实在内部使用了嵌入？\n首先，我们来看看该模型中的张量：\nclassifier.get_variable_names()\n[&#039;dnn/hiddenlayer_0/bias&#039;,\n &#039;dnn/hiddenlayer_0/bias/t_0/Adagrad&#039;,\n &#039;dnn/hiddenlayer_0/kernel&#039;,\n &#039;dnn/hiddenlayer_0/kernel/t_0/Adagrad&#039;,\n &#039;dnn/hiddenlayer_1/bias&#039;,\n &#039;dnn/hiddenlayer_1/bias/t_0/Adagrad&#039;,\n &#039;dnn/hiddenlayer_1/kernel&#039;,\n &#039;dnn/hiddenlayer_1/kernel/t_0/Adagrad&#039;,\n &#039;dnn/input_from_feature_columns/input_layer/terms_embedding/embedding_weights&#039;,\n &#039;dnn/input_from_feature_columns/input_layer/terms_embedding/embedding_weights/t_0/Adagrad&#039;,\n &#039;dnn/logits/bias&#039;,\n &#039;dnn/logits/bias/t_0/Adagrad&#039;,\n &#039;dnn/logits/kernel&#039;,\n &#039;dnn/logits/kernel/t_0/Adagrad&#039;,\n &#039;global_step&#039;]\n\n好的，我们可以看到这里有一个嵌入层：&#039;dnn/input_from_feature_columns/input_layer/terms_embedding/...&#039;。（顺便说一下，有趣的是，该层可以与模型的其他层一起训练，就像所有隐藏层一样。）\n嵌入层的形状是否正确？请运行以下代码来查明。\n注意：在我们的示例中，嵌入是一个矩阵，可让我们将一个 50 维矢量投射到 2 维空间。\nfor neure in classifier.get_variable_names():\n    print(classifier.get_variable_value(neure).shape, &quot;: &quot; + neure)\n(20,) : dnn/hiddenlayer_0/bias\n(20,) : dnn/hiddenlayer_0/bias/t_0/Adagrad\n(2, 20) : dnn/hiddenlayer_0/kernel\n(2, 20) : dnn/hiddenlayer_0/kernel/t_0/Adagrad\n(20,) : dnn/hiddenlayer_1/bias\n(20,) : dnn/hiddenlayer_1/bias/t_0/Adagrad\n(20, 20) : dnn/hiddenlayer_1/kernel\n(20, 20) : dnn/hiddenlayer_1/kernel/t_0/Adagrad\n(50, 2) : dnn/input_from_feature_columns/input_layer/terms_embedding/embedding_weights\n(50, 2) : dnn/input_from_feature_columns/input_layer/terms_embedding/embedding_weights/t_0/Adagrad\n(1,) : dnn/logits/bias\n(1,) : dnn/logits/bias/t_0/Adagrad\n(20, 1) : dnn/logits/kernel\n(20, 1) : dnn/logits/kernel/t_0/Adagrad\n() : global_step\n\n花些时间来手动检查各个层及其形状，以确保一切都按照您预期的方式互相连接。\n检查嵌入\n现在，我们来看看实际嵌入空间，并了解术语最终所在的位置。请执行以下操作：\n\n\n运行以下代码来查看我们在训练的嵌入。一切最终是否如您所预期的那样？\n\n\n重新运行**在 DNN 模型中使用嵌 ** 中的代码来重新训练该模型，然后再次运行下面的嵌入可视化。哪些保持不变？哪些发生了变化？\n\n\n最后，仅使用 10 步来重新训练该模型（这将产生一个糟糕的模型）。再次运行下面的嵌入可视化。您现在看到了什么？为什么？\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n \nembedding_matrix = classifier.get_variable_value(&#039;dnn/input_from_feature_columns/input_layer/terms_embedding/embedding_weights&#039;)\n \nfor term_index in range(len(informative_terms)):\n  # Create a one-hot encoding for our term.  It has 0s everywhere, except for\n  # a single 1 in the coordinate that corresponds to that term.\n  term_vector = np.zeros(len(informative_terms))\n  term_vector[term_index] = 1\n  # We&#039;ll now project that one-hot vector into the embedding space.\n  embedding_xy = np.matmul(term_vector, embedding_matrix)\n  plt.text(embedding_xy[0],\n           embedding_xy[1],\n           informative_terms[term_index])\n \n# Do a little setup to make sure the plot displays nicely.\nplt.rcParams[&quot;figure.figsize&quot;] = (15, 15)\nplt.xlim(1.2 * embedding_matrix.min(), 1.2 * embedding_matrix.max())\nplt.ylim(1.2 * embedding_matrix.min(), 1.2 * embedding_matrix.max())\nplt.show() \n\nimport numpy as np\nimport matplotlib.pyplot as plt\n \nembedding_matrix = classifier.get_variable_value(&#039;dnn/input_from_feature_columns/input_layer/terms_embedding/embedding_weights&#039;)\n \nfor term_index in range(len(informative_terms)):\n  # Create a one-hot encoding for our term.  It has 0s everywhere, except for\n  # a single 1 in the coordinate that corresponds to that term.\n  term_vector = np.zeros(len(informative_terms))\n  term_vector[term_index] = 1\n  # We&#039;ll now project that one-hot vector into the embedding space.\n  embedding_xy = np.matmul(term_vector, embedding_matrix)\n  plt.text(embedding_xy[0],\n           embedding_xy[1],\n           informative_terms[term_index])\n \n# Do a little setup to make sure the plot displays nicely.\nplt.rcParams[&quot;figure.figsize&quot;] = (15, 15)\nplt.xlim(1.2 * embedding_matrix.min(), 1.2 * embedding_matrix.max())\nplt.ylim(1.2 * embedding_matrix.min(), 1.2 * embedding_matrix.max())\nplt.show() \n\n任务 6：尝试改进模型的效果\n看看您能否优化该模型以改进其效果。您可以尝试以下几种做法：\n\n更改超参数或使用其他优化工具，比如 Adam（通过遵循这些策略，您的准确率可能只会提高一两个百分点）。\n**向 informative_terms 中添加其他术语。**此数据集有一个完整的词汇表文件，其中包含 30716 个术语，您可以在以下位置找到该文件：download.mlcc.google.cn/mledu-datasets/sparse-data-embedding/terms.txt 您可以从该词汇表文件中挑选出其他术语，也可以通过 categorical_column_with_vocabulary_file 特征列使用整个词汇表文件。\n\n# Download the vocabulary file.\nterms_url = &#039;download.mlcc.google.cn/mledu-datasets/sparse-data-embedding/terms.txt&#039;\nterms_path = tf.keras.utils.get_file(terms_url.split(&#039;/&#039;)[-1], terms_url)\nDownloading data from download.mlcc.google.cn/mledu-datasets/sparse-data-embedding/terms.txt\n253952/253538 [==============================] - 0s 0us/step\n\n# Create a feature column from &quot;terms&quot;, using a full vocabulary file.\ninformative_terms = None\nwith io.open(terms_path, &#039;r&#039;, encoding=&#039;utf8&#039;) as f:\n  # Convert it to a set first to remove duplicates.\n  informative_terms = list(set(f.read().split()))\n  \nterms_feature_column = tf.feature_column.categorical_column_with_vocabulary_list(key=&quot;terms&quot;, \n                                                                                 vocabulary_list=informative_terms)\n \nterms_embedding_column = tf.feature_column.embedding_column(terms_feature_column, dimension=2)\nfeature_columns = [ terms_embedding_column ]\n \nmy_optimizer = tf.train.AdagradOptimizer(learning_rate=0.1)\nmy_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)\n \nclassifier = tf.estimator.DNNClassifier(\n  feature_columns=feature_columns,\n  hidden_units=[10,10],\n  optimizer=my_optimizer\n)\n \nclassifier.train(\n  input_fn=lambda: _input_fn([train_path]),\n  steps=1000)\n \nevaluation_metrics = classifier.evaluate(\n  input_fn=lambda: _input_fn([train_path]),\n  steps=1000)\nprint(&quot;Training set metrics:&quot;)\nfor m in evaluation_metrics:\n  print(m, evaluation_metrics[m])\nprint(&quot;---&quot;)\n \nevaluation_metrics = classifier.evaluate(\n  input_fn=lambda: _input_fn([test_path]),\n  steps=1000)\n \nprint(&quot;Test set metrics:&quot;)\nfor m in evaluation_metrics:\n  print(m, evaluation_metrics[m])\nprint(&quot;---&quot;)\nTraining set metrics:\naccuracy 0.82\naccuracy_baseline 0.5\nauc 0.89789164\nauc_precision_recall 0.8937925\naverage_loss 0.4075714\nlabel/mean 0.5\nloss 10.189285\nprecision 0.83664364\nprediction/mean 0.4748372\nrecall 0.79528\nglobal_step 1000\n---\nTest set metrics:\naccuracy 0.8048\naccuracy_baseline 0.5\nauc 0.88663244\nauc_precision_recall 0.8821298\naverage_loss 0.42734343\nlabel/mean 0.5\nloss 10.683586\nprecision 0.8235394\nprediction/mean 0.47395515\nrecall 0.77584\nglobal_step 1000\n---\n\n总结\n我们可能获得了比我们原来的线性模型更好且具有嵌入的 DNN 解决方案，但线性模型也相当不错，而且训练速度快得多。线性模型的训练速度之所以更快，是因为它们没有太多要更新的参数或要反向传播的层。\n在有些应用中，线性模型的速度可能非常关键，或者从质量的角度来看，线性模型可能完全够用。在其他领域，DNN 提供的额外模型复杂性和能力可能更重要。在定义模型架构时，请记得要充分探讨您的问题，以便知道自己所处的情形。\n*可选内容：*在 embedding_column 与 indicator_column 之间进行权衡\n从概念上讲，在训练 LinearClassifier 或 DNNClassifier 时，需要根据实际情况使用稀疏列。TF 提供了两个选项：embedding_column 或 indicator_column。\n在训练 LinearClassifier（如使用具有稀疏输入和显式词汇表的线性模型 中所示）时，系统在后台使用了 embedding_column。正如使用深度神经网络 (DNN) 模型 中所示，在训练 DNNClassifier 时，您必须明确选择 embedding_column 或 indicator_column。本部分通过一个简单的示例讨论了这两者之间的区别，以及如何在二者之间进行权衡。\n假设我们的稀疏数据包含 &quot;great&quot;、&quot;beautiful&quot; 和 &quot;excellent&quot; 这几个值。由于我们在此处使用的词汇表大小为 V = 50，因此第一层中的每个单元（神经元）的权重将为 50。我们用 s 表示稀疏输入中的项数。对于此示例稀疏数据，s = 3。对于具有 V 个可能值的输入层，带有 d 个单元的隐藏层需要运行一次“矢量 - 矩阵”乘法运算：(1 \\times V) * (V \\times d)。此运算会产生 O(V * d) 的计算成本。请注意，此成本与隐藏层中的权重数成正比，而与 s 无关。\n如果输入使用 indicator_column 进行了独热编码（长度为 V 的布尔型矢量，存在用 1 表示，其余则为 0），这表示很多零进行了相乘和相加运算。\n当我们通过使用大小为 d 的 embedding_column 获得完全相同的结果时，我们将仅查询与示例输入中存在的 3 个特征 &quot;great&quot;、&quot;beautiful&quot; 和 &quot;excellent&quot; 相对应的嵌入并将这三个嵌入相加：(1 \\times d) + (1 \\times d) + (1 \\times d)。由于不存在的特征的权重在“矢量-矩阵”乘法中与 0 相乘，因此对结果没有任何影响；而存在的特征的权重在“矢量-矩阵”乘法中与 1 相乘。因此，将通过嵌入查询获得的权重相加会获得与“矢量-矩阵”乘法相同的结果。\n当使用嵌入时，计算嵌入查询是一个 O(s * d) 计算；从计算方面而言，它比稀疏数据中的 indicator_column 的 O(V * d) 更具成本效益，因为 s 远远小于 V。（请注意，这些嵌入是临时学习的结果。在任何指定的训练迭代中，都是当前查询的权重。\n正如我们在在 DNN 模型中使用嵌入 中看到的，通过在训练 DNNClassifier 过程中使用 embedding_column，我们的模型学习了特征的低维度表示法，其中点积定义了一个针对目标任务的相似性指标。在本例中，影评中使用的相似术语（例如 &quot;great&quot; 和 &quot;excellent&quot;）在嵌入空间中彼此之间距离较近（即具有较大的点积），而相异的术语（例如 &quot;great&quot; 和 &quot;bad&quot;）在嵌入空间中彼此之间距离较远（即具有较小的点积）。"},"Tensorflow/TensorFlow_2":{"slug":"Tensorflow/TensorFlow_2","filePath":"Tensorflow/TensorFlow_2.md","title":"TensorFlow 2","links":[],"tags":["Tensorflow"],"content":"创建和控制张量\n矢量加法\n可以对张量执行金典的数学运算，试着创建一些矢量。\nfrom __future__ import print_function\n \nimport tensorflow as tf\ntry:\n  tf.contrib.eager.enable_eager_execution()\n  print(&quot;TF imported with eager execution!&quot;)\nexcept ValueError:\n  print(&quot;TF already imported with eager execution!&quot;)\n \n# 一个包含质数的‘primes’矢量\nprimes = tf.constant([2, 3, 5, 7, 11 ,13], dtype=tf.int32)\nprint(&quot;primes:&quot;, primes)\n \n# 一个值全为 1 的 ones 矢量\nones = tf.ones([6], dtype=tf.int32)\nprint(ones)\n \n# 一个通过对前两个矢量执行元素级加法而创建的矢量。\njust_beyond_primes = tf.add(primes, ones)\nprint(just_beyond_primes)\n \n# 把primes中的元素乘二\ntwos = tf.constant([2, 2, 2, 2, 2, 2], dtype=tf.int32)\nprimes_doubled = primes * twos\nprint(primes_doubled)\nTF imported with eager execution!\nprimes: tf.Tensor([ 2  3  5  7 11 13], shape=(6,), dtype=int32)\ntf.Tensor([1 1 1 1 1 1], shape=(6,), dtype=int32)\ntf.Tensor([ 3  4  6  8 12 14], shape=(6,), dtype=int32)\ntf.Tensor([ 4  6 10 14 22 26], shape=(6,), dtype=int32)\n\n输出的张量不仅会返回值，还会返回形状shape，以及储存在张量中的值的类型。调用numpy方法会以NumPy数组的形式返回。\nsome_matrix = tf.constant([[1, 2, 3], [4, 5, 6]], dtype=tf.int32)\nprint(some_matrix)\nprint(&quot;\\nnumpy matrix:\\n&quot;, some_matrix.numpy())\ntf.Tensor(\n[[1 2 3]\n [4 5 6]], shape=(2, 3), dtype=int32)\n\nnumpy matrix:\n [[1 2 3]\n [4 5 6]]\n\n张量形状\nshape 是用来描述张量维度大小和数量。张量的形状表示为list，其中第 i 个元素表示维度 i 的大小。列表的长度表示张量的阶（即维数）。\n如果是二维的则shape=(行数， 列数)\n例如shape=(n1, n2, n3, …, x, y)\n则说明 一共有 (n1 x n2 x n3 x …)个x行y列的数组构成。\n例：\n# 一个标量\nscalar = tf.zeros([])\n \n# 一个有三个元素的向量\nvector = tf.zeros([3])\n \n# 一个两行三列的矩阵\nmatrix = tf.zeros([2, 3])\n \nmatrix2 = tf.zeros([2, 3, 4, 5])\n \nprint(&#039;scalar has shape&#039;, scalar.get_shape(), &#039;and value:\\n&#039;, scalar.numpy())\nprint(&#039;vector has shape&#039;, vector.get_shape(), &#039;and value:\\n&#039;, vector.numpy())\nprint(&#039;matrix has shape&#039;, matrix.get_shape(), &#039;and value:\\n&#039;, matrix.numpy())\nprint(&#039;matrix2 has shape&#039;, matrix2.get_shape(), &#039;and value:\\n&#039;, matrix2.numpy())\n \nscalar has shape () and value:\n 0.0\nvector has shape (3,) and value:\n [0. 0. 0.]\nmatrix has shape (2, 3) and value:\n [[0. 0. 0.]\n [0. 0. 0.]]\nmatrix2 has shape (2, 3, 4, 5) and value:\n [[[[0. 0. 0. 0. 0.]\n   [0. 0. 0. 0. 0.]\n   [0. 0. 0. 0. 0.]\n   [0. 0. 0. 0. 0.]]\n\n  [[0. 0. 0. 0. 0.]\n   [0. 0. 0. 0. 0.]\n   [0. 0. 0. 0. 0.]\n   [0. 0. 0. 0. 0.]]\n\n  [[0. 0. 0. 0. 0.]\n   [0. 0. 0. 0. 0.]\n   [0. 0. 0. 0. 0.]\n   [0. 0. 0. 0. 0.]]]\n\n\n [[[0. 0. 0. 0. 0.]\n   [0. 0. 0. 0. 0.]\n   [0. 0. 0. 0. 0.]\n   [0. 0. 0. 0. 0.]]\n\n  [[0. 0. 0. 0. 0.]\n   [0. 0. 0. 0. 0.]\n   [0. 0. 0. 0. 0.]\n   [0. 0. 0. 0. 0.]]\n\n  [[0. 0. 0. 0. 0.]\n   [0. 0. 0. 0. 0.]\n   [0. 0. 0. 0. 0.]\n   [0. 0. 0. 0. 0.]]]]\n\n广播\n在数学中，您只能对形状相同的张量执行元素级运算（例如，相加和等于）。不过，在 TensorFlow 中，您可以对张量执行传统意义上不可行的运算。TensorFlow 支持广播（一种借鉴自 NumPy 的概念）。利用广播，元素级运算中的较小数组会增大到与较大数组具有相同的形状。例如，通过广播：\n\n\n如果运算需要大小为 [6] 的张量，则大小为 [1] 或 [] 的张量可以作为运算数。\n\n\n如果运算需要大小为 [4, 6] 的张量，则以下任何大小的张量都可以作为运算数：\n\n[1, 6]\n[6]\n[]\n\n\n\n如果运算需要大小为 [3, 5, 6] 的张量，则以下任何大小的张量都可以作为运算数：\n\n[1, 5, 6]\n[3, 1, 6]\n[3, 5, 1]\n[1, 1, 1]\n[5, 6]\n[1, 6]\n[6]\n[1]\n[]\n\n\n\n注意：当张量被广播时，从概念上来说，系统会复制其条目（出于性能考虑，实际并不复制。广播专为实现性能优化而设计）。\n有关完整的广播规则集，请参阅简单易懂的 NumPy 广播文档。\n以下代码执行了与之前一样的张量运算，不过使用的是标量值（而不是全包含 1 或全包含 2 的矢量）和广播。\nprimes = tf.constant([2, 3, 5, 7, 11, 13], dtype=tf.int32)\nprint(&quot;primes:&quot;, primes)\n \none = tf.constant(1, dtype=tf.int32)\nprint(&quot;one:&quot;, one)\n \njust_beyond_primes = tf.add(primes, one)\nprint(&quot;just_beyond_primes:&quot;, just_beyond_primes)\n \ntwo = tf.constant(2, dtype=tf.int32)\nprimes_doubled = primes * two\nprint(primes_doubled)\nprimes: tf.Tensor([ 2  3  5  7 11 13], shape=(6,), dtype=int32)\none: tf.Tensor(1, shape=(), dtype=int32)\njust_beyond_primes: tf.Tensor([ 3  4  6  8 12 14], shape=(6,), dtype=int32)\ntf.Tensor([ 4  6 10 14 22 26], shape=(6,), dtype=int32)\n\n练习 1：矢量运算。\n执行矢量运算以创建一个“just_under_primes_squared”矢量，其中第 i 个元素等于 primes 中第 i 个元素的平方减 1。例如，第二个元素为 3 * 3 - 1 = 8。\n使用 tf.multiply 或 tf.pow 操作可求得 primes 矢量中每个元素值的平方。\ndef solution(primes):\n    primes_squared = tf.pow(primes, 2)  # or tf.multiply(primes, primes)\n    one = tf.constant(1, dtype=tf.int32)\n    just_under_primes_squared = tf.subtract(primes_squared, one)\n    return just_under_primes_squared\n \nprimes = tf.constant([2, 3, 5, 7, 11, 13], dtype=tf.int32)\njust_under_primes_squared = solution(primes)\nprint(just_under_primes_squared)\ntf.Tensor([  3   8  24  48 120 168], shape=(6,), dtype=int32)\n\n矩阵乘法\n在线性代数中，当两个矩阵相乘时，第一个矩阵的列数必须等于第二个矩阵的行数。\n\n3x4 矩阵乘以 4x2 矩阵是 有效 的，可以得出一个 3x2 矩阵。\n4x2 矩阵乘以 3x4 矩阵是 无效 的。\n\n# 一个3x4的矩阵\nx = tf.constant([[5, 2, 4, 3], [5, 1, 6, -2], [-1, 3, -1, -2]],\n                dtype=tf.int32)\n \n# 一个4x2的矩阵\ny = tf.constant([[2, 2], [3, 5], [4, 5], [1, 6]], dtype=tf.int32)\n \n# 结果是一个3x2的矩阵\nmatrix_multiply_result = tf.matmul(x, y)\n \nprint(matrix_multiply_result)\ntf.Tensor(\n[[35 58]\n [35 33]\n [ 1 -4]], shape=(3, 2), dtype=int32)\n\n张量变形\n由于张量加法和矩阵乘法均对运算数施加了限制条件，TensorFlow 编程者需要频繁改变张量的形状。\n您可以使用 tf.reshape 方法改变张量的形状。\n例如，您可以将 8x2 张量变形为 2x8 张量或 4x4 张量(改变形状形成的新矩阵元素数和之前必须一样)：\n此外，您还可以使用 tf.reshape 更改张量的维数（“阶”）。\n例如，您可以将 8x2 张量变形为三维 2x2x4 张量或一维 16 元素张量。\n# 创建一个8x2的矩阵\nmatrix = tf.constant(\n    [[1, 2], [3, 4], [5, 6], [7, 8], [9, 10], [11, 12], [13, 14], [15, 16]],\n    dtype=tf.int32)\n \nreshaped_2x8_matrix = tf.reshape(matrix, [2, 8])\nreshaped_4x4_matrix = tf.reshape(matrix, [4, 4])\n \nprint(&quot;Original matrix (8x2):&quot;)\nprint(matrix.numpy())\nprint(&quot;Reshaped matrix (2x8):&quot;)\nprint(reshaped_2x8_matrix.numpy())\nprint(&quot;Reshaped matrix (4x4):&quot;)\nprint(reshaped_4x4_matrix.numpy())\n \nreshaped_2x2x4_tensor = tf.reshape(matrix, [2, 2, 4])\none_dimensional_vector = tf.reshape(matrix, [16])\nprint(&quot;Reshaped 3-D tensor (2x2x4):&quot;)\nprint(reshaped_2x2x4_tensor.numpy())\nprint(&quot;1-D vector:&quot;)\nprint(one_dimensional_vector.numpy())\nOriginal matrix (8x2):\n[[ 1  2]\n [ 3  4]\n [ 5  6]\n [ 7  8]\n [ 9 10]\n [11 12]\n [13 14]\n [15 16]]\nReshaped matrix (2x8):\n[[ 1  2  3  4  5  6  7  8]\n [ 9 10 11 12 13 14 15 16]]\nReshaped matrix (4x4):\n[[ 1  2  3  4]\n [ 5  6  7  8]\n [ 9 10 11 12]\n [13 14 15 16]]\nReshaped 3-D tensor (2x2x4):\n[[[ 1  2  3  4]\n  [ 5  6  7  8]]\n\n [[ 9 10 11 12]\n  [13 14 15 16]]]\n1-D vector:\n[ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16]\n\n练习 2：改变两个张量的形状，使其能够相乘。\n下面两个矢量无法进行矩阵乘法运算：\n\na = tf.constant([5, 3, 2, 7, 1, 4])\nb = tf.constant([4, 6, 3])\n\n请改变这两个矢量的形状，使其成为可以进行矩阵乘法运算的运算数。\n然后，对变形后的张量调用矩阵乘法运算。\na = tf.constant([5, 3, 2, 7, 1, 4])\nb = tf.constant([4, 6, 3])\n \nreshaped_a = tf.reshape(a, [2, 3])\nreshaped_b = tf.reshape(b, [3, 1])\nc = tf.matmul(reshaped_a, reshaped_b)\n \nprint(&quot;reshaped_a (2x3):&quot;)\nprint(reshaped_a.numpy())\nprint(&quot;reshaped_b (3x1):&quot;)\nprint(reshaped_b.numpy())\nprint(&quot;reshaped_a x reshaped_b (2x1):&quot;)\nprint(c.numpy())\nreshaped_a (2x3):\n[[5 3 2]\n [7 1 4]]\nreshaped_b (3x1):\n[[4]\n [6]\n [3]]\nreshaped_a x reshaped_b (2x1):\n[[44]\n [46]]\n\n变量、初始化和赋值\n到目前为止，我们执行的所有运算都针对的是静态值 (tf.constant)；调用 numpy() 始终返回同一结果。在 TensorFlow 中可以定义 Variable 对象，它的值是可以更改的。\n创建变量时，您可以明确设置一个初始值，也可以使用初始化程序（例如分布）：\n# 创建初始值为3的标量变量\nv = tf.contrib.eager.Variable([3])\n \n# 创建一个形状为[1,4]的矢量变量，其初始值为随机的\n# 从均值为1，标准差为0.35的正态分布中取样\nw = tf.contrib.eager.Variable(tf.random_normal([1, 4], mean=1.0, stddev=0.35))\n \nprint(&quot;v:&quot;, v.numpy())\nprint(&quot;w:&quot;, w.numpy())\nv: [3]\nw: [[0.7752843 1.516361  1.1726708 0.9872638]]\n\n要更改变量的值，请使用 assign 操作，并且向变量赋予新值时，其形状必须和之前的形状一致。\nv = tf.contrib.eager.Variable([3])\nprint(v.numpy())\n \ntf.assign(v, [7])\nprint(v.numpy())\n \nv.assign([5])\nprint(v.numpy())\n \nv = tf.contrib.eager.Variable([[1, 2, 3], [4, 5, 6]])\nprint(v.numpy())\n \ntry:\n  print(&quot;Assigning [7, 8, 9] to v&quot;)\n  v.assign([7, 8, 9])\nexcept ValueError as e:\n  print(&quot;Exception:&quot;, e)\n[3]\n[7]\n[5]\n[[1 2 3]\n [4 5 6]]\nAssigning [7, 8, 9] to v\nException: Shapes (2, 3) and (3,) are incompatible\n\n练习 3：模拟投掷两个骰子 10 次。\n创建一个骰子模拟，在模拟中生成一个 10x3 二维张量，其中：\n\n列 1 和 2 均存储一个六面骰子（值为 1-6）的一次投掷值。\n列 3 存储同一行中列 1 和 2 的值的总和。\n\n例如，第一行中可能会包含以下值：\n\n列 1 存储 4\n列 2 存储 3\n列 3 存储 7\n\n要完成此任务，您需要浏览 TensorFlow 文档。\ndie1 = tf.contrib.eager.Variable(\n    tf.random_uniform([10, 1], minval=1, maxval=7, dtype=tf.int32))\ndie2 = tf.contrib.eager.Variable(\n    tf.random_uniform([10, 1], minval=1, maxval=7, dtype=tf.int32))\n \ndice_sum = tf.add(die1, die2)\nresulting_matrix = tf.concat(values=[die1, die2, dice_sum], axis=1)\n \nprint(resulting_matrix)\ntf.Tensor(\n[[ 2  3  5]\n [ 5  6 11]\n [ 3  3  6]\n [ 5  6 11]\n [ 2  1  3]\n [ 6  5 11]\n [ 1  4  5]\n [ 1  1  2]\n [ 4  6 10]\n [ 5  4  9]], shape=(10, 3), dtype=int32)\n"},"Tensorflow/TensorFlow_3":{"slug":"Tensorflow/TensorFlow_3","filePath":"Tensorflow/TensorFlow_3.md","title":"TensorFlow 3","links":[],"tags":["Tensorflow"],"content":"使用TensorFlow的基本步骤\n添加必要的库\nfrom __future__ import print_function\n \nimport math\n \nfrom IPython import display\nfrom matplotlib import cm\nfrom matplotlib import gridspec\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn import metrics\nimport tensorflow as tf\nfrom tensorflow.python.data import Dataset\n \ntf.logging.set_verbosity(tf.logging.ERROR)\npd.options.display.max_rows = 10  # 最大显示行数\npd.options.display.float_format = &#039;{:.1f}&#039;.format  # 精确度 保留一位小数\n \n加载数据集\n加载的数据集，数据基于加利福尼亚州1990年的人口普查数据\ncalifornia_housing_dataframe = pd.read_csv(&quot;download.mlcc.google.cn/mledu-datasets/california_housing_train.csv&quot;, sep=&quot;,&quot;)\n初始化数据集，对数据集进行随机化处理，以确保不会出现损害随机梯度下降的效果。此外，我们会将 median_house_value 调整为以千为单位，这样，模型就能够以常用范围内的学习速率较为轻松地学习这些数据。\ncalifornia_housing_dataframe = california_housing_dataframe.reindex(\n    np.random.permutation(california_housing_dataframe.index))\ncalifornia_housing_dataframe[&quot;median_house_value&quot;] /= 1000.0\ncalifornia_housing_dataframe\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      longitude\n      latitude\n      housing_median_age\n      total_rooms\n      total_bedrooms\n      population\n      households\n      median_income\n      median_house_value\n    \n  \n  \n    \n      13098\n      -121.9\n      37.6\n      20.0\n      1309.0\n      184.0\n      514.0\n      172.0\n      11.0\n      475.8\n    \n    \n      6576\n      -118.3\n      34.1\n      52.0\n      1261.0\n      616.0\n      2309.0\n      581.0\n      1.6\n      225.0\n    \n    \n      12732\n      -121.8\n      37.7\n      17.0\n      3112.0\n      872.0\n      1392.0\n      680.0\n      3.0\n      172.5\n    \n    \n      6505\n      -118.3\n      34.0\n      34.0\n      1462.0\n      394.0\n      1310.0\n      351.0\n      1.2\n      90.1\n    \n    \n      339\n      -116.9\n      32.7\n      9.0\n      2652.0\n      393.0\n      1355.0\n      362.0\n      6.3\n      293.1\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      6741\n      -118.3\n      33.9\n      41.0\n      896.0\n      198.0\n      605.0\n      168.0\n      2.3\n      128.1\n    \n    \n      496\n      -117.0\n      33.7\n      13.0\n      16148.0\n      3474.0\n      6159.0\n      3232.0\n      2.0\n      97.8\n    \n    \n      9140\n      -119.0\n      35.4\n      30.0\n      227.0\n      75.0\n      169.0\n      101.0\n      1.4\n      60.0\n    \n    \n      2610\n      -117.7\n      34.1\n      33.0\n      2081.0\n      409.0\n      1008.0\n      375.0\n      2.6\n      138.1\n    \n    \n      6827\n      -118.3\n      34.0\n      35.0\n      1090.0\n      345.0\n      1605.0\n      330.0\n      2.2\n      152.8\n    \n  \n\n17000 rows × 9 columns\n\n检查数据\n建议在使用数据之前，先对它有一个初步的了解。\n输出关于各列的一些实用统计信息快速摘要：样本数，均值，标准偏差，最大值，最小值和各种分位数。\ncalifornia_housing_dataframe.describe()\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      longitude\n      latitude\n      housing_median_age\n      total_rooms\n      total_bedrooms\n      population\n      households\n      median_income\n      median_house_value\n    \n  \n  \n    \n      count\n      17000.0\n      17000.0\n      17000.0\n      17000.0\n      17000.0\n      17000.0\n      17000.0\n      17000.0\n      17000.0\n    \n    \n      mean\n      -119.6\n      35.6\n      28.6\n      2643.7\n      539.4\n      1429.6\n      501.2\n      3.9\n      207.3\n    \n    \n      std\n      2.0\n      2.1\n      12.6\n      2179.9\n      421.5\n      1147.9\n      384.5\n      1.9\n      116.0\n    \n    \n      min\n      -124.3\n      32.5\n      1.0\n      2.0\n      1.0\n      3.0\n      1.0\n      0.5\n      15.0\n    \n    \n      25%\n      -121.8\n      33.9\n      18.0\n      1462.0\n      297.0\n      790.0\n      282.0\n      2.6\n      119.4\n    \n    \n      50%\n      -118.5\n      34.2\n      29.0\n      2127.0\n      434.0\n      1167.0\n      409.0\n      3.5\n      180.4\n    \n    \n      75%\n      -118.0\n      37.7\n      37.0\n      3151.2\n      648.2\n      1721.0\n      605.2\n      4.8\n      265.0\n    \n    \n      max\n      -114.3\n      42.0\n      52.0\n      37937.0\n      6445.0\n      35682.0\n      6082.0\n      15.0\n      500.0\n    \n  \n\n\n构建第一个模型\n尝试预测median_house_value，它将是我们的标签，也称为目标。我们将使用total_rooms作为输入特征。\n注意：我们使用的是城市街区级别的数据，因此该特征表示相应街区的房间总数。\n为了训练模型，我们将使用 TensorFlow Estimator API 提供的 LinearRegressor 接口。此 API 负责处理大量低级别模型搭建工作，并会提供执行模型训练、评估和推理的便利方法。\n第一步：定义特征并配置特征列\n为了将我们的训练数据导入TensorFlow，我们需要指定每个特征包含的数据类型。在练习中，主要使用一下两类数据：\n分类数据： 一种文字数据。\n数值数据：一种数字（整数或浮点数）数据以及希望是为数字的数据。\n\n在 TenssorFlow中，使用“特征列”的结构来表示特征的数据类型。特征列仅储存对特征数据的描述；不包含特征数据本身。\n一开始，只使用一个数值输入特征total_rooms。以下代码会从 california_housing_dataframe 中提取 total_rooms 数据，并使用 numeric_column 定义特征列，这样会将其数据指定为数值：\n# 定义输入特征 total_rooms.\nmy_feature = california_housing_dataframe[[&quot;total_rooms&quot;]]\n \n# 为total_rooms配置一个由数字构成的feature column\nfeature_columns = [tf.feature_column.numeric_column(&quot;total_rooms&quot;)]\n注意：total_rooms 数据的形状是一维数组（每个街区的房间总数列表）。这是 numeric_column 的默认形状，因此我们不必将其作为参数传递。\n第二步：定义目标\n接下来，我们将定义目标，也就是 median_house_value。同样，我们可以从 california_housing_dataframe 中提取它：\n# 定义目标\ntargets = california_housing_dataframe[&quot;median_house_value&quot;]\n第三步：配置LinearRegressor\n接下来，我们将使用 LinearRegressor 配置线性回归模型，并使用 GradientDescentOptimizer（它会实现小批量随机梯度下降法 (SGD)）训练该模型。learning_rate 参数可控制梯度步长的大小。\n注意：为了安全起见，我们还会通过 clip_gradients_by_norm 将梯度裁剪应用到我们的优化器。梯度裁剪可确保梯度大小在训练期间不会变得过大，梯度过大会导致梯度下降法失败。\n# 使用梯度下降作为训练模型的优化器\nmy_optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.0000001)\nmy_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)\n \n# 使用我们的特性列和优化器配置线性回归模型\n# 为梯度下降设置0.0000001的学习率\nlinear_regresor = tf.estimator.LinearRegressor(\n    feature_columns=feature_columns,\n    optimizer=my_optimizer\n)\n第四步：定义输入函数\n要将加利福尼亚州住房数据导入 LinearRegressor，我们需要定义一个输入函数，让它告诉 TensorFlow 如何对数据进行预处理，以及在模型训练期间如何批处理、随机处理和重复数据。\n首先，我们将 Pandas 特征数据转换成 NumPy 数组字典。然后，我们可以使用 TensorFlow Dataset API 根据我们的数据构建 Dataset 对象，并将数据拆分成大小为 batch_size 的多批数据，以按照指定周期数 (num_epochs) 进行重复。\n注意：如果将默认值 num_epochs=None 传递到 repeat()，输入数据会无限期重复。\n然后，如果 shuffle 设置为 True，则我们会对数据进行随机处理，以便数据在训练期间以随机方式传递到模型。buffer_size 参数会指定 shuffle 将从中随机抽样的数据集的大小。\n最后，输入函数会为该数据集构建一个迭代器，并向 LinearRegressor 返回下一批数据。\ndef my_input_fn(features, targets, batch_size=1, shuffle=True, num_epochs=None):\n    &quot;&quot;&quot;训练一个特征的线性回归模型\n    参数:\n      features: pandas DataFrame 类型的 features\n      targets: pandas DataFrame 类型的 targets\n      batch_size: Size of batches to be passed to the model\n      shuffle: True or False. 是否随机化 data.\n      num_epochs: 数据应重复的周期数. None = repeat indefinitely\n    Returns:\n      Tuple of (features, labels) for next data batch\n      元组：下一个数据批处理的(特征、标签/目标)\n    &quot;&quot;&quot;\n  \n    # 将pandas数据转换为numpy数组字典\n    features = {key: np.array(value) for key, value in dict(features).items()}\n    \n    # 构建dataset，并且拆分为batch_size个数据    \n    ds = Dataset.from_tensor_slices((features, targets))  # 限制为最大2GB\n    ds = ds.batch(batch_size).repeat(num_epochs)\n    \n    # shuffle\n    if shuffle:\n        ds = ds.shuffle(buffer_size=10000)\n        \n    # 返回下一批数据\n    features, labels = ds.make_one_shot_iterator().get_next()\n    return features, labels\n第五步：训练数据\n现在，我们可以在 linear_regressor 上调用 train() 来训练模型。我们会将 my_input_fn 封装在 lambda 中，以便可以将 my_feature 和 target 作为参数传入（有关详情，请参阅此 TensorFlow 输入函数教程），首先，我们会训练 100 步。\n_ = linear_regresor.train(\n    input_fn = lambda: my_input_fn(my_feature, targets),\n    steps=100\n)\n第六步： 评估模型\n我们基于该训练数据做一次预测，看看我们的模型在训练期间与这些数据的拟合情况。\n注意：训练误差可以衡量您的模型与训练数据的拟合情况，但并不能衡量模型泛化到新数据的效果。在后面的练习中，您将探索如何拆分数据以评估模型的泛化能力。\n# 为预测创建一个输入函数\npredication_input_fn = lambda: my_input_fn(my_feature, targets, num_epochs=1, shuffle=False)\n \n# 在 linear_regressor 上调用predict()进行预测\npredictions = linear_regresor.predict(input_fn=predication_input_fn)\n \n# 将预测格式化为一个NumPy的数组，这样我们就可以计算错误度量\npredictions = np.array([item[&#039;predictions&#039;][0] for item in predictions])\n \n# 输出方差\nmean_squared_error = metrics.mean_squared_error(predictions, targets)\nroot_mean_squared_error = math.sqrt(mean_squared_error)\n \nprint(&quot;Mean Squared Error (on training data): %0.3f&quot; % mean_squared_error)\nprint(&quot;Root Mean Squared Error (on training data): %0.3f&quot; % root_mean_squared_error)\nMean Squared Error (on training data): 56367.025\nRoot Mean Squared Error (on training data): 237.417\n\n如何判断误差有多大？\n由于均方误差 (MSE) 很难解读，因此我们经常查看的是均方根误差 (RMSE)。RMSE 的一个很好的特性是，它可以在与原目标相同的规模下解读。\n我们来比较一下 RMSE 与目标最大值和最小值的差值：\nmin_house_value = california_housing_dataframe[&quot;median_house_value&quot;].min()\nmax_house_value = california_housing_dataframe[&quot;median_house_value&quot;].max()\nmin_max_difference = max_house_value - min_house_value\n \nprint(&quot;Min. Median House Value: %0.3f&quot; % min_house_value)\nprint(&quot;Max. Median House Value: %0.3f&quot; % max_house_value)\nprint(&quot;Difference between Min. and Max.: %0.3f&quot; % min_max_difference)\nprint(&quot;Root Mean Squared Error: %0.3f&quot; % root_mean_squared_error)\nMin. Median House Value: 14.999\nMax. Median House Value: 500.001\nDifference between Min. and Max.: 485.002\nRoot Mean Squared Error: 237.417\n\n我们的误差跨越目标值近一般范围，为了进一步缩小误差，首先了解一下我们的预测predictions和targets的总体统计信息。\ncalibration_data = pd.DataFrame()\ncalibration_data[&quot;predictions&quot;] = pd.Series(predictions)\ncalibration_data[&quot;targets&quot;] = pd.Series(targets)\ncalibration_data.describe()\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      predictions\n      targets\n    \n  \n  \n    \n      count\n      17000.0\n      17000.0\n    \n    \n      mean\n      0.1\n      207.3\n    \n    \n      std\n      0.1\n      116.0\n    \n    \n      min\n      0.0\n      15.0\n    \n    \n      25%\n      0.1\n      119.4\n    \n    \n      50%\n      0.1\n      180.4\n    \n    \n      75%\n      0.2\n      265.0\n    \n    \n      max\n      1.9\n      500.0\n    \n  \n\n\n此信息也许有些帮助，比较看相差还是蛮大的，通过数据可视化来观察下\n我们知道，单个特征的线性回归可以绘制一条将输入 x 映射到输出 y 的线。\n首先，获得均匀分布的随机数据样本，以便绘制可辨识的散点图。\nsample = california_housing_dataframe.sample(n=300)  # 随机抽样300个来观察\n然后，根据模型的偏差项和特征权重绘制学习线，并绘制散点图。\n# 获取 total_rooms 最大与最小值\nx_0 = sample[&quot;total_rooms&quot;].min()\nx_1 = sample[&quot;total_rooms&quot;].max()\n \n# 检索训练过程中产生的最终权重和偏差\nweight = linear_regresor.get_variable_value(&quot;linear/linear_model/total_rooms/weights&quot;)[0]\nbias = linear_regresor.get_variable_value(&quot;linear/linear_model/bias_weights&quot;)\n \n# 获取total_rooms预测的最小和最大值\ny_0 = weight * x_0 + bias\ny_1 = weight * x_1 + bias\n \n# 现在我们有两个坐标后 画出回归线\nplt.plot([x_0, x_1], [y_0, y_1], c=&#039;r&#039;)\n \n# 写上每个轴代表的含义\nplt.ylabel(&quot;median_house_value&quot;)\nplt.xlabel(&quot;total_rooms&quot;)\n \n# 画出sample 数据的散点图\nplt.scatter(sample[&quot;total_rooms&quot;], sample[&quot;median_house_value&quot;])\n \nplt.show()\n \n\n这条线看起来明显和目标相差很大，综上所述，这些初级健全性检查提示我们也许可以找到更好的线。\n调整模型超参数\n我们把以上所学的东西整理到一个函数中，以方便我们更容易的调整参数和观察变化。\ndef train_model(learning_rate, steps, batch_size, input_feature=&quot;total_rooms&quot;):\n    &quot;&quot;&quot;一个线性回归的训练模型\n    参数：\n        learning_rate: 学习速率 float\n        steps: 训练总次数 int\n        batch_size: 批处理大小 非0 int\n        input_feature: 一个&#039; string &#039;，指定一个来自&#039; california_housing_dataframe &#039;的列用作输入特性。          \n    &quot;&quot;&quot;\n    periods = 10  # 周期\n    steps_per_period = steps / periods\n    \n    my_feature = input_feature\n    my_feature_data = california_housing_dataframe[[my_feature]]\n    my_label = &quot;median_house_value&quot;\n    targets = california_housing_dataframe[my_label]\n    \n    # 创建 feature columns\n    feature_columns = [tf.feature_column.numeric_column(my_feature)]\n    \n    # 创建 input feature\n    training_input_fn = lambda: my_input_fn(my_feature_data, targets, batch_size=batch_size)\n    prediction_input_fn = lambda: my_input_fn(my_feature_data, targets, num_epochs=1, shuffle=True)\n    \n    # 创建 一个线性回归对象\n    my_optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n    my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)\n    linear_regressor = tf.estimator.LinearRegressor(\n        feature_columns=feature_columns,\n        optimizer=my_optimizer\n    )\n    \n    # 设置回归线的状态\n    plt.figure(figsize=(15, 6))\n    plt.subplot(1, 2, 1)\n    plt.title(&quot;Learned Line by Period&quot;)\n    plt.ylabel(my_label)\n    plt.xlabel(my_feature)\n    sample = california_housing_dataframe.sample(n=300)\n    plt.scatter(sample[my_feature], sample[my_label])\n    colors = [cm.coolwarm(x) for x in np.linspace(-1, 1, periods)]\n    # 训练模型，但是在循环中这样做，这样我们就可以周期性地评估\n    # 损失指标\n    print(&quot;Training model...&quot;)\n    print(&quot;RMSE (on training data):&quot;)\n    root_mean_squared_errors = [] \n    for period in range(0, periods):\n        # 训练模型，从之前的状态开始\n        linear_regressor.train(\n            input_fn=training_input_fn,\n            steps=steps_per_period\n        )\n        # 计算预测值\n        predictions = linear_regressor.predict(input_fn=prediction_input_fn)\n        predictions = np.array([item[&#039;predictions&#039;][0] for item in predictions])\n        # 计算损失\n        root_mean_squared_error = math.sqrt(\n            metrics.mean_squared_error(predictions, targets))\n        print(&quot;   period %02d : %0.2f&quot; % (period, root_mean_squared_error))\n        # 添加loss 到list\n        root_mean_squared_errors.append(root_mean_squared_error)\n        # 纪录权重和偏差\n        y_extents = np.array([0, sample[my_label].max()])\n \n        weight = linear_regressor.get_variable_value(&#039;linear/linear_model/%s/weights&#039; % input_feature)[0]\n        bias = linear_regressor.get_variable_value(&#039;linear/linear_model/bias_weights&#039;)\n \n        x_extents = (y_extents - bias) / weight\n        x_extents = np.maximum(np.minimum(x_extents,\n                                          sample[my_feature].max()),\n                               sample[my_feature].min())\n        y_extents = weight * x_extents + bias\n        plt.plot(x_extents, y_extents, color=colors[period]) \n    \n    print(&quot;Model training finished.&quot;)\n    \n    # 输出一个周期内损失指标的图表。\n    plt.subplot(1, 2, 2)\n    plt.ylabel(&#039;RMSE&#039;)\n    plt.xlabel(&#039;Periods&#039;)\n    plt.title(&quot;Root Mean Squared Error vs. Periods&quot;)\n    plt.tight_layout()\n    plt.plot(root_mean_squared_errors)\n \n    # 输出带有校准数据的表\n    calibration_data = pd.DataFrame()\n    calibration_data[&quot;predictions&quot;] = pd.Series(predictions)\n    calibration_data[&quot;targets&quot;] = pd.Series(targets)\n    display.display(calibration_data.describe())\n \n    print(&quot;Final RMSE (on training data): %0.2f&quot; % root_mean_squared_error)  \n    return calibration_data\n设置参数初步训练下一试试\ncalibration_data = train_model(\n    learning_rate=0.00001,\n    steps=100,\n    batch_size=1\n)\nTraining model...\nRMSE (on training data):\n   period 00 : 236.40\n   period 01 : 235.26\n   period 02 : 234.10\n   period 03 : 232.96\n   period 04 : 231.87\n   period 05 : 230.78\n   period 06 : 229.62\n   period 07 : 228.54\n   period 08 : 227.36\n   period 09 : 226.38\nModel training finished.\n\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      predictions\n      targets\n    \n  \n  \n    \n      count\n      17000.0\n      17000.0\n    \n    \n      mean\n      13.2\n      207.3\n    \n    \n      std\n      10.9\n      116.0\n    \n    \n      min\n      0.0\n      15.0\n    \n    \n      25%\n      7.3\n      119.4\n    \n    \n      50%\n      10.6\n      180.4\n    \n    \n      75%\n      15.8\n      265.0\n    \n    \n      max\n      189.7\n      500.0\n    \n  \n\n\nFinal RMSE (on training data): 226.38\n\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      predictions\n      targets\n    \n  \n  \n    \n      0\n      9.7\n      66.9\n    \n    \n      1\n      10.6\n      80.1\n    \n    \n      2\n      7.0\n      85.7\n    \n    \n      3\n      13.6\n      73.4\n    \n    \n      4\n      14.0\n      65.5\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      16995\n      14.1\n      111.4\n    \n    \n      16996\n      7.5\n      79.0\n    \n    \n      16997\n      20.9\n      103.6\n    \n    \n      16998\n      12.6\n      85.8\n    \n    \n      16999\n      10.9\n      94.6\n    \n  \n\n17000 rows × 2 columns\n\n\n差距还是蛮大的，改变参数试试\ncalibration_data = train_model(\n    learning_rate=0.00002,\n    steps=500,\n    batch_size=5\n)\nTraining model...\nRMSE (on training data):\n   period 00 : 226.36\n   period 01 : 216.00\n   period 02 : 206.55\n   period 03 : 198.01\n   period 04 : 191.20\n   period 05 : 185.74\n   period 06 : 182.91\n   period 07 : 179.02\n   period 08 : 176.59\n   period 09 : 175.67\nModel training finished.\n\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      predictions\n      targets\n    \n  \n  \n    \n      count\n      17000.0\n      17000.0\n    \n    \n      mean\n      115.8\n      207.3\n    \n    \n      std\n      95.5\n      116.0\n    \n    \n      min\n      0.1\n      15.0\n    \n    \n      25%\n      64.0\n      119.4\n    \n    \n      50%\n      93.2\n      180.4\n    \n    \n      75%\n      138.0\n      265.0\n    \n    \n      max\n      1661.6\n      500.0\n    \n  \n\n\nFinal RMSE (on training data): 175.67\n\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      predictions\n      targets\n    \n  \n  \n    \n      0\n      110.0\n      66.9\n    \n    \n      1\n      51.7\n      80.1\n    \n    \n      2\n      144.4\n      85.7\n    \n    \n      3\n      79.7\n      73.4\n    \n    \n      4\n      77.4\n      65.5\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      16995\n      47.5\n      111.4\n    \n    \n      16996\n      138.7\n      79.0\n    \n    \n      16997\n      248.7\n      103.6\n    \n    \n      16998\n      117.8\n      85.8\n    \n    \n      16999\n      46.4\n      94.6\n    \n  \n\n17000 rows × 2 columns\n\n\n有适用于模型调整的标准启发法吗？\n降低RMSE，这是一个常见的问题。简短的答案是，不同超参数的效果取决于数据。因此，不存在必须遵循的规则，您需要对自己的数据进行测试。\n即便如此，我们仍在下面列出了几条可为您提供指导的经验法则：\n\n训练误差应该稳步减小，刚开始是急剧减小，最终应随着训练收敛达到平稳状态。\n如果训练尚未收敛，尝试运行更长的时间。\n如果训练误差减小速度过慢，则提高学习速率也许有助于加快其减小速度。\n\n但有时如果学习速率过高，训练误差的减小速度反而会变慢。\n\n\n如果训练误差变化很大，尝试降低学习速率。\n\n较低的学习速率和较大的步数/较大的批量大小通常是不错的组合。\n\n\n批量大小过小也会导致不稳定情况。不妨先尝试 100 或 1000 等较大的值，然后逐渐减小值的大小，直到出现性能降低的情况。\n\n重申一下，切勿严格遵循这些经验法则，因为效果取决于数据。请始终进行试验和验证。\n尝试其他特征\n使用 population 特征替换 total_rooms 特征，看看能否取得更好的效果。\ncalibration_data = train_model(\n    learning_rate=0.00002,\n    steps=1000,\n    batch_size=5,\n    input_feature=&quot;population&quot;\n)\nTraining model...\nRMSE (on training data):\n   period 00 : 225.40\n   period 01 : 214.37\n   period 02 : 204.42\n   period 03 : 195.76\n   period 04 : 188.29\n   period 05 : 182.98\n   period 06 : 178.93\n   period 07 : 175.94\n   period 08 : 174.97\n   period 09 : 175.14\nModel training finished.\n\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      predictions\n      targets\n    \n  \n  \n    \n      count\n      17000.0\n      17000.0\n    \n    \n      mean\n      120.1\n      207.3\n    \n    \n      std\n      96.4\n      116.0\n    \n    \n      min\n      0.3\n      15.0\n    \n    \n      25%\n      66.4\n      119.4\n    \n    \n      50%\n      98.0\n      180.4\n    \n    \n      75%\n      144.6\n      265.0\n    \n    \n      max\n      2997.3\n      500.0\n    \n  \n\n\nFinal RMSE (on training data): 175.14\n\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      predictions\n      targets\n    \n  \n  \n    \n      0\n      116.6\n      66.9\n    \n    \n      1\n      829.3\n      80.1\n    \n    \n      2\n      97.7\n      85.7\n    \n    \n      3\n      136.3\n      73.4\n    \n    \n      4\n      41.7\n      65.5\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      16995\n      56.4\n      111.4\n    \n    \n      16996\n      12.3\n      79.0\n    \n    \n      16997\n      83.9\n      103.6\n    \n    \n      16998\n      84.3\n      85.8\n    \n    \n      16999\n      70.1\n      94.6\n    \n  \n\n17000 rows × 2 columns\n\n\n合成特征和离群值\n尝试合成特征\ntotal_rooms 和 population 特征都会统计指定街区的相关总计数据。\n但是，如果一个街区比另一个街区的人口更密集，会怎么样？我们可以创建一个合成特征（即 total_rooms 与 population 的比例）来探索街区人口密度与房屋价值中位数之间的关系。\n在以下单元格中，创建一个名为 rooms_per_person 的特征，并将其用作 train_model() 的 input_feature。\n通过调整学习速率，您使用这一特征可以获得的最佳效果是什么？（效果越好，回归线与数据的拟合度就越高，最终 RMSE 也会越低。）\ncalifornia_housing_dataframe[&quot;rooms_per_person&quot;] = (\n    california_housing_dataframe[&quot;total_rooms&quot;] / california_housing_dataframe[&quot;population&quot;])\n \ncalibration_data = train_model(\n    learning_rate=0.05,\n    steps=500,\n    batch_size=5,\n    input_feature=&quot;rooms_per_person&quot;)\nTraining model...\nRMSE (on training data):\n   period 00 : 214.75\n   period 01 : 193.24\n   period 02 : 176.45\n   period 03 : 160.89\n   period 04 : 150.13\n   period 05 : 146.30\n   period 06 : 145.44\n   period 07 : 146.13\n   period 08 : 147.65\n   period 09 : 149.22\nModel training finished.\n\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      predictions\n      targets\n    \n  \n  \n    \n      count\n      17000.0\n      17000.0\n    \n    \n      mean\n      200.9\n      207.3\n    \n    \n      std\n      93.2\n      116.0\n    \n    \n      min\n      44.5\n      15.0\n    \n    \n      25%\n      164.4\n      119.4\n    \n    \n      50%\n      197.8\n      180.4\n    \n    \n      75%\n      226.2\n      265.0\n    \n    \n      max\n      4443.7\n      500.0\n    \n  \n\n\nFinal RMSE (on training data): 149.22\n\n\n识别离群值\n通过创建预测值与目标值的散点图来可视化模型效果。理想情况下，这些值将位于一条完全相关的对角线上。\n使用您在任务 1 中训练过的人均房间数模型，并使用 Pyplot 的 scatter() 创建预测值与目标值的散点图。\n您是否看到任何异常情况？通过查看 rooms_per_person 中值的分布情况，将这些异常情况追溯到源数据。\nplt.figure(figsize=(15, 6))\nplt.subplot(1, 2, 1)\nplt.scatter(calibration_data[&quot;predictions&quot;], calibration_data[&quot;targets&quot;])\nplt.subplot(1, 2, 2)\n_ = california_housing_dataframe[&quot;rooms_per_person&quot;].hist()\n\n校准数据显示，大多数散点与一条线对齐。这条线几乎是垂直的，我们稍后再讲解。现在，我们重点关注偏离这条线的点。我们注意到这些点的数量相对较少。\n观察我们绘制 rooms_per_person 的直方图，则会发现我们的输入数据中有少量离群值\n截取离群值\n将 rooms_per_person 的离群值设置为相对合理的最小值或最大值来进一步改进模型拟合情况。\n以下是一个如何将函数应用于 Pandas Series 的简单示例，供您参考：\nclipped_feature = my_dataframe[&quot;my_feature_name&quot;].apply(lambda x: max(x, 0))\n\n上述 clipped_feature 没有小于 0 的值。\n观察直方图发现大多数数值都小于5，我们将rooms_per_person 的值截取为5，然后绘制直方图再次检查结果。\ncalifornia_housing_dataframe[&quot;rooms_per_person&quot;] = (\n    california_housing_dataframe[&quot;rooms_per_person&quot;]).apply(lambda x: min(x, 5))\n \n_ = california_housing_dataframe[&quot;rooms_per_person&quot;].hist()\n\n验证截取是否有效，我们再训练一次模型，并再次输出校准数据：\ncalibration_data = train_model(\n    learning_rate=0.05,\n    steps=500,\n    batch_size=5,\n    input_feature=&quot;rooms_per_person&quot;)\nTraining model...\nRMSE (on training data):\n   period 00 : 214.33\n   period 01 : 192.43\n   period 02 : 172.30\n   period 03 : 155.07\n   period 04 : 142.01\n   period 05 : 134.29\n   period 06 : 129.31\n   period 07 : 128.99\n   period 08 : 127.60\n   period 09 : 126.94\nModel training finished.\n\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      predictions\n      targets\n    \n  \n  \n    \n      count\n      17000.0\n      17000.0\n    \n    \n      mean\n      189.5\n      207.3\n    \n    \n      std\n      49.3\n      116.0\n    \n    \n      min\n      45.1\n      15.0\n    \n    \n      25%\n      158.1\n      119.4\n    \n    \n      50%\n      189.6\n      180.4\n    \n    \n      75%\n      216.4\n      265.0\n    \n    \n      max\n      419.5\n      500.0\n    \n  \n\n\nFinal RMSE (on training data): 126.94\n\n\n_ = plt.scatter(calibration_data[&quot;predictions&quot;], calibration_data[&quot;targets&quot;])\n"},"Tensorflow/TensorFlow_4":{"slug":"Tensorflow/TensorFlow_4","filePath":"Tensorflow/TensorFlow_4.md","title":"TensorFlow 4","links":[],"tags":["Tensorflow"],"content":"验证\n通常我们会把数据分配为三分，训练集，交叉验证集和测试集，这样做的好处是为了避免过拟合，能够更好的泛化。\n\n添加验证集后，我们的工作流程大概是这样的：\n\n接下来的练习是尝试使用这个流程来训练\n与在之前的练习中一样，我们将使用加利福尼亚州住房数据集，尝试根据 1990 年的人口普查数据在城市街区级别预测 median_house_value。\n设置\n首先加载并准备数据。这一次使用多个特征，因此把逻辑模块化，以方便对特征进行预处理\nfrom __future__ import print_function\n \nimport math\n \nfrom IPython import display\nfrom matplotlib import cm\nfrom matplotlib import gridspec\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn import metrics\nimport tensorflow as tf\nfrom tensorflow.python.data import Dataset\n \ntf.logging.set_verbosity(tf.logging.ERROR)\npd.options.display.max_rows = 10\npd.options.display.float_format = &#039;{:.1f}&#039;.format\n \ncalifornia_housing_dataframe = pd.read_csv(&quot;download.mlcc.google.cn/mledu-datasets/california_housing_train.csv&quot;, sep=&quot;,&quot;)\ndef preprocess_features(california_housing_dataframe):\n    &quot;&quot;&quot;从加州住房数据集获取输入数据\n    参数:\n        california_housing_dataframe:panda的 DataFrame 类型的数据集\n    返回:\n        用于模型feature的DataFrame        \n    &quot;&quot;&quot;\n    selected_features = california_housing_dataframe[\n        [&quot;latitude&quot;,\n         &quot;longitude&quot;,\n         &quot;housing_median_age&quot;,\n         &quot;total_rooms&quot;,\n         &quot;total_bedrooms&quot;,\n         &quot;population&quot;,\n         &quot;households&quot;,\n         &quot;median_income&quot;]]\n    \n    processed_features = selected_features.copy()\n    # Create a synthetic feature.\n    processed_features[&quot;rooms_per_person&quot;] = (\n        california_housing_dataframe[&quot;total_rooms&quot;] /\n        california_housing_dataframe[&quot;population&quot;])\n    return processed_features\n \ndef preprocess_targets(california_housing_dataframe):\n    &quot;&quot;&quot;准备目标特性(即来自加州住房数据集。\n    参数:\n        california_housing_dataframe:panda的 DataFrame 类型的数据集\n    来自加州住房数据集。\n    返回:\n        用于模型feature的DataFrame \n    &quot;&quot;&quot;\n    output_targets = pd.DataFrame()\n    # 将targets的单位扩到到千为单位\n    output_targets[&quot;median_house_value&quot;] = (\n        california_housing_dataframe[&quot;median_house_value&quot;] / 1000.0)\n    return output_targets\n我们从 17000 个样本中选前 12000 个样本 作 训练集\ntraining_examples = preprocess_features(california_housing_dataframe.head(12000))\ntraining_examples.describe()\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      latitude\n      longitude\n      housing_median_age\n      total_rooms\n      total_bedrooms\n      population\n      households\n      median_income\n      rooms_per_person\n    \n  \n  \n    \n      count\n      12000.0\n      12000.0\n      12000.0\n      12000.0\n      12000.0\n      12000.0\n      12000.0\n      12000.0\n      12000.0\n    \n    \n      mean\n      34.6\n      -118.5\n      27.5\n      2655.7\n      547.1\n      1476.0\n      505.4\n      3.8\n      1.9\n    \n    \n      std\n      1.6\n      1.2\n      12.1\n      2258.1\n      434.3\n      1174.3\n      391.7\n      1.9\n      1.3\n    \n    \n      min\n      32.5\n      -121.4\n      1.0\n      2.0\n      2.0\n      3.0\n      2.0\n      0.5\n      0.0\n    \n    \n      25%\n      33.8\n      -118.9\n      17.0\n      1451.8\n      299.0\n      815.0\n      283.0\n      2.5\n      1.4\n    \n    \n      50%\n      34.0\n      -118.2\n      28.0\n      2113.5\n      438.0\n      1207.0\n      411.0\n      3.5\n      1.9\n    \n    \n      75%\n      34.4\n      -117.8\n      36.0\n      3146.0\n      653.0\n      1777.0\n      606.0\n      4.6\n      2.3\n    \n    \n      max\n      41.8\n      -114.3\n      52.0\n      37937.0\n      5471.0\n      35682.0\n      5189.0\n      15.0\n      55.2\n    \n  \n\n\ntraining_targets = preprocess_targets(california_housing_dataframe.head(12000))\ntraining_targets.describe()\n\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      median_house_value\n    \n  \n  \n    \n      count\n      12000.0\n    \n    \n      mean\n      198.0\n    \n    \n      std\n      111.9\n    \n    \n      min\n      15.0\n    \n    \n      25%\n      117.1\n    \n    \n      50%\n      170.5\n    \n    \n      75%\n      244.4\n    \n    \n      max\n      500.0\n    \n  \n\n\n我们从 17000 个样本中选择后 5000 个为 验证集\nvalidation_examples = preprocess_features(california_housing_dataframe.tail(5000))\nvalidation_examples.describe()\n\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      latitude\n      longitude\n      housing_median_age\n      total_rooms\n      total_bedrooms\n      population\n      households\n      median_income\n      rooms_per_person\n    \n  \n  \n    \n      count\n      5000.0\n      5000.0\n      5000.0\n      5000.0\n      5000.0\n      5000.0\n      5000.0\n      5000.0\n      5000.0\n    \n    \n      mean\n      38.1\n      -122.2\n      31.3\n      2614.8\n      521.1\n      1318.1\n      491.2\n      4.1\n      2.1\n    \n    \n      std\n      0.9\n      0.5\n      13.4\n      1979.6\n      388.5\n      1073.7\n      366.5\n      2.0\n      0.6\n    \n    \n      min\n      36.1\n      -124.3\n      1.0\n      8.0\n      1.0\n      8.0\n      1.0\n      0.5\n      0.1\n    \n    \n      25%\n      37.5\n      -122.4\n      20.0\n      1481.0\n      292.0\n      731.0\n      278.0\n      2.7\n      1.7\n    \n    \n      50%\n      37.8\n      -122.1\n      31.0\n      2164.0\n      424.0\n      1074.0\n      403.0\n      3.7\n      2.1\n    \n    \n      75%\n      38.4\n      -121.9\n      42.0\n      3161.2\n      635.0\n      1590.2\n      603.0\n      5.1\n      2.4\n    \n    \n      max\n      42.0\n      -121.4\n      52.0\n      32627.0\n      6445.0\n      28566.0\n      6082.0\n      15.0\n      18.3\n    \n  \n\n\nvalidation_targets = preprocess_targets(california_housing_dataframe.tail(5000))\nvalidation_targets.describe()\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      median_house_value\n    \n  \n  \n    \n      count\n      5000.0\n    \n    \n      mean\n      229.5\n    \n    \n      std\n      122.5\n    \n    \n      min\n      15.0\n    \n    \n      25%\n      130.4\n    \n    \n      50%\n      213.0\n    \n    \n      75%\n      303.2\n    \n    \n      max\n      500.0\n    \n  \n\n\n检查数据\n我们根据基准预期情况检查一下我们的数据：\n\n\n对于一些值（例如 median_house_value），我们可以检查这些值是否位于合理的范围内（请注意，这是 1990 年的数据，不是现在的！）。\n\n\n对于 latitude 和 longitude 等其他值，我们可以通过 Google 进行快速搜索，并快速检查一下它们与预期值是否一致。\n\n\n如果您仔细看，可能会发现下列异常情况：\n\n\nmedian_income 位于 3 到 15 的范围内。我们完全不清楚此范围究竟指的是什么，看起来可能是某对数尺度？无法找到相关记录；我们所能假设的只是，值越高，相应的收入越高。\n\n\nmedian_house_value 的最大值是 500001。这看起来像是某种人为设定的上限。\n\n\nrooms_per_person 特征通常在正常范围内，其中第 75 百分位数的值约为 2。但也有一些非常大的值（例如 18 或 55），这可能表明数据有一定程度的损坏。\n\n\n绘制维度/经度与房屋价值中位数的曲线图\n我们来详细了解一下 latitude 和 longitude 这两个特征。它们是相关城市街区的地理坐标。\n利用这两个特征可以提供出色的可视化结果 - 我们来绘制 latitude 和 longitude 的曲线图，然后用颜色标注 median_house_value。\ndef plot_scatter(training_examples, training_targets, validation_examples, validation_targets):\n    plt.figure(figsize=(13, 8))\n \n    ax = plt.subplot(1, 2, 1)\n    ax.set_title(&quot;Validation Data&quot;)\n \n    ax.set_autoscaley_on(False)\n    ax.set_ylim([32, 43])\n    ax.set_autoscalex_on(False)\n    ax.set_xlim([-126, -112])\n    plt.scatter(validation_examples[&quot;longitude&quot;],\n                validation_examples[&quot;latitude&quot;],\n                cmap=&quot;coolwarm&quot;,\n                c=validation_targets[&quot;median_house_value&quot;] / validation_targets[&quot;median_house_value&quot;].max())\n \n    ax = plt.subplot(1, 2, 2)\n    ax.set_title(&quot;Training Date&quot;)\n \n    ax.set_autoscaley_on(False)\n    ax.set_ylim([32, 43])\n    ax.set_autoscalex_on(False)\n    ax.set_xlim([-126, -112])\n    plt.scatter(training_examples[&quot;longitude&quot;],\n                training_examples[&quot;latitude&quot;],\n                cmap=&quot;coolwarm&quot;,\n                c=training_targets[&quot;median_house_value&quot;] / training_targets[&quot;median_house_value&quot;].max())\n    _ = plt.plot()\nplot_scatter(training_examples, training_targets, validation_examples, validation_targets)\n\n现在应该已经呈现出一幅不错的加利福尼亚州地图了，其中旧金山和洛杉矶等住房成本高昂的地区用红色表示。\n根据训练集呈现的地图有几分像真正的地图，但根据验证集呈现的明显不像。\n查看上面的摘要统计信息表格时，很容易产生想知道如何进行有用的数据检查的想法。每个街区 total_rooms 的第 75 百分位的正确值是什么？\n需要注意的关键一点是，对于任何指定特征或列，训练集和验证集之间的值的分布应该大致相同。\n我们真正需要担心的是，真实情况并非这样，这一事实表明我们创建训练集和验证集的拆分方式很可能存在问题。\n随机化处理数据\n我们需要在读入数据时，对数据进行随机化处理的。\n如果我们在创建训练集和验证集之前，没有对数据进行正确的随机化处理，那么以某种特定顺序接收数据可能会导致出现问题（似乎就是此时的问题）。\n发现并解决问题后，重新运行上面的 latitude/longitude 绘图单元格，并确认我们的健全性检查的结果看上去更好了。\n顺便提一下，在这一步中，我们会学到一项重要经验。\n机器学习中的调试通常是数据调试而不是代码调试。\n如果数据有误，即使最高级的机器学习代码也挽救不了局面。\ncalifornia_housing_dataframe = california_housing_dataframe.reindex(\n     np.random.permutation(california_housing_dataframe.index))\n \ntraining_examples = preprocess_features(california_housing_dataframe.head(12000))\ntraining_targets = preprocess_targets(california_housing_dataframe.head(12000))\n \nvalidation_examples = preprocess_features(california_housing_dataframe.tail(5000))\nvalidation_targets = preprocess_targets(california_housing_dataframe.tail(5000))\n \nplot_scatter(training_examples, training_targets, validation_examples, validation_targets)\n\n好的，这次的结果来看训练集和验证集都有相似的分布。\n训练和评估模型\n尝试不同的超参数，获得最佳验证效果。\n首先定义输入函数\ndef my_input_fn(features, targets, batch_size=1, shuffle=True, num_epochs=None):\n    features = {key: np.array(value) for key, value in dict(features).items()}\n    \n    ds = Dataset.from_tensor_slices((features, targets))\n    ds = ds.batch(batch_size).repeat(num_epochs)\n    \n    if shuffle:\n        ds = ds.shuffle(10000)\n    \n    features, labels = ds.make_one_shot_iterator().get_next()\n    return features, labels\n由于我们现在使用的是多个输入特征，因此需要把用于将特征列配置为独立函数的代码模块化。（目前此代码相当简单，因为我们的所有特征都是数值，但当我们在今后的练习中使用其他类型的特征时，会基于此代码进行构建。）\ndef construct_feature_columns(input_features):\n    &quot;&quot;&quot;构造TensorFlow特征列\n        参数:\n            input_features:要使用的数字输入特性的名称。\n        返回:\n            一个 feature columns 集合\n    &quot;&quot;&quot;\n    return set([tf.feature_column.numeric_column(my_feature) \n                for my_feature in input_features])\n接下来，继续完成 train_model() 代码，以设置输入函数和计算预测。\n注意：可以参考以前的练习中的代码，但要确保针对相应数据集调用 predict()。\n比较训练数据和验证数据的损失。使用一个原始特征时，我们得到的最佳均方根误差 (RMSE) 约为 180。\n现在我们可以使用多个特征，不妨看一下可以获得多好的结果。\n使用我们之前了解的一些方法检查数据。这些方法可能包括：\n\n\n比较预测值和实际目标值的分布情况\n\n\n绘制预测值和目标值的散点图\n\n\n使用 latitude 和 longitude 绘制两个验证数据散点图：\n\n一个散点图将颜色映射到实际目标 median_house_value\n另一个散点图将颜色映射到预测的 median_house_value，并排进行比较。\n\n\n\ndef train_model(\n    learning_rate,\n    strps,\n    batch_size,\n    training_examples,\n    training_targets,\n    validation_examples,\n    validation_targets):\n    &quot;&quot;&quot;训练多元特征的线性回归模型\n        除训练外，此功能还打印训练进度信息，\n        以及随着时间的推移而失去的训练和验证。\n    参数:\n        learning_rate:一个float，表示学习率\n        steps:一个非零的int，训练步骤的总数。训练步骤\n            由使用单个批处理的向前和向后传递组成。\n        batch_size:一个非零的int\n        training_example: DataFrame 包含一个或多个列\n        &#039;  california_housing_dataframe &#039;作为训练的输入feature\n        training_targets:一个&#039; DataFrame &#039;，它只包含一列\n        &#039; california_housing_dataframe &#039;作为训练的目标。\n        validation_example: &#039; DataFrame &#039;包含一个或多个列\n        &#039; california_housing_dataframe &#039;作为验证的输入feature\n        validation_targets: &#039; DataFrame &#039;，仅包含来自其中的一列\n        &#039; california_housing_dataframe &#039;作为验证的目标。\n    返回:\n        在训练数据上训练的“线性回归器”对象\n    &quot;&quot;&quot;\n    periods = 10\n    steps_per_period = strps / periods\n    \n    # 创建一个线性回归对象\n    my_optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n    my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)\n    linear_regressor = tf.estimator.LinearRegressor(\n        feature_columns=construct_feature_columns(training_examples),\n        optimizer=my_optimizer\n    )\n    \n    # 创建输入函数\n    training_input_fn = lambda: my_input_fn(\n        training_examples,\n        training_targets[&quot;median_house_value&quot;],\n        batch_size=batch_size)\n    \n    predict_training_input_fn = lambda: my_input_fn(\n      training_examples, \n      training_targets[&quot;median_house_value&quot;], \n      num_epochs=1, \n      shuffle=False)\n    \n    predict_validation_input_fn = lambda: my_input_fn(\n        validation_examples, \n        validation_targets[&quot;median_house_value&quot;],\n        num_epochs=1,\n        shuffle=False)\n    \n    #训练模型，但要在循环中进行，这样我们才能定期评估\n    #损失指标\n    print(&quot;Training model...&quot;)\n    print(&quot;RMSE (on training data):&quot;)\n    training_rmse = []\n    validation_rmse = []\n    for period in range (0, periods):\n      # Train the model, starting from the prior state.\n      linear_regressor.train(\n          input_fn=training_input_fn,\n          steps=steps_per_period,\n      )\n      # Take a break and compute predictions.\n      training_predictions = linear_regressor.predict(input_fn=predict_training_input_fn)\n      training_predictions = np.array([item[&#039;predictions&#039;][0] for item in training_predictions])\n      \n      validation_predictions = linear_regressor.predict(input_fn=predict_validation_input_fn)\n      validation_predictions = np.array([item[&#039;predictions&#039;][0] for item in validation_predictions])\n      \n      \n      # Compute training and validation loss.\n      training_root_mean_squared_error = math.sqrt(\n          metrics.mean_squared_error(training_predictions, training_targets))\n      validation_root_mean_squared_error = math.sqrt(\n          metrics.mean_squared_error(validation_predictions, validation_targets))\n      # Occasionally print the current loss.\n      print(&quot;  period %02d : %0.2f&quot; % (period, training_root_mean_squared_error))\n      # Add the loss metrics from this period to our list.\n      training_rmse.append(training_root_mean_squared_error)\n      validation_rmse.append(validation_root_mean_squared_error)\n    print(&quot;Model training finished.&quot;)\n \n    # Output a graph of loss metrics over periods.\n    plt.ylabel(&quot;RMSE&quot;)\n    plt.xlabel(&quot;Periods&quot;)\n    plt.title(&quot;Root Mean Squared Error vs. Periods&quot;)\n    plt.tight_layout()\n    plt.plot(training_rmse, label=&quot;training&quot;)\n    plt.plot(validation_rmse, label=&quot;validation&quot;)\n    plt.legend()\n \n    return linear_regressor    \nlinear_regressor = train_model(\n    learning_rate=0.00003,\n    strps=500,\n    batch_size=5,\n    training_examples=training_examples,\n    training_targets=training_targets,\n    validation_examples=validation_examples,\n    validation_targets=validation_targets)\nTraining model...\nRMSE (on training data):\n  period 00 : 217.57\n  period 01 : 200.14\n  period 02 : 185.74\n  period 03 : 175.52\n  period 04 : 170.71\n  period 05 : 167.06\n  period 06 : 165.72\n  period 07 : 165.65\n  period 08 : 166.77\n  period 09 : 168.40\nModel training finished.\n\n\n基于测试数据进行评估\n载入测试数据集并据此评估模型。\n我们已对验证数据进行了大量迭代。接下来确保我们没有过拟合该特定样本集的特性。\n测试数据集位于此处。\ncalifornia_housing_test_data = pd.read_csv(&quot;download.mlcc.google.cn/mledu-datasets/california_housing_test.csv&quot;, sep=&quot;,&quot;)\n \ntest_examples = preprocess_features(california_housing_test_data)\ntest_targets = preprocess_targets(california_housing_test_data)\n \npredict_test_input_fn = lambda: my_input_fn(\n      test_examples, \n      test_targets[&quot;median_house_value&quot;], \n      num_epochs=1, \n      shuffle=False)\n \ntest_predictions = linear_regressor.predict(input_fn=predict_test_input_fn)\ntest_predictions = np.array([item[&#039;predictions&#039;][0] for item in test_predictions])\n \nroot_mean_squared_error = math.sqrt(\n    metrics.mean_squared_error(test_predictions, test_targets))\n \nprint(&quot;Final RMSE (on test data): %0.2f&quot; % root_mean_squared_error)\nFinal RMSE (on test data): 162.99\n"},"Tensorflow/TensorFlow_5":{"slug":"Tensorflow/TensorFlow_5","filePath":"Tensorflow/TensorFlow_5.md","title":"TensorFlow 5","links":[],"tags":["Tensorflow"],"content":"表示法（Representation）\n机器学习模型不能直接看到、听到或感知输入样本。必须创建数据表示，为模型提供有用的信号来了解数据的关键特性。也就是说，为了训练模型，必须选择最能代表数据的特征集。\n特征工程\n传统编程的关注点是代码。在机器学习项目中，关注点变成了表示。也就是说，开发者通过添加和改善特征来调整模型。\n将原始数据映射到特征\n下图左侧边是来自数据源的原始数据，右侧表示特征矢量，也就是组成数据集中样本的浮点值集。 特征工程指的是将原始数据转换为特征矢量。进行特征工程预计需要大量时间。\n机器学习模型通常期望样本表示为实数矢量。这种矢量的构建方法如下：为每个字段衍生特征，然后将它们全部连接到一起\n\n映射数值\n机器学习模型根据浮点值进行训练，因此整数和浮点原始数据不需要特殊编码。正如下图 所示，将原始整数值 6 转换为特征值 6.0 是没有意义的：\n\n映射字符串值\n模型无法通过字符串值学习规律，因此您需要进行一些特征工程来将这些值转换为数字形式：\n1.首先，为您要表示的所有特征的字符串值定义一个词汇表。对于 street_name 特征，该词汇表中将包含您知道的所有街道。\n2.然后，使用该词汇表创建一个独热编码，用于将指定字符串值表示为二元矢量。在该矢量（与指定的字符串值对应）中：\n\n    ·只有一个元素设为 1。\n    ·其他所有元素均设为 0。\n    ·该矢量的长度等于词汇表中的元素数。\n    \n\n下图显示了某条特定街道 (Shorebird Way) 的独热编码。在此二元矢量中，代表 Shorebird Way 的元素的值为 1，而代表所有其他街道的元素的值为 θ。\n\n映射分类（枚举）值\n分类特征具有一组离散的可能值。例如，名为 Lowland Countries 的特征只包含 3 个可能值：\n{&#039;Netherlands&#039;, &#039;Belgium&#039;, &#039;Luxembourg&#039;}\n\n您可能会将分类特征（如 Lowland Countries）编码为枚举类型或表示不同值的整数离散集。例如：\n将荷兰表示为 0\n将比利时表示为 1\n将卢森堡表示为 2\n\n不过，机器学习模型通常将每个分类特征表示为单独的布尔值。例如，Lowland Countries 在模型中可以表示为 3 个单独的布尔值特征：\nx1：是荷兰吗？\nx2：是比利时吗？\nx3：是卢森堡吗？\n\n采用这种方法编码还可以简化某个值可能属于多个分类这种情况（例如，“与法国接壤”对于比利时和卢森堡来说都是 True）。\n良好特征的特点\n我们探索了将原始数据映射到合适特征矢量的方法，但这只是工作的一部分。现在，我们必须探索什么样的值才算这些特征矢量中良好的特征。\n避免很少使用的离散特征值\n良好的特征值应该在数据集中出现大约 5 次以上。这样一来，模型就可以学习该特征值与标签是如何关联的。也就是说，大量离散值相同的样本可让模型有机会了解不同设置中的特征，从而判断何时可以对标签很好地做出预测。\n例如，house_type 特征可能包含大量样本，其中它的值为 victorian：\nhouse_type: victorian\n\n相反，如果某个特征的值仅出现一次或者很少出现，则模型就无法根据该特征进行预测。\n例如，unique_house_id 就不适合作为特征，因为每个值只使用一次，模型无法从中学习任何规律：\nunique_house_id: 8SK982ZZ1242Z\n\n最好具有清晰明确的含义\n每个特征对于项目中的任何人来说都应该具有清晰明确的含义。\n例如，下面的房龄适合作为特征，可立即识别为年龄：\nhouse_age: 27\n\n相反，对于下方特征值的含义，除了创建它的工程师，其他人恐怕辨识不出：\nhouse_age: 851472000\n\n在某些情况下，混乱的数据（而不是糟糕的工程选择）会导致含义不清晰的值。\n例如，以下 user_age 的来源没有检查值恰当与否：\nuser_age: 277\n\n不要将“神奇”的值与实际数据混为一谈\n良好的浮点特征不包含超出范围的异常断点或“神奇”的值。\n例如，假设一个特征具有 0 到 1 之间的浮点值。那么，如下值是可以接受的：\nquality_rating: 0.82\nquality_rating: 0.37\n\n不过，如果用户没有输入 quality_rating，则数据集可能使用如下神奇值来表示不存在该值：\nquality_rating: -1\n\n为解决神奇值的问题，需将该特征转换为两个特征：\n一个特征只存储质量评分，不含神奇值。\n一个特征存储布尔值，表示是否提供了 quality_rating。为该布尔值特征指定一个名称，例如 is_quality_rating_defined。\n\n考虑上游不稳定性\n特征的定义不应随时间发生变化。\n例如，下列值是有用的，因为城市名称一般不会改变。（注意，我们仍然需要将“br/sao_paulo”这样的字符串转换为独热矢量。）\ncity_id: &quot;br/sao_paulo&quot;\n\n但收集由其他模型推理的值会产生额外成本。可能值“219”目前代表圣保罗，但这种表示在未来运行其他模型时可能轻易发生变化：\ninferred_city_cluster: &quot;219&quot;\n\n数据清理\n苹果树结出的果子有品相上乘的，也有虫蛀坏果。而高端便利店出售的苹果是 100% 完美的水果。从果园到水果店之间，专门有人花费大量时间将坏苹果剔除或给可以挽救的苹果涂上一层薄薄的蜡。作为一名机器学习工程师，您将花费大量的时间挑出坏样本并加工可以挽救的样本。即使是非常少量的“坏苹果”也会破坏掉一个大规模数据集。\n缩放特征值\n缩放是指将浮点特征值从自然范围（例如 100 到 900）转换为标准范围（例如 0 到 1 或 -1 到 +1）。如果某个特征集只包含一个特征，则缩放可以提供的实际好处微乎其微或根本没有。不过，如果特征集包含多个特征，则缩放特征可以带来以下优势：\n·帮助梯度下降法更快速地收敛。\n·帮助避免“NaN 陷阱”。在这种陷阱中，模型中的一个数值变成 NaN（例如，当某个值在训练期间超出浮点精确率限制时），并且模型中的所有其他数值最终也会因数学运算而变成 NaN。\n·帮助模型为每个特征确定合适的权重。如果没有进行特征缩放，则模型会对范围较大的特征投入过多精力。\n\n您不需要对每个浮点特征进行完全相同的缩放。即使特征 A 的范围是 -1 到 +1，同时特征 B 的范围是 -3 到 +3，也不会产生什么恶劣的影响。不过，如果特征 B 的范围是 5000 到 100000，您的模型会出现糟糕的响应。\n要缩放数字数据，一种显而易见的方法是将 [最小值，最大值] 以线性方式映射到较小的范围，例如 [-1，+1]。\n\n另一种热门的缩放策略是计算每个值的 Z 得分。Z 得分与距离均值的标准偏差数相关。换而言之：\n\n    scaled value = (value - mean) / stddev\n    \n例如，给定以下条件：\n\n    ·均值 = 100\n    ·标准偏差 = 20\n    ·原始值 = 130\n    \n则：\n\n  scaled_value = (130 - 100) / 20\n  scaled_value = 1.5\n使用 Z 得分进行缩放意味着，大多数缩放后的值将介于 -3 和 +3 之间，而少量值将略高于或低于该范围。```\n\n\n\n### 处理极端离群值\n\n分箱\n\n\n\n### 清查\n\n截至目前，我们假定用于训练和测试的所有数据都是值得信赖的。在现实生活中，数据集中的很多样本是不可靠的，原因有以下一种或多种：\n\n    遗漏值。 例如，有人忘记为某个房屋的年龄输入值。\n    重复样本。 例如，服务器错误地将同一条记录上传了两次。\n    不良标签。 例如，有人错误地将一颗橡树的图片标记为枫树。\n    不良特征值。 例如，有人输入了多余的位数，或者温度计被遗落在太阳底下。\n\n一旦检测到存在这些问题，您通常需要将相应样本从数据集中移除，从而“修正”不良样本。要检测遗漏值或重复样本，可以编写一个简单的程序。检测不良特征值或标签可能会比较棘手。\n\n除了检测各个不良样本之外，还必须检测集合中的不良数据。直方图是一种用于可视化集合中数据的很好机制。此外，收集如下统计信息也会有所帮助：\n\n    最大值和最小值\n    均值和中间值\n    标准偏差\n\n考虑生成离散特征的最常见值列表。例如，country:uk 的样本数是否符合您的预期？language:jp 是否真的应该作为您数据集中的最常用语言？\n\n了解数据\n\n遵循以下规则：\n\n    记住预期的数据状态。\n    确认数据是否满足这些预期（或者您可以解释为何数据不满足预期）。\n    仔细检查训练数据是否与其他来源（例如信息中心）的数据一致。\n    像处理任何任务关键型代码一样谨慎处理您的数据。良好的机器学习依赖于良好的数据。\n\n# 特征集编程练习\n\n创建一个包含极少特征但效果与更复杂的特征集一样出色的集合\n\n## 设置\n\n和之前一样，我们先加载并准备加利福尼亚州住房数据。\n\n\n```python\nfrom __future__ import print_function\n\nimport math\n\nfrom IPython import display\nfrom matplotlib import cm\nfrom matplotlib import gridspec\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn import metrics\nimport tensorflow as tf\nfrom tensorflow.python.data import Dataset\n\ntf.logging.set_verbosity(tf.logging.ERROR)\npd.options.display.max_rows = 10\npd.options.display.float_format = &#039;{:.1f}&#039;.format\n\ncalifornia_housing_dataframe = pd.read_csv(&quot;download.mlcc.google.cn/mledu-datasets/california_housing_train.csv&quot;, sep=&quot;,&quot;)\n\ncalifornia_housing_dataframe = california_housing_dataframe.reindex(\n    np.random.permutation(california_housing_dataframe.index))\n\n# 预处理\ndef preprocess_features(california_housing_dataframe):\n    selected_features = california_housing_dataframe[\n        [&quot;latitude&quot;,\n         &quot;longitude&quot;,\n         &quot;housing_median_age&quot;,\n         &quot;total_rooms&quot;,\n         &quot;total_bedrooms&quot;,\n         &quot;population&quot;,\n         &quot;households&quot;,\n         &quot;median_income&quot;]]\n    \n    processed_features = selected_features.copy()\n    \n    processed_features[&quot;rooms_per_person&quot;] = (\n        california_housing_dataframe[&quot;total_rooms&quot;]/\n        california_housing_dataframe[&quot;population&quot;])\n    return processed_features\n \ndef preprocess_targets(california_housing_dataframe):\n    output_targets = pd.DataFrame()\n    # Scale the target to be in units of thousands of dollars.\n    output_targets[&quot;median_house_value&quot;] = (\n      california_housing_dataframe[&quot;median_house_value&quot;] / 1000.0)\n    return output_targets\n# 抽取前 12000 个数据作训练集\ntraining_examples = preprocess_features(california_housing_dataframe.head(12000))\ntraining_targets = preprocess_targets(california_housing_dataframe.head(12000))\n \n# 抽取最后 5000 个作验证集\nvalidation_examples = preprocess_features(california_housing_dataframe.tail(5000))\nvalidation_targets = preprocess_targets(california_housing_dataframe.tail(5000))\n \n# 检查一下我们的数据是否正常合理\nprint(&quot;Training examples summary:&quot;)\ndisplay.display(training_examples.describe())\nprint(&quot;Validation examples summary:&quot;)\ndisplay.display(validation_examples.describe())\n \nprint(&quot;Training targets summary:&quot;)\ndisplay.display(training_targets.describe())\nprint(&quot;Validation targets summary:&quot;)\ndisplay.display(validation_targets.describe())\nTraining examples summary:\n\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      latitude\n      longitude\n      housing_median_age\n      total_rooms\n      total_bedrooms\n      population\n      households\n      median_income\n      rooms_per_person\n    \n  \n  \n    \n      count\n      12000.0\n      12000.0\n      12000.0\n      12000.0\n      12000.0\n      12000.0\n      12000.0\n      12000.0\n      12000.0\n    \n    \n      mean\n      35.6\n      -119.6\n      28.6\n      2655.8\n      543.1\n      1433.2\n      504.6\n      3.9\n      2.0\n    \n    \n      std\n      2.1\n      2.0\n      12.6\n      2180.4\n      425.2\n      1122.9\n      387.0\n      1.9\n      1.1\n    \n    \n      min\n      32.5\n      -124.3\n      2.0\n      2.0\n      1.0\n      3.0\n      1.0\n      0.5\n      0.1\n    \n    \n      25%\n      33.9\n      -121.8\n      18.0\n      1467.0\n      298.0\n      793.0\n      283.0\n      2.6\n      1.5\n    \n    \n      50%\n      34.2\n      -118.5\n      29.0\n      2127.0\n      435.0\n      1170.0\n      410.0\n      3.5\n      1.9\n    \n    \n      75%\n      37.7\n      -118.0\n      37.0\n      3157.2\n      653.0\n      1726.0\n      608.0\n      4.8\n      2.3\n    \n    \n      max\n      42.0\n      -114.6\n      52.0\n      32627.0\n      6445.0\n      28566.0\n      6082.0\n      15.0\n      55.2\n    \n  \n\n\nValidation examples summary:\n\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      latitude\n      longitude\n      housing_median_age\n      total_rooms\n      total_bedrooms\n      population\n      households\n      median_income\n      rooms_per_person\n    \n  \n  \n    \n      count\n      5000.0\n      5000.0\n      5000.0\n      5000.0\n      5000.0\n      5000.0\n      5000.0\n      5000.0\n      5000.0\n    \n    \n      mean\n      35.7\n      -119.6\n      28.6\n      2614.5\n      530.5\n      1420.8\n      493.0\n      3.9\n      2.0\n    \n    \n      std\n      2.1\n      2.0\n      12.6\n      2178.9\n      412.5\n      1205.6\n      378.5\n      2.0\n      1.3\n    \n    \n      min\n      32.5\n      -124.3\n      1.0\n      18.0\n      3.0\n      8.0\n      3.0\n      0.5\n      0.0\n    \n    \n      25%\n      33.9\n      -121.8\n      18.0\n      1445.0\n      294.0\n      780.0\n      276.8\n      2.6\n      1.5\n    \n    \n      50%\n      34.3\n      -118.5\n      29.0\n      2130.5\n      430.0\n      1159.5\n      406.0\n      3.5\n      1.9\n    \n    \n      75%\n      37.7\n      -118.0\n      37.0\n      3129.2\n      637.0\n      1700.8\n      595.0\n      4.7\n      2.3\n    \n    \n      max\n      41.8\n      -114.3\n      52.0\n      37937.0\n      5471.0\n      35682.0\n      5189.0\n      15.0\n      52.0\n    \n  \n\n\nTraining targets summary:\n\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      median_house_value\n    \n  \n  \n    \n      count\n      12000.0\n    \n    \n      mean\n      207.5\n    \n    \n      std\n      115.4\n    \n    \n      min\n      15.0\n    \n    \n      25%\n      119.9\n    \n    \n      50%\n      181.1\n    \n    \n      75%\n      265.6\n    \n    \n      max\n      500.0\n    \n  \n\n\nValidation targets summary:\n\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      median_house_value\n    \n  \n  \n    \n      count\n      5000.0\n    \n    \n      mean\n      206.9\n    \n    \n      std\n      117.5\n    \n    \n      min\n      15.0\n    \n    \n      25%\n      118.8\n    \n    \n      50%\n      178.4\n    \n    \n      75%\n      263.1\n    \n    \n      max\n      500.0\n    \n  \n\n\n构建良好的特征集\n如果只使用 2 个或 3 个特征，您可以获得的最佳效果是什么？\n相关矩阵展现了两两比较的相关性，既包括每个特征与目标特征之间的比较，也包括每个特征与其他特征之间的比较。\n在这里，相关性被定义为皮尔逊相关系数。您不必理解具体数学原理也可完成本练习。\n相关性值具有以下含义：\n\n-1.0：完全负相关\n0.0：不相关\n1.0：完全正相关\n\ncorrelation_dataframe = training_examples.copy()\ncorrelation_dataframe[&quot;target&quot;] = training_targets[&quot;median_house_value&quot;]\n \ncorrelation_dataframe.corr()\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      latitude\n      longitude\n      housing_median_age\n      total_rooms\n      total_bedrooms\n      population\n      households\n      median_income\n      rooms_per_person\n      target\n    \n  \n  \n    \n      latitude\n      1.0\n      -0.9\n      0.0\n      -0.0\n      -0.1\n      -0.1\n      -0.1\n      -0.1\n      0.1\n      -0.1\n    \n    \n      longitude\n      -0.9\n      1.0\n      -0.1\n      0.0\n      0.1\n      0.1\n      0.1\n      -0.0\n      -0.1\n      -0.0\n    \n    \n      housing_median_age\n      0.0\n      -0.1\n      1.0\n      -0.4\n      -0.3\n      -0.3\n      -0.3\n      -0.1\n      -0.1\n      0.1\n    \n    \n      total_rooms\n      -0.0\n      0.0\n      -0.4\n      1.0\n      0.9\n      0.9\n      0.9\n      0.2\n      0.1\n      0.1\n    \n    \n      total_bedrooms\n      -0.1\n      0.1\n      -0.3\n      0.9\n      1.0\n      0.9\n      1.0\n      -0.0\n      0.1\n      0.0\n    \n    \n      population\n      -0.1\n      0.1\n      -0.3\n      0.9\n      0.9\n      1.0\n      0.9\n      -0.0\n      -0.1\n      -0.0\n    \n    \n      households\n      -0.1\n      0.1\n      -0.3\n      0.9\n      1.0\n      0.9\n      1.0\n      0.0\n      -0.0\n      0.1\n    \n    \n      median_income\n      -0.1\n      -0.0\n      -0.1\n      0.2\n      -0.0\n      -0.0\n      0.0\n      1.0\n      0.3\n      0.7\n    \n    \n      rooms_per_person\n      0.1\n      -0.1\n      -0.1\n      0.1\n      0.1\n      -0.1\n      -0.0\n      0.3\n      1.0\n      0.2\n    \n    \n      target\n      -0.1\n      -0.0\n      0.1\n      0.1\n      0.0\n      -0.0\n      0.1\n      0.7\n      0.2\n      1.0\n    \n  \n\n\n理想情况下，我们希望具有与目标密切相关的特征。\n此外，我们还希望有一些相互之间的相关性不太密切的特征，以便它们添加独立信息。\n利用这些信息来尝试移除特征。您也可以尝试构建其他合成特征，例如两个原始特征的比例。\n为方便起见，我们已经添加了前一个练习的训练代码。\ndef construct_feature_columns(input_features):\n  &quot;&quot;&quot;Construct the TensorFlow Feature Columns.\n \n  Args:\n    input_features: The names of the numerical input features to use.\n  Returns:\n    A set of feature columns\n  &quot;&quot;&quot; \n  return set([tf.feature_column.numeric_column(my_feature)\n              for my_feature in input_features])\n \ndef my_input_fn(features, targets, batch_size=1, shuffle=True, num_epochs=None):\n    &quot;&quot;&quot;Trains a linear regression model.\n  \n    Args:\n      features: pandas DataFrame of features\n      targets: pandas DataFrame of targets\n      batch_size: Size of batches to be passed to the model\n      shuffle: True or False. Whether to shuffle the data.\n      num_epochs: Number of epochs for which data should be repeated. None = repeat indefinitely\n    Returns:\n      Tuple of (features, labels) for next data batch\n    &quot;&quot;&quot;\n    \n    # Convert pandas data into a dict of np arrays.\n    features = {key:np.array(value) for key,value in dict(features).items()}                                           \n    \n    # Construct a dataset, and configure batching/repeating.\n    ds = Dataset.from_tensor_slices((features,targets)) # warning: 2GB limit\n    ds = ds.batch(batch_size).repeat(num_epochs)\n \n    # Shuffle the data, if specified.\n    if shuffle:\n      ds = ds.shuffle(10000)\n    \n    # Return the next batch of data.\n    features, labels = ds.make_one_shot_iterator().get_next()\n    return features, labels\n \ndef train_model(\n    learning_rate,\n    steps,\n    batch_size,\n    training_examples,\n    training_targets,\n    validation_examples,\n    validation_targets):\n  &quot;&quot;&quot;Trains a linear regression model.\n  \n  In addition to training, this function also prints training progress information,\n  as well as a plot of the training and validation loss over time.\n  \n  Args:\n    learning_rate: A `float`, the learning rate.\n    steps: A non-zero `int`, the total number of training steps. A training step\n      consists of a forward and backward pass using a single batch.\n    batch_size: A non-zero `int`, the batch size.\n    training_examples: A `DataFrame` containing one or more columns from\n      `california_housing_dataframe` to use as input features for training.\n    training_targets: A `DataFrame` containing exactly one column from\n      `california_housing_dataframe` to use as target for training.\n    validation_examples: A `DataFrame` containing one or more columns from\n      `california_housing_dataframe` to use as input features for validation.\n    validation_targets: A `DataFrame` containing exactly one column from\n      `california_housing_dataframe` to use as target for validation.\n      \n  Returns:\n    A `LinearRegressor` object trained on the training data.\n  &quot;&quot;&quot;\n \n  periods = 10\n  steps_per_period = steps / periods\n \n  # Create a linear regressor object.\n  my_optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n  my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)\n  linear_regressor = tf.estimator.LinearRegressor(\n      feature_columns=construct_feature_columns(training_examples),\n      optimizer=my_optimizer\n  )\n    \n  # Create input functions.\n  training_input_fn = lambda: my_input_fn(training_examples, \n                                          training_targets[&quot;median_house_value&quot;], \n                                          batch_size=batch_size)\n  predict_training_input_fn = lambda: my_input_fn(training_examples, \n                                                  training_targets[&quot;median_house_value&quot;], \n                                                  num_epochs=1, \n                                                  shuffle=False)\n  predict_validation_input_fn = lambda: my_input_fn(validation_examples, \n                                                    validation_targets[&quot;median_house_value&quot;], \n                                                    num_epochs=1, \n                                                    shuffle=False)\n \n  # Train the model, but do so inside a loop so that we can periodically assess\n  # loss metrics.\n  print(&quot;Training model...&quot;)\n  print(&quot;RMSE (on training data):&quot;)\n  training_rmse = []\n  validation_rmse = []\n  for period in range (0, periods):\n    # Train the model, starting from the prior state.\n    linear_regressor.train(\n        input_fn=training_input_fn,\n        steps=steps_per_period,\n    )\n    # Take a break and compute predictions.\n    training_predictions = linear_regressor.predict(input_fn=predict_training_input_fn)\n    training_predictions = np.array([item[&#039;predictions&#039;][0] for item in training_predictions])\n    \n    validation_predictions = linear_regressor.predict(input_fn=predict_validation_input_fn)\n    validation_predictions = np.array([item[&#039;predictions&#039;][0] for item in validation_predictions])\n    \n    # Compute training and validation loss.\n    training_root_mean_squared_error = math.sqrt(\n        metrics.mean_squared_error(training_predictions, training_targets))\n    validation_root_mean_squared_error = math.sqrt(\n        metrics.mean_squared_error(validation_predictions, validation_targets))\n    # Occasionally print the current loss.\n    print(&quot;  period %02d : %0.2f&quot; % (period, training_root_mean_squared_error))\n    # Add the loss metrics from this period to our list.\n    training_rmse.append(training_root_mean_squared_error)\n    validation_rmse.append(validation_root_mean_squared_error)\n  print(&quot;Model training finished.&quot;)\n \n  \n  # Output a graph of loss metrics over periods.\n  plt.ylabel(&quot;RMSE&quot;)\n  plt.xlabel(&quot;Periods&quot;)\n  plt.title(&quot;Root Mean Squared Error vs. Periods&quot;)\n  plt.tight_layout()\n  plt.plot(training_rmse, label=&quot;training&quot;)\n  plt.plot(validation_rmse, label=&quot;validation&quot;)\n  plt.legend()\n \n  return linear_regressor\n搜索一组效果良好的特征和训练参数\nminimal_features = [\n  &quot;latitude&quot;,\n  &quot;longitude&quot;,\n]\n \nminimal_training_examples = training_examples[minimal_features]\nminimal_validation_examples = validation_examples[minimal_features]\n \n_ = train_model(\n    learning_rate=0.01,\n    steps=500,\n    batch_size=5,\n    training_examples=minimal_training_examples,\n    training_targets=training_targets,\n    validation_examples=minimal_validation_examples,\n    validation_targets=validation_targets)\n \nplt.show()\n \nminimal_features = [\n  &quot;median_income&quot;,\n  &quot;latitude&quot;,\n]\n \nminimal_training_examples = training_examples[minimal_features]\nminimal_validation_examples = validation_examples[minimal_features]\n \n_ = train_model(\n    learning_rate=0.01,\n    steps=500,\n    batch_size=5,\n    training_examples=minimal_training_examples,\n    training_targets=training_targets,\n    validation_examples=minimal_validation_examples,\n    validation_targets=validation_targets)\n \nplt.show()\nTraining model...\nRMSE (on training data):\n  period 00 : 115.69\n  period 01 : 115.56\n  period 02 : 116.82\n  period 03 : 115.67\n  period 04 : 115.57\n  period 05 : 116.72\n  period 06 : 116.71\n  period 07 : 119.22\n  period 08 : 115.56\n  period 09 : 115.40\nModel training finished.\n\n\nTraining model...\nRMSE (on training data):\n  period 00 : 165.32\n  period 01 : 125.38\n  period 02 : 116.53\n  period 03 : 115.99\n  period 04 : 115.63\n  period 05 : 114.81\n  period 06 : 114.06\n  period 07 : 113.58\n  period 08 : 114.35\n  period 09 : 112.88\nModel training finished.\n\n\n观察发现 经度 和 维度 貌似是没有什么相关度的，个人收入的中位数 和 维度 是一组比较好的特征组。\n更好地利用纬度\n绘制 latitude 与 median_house_value 的图形后，表明两者确实不存在线性关系。\n不过，有几个峰值与洛杉矶和旧金山大致相对应。\nplt.scatter(training_examples[&quot;latitude&quot;], training_targets[&quot;median_house_value&quot;])\n\n&lt;matplotlib.collections.PathCollection at 0x7fb5a238afd0&gt;\n\n\n尝试创建一些能够更好地利用纬度的合成特征。\n例如，您可以创建某个特征，将 latitude 映射到值 |latitude - 38|，并将该特征命名为 distance_from_san_francisco。\n或者，您可以将该空间分成 10 个不同的分桶（例如 latitude_32_to_33、latitude_33_to_34 等）：如果 latitude 位于相应分桶范围内，则显示值 1.0；如果不在范围内，则显示值 0.0。\n使用相关矩阵来指导您构建合成特征；如果您发现效果还不错的合成特征，可以将其添加到您的模型中。\n除了 latitude 之外，我们还会保留 median_income，以便与之前的结果进行比较。\n我们决定对纬度进行分桶。在 Pandas 中使用 Series.apply 执行此操作相当简单。\nLATITUDE_RANGES = zip(range(32, 44), range(33, 45))\ndef select_and_transform_features(source_df):\n    selected_examples = pd.DataFrame()\n    selected_examples[&quot;median_income&quot;] = source_df[&quot;median_income&quot;]\n    for r in LATITUDE_RANGES:\n      selected_examples[&quot;latitude_%d_to_%d&quot; % r] = source_df[&quot;latitude&quot;].apply(\n        lambda l: 1.0 if l &gt;= r[0] and l &lt; r[1] else 0.0)\n    return selected_examples\nselected_training_examples = select_and_transform_features(training_examples)\nselected_training_examples.head(2)\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      median_income\n      latitude_32_to_33\n      latitude_33_to_34\n      latitude_34_to_35\n      latitude_35_to_36\n      latitude_36_to_37\n      latitude_37_to_38\n      latitude_38_to_39\n      latitude_39_to_40\n      latitude_40_to_41\n      latitude_41_to_42\n      latitude_42_to_43\n      latitude_43_to_44\n    \n  \n  \n    \n      4828\n      3.4\n      0.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      6103\n      4.7\n      0.0\n      0.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n  \n\n\nselected_validation_examples = select_and_transform_features(validation_examples)\nselected_validation_examples.head(2)\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      median_income\n      latitude_32_to_33\n      latitude_33_to_34\n      latitude_34_to_35\n      latitude_35_to_36\n      latitude_36_to_37\n      latitude_37_to_38\n      latitude_38_to_39\n      latitude_39_to_40\n      latitude_40_to_41\n      latitude_41_to_42\n      latitude_42_to_43\n      latitude_43_to_44\n    \n  \n  \n    \n      8959\n      7.0\n      0.0\n      0.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      8330\n      2.6\n      0.0\n      0.0\n      1.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n  \n\n\n_ = train_model(\n    learning_rate=0.01,\n    steps=500,\n    batch_size=5,\n    training_examples=selected_training_examples,\n    training_targets=training_targets,\n    validation_examples=selected_validation_examples,\n    validation_targets=validation_targets)\nTraining model...\nRMSE (on training data):\n  period 00 : 227.14\n  period 01 : 216.98\n  period 02 : 206.92\n  period 03 : 196.96\n  period 04 : 187.10\n  period 05 : 177.38\n  period 06 : 167.81\n  period 07 : 158.44\n  period 08 : 149.28\n  period 09 : 140.38\nModel training finished.\n\n"},"Tensorflow/TensorFlow_6":{"slug":"Tensorflow/TensorFlow_6","filePath":"Tensorflow/TensorFlow_6.md","title":"TensorFlow 6","links":[],"tags":["Tensorflow"],"content":"特征组合 (Feature Crosses)\n特征组合是指两个或多个特征相乘形成的合成特征。特征的相乘组合可以提供超出这些特征单独能够提供的预测能力。\n对非线性规律进行编码\n组合独热矢量\n到目前为止，我们已经重点介绍了如何对两个单独的浮点特征进行特征组合。在实践中，机器学习模型很少会组合连续特征。不过，机器学习模型却经常组合独热特征矢量，将独热特征矢量的特征组合视为逻辑连接。例如，假设我们具有以下两个特征：国家/地区和语言。对每个特征进行独热编码会生成具有二元特征的矢量，这些二元特征可解读为 country=USA, country=France 或 language=English, language=Spanish。然后，如果您对这些独热编码进行特征组合，则会得到可解读为逻辑连接的二元特征，如下所示：\n  country:usa AND language:spanish\n\n再举一个例子，假设您对纬度和经度进行分箱，获得单独的独热 5 元素特征矢量。例如，指定的纬度和经度可以表示如下：\n  binned_latitude = [0, 0, 0, 1, 0]\n  binned_longitude = [0, 1, 0, 0, 0]\n\n假设您对这两个特征矢量创建了特征组合：\nbinned_latitude X binned_longitude\n\n此特征组合是一个 25 元素独热矢量（24 个 0 和 1 个 1）。该组合中的单个 1 表示纬度与经度的特定连接。然后，您的模型就可以了解到有关这种连接的特定关联性。\n假设我们更粗略地对纬度和经度进行分箱，如下所示：\nbinned_latitude(lat) = [\n  0  &lt; lat &lt;= 10\n  10 &lt; lat &lt;= 20\n  20 &lt; lat &lt;= 30\n]\n\nbinned_longitude(lon) = [\n  0  &lt; lon &lt;= 15\n  15 &lt; lon &lt;= 30\n]\n\n针对这些粗略分箱创建特征组合会生成具有以下含义的合成特征：\nbinned_latitude_X_longitude(lat, lon) = [\n  0  &lt; lat &lt;= 10 AND 0  &lt; lon &lt;= 15\n  0  &lt; lat &lt;= 10 AND 15 &lt; lon &lt;= 30\n  10 &lt; lat &lt;= 20 AND 0  &lt; lon &lt;= 15\n  10 &lt; lat &lt;= 20 AND 15 &lt; lon &lt;= 30\n  20 &lt; lat &lt;= 30 AND 0  &lt; lon &lt;= 15\n  20 &lt; lat &lt;= 30 AND 15 &lt; lon &lt;= 30\n]\n\n现在，假设我们的模型需要根据以下两个特征来预测狗主人对狗狗的满意程度：\n行为类型（吠叫、叫、偎依等）\n时段\n如果我们根据这两个特征构建以下特征组合：\n  [behavior type X time of day]\n  \n\n我们最终获得的预测能力将远远超过任一特征单独的预测能力。例如，如果狗狗在下午 5 点主人下班回来时（快乐地）叫喊，可能表示对主人满意度的正面预测结果。如果狗狗在凌晨 3 点主人熟睡时（也许痛苦地）哀叫，可能表示对主人满意度的强烈负面预测结果。\n线性学习器可以很好地扩展到大量数据。对大规模数据集使用特征组合是学习高度复杂模型的一种有效策略。神经网络可提供另一种策略。\n编程练习\n\n通过添加其他合成特征来改进线性回归模型（这是前一个练习的延续）\n使用输入函数将 Pandas DataFrame 对象转换为 Tensors，并在 fit() 和 predict() 中调用输入函数\n使用 FTRL 优化算法进行模型训练\n通过独热编码、分箱和特征组合创建新的合成特征\n\n设置\n首先，我们来定义输入并创建数据加载代码，正如我们在之前的练习中所做的那样。\nfrom __future__ import print_function\n \nimport math\n \nfrom IPython import display\nfrom matplotlib import cm\nfrom matplotlib import gridspec\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn import metrics\nimport tensorflow as tf\nfrom tensorflow.python.data import Dataset\n \ntf.logging.set_verbosity(tf.logging.ERROR)\npd.options.display.max_rows = 10\npd.options.display.float_format = &#039;{:.1f}&#039;.format\n \ncalifornia_housing_dataframe = pd.read_csv(&quot;download.mlcc.google.cn/mledu-datasets/california_housing_train.csv&quot;, sep=&quot;,&quot;)\n \ncalifornia_housing_dataframe = california_housing_dataframe.reindex(\n    np.random.permutation(california_housing_dataframe.index))\ndef preprocess_features(california_housing_dataframe):\n  &quot;&quot;&quot;Prepares input features from California housing data set.\n \n  Args:\n    california_housing_dataframe: A Pandas DataFrame expected to contain data\n      from the California housing data set.\n  Returns:\n    A DataFrame that contains the features to be used for the model, including\n    synthetic features.\n  &quot;&quot;&quot;\n  selected_features = california_housing_dataframe[\n    [&quot;latitude&quot;,\n     &quot;longitude&quot;,\n     &quot;housing_median_age&quot;,\n     &quot;total_rooms&quot;,\n     &quot;total_bedrooms&quot;,\n     &quot;population&quot;,\n     &quot;households&quot;,\n     &quot;median_income&quot;]]\n  processed_features = selected_features.copy()\n  # Create a synthetic feature.\n  processed_features[&quot;rooms_per_person&quot;] = (\n    california_housing_dataframe[&quot;total_rooms&quot;] /\n    california_housing_dataframe[&quot;population&quot;])\n  return processed_features\n \ndef preprocess_targets(california_housing_dataframe):\n  &quot;&quot;&quot;Prepares target features (i.e., labels) from California housing data set.\n \n  Args:\n    california_housing_dataframe: A Pandas DataFrame expected to contain data\n      from the California housing data set.\n  Returns:\n    A DataFrame that contains the target feature.\n  &quot;&quot;&quot;\n  output_targets = pd.DataFrame()\n  # Scale the target to be in units of thousands of dollars.\n  output_targets[&quot;median_house_value&quot;] = (\n    california_housing_dataframe[&quot;median_house_value&quot;] / 1000.0)\n  return output_targets\n# Choose the first 12000 (out of 17000) examples for training.\ntraining_examples = preprocess_features(california_housing_dataframe.head(12000))\ntraining_targets = preprocess_targets(california_housing_dataframe.head(12000))\n \n# Choose the last 5000 (out of 17000) examples for validation.\nvalidation_examples = preprocess_features(california_housing_dataframe.tail(5000))\nvalidation_targets = preprocess_targets(california_housing_dataframe.tail(5000))\n \n# Double-check that we&#039;ve done the right thing.\nprint(&quot;Training examples summary:&quot;)\ndisplay.display(training_examples.describe())\nprint(&quot;Validation examples summary:&quot;)\ndisplay.display(validation_examples.describe())\n \nprint(&quot;Training targets summary:&quot;)\ndisplay.display(training_targets.describe())\nprint(&quot;Validation targets summary:&quot;)\ndisplay.display(validation_targets.describe())\nTraining examples summary:\n\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      latitude\n      longitude\n      housing_median_age\n      total_rooms\n      total_bedrooms\n      population\n      households\n      median_income\n      rooms_per_person\n    \n  \n  \n    \n      count\n      12000.0\n      12000.0\n      12000.0\n      12000.0\n      12000.0\n      12000.0\n      12000.0\n      12000.0\n      12000.0\n    \n    \n      mean\n      35.6\n      -119.5\n      28.6\n      2643.5\n      539.4\n      1430.0\n      501.4\n      3.9\n      2.0\n    \n    \n      std\n      2.1\n      2.0\n      12.6\n      2203.2\n      423.9\n      1153.9\n      388.0\n      1.9\n      1.2\n    \n    \n      min\n      32.5\n      -124.3\n      1.0\n      8.0\n      1.0\n      3.0\n      1.0\n      0.5\n      0.1\n    \n    \n      25%\n      33.9\n      -121.8\n      18.0\n      1457.8\n      295.0\n      788.0\n      280.0\n      2.6\n      1.5\n    \n    \n      50%\n      34.2\n      -118.5\n      29.0\n      2121.0\n      432.0\n      1165.0\n      407.0\n      3.5\n      1.9\n    \n    \n      75%\n      37.7\n      -118.0\n      37.0\n      3149.2\n      647.0\n      1717.0\n      603.2\n      4.8\n      2.3\n    \n    \n      max\n      42.0\n      -114.3\n      52.0\n      37937.0\n      5471.0\n      35682.0\n      5189.0\n      15.0\n      55.2\n    \n  \n\n\nValidation examples summary:\n\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      latitude\n      longitude\n      housing_median_age\n      total_rooms\n      total_bedrooms\n      population\n      households\n      median_income\n      rooms_per_person\n    \n  \n  \n    \n      count\n      5000.0\n      5000.0\n      5000.0\n      5000.0\n      5000.0\n      5000.0\n      5000.0\n      5000.0\n      5000.0\n    \n    \n      mean\n      35.7\n      -119.6\n      28.7\n      2644.1\n      539.3\n      1428.5\n      500.7\n      3.9\n      2.0\n    \n    \n      std\n      2.1\n      2.0\n      12.6\n      2123.4\n      415.6\n      1133.3\n      376.0\n      1.9\n      1.1\n    \n    \n      min\n      32.5\n      -124.3\n      1.0\n      2.0\n      2.0\n      6.0\n      2.0\n      0.5\n      0.0\n    \n    \n      25%\n      33.9\n      -121.8\n      18.0\n      1473.0\n      300.0\n      792.0\n      285.0\n      2.6\n      1.5\n    \n    \n      50%\n      34.3\n      -118.5\n      29.0\n      2148.5\n      437.0\n      1172.0\n      413.5\n      3.5\n      2.0\n    \n    \n      75%\n      37.7\n      -118.0\n      37.0\n      3153.2\n      652.2\n      1737.0\n      608.0\n      4.8\n      2.3\n    \n    \n      max\n      42.0\n      -114.6\n      52.0\n      32627.0\n      6445.0\n      28566.0\n      6082.0\n      15.0\n      41.3\n    \n  \n\n\nTraining targets summary:\n\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      median_house_value\n    \n  \n  \n    \n      count\n      12000.0\n    \n    \n      mean\n      207.6\n    \n    \n      std\n      116.2\n    \n    \n      min\n      15.0\n    \n    \n      25%\n      119.8\n    \n    \n      50%\n      180.4\n    \n    \n      75%\n      265.7\n    \n    \n      max\n      500.0\n    \n  \n\n\nValidation targets summary:\n\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      median_house_value\n    \n  \n  \n    \n      count\n      5000.0\n    \n    \n      mean\n      206.5\n    \n    \n      std\n      115.6\n    \n    \n      min\n      15.0\n    \n    \n      25%\n      118.8\n    \n    \n      50%\n      180.1\n    \n    \n      75%\n      263.4\n    \n    \n      max\n      500.0\n    \n  \n\n\ndef construct_feature_columns(input_features):\n  &quot;&quot;&quot;Construct the TensorFlow Feature Columns.\n \n  Args:\n    input_features: The names of the numerical input features to use.\n  Returns:\n    A set of feature columns\n  &quot;&quot;&quot;\n  return set([tf.feature_column.numeric_column(my_feature)\n              for my_feature in input_features])\ndef my_input_fn(features, targets, batch_size=1, shuffle=True, num_epochs=None):\n    &quot;&quot;&quot;Trains a linear regression model.\n  \n    Args:\n      features: pandas DataFrame of features\n      targets: pandas DataFrame of targets\n      batch_size: Size of batches to be passed to the model\n      shuffle: True or False. Whether to shuffle the data.\n      num_epochs: Number of epochs for which data should be repeated. None = repeat indefinitely\n    Returns:\n      Tuple of (features, labels) for next data batch\n    &quot;&quot;&quot;\n    \n    # Convert pandas data into a dict of np arrays.\n    features = {key:np.array(value) for key,value in dict(features).items()}                                           \n \n    # Construct a dataset, and configure batching/repeating.\n    ds = Dataset.from_tensor_slices((features,targets)) # warning: 2GB limit\n    ds = ds.batch(batch_size).repeat(num_epochs)\n    \n    # Shuffle the data, if specified.\n    if shuffle:\n      ds = ds.shuffle(10000)\n    \n    # Return the next batch of data.\n    features, labels = ds.make_one_shot_iterator().get_next()\n    return features, labels\nFTRL 优化算法\n高维度线性模型可受益于使用一种基于梯度的优化方法，叫做 FTRL。该算法的优势是针对不同系数以不同方式调整学习速率，如果某些特征很少采用非零值，该算法可能比较实用（也非常适合支持 L1 正则化）。我们可以使用 FtrlOptimizer 来应用 FTRL。\ndef train_model(\n    learning_rate,\n    steps,\n    batch_size,\n    feature_columns,\n    training_examples,\n    training_targets,\n    validation_examples,\n    validation_targets):\n  &quot;&quot;&quot;Trains a linear regression model.\n  \n  In addition to training, this function also prints training progress information,\n  as well as a plot of the training and validation loss over time.\n  \n  Args:\n    learning_rate: A `float`, the learning rate.\n    steps: A non-zero `int`, the total number of training steps. A training step\n      consists of a forward and backward pass using a single batch.\n    feature_columns: A `set` specifying the input feature columns to use.\n    training_examples: A `DataFrame` containing one or more columns from\n      `california_housing_dataframe` to use as input features for training.\n    training_targets: A `DataFrame` containing exactly one column from\n      `california_housing_dataframe` to use as target for training.\n    validation_examples: A `DataFrame` containing one or more columns from\n      `california_housing_dataframe` to use as input features for validation.\n    validation_targets: A `DataFrame` containing exactly one column from\n      `california_housing_dataframe` to use as target for validation.\n      \n  Returns:\n    A `LinearRegressor` object trained on the training data.\n  &quot;&quot;&quot;\n \n  periods = 10\n  steps_per_period = steps / periods\n \n  # Create a linear regressor object.\n  my_optimizer = tf.train.FtrlOptimizer(learning_rate=learning_rate)\n  my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)\n  linear_regressor = tf.estimator.LinearRegressor(\n      feature_columns=feature_columns,\n      optimizer=my_optimizer\n  )\n  \n  training_input_fn = lambda: my_input_fn(training_examples, \n                                          training_targets[&quot;median_house_value&quot;], \n                                          batch_size=batch_size)\n  predict_training_input_fn = lambda: my_input_fn(training_examples, \n                                                  training_targets[&quot;median_house_value&quot;], \n                                                  num_epochs=1, \n                                                  shuffle=False)\n  predict_validation_input_fn = lambda: my_input_fn(validation_examples, \n                                                    validation_targets[&quot;median_house_value&quot;], \n                                                    num_epochs=1, \n                                                    shuffle=False)\n \n  # Train the model, but do so inside a loop so that we can periodically assess\n  # loss metrics.\n  print(&quot;Training model...&quot;)\n  print(&quot;RMSE (on training data):&quot;)\n  training_rmse = []\n  validation_rmse = []\n  for period in range (0, periods):\n    # Train the model, starting from the prior state.\n    linear_regressor.train(\n        input_fn=training_input_fn,\n        steps=steps_per_period\n    )\n    # Take a break and compute predictions.\n    training_predictions = linear_regressor.predict(input_fn=predict_training_input_fn)\n    training_predictions = np.array([item[&#039;predictions&#039;][0] for item in training_predictions])\n    validation_predictions = linear_regressor.predict(input_fn=predict_validation_input_fn)\n    validation_predictions = np.array([item[&#039;predictions&#039;][0] for item in validation_predictions])\n    \n    # Compute training and validation loss.\n    training_root_mean_squared_error = math.sqrt(\n        metrics.mean_squared_error(training_predictions, training_targets))\n    validation_root_mean_squared_error = math.sqrt(\n        metrics.mean_squared_error(validation_predictions, validation_targets))\n    # Occasionally print the current loss.\n    print(&quot;  period %02d : %0.2f&quot; % (period, training_root_mean_squared_error))\n    # Add the loss metrics from this period to our list.\n    training_rmse.append(training_root_mean_squared_error)\n    validation_rmse.append(validation_root_mean_squared_error)\n  print(&quot;Model training finished.&quot;)\n \n  \n  # Output a graph of loss metrics over periods.\n  plt.ylabel(&quot;RMSE&quot;)\n  plt.xlabel(&quot;Periods&quot;)\n  plt.title(&quot;Root Mean Squared Error vs. Periods&quot;)\n  plt.tight_layout()\n  plt.plot(training_rmse, label=&quot;training&quot;)\n  plt.plot(validation_rmse, label=&quot;validation&quot;)\n  plt.legend()\n \n  return linear_regressor\n_ = train_model(\n    learning_rate=1.0,\n    steps=500,\n    batch_size=100,\n    feature_columns=construct_feature_columns(training_examples),\n    training_examples=training_examples,\n    training_targets=training_targets,\n    validation_examples=validation_examples,\n    validation_targets=validation_targets)\nTraining model...\nRMSE (on training data):\n  period 00 : 180.73\n  period 01 : 170.12\n  period 02 : 111.43\n  period 03 : 157.79\n  period 04 : 147.28\n  period 05 : 139.07\n  period 06 : 120.82\n  period 07 : 148.39\n  period 08 : 118.07\n  period 09 : 117.32\nModel training finished.\n\n\n离散特征的独热编码\n通常，在训练逻辑回归模型之前，离散（即字符串、枚举、整数）特征会转换为二元特征系列。\n例如，假设我们创建了一个合成特征，可以采用 0、1 或 2 中的任何值，并且我们还具有以下几个训练点：\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfeature_value021021\n对于每个可能的分类值，我们都会创建一个新的二元实值特征，该特征只能采用两个可能值中的一个：如果示例中包含该值，则值为 1.0；如果不包含，则值为 0.0。在上述示例中，分类特征会被转换成三个特征，现在训练点如下所示：\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nfeature_value_0feature_value_1feature_value_200.00.01.011.00.00.020.01.00.0\n分桶（分箱）特征\n分桶也称为分箱。\n例如，我们可以将 population 分为以下 3 个分桶：\n\nbucket_0 (&lt; 5000)：对应于人口分布较少的街区\nbucket_1 (5000 - 25000)：对应于人口分布适中的街区\nbucket_2 (&gt; 25000)：对应于人口分布较多的街区\n\n根据前面的分桶定义，以下 population 矢量：\n[[10001], [42004], [2500], [18000]]\n\n将变成以下经过分桶的特征矢量：\n[[1], [2], [0], [1]]\n\n这些特征值现在是分桶索引。请注意，这些索引被视为离散特征。通常情况下，这些特征将被进一步转换为上述独热表示法，但这是以透明方式实现的。\n要为分桶特征定义特征列，我们可以使用 bucketized_column（而不是使用 numeric_column），该列将数字列作为输入，并使用 boundaries 参数中指定的分桶边界将其转换为分桶特征。以下代码为 households 和 longitude 定义了分桶特征列；get_quantile_based_boundaries 函数会根据分位数计算边界，以便每个分桶包含相同数量的元素。\ndef get_quantile_based_boundaries(feature_values, num_buckets):\n    boundaries = np.arange(1.0, num_buckets) / num_buckets\n    quantiles = feature_values.quantile(boundaries)\n    return [quantiles[q] for q in quantiles.keys()]\n \n# 把 households 分成7桶\nhouseholds = tf.feature_column.numeric_column(&quot;housholds&quot;)\nbucketized_households = tf.feature_column.bucketized_column(\n    households, boundaries=get_quantile_based_boundaries(\n    california_housing_dataframe[&quot;households&quot;], 7))\n \n# 把 longitude 分成10桶\nlongitude = tf.feature_column.numeric_column(&quot;longitude&quot;)\nbucketized_longitude = tf.feature_column.bucketized_column(\n    longitude, boundaries=get_quantile_based_boundaries(\n    california_housing_dataframe[&quot;longitude&quot;], 10))\n任务 1：使用分桶特征列训练模型\n将我们示例中的所有实值特征进行分桶，训练模型，然后查看结果是否有所改善。\n在前面的代码块中，两个实值列（即 households 和 longitude）已被转换为分桶特征列。您的任务是对其余的列进行分桶，然后运行代码来训练模型。您可以采用各种启发法来确定分桶的范围。本练习使用了分位数技巧，通过这种方式选择分桶边界后，每个分桶将包含相同数量的样本。\ndef construct_feature_columns():\n  &quot;&quot;&quot;Construct the TensorFlow Feature Columns.\n \n  Returns:\n    A set of feature columns\n  &quot;&quot;&quot; \n  households = tf.feature_column.numeric_column(&quot;households&quot;)\n  longitude = tf.feature_column.numeric_column(&quot;longitude&quot;)\n  latitude = tf.feature_column.numeric_column(&quot;latitude&quot;)\n  housing_median_age = tf.feature_column.numeric_column(&quot;housing_median_age&quot;)\n  median_income = tf.feature_column.numeric_column(&quot;median_income&quot;)\n  rooms_per_person = tf.feature_column.numeric_column(&quot;rooms_per_person&quot;)\n  \n  # Divide households into 7 buckets.\n  bucketized_households = tf.feature_column.bucketized_column(\n    households, boundaries=get_quantile_based_boundaries(\n      training_examples[&quot;households&quot;], 7))\n \n  # Divide longitude into 10 buckets.\n  bucketized_longitude = tf.feature_column.bucketized_column(\n    longitude, boundaries=get_quantile_based_boundaries(\n      training_examples[&quot;longitude&quot;], 10))\n  \n  # Divide latitude into 10 buckets.\n  bucketized_latitude = tf.feature_column.bucketized_column(\n    latitude, boundaries=get_quantile_based_boundaries(\n      training_examples[&quot;latitude&quot;], 10))\n \n  # Divide housing_median_age into 7 buckets.\n  bucketized_housing_median_age = tf.feature_column.bucketized_column(\n    housing_median_age, boundaries=get_quantile_based_boundaries(\n      training_examples[&quot;housing_median_age&quot;], 7))\n  \n  # Divide median_income into 7 buckets.\n  bucketized_median_income = tf.feature_column.bucketized_column(\n    median_income, boundaries=get_quantile_based_boundaries(\n      training_examples[&quot;median_income&quot;], 7))\n  \n  # Divide rooms_per_person into 7 buckets.\n  bucketized_rooms_per_person = tf.feature_column.bucketized_column(\n    rooms_per_person, boundaries=get_quantile_based_boundaries(\n      training_examples[&quot;rooms_per_person&quot;], 7))\n  \n  feature_columns = set([\n    bucketized_longitude,\n    bucketized_latitude,\n    bucketized_housing_median_age,\n    bucketized_households,\n    bucketized_median_income,\n    bucketized_rooms_per_person])\n  \n  return feature_columns\n_ = train_model(\n    learning_rate=1.0,\n    steps=500,\n    batch_size=100,\n    feature_columns=construct_feature_columns(),\n    training_examples=training_examples,\n    training_targets=training_targets,\n    validation_examples=validation_examples,\n    validation_targets=validation_targets)\nTraining model...\nRMSE (on training data):\n  period 00 : 169.83\n  period 01 : 143.51\n  period 02 : 126.97\n  period 03 : 115.77\n  period 04 : 107.83\n  period 05 : 101.97\n  period 06 : 97.44\n  period 07 : 93.81\n  period 08 : 90.89\n  period 09 : 88.49\nModel training finished.\n\n\n特征组合\n组合两个（或更多个）特征是使用线性模型来学习非线性关系的一种聪明做法。在我们的问题中，如果我们只使用 latitude 特征进行学习，那么该模型可能会发现特定纬度（或特定纬度范围内，因为我们已经将其分桶）的城市街区更可能比其他街区住房成本高昂。longitude 特征的情况与此类似。但是，如果我们将 longitude 与 latitude 组合，产生的组合特征则代表一个明确的城市街区。如果模型发现某些城市街区（位于特定纬度和经度范围内）更可能比其他街区住房成本高昂，那么这将是比单独考虑两个特征更强烈的信号。\n目前，特征列 API 仅支持组合离散特征。要组合两个连续的值（比如 latitude 或 longitude），我们可以对其进行分桶。\n如果我们组合 latitude 和 longitude 特征（例如，假设 longitude 被分到 2 个分桶中，而 latitude 有 3 个分桶），我们实际上会得到 6 个组合的二元特征。当我们训练模型时，每个特征都会分别获得自己的权重。\ndef construct_feature_columns():\n  &quot;&quot;&quot;Construct the TensorFlow Feature Columns.\n \n  Returns:\n    A set of feature columns\n  &quot;&quot;&quot; \n  households = tf.feature_column.numeric_column(&quot;households&quot;)\n  longitude = tf.feature_column.numeric_column(&quot;longitude&quot;)\n  latitude = tf.feature_column.numeric_column(&quot;latitude&quot;)\n  housing_median_age = tf.feature_column.numeric_column(&quot;housing_median_age&quot;)\n  median_income = tf.feature_column.numeric_column(&quot;median_income&quot;)\n  rooms_per_person = tf.feature_column.numeric_column(&quot;rooms_per_person&quot;)\n  \n  # Divide households into 7 buckets.\n  bucketized_households = tf.feature_column.bucketized_column(\n    households, boundaries=get_quantile_based_boundaries(\n      training_examples[&quot;households&quot;], 7))\n \n  # Divide longitude into 10 buckets.\n  bucketized_longitude = tf.feature_column.bucketized_column(\n    longitude, boundaries=get_quantile_based_boundaries(\n      training_examples[&quot;longitude&quot;], 10))\n  \n  # Divide latitude into 10 buckets.\n  bucketized_latitude = tf.feature_column.bucketized_column(\n    latitude, boundaries=get_quantile_based_boundaries(\n      training_examples[&quot;latitude&quot;], 10))\n \n  # Divide housing_median_age into 7 buckets.\n  bucketized_housing_median_age = tf.feature_column.bucketized_column(\n    housing_median_age, boundaries=get_quantile_based_boundaries(\n      training_examples[&quot;housing_median_age&quot;], 7))\n  \n  # Divide median_income into 7 buckets.\n  bucketized_median_income = tf.feature_column.bucketized_column(\n    median_income, boundaries=get_quantile_based_boundaries(\n      training_examples[&quot;median_income&quot;], 7))\n  \n  # Divide rooms_per_person into 7 buckets.\n  bucketized_rooms_per_person = tf.feature_column.bucketized_column(\n    rooms_per_person, boundaries=get_quantile_based_boundaries(\n      training_examples[&quot;rooms_per_person&quot;], 7))\n  \n  # 为long_x_lat feature cross做一个特性列\n  long_x_lat = tf.feature_column.crossed_column(\n  set([bucketized_longitude, bucketized_latitude]), hash_bucket_size=1000) \n  \n  feature_columns = set([\n    bucketized_longitude,\n    bucketized_latitude,\n    bucketized_housing_median_age,\n    bucketized_households,\n    bucketized_median_income,\n    bucketized_rooms_per_person,\n    long_x_lat])\n  \n  return feature_columns\n_ = train_model(\n    learning_rate=1.0,\n    steps=500,\n    batch_size=100,\n    feature_columns=construct_feature_columns(),\n    training_examples=training_examples,\n    training_targets=training_targets,\n    validation_examples=validation_examples,\n    validation_targets=validation_targets)\nTraining model...\nRMSE (on training data):\n  period 00 : 163.99\n  period 01 : 135.79\n  period 02 : 118.69\n  period 03 : 107.28\n  period 04 : 99.33\n  period 05 : 93.53\n  period 06 : 89.03\n  period 07 : 85.54\n  period 08 : 82.65\n  period 09 : 80.32\nModel training finished.\n\n"},"Tensorflow/TensorFlow_7":{"slug":"Tensorflow/TensorFlow_7","filePath":"Tensorflow/TensorFlow_7.md","title":"TensorFlow 7","links":[],"tags":["Tensorflow"],"content":"正则化 (Regularization for Simplicity)\n略\n查准率和召回率\n略\n逻辑回归\n与在之前的练习中一样，我们将使用加利福尼亚州住房数据集，但这次我们会预测某个城市街区的住房成本是否高昂，从而将其转换成一个二元分类问题。此外，我们还会暂时恢复使用默认特征。\n构建二元分类问题\n数据集的目标是 median_house_value，它是一个数值（连续值）特征。我们可以通过向此连续值使用阈值来创建一个布尔值标签。\n我们希望通过某个城市街区的特征预测该街区的住房成本是否高昂。为了给训练数据和评估数据准备目标，我们针对房屋价值中位数定义了分类阈值 - 第 75 百分位数（约为 265000）。所有高于此阈值的房屋价值标记为 1，其他值标记为 0。\n设置\nfrom __future__ import print_function\n \nimport math\n \nfrom IPython import display\nfrom matplotlib import cm\nfrom matplotlib import gridspec\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn import metrics\nimport tensorflow as tf\nfrom tensorflow.python.data import Dataset\n \ntf.logging.set_verbosity(tf.logging.ERROR)\npd.options.display.max_rows = 10\npd.options.display.float_format = &#039;{:.1f}&#039;.format\n \ncalifornia_housing_dataframe = pd.read_csv(&quot;download.mlcc.google.cn/mledu-datasets/california_housing_train.csv&quot;, sep=&quot;,&quot;)\n \ncalifornia_housing_dataframe = california_housing_dataframe.reindex(\n    np.random.permutation(california_housing_dataframe.index))\n注意以下代码与之前练习中的代码之间稍有不同。我们并没有将 median_house_value 用作目标，而是创建了一个新的二元目标 median_house_value_is_high。\ndef preprocess_features(california_housing_dataframe):\n    selected_features = california_housing_dataframe[\n    [&quot;latitude&quot;,\n     &quot;longitude&quot;,\n     &quot;housing_median_age&quot;,\n     &quot;total_rooms&quot;,\n     &quot;total_bedrooms&quot;,\n     &quot;population&quot;,\n     &quot;households&quot;,\n     &quot;median_income&quot;]]    \n    processed_features = selected_features.copy()\n    processed_features[&quot;rooms_per_person&quot;] = (\n        california_housing_dataframe[&quot;total_rooms&quot;] /\n        california_housing_dataframe[&quot;population&quot;])\n    return processed_features\n \ndef preprocess_targets(california_housing_dataframe):\n    output_targets = pd.DataFrame()\n    output_targets[&quot;median_house_value_is_high&quot;] = (\n    california_housing_dataframe[&quot;median_house_value&quot;] &gt; 265000).astype(float)\n    return output_targets\ntraining_examples = preprocess_features(california_housing_dataframe.head(12000))\ntraining_targets = preprocess_targets(california_housing_dataframe.head(12000))\n \nvalidation_examples = preprocess_features(california_housing_dataframe.tail(5000))\nvalidation_targets = preprocess_targets(california_housing_dataframe.tail(5000))\n \nprint(&quot;Training examples summary:&quot;)\ndisplay.display(training_examples.describe())\nprint(&quot;Validation examples summary:&quot;)\ndisplay.display(validation_examples.describe())\n \nprint(&quot;Training targets summary:&quot;)\ndisplay.display(training_targets.describe())\nprint(&quot;Validation targets summary:&quot;)\ndisplay.display(validation_targets.describe())\nTraining examples summary:\n\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      latitude\n      longitude\n      housing_median_age\n      total_rooms\n      total_bedrooms\n      population\n      households\n      median_income\n      rooms_per_person\n    \n  \n  \n    \n      count\n      12000.0\n      12000.0\n      12000.0\n      12000.0\n      12000.0\n      12000.0\n      12000.0\n      12000.0\n      12000.0\n    \n    \n      mean\n      35.6\n      -119.6\n      28.6\n      2630.6\n      537.6\n      1426.3\n      499.6\n      3.9\n      2.0\n    \n    \n      std\n      2.1\n      2.0\n      12.6\n      2156.5\n      415.3\n      1158.6\n      379.3\n      1.9\n      1.2\n    \n    \n      min\n      32.5\n      -124.3\n      1.0\n      2.0\n      1.0\n      6.0\n      1.0\n      0.5\n      0.0\n    \n    \n      25%\n      33.9\n      -121.8\n      18.0\n      1460.8\n      297.0\n      790.0\n      282.0\n      2.6\n      1.5\n    \n    \n      50%\n      34.2\n      -118.5\n      29.0\n      2113.5\n      432.0\n      1168.0\n      408.0\n      3.5\n      1.9\n    \n    \n      75%\n      37.7\n      -118.0\n      37.0\n      3138.2\n      647.0\n      1717.2\n      603.0\n      4.8\n      2.3\n    \n    \n      max\n      42.0\n      -114.3\n      52.0\n      37937.0\n      6445.0\n      35682.0\n      6082.0\n      15.0\n      55.2\n    \n  \n\n\nValidation examples summary:\n\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      latitude\n      longitude\n      housing_median_age\n      total_rooms\n      total_bedrooms\n      population\n      households\n      median_income\n      rooms_per_person\n    \n  \n  \n    \n      count\n      5000.0\n      5000.0\n      5000.0\n      5000.0\n      5000.0\n      5000.0\n      5000.0\n      5000.0\n      5000.0\n    \n    \n      mean\n      35.7\n      -119.6\n      28.6\n      2675.1\n      543.8\n      1437.5\n      505.0\n      3.9\n      2.0\n    \n    \n      std\n      2.1\n      2.0\n      12.5\n      2235.0\n      436.1\n      1121.7\n      396.8\n      1.9\n      1.0\n    \n    \n      min\n      32.5\n      -124.3\n      2.0\n      12.0\n      4.0\n      3.0\n      3.0\n      0.5\n      0.3\n    \n    \n      25%\n      33.9\n      -121.8\n      18.0\n      1465.8\n      295.8\n      788.0\n      278.0\n      2.6\n      1.5\n    \n    \n      50%\n      34.3\n      -118.5\n      28.0\n      2172.0\n      438.0\n      1165.0\n      411.0\n      3.5\n      1.9\n    \n    \n      75%\n      37.7\n      -118.0\n      37.0\n      3176.0\n      652.0\n      1732.2\n      609.0\n      4.8\n      2.3\n    \n    \n      max\n      41.9\n      -114.6\n      52.0\n      32054.0\n      5290.0\n      15507.0\n      5050.0\n      15.0\n      27.1\n    \n  \n\n\nTraining targets summary:\n\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      median_house_value_is_high\n    \n  \n  \n    \n      count\n      12000.0\n    \n    \n      mean\n      0.2\n    \n    \n      std\n      0.4\n    \n    \n      min\n      0.0\n    \n    \n      25%\n      0.0\n    \n    \n      50%\n      0.0\n    \n    \n      75%\n      0.0\n    \n    \n      max\n      1.0\n    \n  \n\n\nValidation targets summary:\n\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      median_house_value_is_high\n    \n  \n  \n    \n      count\n      5000.0\n    \n    \n      mean\n      0.3\n    \n    \n      std\n      0.4\n    \n    \n      min\n      0.0\n    \n    \n      25%\n      0.0\n    \n    \n      50%\n      0.0\n    \n    \n      75%\n      1.0\n    \n    \n      max\n      1.0\n    \n  \n\n\n线性回归的表现\n为了了解逻辑回归为什么有效，我们首先训练一个使用线性回归的简单模型。该模型将使用 {0, 1} 中的值为标签，并尝试预测一个尽可能接近 0 或 1 的连续值。此外，我们希望将输出解读为概率，所以最好模型的输出值可以位于 (0, 1) 范围内。然后我们会应用阈值 0.5，以确定标签。\n运行以下单元格，以使用 LinearRegressor 训练线性回归模型。\ndef construct_feature_columns(input_features):\n  &quot;&quot;&quot;Construct the TensorFlow Feature Columns.\n \n  Args:\n    input_features: The names of the numerical input features to use.\n  Returns:\n    A set of feature columns\n  &quot;&quot;&quot;\n  return set([tf.feature_column.numeric_column(my_feature)\n              for my_feature in input_features])\n \ndef my_input_fn(features, targets, batch_size=1, shuffle=True, num_epochs=None):\n    &quot;&quot;&quot;Trains a linear regression model.\n  \n    Args:\n      features: pandas DataFrame of features\n      targets: pandas DataFrame of targets\n      batch_size: Size of batches to be passed to the model\n      shuffle: True or False. Whether to shuffle the data.\n      num_epochs: Number of epochs for which data should be repeated. None = repeat indefinitely\n    Returns:\n      Tuple of (features, labels) for next data batch\n    &quot;&quot;&quot;\n    \n    # Convert pandas data into a dict of np arrays.\n    features = {key:np.array(value) for key,value in dict(features).items()}                                            \n \n    # Construct a dataset, and configure batching/repeating.\n    ds = Dataset.from_tensor_slices((features,targets)) # warning: 2GB limit\n    ds = ds.batch(batch_size).repeat(num_epochs)\n    \n    # Shuffle the data, if specified.\n    if shuffle:\n      ds = ds.shuffle(10000)\n    \n    # Return the next batch of data.\n    features, labels = ds.make_one_shot_iterator().get_next()\n    return features, labels\n \ndef train_linear_regressor_model(\n    learning_rate,\n    steps,\n    batch_size,\n    training_examples,\n    training_targets,\n    validation_examples,\n    validation_targets):\n  &quot;&quot;&quot;Trains a linear regression model.\n  \n  In addition to training, this function also prints training progress information,\n  as well as a plot of the training and validation loss over time.\n  \n  Args:\n    learning_rate: A `float`, the learning rate.\n    steps: A non-zero `int`, the total number of training steps. A training step\n      consists of a forward and backward pass using a single batch.\n    batch_size: A non-zero `int`, the batch size.\n    training_examples: A `DataFrame` containing one or more columns from\n      `california_housing_dataframe` to use as input features for training.\n    training_targets: A `DataFrame` containing exactly one column from\n      `california_housing_dataframe` to use as target for training.\n    validation_examples: A `DataFrame` containing one or more columns from\n      `california_housing_dataframe` to use as input features for validation.\n    validation_targets: A `DataFrame` containing exactly one column from\n      `california_housing_dataframe` to use as target for validation.\n      \n  Returns:\n    A `LinearRegressor` object trained on the training data.\n  &quot;&quot;&quot;\n \n  periods = 10\n  steps_per_period = steps / periods\n \n  # Create a linear regressor object.\n  my_optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n  my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)\n  linear_regressor = tf.estimator.LinearRegressor(\n      feature_columns=construct_feature_columns(training_examples),\n      optimizer=my_optimizer\n  )\n    \n  # Create input functions.  \n  training_input_fn = lambda: my_input_fn(training_examples, \n                                          training_targets[&quot;median_house_value_is_high&quot;], \n                                          batch_size=batch_size)\n  predict_training_input_fn = lambda: my_input_fn(training_examples, \n                                                  training_targets[&quot;median_house_value_is_high&quot;], \n                                                  num_epochs=1, \n                                                  shuffle=False)\n  predict_validation_input_fn = lambda: my_input_fn(validation_examples, \n                                                    validation_targets[&quot;median_house_value_is_high&quot;], \n                                                    num_epochs=1, \n                                                    shuffle=False)\n \n  # Train the model, but do so inside a loop so that we can periodically assess\n  # loss metrics.\n  print(&quot;Training model...&quot;)\n  print(&quot;RMSE (on training data):&quot;)\n  training_rmse = []\n  validation_rmse = []\n  for period in range (0, periods):\n    # Train the model, starting from the prior state.\n    linear_regressor.train(\n        input_fn=training_input_fn,\n        steps=steps_per_period\n    )\n    \n    # Take a break and compute predictions.\n    training_predictions = linear_regressor.predict(input_fn=predict_training_input_fn)\n    training_predictions = np.array([item[&#039;predictions&#039;][0] for item in training_predictions])\n    \n    validation_predictions = linear_regressor.predict(input_fn=predict_validation_input_fn)\n    validation_predictions = np.array([item[&#039;predictions&#039;][0] for item in validation_predictions])\n    \n    # Compute training and validation loss.\n    training_root_mean_squared_error = math.sqrt(\n        metrics.mean_squared_error(training_predictions, training_targets))\n    validation_root_mean_squared_error = math.sqrt(\n        metrics.mean_squared_error(validation_predictions, validation_targets))\n    # Occasionally print the current loss.\n    print(&quot;  period %02d : %0.2f&quot; % (period, training_root_mean_squared_error))\n    # Add the loss metrics from this period to our list.\n    training_rmse.append(training_root_mean_squared_error)\n    validation_rmse.append(validation_root_mean_squared_error)\n  print(&quot;Model training finished.&quot;)\n  \n  # Output a graph of loss metrics over periods.\n  plt.ylabel(&quot;RMSE&quot;)\n  plt.xlabel(&quot;Periods&quot;)\n  plt.title(&quot;Root Mean Squared Error vs. Periods&quot;)\n  plt.tight_layout()\n  plt.plot(training_rmse, label=&quot;training&quot;)\n  plt.plot(validation_rmse, label=&quot;validation&quot;)\n  plt.legend()\n \n  return linear_regressor\nlinear_regressor = train_linear_regressor_model(\n    learning_rate=0.000001,\n    steps=200,\n    batch_size=20,\n    training_examples=training_examples,\n    training_targets=training_targets,\n    validation_examples=validation_examples,\n    validation_targets=validation_targets)\nTraining model...\nRMSE (on training data):\n  period 00 : 0.45\n  period 01 : 0.45\n  period 02 : 0.45\n  period 03 : 0.45\n  period 04 : 0.46\n  period 05 : 0.44\n  period 06 : 0.44\n  period 07 : 0.44\n  period 08 : 0.45\n  period 09 : 0.44\nModel training finished.\n\n\n计算预测的对数损失函数\n检查预测，并确定是否可以使用它们来计算对数损失函数。\nLinearRegressor 使用的是 L2 损失，在将输出解读为概率时，它并不能有效地惩罚误分类。例如，对于概率分别为 0.9 和 0.9999 的负分类样本是否被分类为正分类，二者之间的差异应该很大，但 L2 损失并不会明显区分这些情况。\n相比之下，LogLoss（对数损失函数）对这些”置信错误”的惩罚力度更大。请注意，LogLoss 的定义如下：\nLog Loss = \\sum_{(x,y)\\in D} -y \\cdot log(y_{pred}) - (1 - y) \\cdot log(1 - y_{pred})\n但我们首先需要获得预测值。我们可以使用 LinearRegressor.predict 获得预测值。\n我们可以使用预测和相应目标计算 LogLoss 吗？\npredict_validation_input_fn = lambda: my_input_fn(validation_examples,\n                                                  validation_targets[&quot;median_house_value_is_high&quot;],\n                                                  num_epochs=1,\n                                                  shuffle=False)\n \nvalidation_predictions = linear_regressor.predict(input_fn=predict_validation_input_fn)\nvalidation_predictions = np.array([item[&#039;predictions&#039;][0] for item in validation_predictions])\n_ = plt.hist(validation_predictions)\n\n训练逻辑回归模型并计算验证集的对数损失函数\n要使用逻辑回归非常简单，用 LinearClassifier 替代 LinearRegressor 即可。完成以下代码。\n注意：在 LinearClassifier 模型上运行 train() 和 predict() 时，您可以通过返回的字典（例如 predictions[&quot;probabilities&quot;]）中的 &quot;probabilities&quot; 键获取实值预测概率。Sklearn 的 log_loss 函数可基于这些概率计算对数损失函数，非常方便。\ndef train_linear_classifier_model(\n    learning_rate,\n    steps,\n    batch_size,\n    training_examples,\n    training_targets,\n    validation_examples,\n    validation_targets):\n  &quot;&quot;&quot;Trains a linear classification model.\n  \n  In addition to training, this function also prints training progress information,\n  as well as a plot of the training and validation loss over time.\n  \n  Args:\n    learning_rate: A `float`, the learning rate.\n    steps: A non-zero `int`, the total number of training steps. A training step\n      consists of a forward and backward pass using a single batch.\n    batch_size: A non-zero `int`, the batch size.\n    training_examples: A `DataFrame` containing one or more columns from\n      `california_housing_dataframe` to use as input features for training.\n    training_targets: A `DataFrame` containing exactly one column from\n      `california_housing_dataframe` to use as target for training.\n    validation_examples: A `DataFrame` containing one or more columns from\n      `california_housing_dataframe` to use as input features for validation.\n    validation_targets: A `DataFrame` containing exactly one column from\n      `california_housing_dataframe` to use as target for validation.\n      \n  Returns:\n    A `LinearClassifier` object trained on the training data.\n  &quot;&quot;&quot;\n \n  periods = 10\n  steps_per_period = steps / periods\n  \n  # Create a linear classifier object.\n  my_optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n  my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)  \n  linear_classifier = tf.estimator.LinearClassifier(\n      feature_columns=construct_feature_columns(training_examples),\n      optimizer=my_optimizer\n  )\n  \n  # Create input functions.\n  training_input_fn = lambda: my_input_fn(training_examples, \n                                          training_targets[&quot;median_house_value_is_high&quot;], \n                                          batch_size=batch_size)\n  predict_training_input_fn = lambda: my_input_fn(training_examples, \n                                                  training_targets[&quot;median_house_value_is_high&quot;], \n                                                  num_epochs=1, \n                                                  shuffle=False)\n  predict_validation_input_fn = lambda: my_input_fn(validation_examples, \n                                                    validation_targets[&quot;median_house_value_is_high&quot;], \n                                                    num_epochs=1, \n                                                    shuffle=False)\n  \n  # Train the model, but do so inside a loop so that we can periodically assess\n  # loss metrics.\n  print(&quot;Training model...&quot;)\n  print(&quot;LogLoss (on training data):&quot;)\n  training_log_losses = []\n  validation_log_losses = []\n  for period in range (0, periods):\n    # Train the model, starting from the prior state.\n    linear_classifier.train(\n        input_fn=training_input_fn,\n        steps=steps_per_period\n    )\n    # Take a break and compute predictions.    \n    training_probabilities = linear_classifier.predict(input_fn=predict_training_input_fn)\n    training_probabilities = np.array([item[&#039;probabilities&#039;] for item in training_probabilities])\n    \n    validation_probabilities = linear_classifier.predict(input_fn=predict_validation_input_fn)\n    validation_probabilities = np.array([item[&#039;probabilities&#039;] for item in validation_probabilities])\n    \n    training_log_loss = metrics.log_loss(training_targets, training_probabilities)\n    validation_log_loss = metrics.log_loss(validation_targets, validation_probabilities)\n    # Occasionally print the current loss.\n    print(&quot;  period %02d : %0.2f&quot; % (period, training_log_loss))\n    # Add the loss metrics from this period to our list.\n    training_log_losses.append(training_log_loss)\n    validation_log_losses.append(validation_log_loss)\n  print(&quot;Model training finished.&quot;)\n  \n  # Output a graph of loss metrics over periods.\n  plt.ylabel(&quot;LogLoss&quot;)\n  plt.xlabel(&quot;Periods&quot;)\n  plt.title(&quot;LogLoss vs. Periods&quot;)\n  plt.tight_layout()\n  plt.plot(training_log_losses, label=&quot;training&quot;)\n  plt.plot(validation_log_losses, label=&quot;validation&quot;)\n  plt.legend()\n \n  return linear_classifier\nlinear_classifier = train_linear_classifier_model(\n    learning_rate=0.000005,\n    steps=500,\n    batch_size=20,\n    training_examples=training_examples,\n    training_targets=training_targets,\n    validation_examples=validation_examples,\n    validation_targets=validation_targets)\nTraining model...\nLogLoss (on training data):\n  period 00 : 0.60\n  period 01 : 0.58\n  period 02 : 0.57\n  period 03 : 0.56\n  period 04 : 0.55\n  period 05 : 0.55\n  period 06 : 0.54\n  period 07 : 0.55\n  period 08 : 0.54\n  period 09 : 0.53\nModel training finished.\n\n\n计算查准率并为验证集绘制 ROC 曲线\n分类时非常有用的一些指标包括：模型准确率、ROC 曲线和 ROC 曲线下面积 (AUC)。我们会检查这些指标。\nLinearClassifier.evaluate 可计算准确率和 AUC 等实用指标。\nevaluation_metrics = linear_classifier.evaluate(input_fn=predict_validation_input_fn)\n \nprint(&quot;AUC on the validation set: %0.2f&quot; % evaluation_metrics[&#039;auc&#039;])\nprint(&quot;Accuracy on the validation set: %0.2f&quot; % evaluation_metrics[&#039;accuracy&#039;])\nAUC on the validation set: 0.73\nAccuracy on the validation set: 0.76\n\n您可以使用类别概率（例如由 LinearClassifier.predict\n和 Sklearn 的 roc_curve 计算的概率）来获得绘制 ROC 曲线所需的真正例率和假正例率。\nvalidation_probabilities = linear_classifier.predict(input_fn=predict_validation_input_fn)\n# Get just the probabilities for the positive class.\nvalidation_probabilities = np.array([item[&#039;probabilities&#039;][1] for item in validation_probabilities])\n \nfalse_positive_rate, true_positive_rate, thresholds = metrics.roc_curve(\n    validation_targets, validation_probabilities)\nplt.plot(false_positive_rate, true_positive_rate, label=&quot;our model&quot;)\nplt.plot([0, 1], [0, 1], label=&quot;random classifier&quot;)\n_ = plt.legend(loc=2)\n\n看看您是否可以调整训练的模型的学习设置，以改善 AUC。\n通常情况下，某些指标在提升的同时会损害其他指标，因此您需要找到可以实现理想折中情况的设置。\n验证所有指标是否同时有所提升。\n一个可能有用的解决方案是，只要不过拟合，就训练更长时间。\n要做到这一点，我们可以增加步数和/或批量大小。\n所有指标同时提升，这样，我们的损失指标就可以很好地代理 AUC 和准确率了。\n注意它是如何进行很多很多次迭代，只是为了再尽量增加一点 AUC。这种情况很常见，但通常情况下，即使只有一点小小的收获，投入的成本也是值得的。\n# TUNE THE SETTINGS BELOW TO IMPROVE AUC\nlinear_classifier = train_linear_classifier_model(\n    learning_rate=0.0000035,\n    steps=20000,\n    batch_size=500,\n    training_examples=training_examples,\n    training_targets=training_targets,\n    validation_examples=validation_examples,\n    validation_targets=validation_targets)\n \nevaluation_metrics = linear_classifier.evaluate(input_fn=predict_validation_input_fn)\n \nprint(&quot;AUC on the validation set: %0.2f&quot; % evaluation_metrics[&#039;auc&#039;])\nprint(&quot;Accuracy on the validation set: %0.2f&quot; % evaluation_metrics[&#039;accuracy&#039;])\nTraining model...\nLogLoss (on training data):\n  period 00 : 0.50\n  period 01 : 0.49\n  period 02 : 0.48\n  period 03 : 0.48\n  period 04 : 0.48\n  period 05 : 0.47\n  period 06 : 0.47\n  period 07 : 0.47\n  period 08 : 0.47\n  period 09 : 0.47\nModel training finished.\nAUC on the validation set: 0.81\nAccuracy on the validation set: 0.79\n\n\n稀疏性和 L1 正则化\n降低复杂性的一种方法是使用正则化函数，它会使权重正好为零。对于线性模型（例如线性回归），权重为零就相当于完全没有使用相应特征。除了可避免过拟合之外，生成的模型还会更加有效。\nL1 正则化是一种增加稀疏性的好方法。\n设置\n加载数据并创建特征定义。\ndef my_input_fn(features, targets, batch_size=1, shuffle=True, num_epochs=None):\n    &quot;&quot;&quot;Trains a linear regression model.\n  \n    Args:\n      features: pandas DataFrame of features\n      targets: pandas DataFrame of targets\n      batch_size: Size of batches to be passed to the model\n      shuffle: True or False. Whether to shuffle the data.\n      num_epochs: Number of epochs for which data should be repeated. None = repeat indefinitely\n    Returns:\n      Tuple of (features, labels) for next data batch\n    &quot;&quot;&quot;\n  \n    # Convert pandas data into a dict of np arrays.\n    features = {key:np.array(value) for key,value in dict(features).items()}                                            \n \n    # Construct a dataset, and configure batching/repeating.\n    ds = Dataset.from_tensor_slices((features,targets)) # warning: 2GB limit\n    ds = ds.batch(batch_size).repeat(num_epochs)\n    \n    # Shuffle the data, if specified.\n    if shuffle:\n      ds = ds.shuffle(10000)\n    \n    # Return the next batch of data.\n    features, labels = ds.make_one_shot_iterator().get_next()\n    return features, labels\n \n# 分桶\ndef get_quantile_based_buckets(feature_values, num_buckets):\n  quantiles = feature_values.quantile(\n    [(i+1.)/(num_buckets + 1.) for i in range(num_buckets)])\n  return [quantiles[q] for q in quantiles.keys()]\n \n# 构造特征列\ndef construct_feature_columns():\n  &quot;&quot;&quot;Construct the TensorFlow Feature Columns.\n \n  Returns:\n    A set of feature columns\n  &quot;&quot;&quot;\n \n  bucketized_households = tf.feature_column.bucketized_column(\n    tf.feature_column.numeric_column(&quot;households&quot;),\n    boundaries=get_quantile_based_buckets(training_examples[&quot;households&quot;], 10))\n  bucketized_longitude = tf.feature_column.bucketized_column(\n    tf.feature_column.numeric_column(&quot;longitude&quot;),\n    boundaries=get_quantile_based_buckets(training_examples[&quot;longitude&quot;], 50))\n  bucketized_latitude = tf.feature_column.bucketized_column(\n    tf.feature_column.numeric_column(&quot;latitude&quot;),\n    boundaries=get_quantile_based_buckets(training_examples[&quot;latitude&quot;], 50))\n  bucketized_housing_median_age = tf.feature_column.bucketized_column(\n    tf.feature_column.numeric_column(&quot;housing_median_age&quot;),\n    boundaries=get_quantile_based_buckets(\n      training_examples[&quot;housing_median_age&quot;], 10))\n  bucketized_total_rooms = tf.feature_column.bucketized_column(\n    tf.feature_column.numeric_column(&quot;total_rooms&quot;),\n    boundaries=get_quantile_based_buckets(training_examples[&quot;total_rooms&quot;], 10))\n  bucketized_total_bedrooms = tf.feature_column.bucketized_column(\n    tf.feature_column.numeric_column(&quot;total_bedrooms&quot;),\n    boundaries=get_quantile_based_buckets(training_examples[&quot;total_bedrooms&quot;], 10))\n  bucketized_population = tf.feature_column.bucketized_column(\n    tf.feature_column.numeric_column(&quot;population&quot;),\n    boundaries=get_quantile_based_buckets(training_examples[&quot;population&quot;], 10))\n  bucketized_median_income = tf.feature_column.bucketized_column(\n    tf.feature_column.numeric_column(&quot;median_income&quot;),\n    boundaries=get_quantile_based_buckets(training_examples[&quot;median_income&quot;], 10))\n  bucketized_rooms_per_person = tf.feature_column.bucketized_column(\n    tf.feature_column.numeric_column(&quot;rooms_per_person&quot;),\n    boundaries=get_quantile_based_buckets(\n      training_examples[&quot;rooms_per_person&quot;], 10))\n \n  long_x_lat = tf.feature_column.crossed_column(\n    set([bucketized_longitude, bucketized_latitude]), hash_bucket_size=1000)\n \n  feature_columns = set([\n    long_x_lat,\n    bucketized_longitude,\n    bucketized_latitude,\n    bucketized_housing_median_age,\n    bucketized_total_rooms,\n    bucketized_total_bedrooms,\n    bucketized_population,\n    bucketized_households,\n    bucketized_median_income,\n    bucketized_rooms_per_person])\n  \n  return feature_columns\n \ndef train_linear_classifier_model(\n    learning_rate,\n    regularization_strength,\n    steps,\n    batch_size,\n    feature_columns,\n    training_examples,\n    training_targets,\n    validation_examples,\n    validation_targets):\n  &quot;&quot;&quot;Trains a linear regression model.\n  \n  In addition to training, this function also prints training progress information,\n  as well as a plot of the training and validation loss over time.\n  \n  Args:\n    learning_rate: A `float`, the learning rate.\n    regularization_strength: A `float` that indicates the strength of the L1\n       regularization. A value of `0.0` means no regularization.\n    steps: A non-zero `int`, the total number of training steps. A training step\n      consists of a forward and backward pass using a single batch.\n    feature_columns: A `set` specifying the input feature columns to use.\n    training_examples: A `DataFrame` containing one or more columns from\n      `california_housing_dataframe` to use as input features for training.\n    training_targets: A `DataFrame` containing exactly one column from\n      `california_housing_dataframe` to use as target for training.\n    validation_examples: A `DataFrame` containing one or more columns from\n      `california_housing_dataframe` to use as input features for validation.\n    validation_targets: A `DataFrame` containing exactly one column from\n      `california_housing_dataframe` to use as target for validation.\n      \n  Returns:\n    A `LinearClassifier` object trained on the training data.\n  &quot;&quot;&quot;\n \n  periods = 7\n  steps_per_period = steps / periods\n \n  # Create a linear classifier object.\n  my_optimizer = tf.train.FtrlOptimizer(learning_rate=learning_rate, l1_regularization_strength=regularization_strength)\n  my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)\n  linear_classifier = tf.estimator.LinearClassifier(\n      feature_columns=feature_columns,\n      optimizer=my_optimizer\n  )\n  \n  # Create input functions.\n  training_input_fn = lambda: my_input_fn(training_examples, \n                                          training_targets[&quot;median_house_value_is_high&quot;], \n                                          batch_size=batch_size)\n  predict_training_input_fn = lambda: my_input_fn(training_examples, \n                                                  training_targets[&quot;median_house_value_is_high&quot;], \n                                                  num_epochs=1, \n                                                  shuffle=False)\n  predict_validation_input_fn = lambda: my_input_fn(validation_examples, \n                                                    validation_targets[&quot;median_house_value_is_high&quot;], \n                                                    num_epochs=1, \n                                                    shuffle=False)\n  \n  # Train the model, but do so inside a loop so that we can periodically assess\n  # loss metrics.\n  print(&quot;Training model...&quot;)\n  print(&quot;LogLoss (on validation data):&quot;)\n  training_log_losses = []\n  validation_log_losses = []\n  for period in range (0, periods):\n    # Train the model, starting from the prior state.\n    linear_classifier.train(\n        input_fn=training_input_fn,\n        steps=steps_per_period\n    )\n    # Take a break and compute predictions.\n    training_probabilities = linear_classifier.predict(input_fn=predict_training_input_fn)\n    training_probabilities = np.array([item[&#039;probabilities&#039;] for item in training_probabilities])\n    \n    validation_probabilities = linear_classifier.predict(input_fn=predict_validation_input_fn)\n    validation_probabilities = np.array([item[&#039;probabilities&#039;] for item in validation_probabilities])\n    \n    # Compute training and validation loss.\n    training_log_loss = metrics.log_loss(training_targets, training_probabilities)\n    validation_log_loss = metrics.log_loss(validation_targets, validation_probabilities)\n    # Occasionally print the current loss.\n    print(&quot;  period %02d : %0.2f&quot; % (period, validation_log_loss))\n    # Add the loss metrics from this period to our list.\n    training_log_losses.append(training_log_loss)\n    validation_log_losses.append(validation_log_loss)\n  print(&quot;Model training finished.&quot;)\n \n  # Output a graph of loss metrics over periods.\n  plt.ylabel(&quot;LogLoss&quot;)\n  plt.xlabel(&quot;Periods&quot;)\n  plt.title(&quot;LogLoss vs. Periods&quot;)\n  plt.tight_layout()\n  plt.plot(training_log_losses, label=&quot;training&quot;)\n  plt.plot(validation_log_losses, label=&quot;validation&quot;)\n  plt.legend()\n \n  return linear_classifier\n计算模型大小\n要计算模型大小，只需计算非零参数的数量即可。为此，我们在下面提供了一个辅助函数。该函数深入使用了 Estimator API，如果不了解它的工作原理，也不用担心。\ndef model_size(estimator):\n  variables = estimator.get_variable_names()\n  size = 0\n  for variable in variables:\n    if not any(x in variable \n               for x in [&#039;global_step&#039;,\n                         &#039;centered_bias_weight&#039;,\n                         &#039;bias_weight&#039;,\n                         &#039;Ftrl&#039;]\n              ):\n      size += np.count_nonzero(estimator.get_variable_value(variable))\n  return size\n减小模型大小\n您的团队需要针对 SmartRing 构建一个准确度高的逻辑回归模型，这种指环非常智能，可以感应城市街区的人口统计特征（median_income、avg_rooms、households 等等），并告诉您指定城市街区的住房成本是否高昂。\n由于 SmartRing 很小，因此工程团队已确定它只能处理参数数量不超过 600 个的模型。另一方面，产品管理团队也已确定，除非所保留测试集的对数损失函数低于 0.35，否则该模型不能发布。\n您可以使用秘密武器“L1 正则化”调整模型，使其同时满足大小和准确率限制条件吗？\n查找合适的正则化系数。\n查找可同时满足以下两种限制条件的 L1 正则化强度参数：模型的参数数量不超过 600 个且验证集的对数损失函数低于 0.35。\n以下代码可帮助您快速开始。您可以通过多种方法向您的模型应用正则化。在此练习中，我们选择使用 FtrlOptimizer 来应用正则化。FtrlOptimizer 是一种设计成使用 L1 正则化比标准梯度下降法得到更好结果的方法。\n重申一次，我们会使用整个数据集来训练该模型，因此预计其运行速度会比通常要慢。\n正则化强度为 0.1 应该就足够了。请注意，有一个需要做出折中选择的地方：正则化越强，我们获得的模型就越小，但会影响分类损失。\nlinear_classifier = train_linear_classifier_model(\n    learning_rate=0.1,\n    # TWEAK THE REGULARIZATION VALUE BELOW\n    regularization_strength=0.0,\n    steps=300,\n    batch_size=100,\n    feature_columns=construct_feature_columns(),\n    training_examples=training_examples,\n    training_targets=training_targets,\n    validation_examples=validation_examples,\n    validation_targets=validation_targets)\nprint(&quot;Model size:&quot;, model_size(linear_classifier))\nTraining model...\nLogLoss (on validation data):\n  period 00 : 0.30\n  period 01 : 0.27\n  period 02 : 0.26\n  period 03 : 0.25\n  period 04 : 0.24\n  period 05 : 0.24\n  period 06 : 0.23\nModel training finished.\nModel size: 790\n\n\nlinear_classifier = train_linear_classifier_model(\n    learning_rate=0.1,\n    regularization_strength=0.1,\n    steps=300,\n    batch_size=100,\n    feature_columns=construct_feature_columns(),\n    training_examples=training_examples,\n    training_targets=training_targets,\n    validation_examples=validation_examples,\n    validation_targets=validation_targets)\nprint(&quot;Model size:&quot;, model_size(linear_classifier))\nTraining model...\nLogLoss (on validation data):\n  period 00 : 0.31\n  period 01 : 0.27\n  period 02 : 0.26\n  period 03 : 0.25\n  period 04 : 0.24\n  period 05 : 0.24\n  period 06 : 0.23\nModel training finished.\nModel size: 764\n\n"},"Tensorflow/TensorFlow_8":{"slug":"Tensorflow/TensorFlow_8","filePath":"Tensorflow/TensorFlow_8.md","title":"TensorFlow 8","links":[],"tags":["Tensorflow"],"content":"神经网络简介\n\n\n使用 TensorFlow DNNRegressor 类定义神经网络 (NN) 及其隐藏层\n\n\n训练神经网络学习数据集中的非线性规律，并实现比线性回归模型更好的效果\n\n\n在之前的练习中，我们使用合成特征来帮助模型学习非线性规律。\n一组重要的非线性关系是纬度和经度的关系，但也可能存在其他非线性关系。\n现在我们从之前练习中的逻辑回归任务回到标准的（线性）回归任务。也就是说，我们将直接预测 median_house_value。\n设置\n加载数据并创建特征定义。\nfrom __future__ import print_function\n \nimport math\n \nfrom IPython import display\nfrom matplotlib import cm\nfrom matplotlib import gridspec\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn import metrics\nimport tensorflow as tf\nfrom tensorflow.python.data import Dataset\n \ntf.logging.set_verbosity(tf.logging.ERROR)\npd.options.display.max_rows = 10\npd.options.display.float_format = &#039;{:.1f}&#039;.format\n \ncalifornia_housing_dataframe = pd.read_csv(&quot;download.mlcc.google.cn/mledu-datasets/california_housing_train.csv&quot;, sep=&quot;,&quot;)\n \ncalifornia_housing_dataframe = california_housing_dataframe.reindex(\n    np.random.permutation(california_housing_dataframe.index))\ndef preprocess_features(california_housing_dataframe):\n  &quot;&quot;&quot;Prepares input features from California housing data set.\n \n  Args:\n    california_housing_dataframe: A Pandas DataFrame expected to contain data\n      from the California housing data set.\n  Returns:\n    A DataFrame that contains the features to be used for the model, including\n    synthetic features.\n  &quot;&quot;&quot;\n  selected_features = california_housing_dataframe[\n    [&quot;latitude&quot;,\n     &quot;longitude&quot;,\n     &quot;housing_median_age&quot;,\n     &quot;total_rooms&quot;,\n     &quot;total_bedrooms&quot;,\n     &quot;population&quot;,\n     &quot;households&quot;,\n     &quot;median_income&quot;]]\n  processed_features = selected_features.copy()\n  # Create a synthetic feature.\n  processed_features[&quot;rooms_per_person&quot;] = (\n    california_housing_dataframe[&quot;total_rooms&quot;] /\n    california_housing_dataframe[&quot;population&quot;])\n  return processed_features\n \ndef preprocess_targets(california_housing_dataframe):\n  &quot;&quot;&quot;Prepares target features (i.e., labels) from California housing data set.\n \n  Args:\n    california_housing_dataframe: A Pandas DataFrame expected to contain data\n      from the California housing data set.\n  Returns:\n    A DataFrame that contains the target feature.\n  &quot;&quot;&quot;\n  output_targets = pd.DataFrame()\n  # Scale the target to be in units of thousands of dollars.\n  output_targets[&quot;median_house_value&quot;] = (\n    california_housing_dataframe[&quot;median_house_value&quot;] / 1000.0)\n  return output_targets\n# Choose the first 12000 (out of 17000) examples for training.\ntraining_examples = preprocess_features(california_housing_dataframe.head(12000))\ntraining_targets = preprocess_targets(california_housing_dataframe.head(12000))\n \n# Choose the last 5000 (out of 17000) examples for validation.\nvalidation_examples = preprocess_features(california_housing_dataframe.tail(5000))\nvalidation_targets = preprocess_targets(california_housing_dataframe.tail(5000))\n \n# Double-check that we&#039;ve done the right thing.\nprint(&quot;Training examples summary:&quot;)\ndisplay.display(training_examples.describe())\nprint(&quot;Validation examples summary:&quot;)\ndisplay.display(validation_examples.describe())\n \nprint(&quot;Training targets summary:&quot;)\ndisplay.display(training_targets.describe())\nprint(&quot;Validation targets summary:&quot;)\ndisplay.display(validation_targets.describe())\nTraining examples summary:\n\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      latitude\n      longitude\n      housing_median_age\n      total_rooms\n      total_bedrooms\n      population\n      households\n      median_income\n      rooms_per_person\n    \n  \n  \n    \n      count\n      12000.0\n      12000.0\n      12000.0\n      12000.0\n      12000.0\n      12000.0\n      12000.0\n      12000.0\n      12000.0\n    \n    \n      mean\n      35.6\n      -119.6\n      28.7\n      2636.9\n      537.9\n      1429.2\n      500.3\n      3.9\n      2.0\n    \n    \n      std\n      2.1\n      2.0\n      12.5\n      2187.4\n      422.5\n      1168.5\n      387.3\n      1.9\n      1.1\n    \n    \n      min\n      32.5\n      -124.3\n      1.0\n      2.0\n      1.0\n      3.0\n      1.0\n      0.5\n      0.0\n    \n    \n      25%\n      33.9\n      -121.8\n      18.0\n      1461.0\n      296.8\n      788.0\n      281.0\n      2.6\n      1.5\n    \n    \n      50%\n      34.2\n      -118.5\n      29.0\n      2116.0\n      431.0\n      1165.0\n      407.5\n      3.5\n      1.9\n    \n    \n      75%\n      37.7\n      -118.0\n      37.0\n      3127.0\n      645.0\n      1713.0\n      601.0\n      4.8\n      2.3\n    \n    \n      max\n      42.0\n      -114.3\n      52.0\n      37937.0\n      6445.0\n      35682.0\n      6082.0\n      15.0\n      55.2\n    \n  \n\n\nValidation examples summary:\n\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      latitude\n      longitude\n      housing_median_age\n      total_rooms\n      total_bedrooms\n      population\n      households\n      median_income\n      rooms_per_person\n    \n  \n  \n    \n      count\n      5000.0\n      5000.0\n      5000.0\n      5000.0\n      5000.0\n      5000.0\n      5000.0\n      5000.0\n      5000.0\n    \n    \n      mean\n      35.6\n      -119.6\n      28.4\n      2659.9\n      542.9\n      1430.4\n      503.5\n      3.9\n      2.0\n    \n    \n      std\n      2.1\n      2.0\n      12.9\n      2162.0\n      419.1\n      1096.8\n      377.9\n      1.9\n      1.3\n    \n    \n      min\n      32.6\n      -124.3\n      2.0\n      15.0\n      4.0\n      8.0\n      2.0\n      0.5\n      0.1\n    \n    \n      25%\n      33.9\n      -121.8\n      18.0\n      1465.8\n      297.0\n      793.0\n      283.0\n      2.6\n      1.5\n    \n    \n      50%\n      34.2\n      -118.5\n      28.0\n      2154.5\n      439.0\n      1173.0\n      413.0\n      3.5\n      1.9\n    \n    \n      75%\n      37.7\n      -118.0\n      37.0\n      3216.0\n      658.2\n      1738.0\n      614.0\n      4.7\n      2.3\n    \n    \n      max\n      41.9\n      -114.6\n      52.0\n      30401.0\n      4957.0\n      13251.0\n      4339.0\n      15.0\n      52.0\n    \n  \n\n\nTraining targets summary:\n\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      median_house_value\n    \n  \n  \n    \n      count\n      12000.0\n    \n    \n      mean\n      206.6\n    \n    \n      std\n      115.5\n    \n    \n      min\n      15.0\n    \n    \n      25%\n      119.2\n    \n    \n      50%\n      180.8\n    \n    \n      75%\n      263.3\n    \n    \n      max\n      500.0\n    \n  \n\n\nValidation targets summary:\n\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      median_house_value\n    \n  \n  \n    \n      count\n      5000.0\n    \n    \n      mean\n      208.9\n    \n    \n      std\n      117.0\n    \n    \n      min\n      15.0\n    \n    \n      25%\n      120.2\n    \n    \n      50%\n      179.2\n    \n    \n      75%\n      268.3\n    \n    \n      max\n      500.0\n    \n  \n\n\n构建神经网络\n神经网络由 DNNRegressor 类定义。\n使用 hidden_units 定义神经网络的结构。hidden_units 参数会创建一个整数列表，其中每个整数对应一个隐藏层，表示其中的节点数。以下面的赋值为例：\nhidden_units=[3,10]\n上述赋值为神经网络指定了两个隐藏层：\n\n第一个隐藏层包含 3 个节点。\n第二个隐藏层包含 10 个节点。\n\n如果我们想要添加更多层，可以向该列表添加更多整数。例如，hidden_units=[10,20,30,40] 会创建 4 个分别包含 10、20、30 和 40 个单元的隐藏层。\n默认情况下，所有隐藏层都会使用 ReLu 激活函数，且是全连接层。\ndef construct_feature_columns(input_features):\n  &quot;&quot;&quot;Construct the TensorFlow Feature Columns.\n \n  Args:\n    input_features: The names of the numerical input features to use.\n  Returns:\n    A set of feature columns\n  &quot;&quot;&quot; \n  return set([tf.feature_column.numeric_column(my_feature)\n              for my_feature in input_features])\n \ndef my_input_fn(features, targets, batch_size=1, shuffle=True, num_epochs=None):\n    &quot;&quot;&quot;Trains a neural net regression model.\n  \n    Args:\n      features: pandas DataFrame of features\n      targets: pandas DataFrame of targets\n      batch_size: Size of batches to be passed to the model\n      shuffle: True or False. Whether to shuffle the data.\n      num_epochs: Number of epochs for which data should be repeated. None = repeat indefinitely\n    Returns:\n      Tuple of (features, labels) for next data batch\n    &quot;&quot;&quot;\n    \n    # Convert pandas data into a dict of np arrays.\n    features = {key:np.array(value) for key,value in dict(features).items()}                                             \n \n    # Construct a dataset, and configure batching/repeating.\n    ds = Dataset.from_tensor_slices((features,targets)) # warning: 2GB limit\n    ds = ds.batch(batch_size).repeat(num_epochs)\n    \n    # Shuffle the data, if specified.\n    if shuffle:\n      ds = ds.shuffle(10000)\n    \n    # Return the next batch of data.\n    features, labels = ds.make_one_shot_iterator().get_next()\n    return features, labels\ndef train_nn_regression_model(\n    learning_rate,\n    steps,\n    batch_size,\n    hidden_units,\n    training_examples,\n    training_targets,\n    validation_examples,\n    validation_targets):\n    &quot;&quot;&quot;Trains a neural network regression model.\n    \n    In addition to training, this function also prints training progress information,\n    as well as a plot of the training and validation loss over time.\n    \n    Args:\n      learning_rate: A `float`, the learning rate.\n      steps: A non-zero `int`, the total number of training steps. A training step\n        consists of a forward and backward pass using a single batch.\n      batch_size: A non-zero `int`, the batch size.\n      hidden_units: A `list` of int values, specifying the number of neurons in each layer.\n      training_examples: A `DataFrame` containing one or more columns from\n        `california_housing_dataframe` to use as input features for training.\n      training_targets: A `DataFrame` containing exactly one column from\n        `california_housing_dataframe` to use as target for training.\n      validation_examples: A `DataFrame` containing one or more columns from\n        `california_housing_dataframe` to use as input features for validation.\n      validation_targets: A `DataFrame` containing exactly one column from\n        `california_housing_dataframe` to use as target for validation.\n        \n    Returns:\n      A `DNNRegressor` object trained on the training data.\n    &quot;&quot;&quot;\n \n    periods = 10\n    steps_per_period = steps / periods\n    \n    # Create a DNNRrgressor object.\n    my_optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n    my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)\n    dnn_regressor = tf.estimator.DNNRegressor(\n        feature_columns=construct_feature_columns(training_examples),\n        hidden_units=hidden_units,\n        optimizer=my_optimizer\n    )\n    \n    # Create input functions.\n    training_input_fn = lambda: my_input_fn(training_examples, \n                                            training_targets[&quot;median_house_value&quot;], \n                                            batch_size=batch_size)\n    predict_training_input_fn = lambda: my_input_fn(training_examples, \n                                                    training_targets[&quot;median_house_value&quot;], \n                                                    num_epochs=1, \n                                                    shuffle=False)\n    predict_validation_input_fn = lambda: my_input_fn(validation_examples, \n                                                    validation_targets[&quot;median_house_value&quot;], \n                                                    num_epochs=1, \n                                                    shuffle=False)    \n    # Train the model, but do so inside a loop so that we can periodically assess\n    # loss metrics. \n    print(&quot;Training model...&quot;)\n    print(&quot;RMSE (on training data):&quot;)\n    \n    training_rmse = []\n    validation_rmse = []\n    \n    for period in range(0, periods):\n        # train the model, starting from the prior state.\n        dnn_regressor.train(\n            input_fn=training_input_fn,\n            steps=steps_per_period\n        )\n        # take abreak and compute perdictions.\n        training_predictions = dnn_regressor.predict(input_fn=predict_training_input_fn)\n        training_predictions = np.array([item[&#039;predictions&#039;][0] for item in training_predictions])\n        \n        validation_predictions = dnn_regressor.predict(input_fn=predict_validation_input_fn)\n        validation_predictions = np.array([item[&#039;predictions&#039;][0] for item in validation_predictions])\n        \n        # compute training an validation loss\n        training_root_mean_squared_error = math.sqrt(\n            metrics.mean_squared_error(training_predictions, training_targets))\n        validation_root_mean_squared_error = math.sqrt(\n            metrics.mean_squared_error(validation_predictions, validation_targets))\n        \n        # Occasionally print the current loss.\n        print(&quot;  period %02d : %0.2f&quot; % (period, training_root_mean_squared_error))\n        # Add the loss metrics from this period to our list.\n        training_rmse.append(training_root_mean_squared_error)\n        validation_rmse.append(validation_root_mean_squared_error)   \n    \n    print(&quot;Model training finished.&quot;)\n    # Output a graph of loss metrics over periods.\n    plt.ylabel(&quot;RMSE&quot;)\n    plt.xlabel(&quot;Periods&quot;)\n    plt.title(&quot;Root Mean Squared Error vs. Periods&quot;)\n    plt.tight_layout()\n    plt.plot(training_rmse, label=&quot;training&quot;)\n    plt.plot(validation_rmse, label=&quot;validation&quot;)\n    plt.legend()\n \n    print(&quot;Final RMSE (on training data):   %0.2f&quot; % training_root_mean_squared_error)\n    print(&quot;Final RMSE (on validation data): %0.2f&quot; % validation_root_mean_squared_error)\n \n    return dnn_regressor\n训练神经网络模型\n调整超参数，目标是将 RMSE 降到 110 以下。\n我们已经知道，在使用了很多特征的线性回归练习中，110 左右的 RMSE 已经是相当不错的结果。现在我们将得到比它更好的结果。\n对于神经网络而言，过拟合是一种真正的潜在危险。您可以查看训练数据损失与验证数据损失之间的差值，以帮助判断模型是否有过拟合的趋势。如果差值开始变大，则通常可以肯定存在过拟合。\n下面参数是我写的，也许有更好的参数会获得更低的RMSE。\ndnn_regressor = train_nn_regression_model(\n    learning_rate=0.002,\n    steps=2000,\n    batch_size=100,\n    hidden_units=[8, 10],\n    training_examples=training_examples,\n    training_targets=training_targets,\n    validation_examples=validation_examples,\n    validation_targets=validation_targets)\nTraining model...\nRMSE (on training data):\n  period 00 : 153.67\n  period 01 : 136.38\n  period 02 : 119.26\n  period 03 : 106.87\n  period 04 : 107.80\n  period 05 : 106.81\n  period 06 : 107.47\n  period 07 : 109.00\n  period 08 : 104.58\n  period 09 : 106.55\nModel training finished.\nFinal RMSE (on training data):   106.55\nFinal RMSE (on validation data): 106.82\n\n\n用测试数据进行评估\n确认验证效果结果经受得住测试数据的检验。\n获得满意的模型后，用测试数据评估该模型，以与验证效果进行比较。\n测试数据集位于此处。\ncalifornia_housing_test_data = pd.read_csv(&quot;download.mlcc.google.cn/mledu-datasets/california_housing_test.csv&quot;, sep=&quot;,&quot;)\n \ntest_examples = preprocess_features(california_housing_test_data)\ntest_targets = preprocess_targets(california_housing_test_data)\n \npredict_testing_input_fn = lambda: my_input_fn(test_examples, \n                                               test_targets[&quot;median_house_value&quot;], \n                                               num_epochs=1, \n                                               shuffle=False)\n \ntest_predictions = dnn_regressor.predict(input_fn=predict_testing_input_fn)\ntest_predictions = np.array([item[&#039;predictions&#039;][0] for item in test_predictions])\n \nroot_mean_squared_error = math.sqrt(\n    metrics.mean_squared_error(test_predictions, test_targets))\n \nprint(&quot;Final RMSE (on test data): %0.2f&quot; % root_mean_squared_error)\nFinal RMSE (on test data): 105.23\n\n提高神经网络性能\n通过将特征标准化并应用各种优化算法来提高神经网络的性能\n注意：本练习中介绍的优化方法并非专门针对神经网络；这些方法可有效改进大多数类型的模型。\ndef train_nn_regression_model_optimize(\n    my_optimizer,\n    steps,\n    batch_size,\n    hidden_units,\n    training_examples,\n    training_targets,\n    validation_examples,\n    validation_targets):\n  &quot;&quot;&quot;Trains a neural network regression model.\n  \n  In addition to training, this function also prints training progress information,\n  as well as a plot of the training and validation loss over time.\n  \n  Args:\n    my_optimizer: An instance of `tf.train.Optimizer`, the optimizer to use.\n    steps: A non-zero `int`, the total number of training steps. A training step\n      consists of a forward and backward pass using a single batch.\n    batch_size: A non-zero `int`, the batch size.\n    hidden_units: A `list` of int values, specifying the number of neurons in each layer.\n    training_examples: A `DataFrame` containing one or more columns from\n      `california_housing_dataframe` to use as input features for training.\n    training_targets: A `DataFrame` containing exactly one column from\n      `california_housing_dataframe` to use as target for training.\n    validation_examples: A `DataFrame` containing one or more columns from\n      `california_housing_dataframe` to use as input features for validation.\n    validation_targets: A `DataFrame` containing exactly one column from\n      `california_housing_dataframe` to use as target for validation.\n      \n  Returns:\n    A tuple `(estimator, training_losses, validation_losses)`:\n      estimator: the trained `DNNRegressor` object.\n      training_losses: a `list` containing the training loss values taken during training.\n      validation_losses: a `list` containing the validation loss values taken during training.\n  &quot;&quot;&quot;\n \n  periods = 10\n  steps_per_period = steps / periods\n  \n  # Create a DNNRegressor object.\n  my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)\n  dnn_regressor = tf.estimator.DNNRegressor(\n      feature_columns=construct_feature_columns(training_examples),\n      hidden_units=hidden_units,\n      optimizer=my_optimizer\n  )\n  \n  # Create input functions.\n  training_input_fn = lambda: my_input_fn(training_examples, \n                                          training_targets[&quot;median_house_value&quot;], \n                                          batch_size=batch_size)\n  predict_training_input_fn = lambda: my_input_fn(training_examples, \n                                                  training_targets[&quot;median_house_value&quot;], \n                                                  num_epochs=1, \n                                                  shuffle=False)\n  predict_validation_input_fn = lambda: my_input_fn(validation_examples, \n                                                    validation_targets[&quot;median_house_value&quot;], \n                                                    num_epochs=1, \n                                                    shuffle=False)\n \n  # Train the model, but do so inside a loop so that we can periodically assess\n  # loss metrics.\n  print(&quot;Training model...&quot;)\n  print(&quot;RMSE (on training data):&quot;)\n  training_rmse = []\n  validation_rmse = []\n  for period in range (0, periods):\n    # Train the model, starting from the prior state.\n    dnn_regressor.train(\n        input_fn=training_input_fn,\n        steps=steps_per_period\n    )\n    # Take a break and compute predictions.\n    training_predictions = dnn_regressor.predict(input_fn=predict_training_input_fn)\n    training_predictions = np.array([item[&#039;predictions&#039;][0] for item in training_predictions])\n    \n    validation_predictions = dnn_regressor.predict(input_fn=predict_validation_input_fn)\n    validation_predictions = np.array([item[&#039;predictions&#039;][0] for item in validation_predictions])\n    \n    # Compute training and validation loss.\n    training_root_mean_squared_error = math.sqrt(\n        metrics.mean_squared_error(training_predictions, training_targets))\n    validation_root_mean_squared_error = math.sqrt(\n        metrics.mean_squared_error(validation_predictions, validation_targets))\n    # Occasionally print the current loss.\n    print(&quot;  period %02d : %0.2f&quot; % (period, training_root_mean_squared_error))\n    # Add the loss metrics from this period to our list.\n    training_rmse.append(training_root_mean_squared_error)\n    validation_rmse.append(validation_root_mean_squared_error)\n  print(&quot;Model training finished.&quot;)\n \n  # Output a graph of loss metrics over periods.\n  plt.ylabel(&quot;RMSE&quot;)\n  plt.xlabel(&quot;Periods&quot;)\n  plt.title(&quot;Root Mean Squared Error vs. Periods&quot;)\n  plt.tight_layout()\n  plt.plot(training_rmse, label=&quot;training&quot;)\n  plt.plot(validation_rmse, label=&quot;validation&quot;)\n  plt.legend()\n \n  print(&quot;Final RMSE (on training data):   %0.2f&quot; % training_root_mean_squared_error)\n  print(&quot;Final RMSE (on validation data): %0.2f&quot; % validation_root_mean_squared_error)\n \n  return dnn_regressor, training_rmse, validation_rmse\n_ = train_nn_regression_model_optimize(\n    my_optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.0007),\n    steps=5000,\n    batch_size=70,\n    hidden_units=[10, 10],\n    training_examples=training_examples,\n    training_targets=training_targets,\n    validation_examples=validation_examples,\n    validation_targets=validation_targets)\nTraining model...\nRMSE (on training data):\n  period 00 : 162.48\n  period 01 : 157.66\n  period 02 : 150.85\n  period 03 : 142.03\n  period 04 : 131.60\n  period 05 : 120.72\n  period 06 : 113.46\n  period 07 : 110.01\n  period 08 : 108.28\n  period 09 : 107.34\nModel training finished.\nFinal RMSE (on training data):   107.34\nFinal RMSE (on validation data): 108.43\n\n\n线性缩放\n将输入标准化以使其位于 (-1, 1) 范围内可能是一种良好的标准做法。这样一来，SGD 在一个维度中采用很大步长（或者在另一维度中采用很小步长）时不会受阻。数值优化的爱好者可能会注意到，这种做法与使用预调节器 (Preconditioner) 的想法是有联系的。\ndef linear_scale(series):\n  min_val = series.min()\n  max_val = series.max()\n  scale = (max_val - min_val) / 2.0\n  return series.apply(lambda x:((x - min_val) / scale) - 1.0)\n使用线性缩放将特征标准化\n将输入标准化到 (-1, 1) 这一范围内。能达到什么程度的效果？\n一般来说，当输入特征大致位于相同范围时，神经网络的训练效果最好。\n对您的标准化数据进行健全性检查。（如果您忘了将某个特征标准化，会发生什么情况？）\n由于标准化会使用最小值和最大值，我们必须确保在整个数据集中一次性完成该操作。\n我们之所以可以这样做，是因为我们所有的数据都在一个 DataFrame 中。如果我们有多个数据集，则最好从训练集中导出标准化参数，然后以相同方式将其应用于测试集。\ndef normalize_linear_scale(examples_dataframe):\n  &quot;&quot;&quot;Returns a version of the input `DataFrame` that has all its features normalized linearly.&quot;&quot;&quot;\n  processed_features = pd.DataFrame()\n  processed_features[&quot;latitude&quot;] = linear_scale(examples_dataframe[&quot;latitude&quot;])\n  processed_features[&quot;longitude&quot;] = linear_scale(examples_dataframe[&quot;longitude&quot;])\n  processed_features[&quot;housing_median_age&quot;] = linear_scale(examples_dataframe[&quot;housing_median_age&quot;])\n  processed_features[&quot;total_rooms&quot;] = linear_scale(examples_dataframe[&quot;total_rooms&quot;])\n  processed_features[&quot;total_bedrooms&quot;] = linear_scale(examples_dataframe[&quot;total_bedrooms&quot;])\n  processed_features[&quot;population&quot;] = linear_scale(examples_dataframe[&quot;population&quot;])\n  processed_features[&quot;households&quot;] = linear_scale(examples_dataframe[&quot;households&quot;])\n  processed_features[&quot;median_income&quot;] = linear_scale(examples_dataframe[&quot;median_income&quot;])\n  processed_features[&quot;rooms_per_person&quot;] = linear_scale(examples_dataframe[&quot;rooms_per_person&quot;])\n  return processed_features\n \nnormalized_dataframe = normalize_linear_scale(preprocess_features(california_housing_dataframe))\nnormalized_training_examples = normalized_dataframe.head(12000)\nnormalized_validation_examples = normalized_dataframe.tail(5000)\n \n_ = train_nn_regression_model_optimize(\n    my_optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.005),\n    steps=2000,\n    batch_size=50,\n    hidden_units=[10, 10],\n    training_examples=normalized_training_examples,\n    training_targets=training_targets,\n    validation_examples=normalized_validation_examples,\n    validation_targets=validation_targets)\nTraining model...\nRMSE (on training data):\n  period 00 : 163.23\n  period 01 : 115.65\n  period 02 : 106.20\n  period 03 : 91.27\n  period 04 : 79.65\n  period 05 : 76.10\n  period 06 : 74.00\n  period 07 : 72.52\n  period 08 : 71.52\n  period 09 : 70.72\nModel training finished.\nFinal RMSE (on training data):   70.72\nFinal RMSE (on validation data): 72.52\n\n\n尝试其他优化器\n** 使用 AdaGrad 和 Adam 优化器并对比其效果。**\nAdaGrad 优化器是一种备选方案。AdaGrad 的核心是灵活地修改模型中每个系数的学习率，从而单调降低有效的学习率。该优化器对于凸优化问题非常有效，但不一定适合非凸优化问题的神经网络训练。您可以通过指定 AdagradOptimizer（而不是 GradientDescentOptimizer）来使用 AdaGrad。请注意，对于 AdaGrad，您可能需要使用较大的学习率。\n对于非凸优化问题，Adam 有时比 AdaGrad 更有效。要使用 Adam，请调用 tf.train.AdamOptimizer 方法。此方法将几个可选超参数作为参数，但我们的解决方案仅指定其中一个 (learning_rate)。在应用设置中，您应该谨慎指定和调整可选超参数。\n# 首先，我们来尝试 AdaGrad。\n_, adagrad_training_losses, adagrad_validation_losses = train_nn_regression_model_optimize(\n    my_optimizer=tf.train.AdagradOptimizer(learning_rate=0.5),\n    steps=500,\n    batch_size=100,\n    hidden_units=[10, 10],\n    training_examples=normalized_training_examples,\n    training_targets=training_targets,\n    validation_examples=normalized_validation_examples,\n    validation_targets=validation_targets)\nTraining model...\nRMSE (on training data):\n  period 00 : 84.89\n  period 01 : 72.08\n  period 02 : 71.06\n  period 03 : 71.18\n  period 04 : 69.57\n  period 05 : 72.57\n  period 06 : 69.04\n  period 07 : 67.49\n  period 08 : 68.83\n  period 09 : 69.24\nModel training finished.\nFinal RMSE (on training data):   69.24\nFinal RMSE (on validation data): 71.91\n\n\n # 现在，我们来尝试 Adam。\n_, adam_training_losses, adam_validation_losses = train_nn_regression_model_optimize(\n    my_optimizer=tf.train.AdamOptimizer(learning_rate=0.009),\n    steps=500,\n    batch_size=100,\n    hidden_units=[10, 10],\n    training_examples=normalized_training_examples,\n    training_targets=training_targets,\n    validation_examples=normalized_validation_examples,\n    validation_targets=validation_targets)\nTraining model...\nRMSE (on training data):\n  period 00 : 171.53\n  period 01 : 108.88\n  period 02 : 100.89\n  period 03 : 88.85\n  period 04 : 76.59\n  period 05 : 72.77\n  period 06 : 70.84\n  period 07 : 70.29\n  period 08 : 69.79\n  period 09 : 68.84\nModel training finished.\nFinal RMSE (on training data):   68.84\nFinal RMSE (on validation data): 70.98\n\n\n# 我们并排输出损失指标的图表。\nplt.ylabel(&quot;RMSE&quot;)\nplt.xlabel(&quot;Periods&quot;)\nplt.title(&quot;Root Mean Squared Error vs. Periods&quot;)\nplt.plot(adagrad_training_losses, label=&#039;Adagrad training&#039;)\nplt.plot(adagrad_validation_losses, label=&#039;Adagrad validation&#039;)\nplt.plot(adam_training_losses, label=&#039;Adam training&#039;)\nplt.plot(adam_validation_losses, label=&#039;Adam validation&#039;)\n_ = plt.legend()\n\n尝试其他标准化方法\n尝试对各种特征使用其他标准化方法，以进一步提高性能。\n如果仔细查看转换后数据的汇总统计信息，您可能会注意到，对某些特征进行线性缩放会使其聚集到接近 -1 的位置。\n例如，很多特征的中位数约为 -0.8，而不是 0.0。\n_ = training_examples.hist(bins=20, figsize=(18, 12), xlabelsize=2)\n\n通过选择其他方式来转换这些特征，我们可能会获得更好的效果。\n例如，对数缩放可能对某些特征有帮助。或者，截取极端值可能会使剩余部分的信息更加丰富。\ndef log_normalize(series):\n  return series.apply(lambda x:math.log(x+1.0))\n \ndef clip(series, clip_to_min, clip_to_max):\n  return series.apply(lambda x:(\n    min(max(x, clip_to_min), clip_to_max)))\n \ndef z_score_normalize(series):\n  mean = series.mean()\n  std_dv = series.std()\n  return series.apply(lambda x:(x - mean) / std_dv)\n \ndef binary_threshold(series, threshold):\n  return series.apply(lambda x:(1 if x &gt; threshold else 0))\n上述部分包含一些额外的标准化函数。\n请注意，如果您将目标标准化，则需要将网络的预测结果非标准化，以便比较损失函数的值。\ndef normalize(examples_dataframe):\n  &quot;&quot;&quot;Returns a version of the input `DataFrame` that has all its features normalized.&quot;&quot;&quot;\n  processed_features = pd.DataFrame()\n \n  processed_features[&quot;households&quot;] = log_normalize(examples_dataframe[&quot;households&quot;])\n  processed_features[&quot;median_income&quot;] = log_normalize(examples_dataframe[&quot;median_income&quot;])\n  processed_features[&quot;total_bedrooms&quot;] = log_normalize(examples_dataframe[&quot;total_bedrooms&quot;])\n  \n  processed_features[&quot;latitude&quot;] = linear_scale(examples_dataframe[&quot;latitude&quot;])\n  processed_features[&quot;longitude&quot;] = linear_scale(examples_dataframe[&quot;longitude&quot;])\n  processed_features[&quot;housing_median_age&quot;] = linear_scale(examples_dataframe[&quot;housing_median_age&quot;])\n \n  processed_features[&quot;population&quot;] = linear_scale(clip(examples_dataframe[&quot;population&quot;], 0, 5000))\n  processed_features[&quot;rooms_per_person&quot;] = linear_scale(clip(examples_dataframe[&quot;rooms_per_person&quot;], 0, 5))\n  processed_features[&quot;total_rooms&quot;] = linear_scale(clip(examples_dataframe[&quot;total_rooms&quot;], 0, 10000))\n \n  return processed_features\n \nnormalized_dataframe = normalize(preprocess_features(california_housing_dataframe))\nnormalized_training_examples = normalized_dataframe.head(12000)\nnormalized_validation_examples = normalized_dataframe.tail(5000)\n \n_ = train_nn_regression_model_optimize(\n    my_optimizer=tf.train.AdagradOptimizer(learning_rate=0.15),\n    steps=1000,\n    batch_size=50,\n    hidden_units=[10, 10],\n    training_examples=normalized_training_examples,\n    training_targets=training_targets,\n    validation_examples=normalized_validation_examples,\n    validation_targets=validation_targets)\nTraining model...\nRMSE (on training data):\n  period 00 : 89.38\n  period 01 : 75.15\n  period 02 : 72.35\n  period 03 : 70.70\n  period 04 : 70.95\n  period 05 : 69.13\n  period 06 : 68.46\n  period 07 : 68.42\n  period 08 : 68.68\n  period 09 : 67.93\nModel training finished.\nFinal RMSE (on training data):   67.93\nFinal RMSE (on validation data): 69.55\n\n\n仅使用纬度和经度特征\n训练仅使用纬度和经度作为特征的神经网络模型。\n房地产商喜欢说，地段是房价的唯一重要特征。\n我们来看看能否通过训练仅使用纬度和经度作为特征的模型来证实这一点。\n只有我们的神经网络模型可以从纬度和经度中学会复杂的非线性规律，才能达到我们想要的效果。\n注意：我们可能需要一个网络结构，其层数比我们之前在练习中使用的要多。\ndef location_location_location(examples_dataframe):\n  &quot;&quot;&quot;Returns a version of the input `DataFrame` that keeps only the latitude and longitude.&quot;&quot;&quot;\n  processed_features = pd.DataFrame()\n  processed_features[&quot;latitude&quot;] = linear_scale(examples_dataframe[&quot;latitude&quot;])\n  processed_features[&quot;longitude&quot;] = linear_scale(examples_dataframe[&quot;longitude&quot;])\n  return processed_features\n \nlll_dataframe = location_location_location(preprocess_features(california_housing_dataframe))\nlll_training_examples = lll_dataframe.head(12000)\nlll_validation_examples = lll_dataframe.tail(5000)\n \n_ = train_nn_regression_model_optimize(\n    my_optimizer=tf.train.AdagradOptimizer(learning_rate=0.05),\n    steps=500,\n    batch_size=50,\n    hidden_units=[10, 10, 5, 5, 5],\n    training_examples=lll_training_examples,\n    training_targets=training_targets,\n    validation_examples=lll_validation_examples,\n    validation_targets=validation_targets)\nTraining model...\nRMSE (on training data):\n  period 00 : 157.77\n  period 01 : 107.67\n  period 02 : 105.43\n  period 03 : 104.46\n  period 04 : 103.15\n  period 05 : 101.82\n  period 06 : 101.01\n  period 07 : 100.61\n  period 08 : 100.10\n  period 09 : 99.77\nModel training finished.\nFinal RMSE (on training data):   99.77\nFinal RMSE (on validation data): 100.48\n\n\n最好使纬度和经度保持标准化状态，对于只有两个特征的模型，结果并不算太糟。当然，地产价值在短距离内仍然可能有较大差异。"},"Tensorflow/TensorFlow_9":{"slug":"Tensorflow/TensorFlow_9","filePath":"Tensorflow/TensorFlow_9.md","title":"TensorFlow 9","links":[],"tags":["Tensorflow"],"content":"使用神经网络对手写数字进行分类\n\n\n训练线性模型和神经网络，以对传统 MNIST 数据集中的手写数字进行分类\n比较线性分类模型和神经网络分类模型的效果\n可视化神经网络隐藏层的权重\n\n我们的目标是将每个输入图片与正确的数字相对应。我们会创建一个包含几个隐藏层的神经网络，并在顶部放置一个归一化指数层，以选出最合适的类别。\n设置\n首先，我们下载数据集、导入 TensorFlow 和其他实用工具，并将数据加载到 Pandas DataFrame。请注意，此数据是原始 MNIST 训练数据的样本；我们随机选择了 20000 行。\nfrom __future__ import print_function\n \nimport glob\nimport math\nimport os\n \nfrom IPython import display\nfrom matplotlib import cm\nfrom matplotlib import gridspec\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn import metrics\nimport tensorflow as tf\nfrom tensorflow.python.data import Dataset\n \ntf.logging.set_verbosity(tf.logging.ERROR)\npd.options.display.max_rows = 10\npd.options.display.float_format = &#039;{:.1f}&#039;.format\n \nmnist_dataframe = pd.read_csv(\n  &quot;download.mlcc.google.cn/mledu-datasets/mnist_train_small.csv&quot;,\n  sep=&quot;,&quot;,\n  header=None)\n \n# Use just the first 10,000 records for training/validation.\nmnist_dataframe = mnist_dataframe.head(10000)\n \nmnist_dataframe = mnist_dataframe.reindex(np.random.permutation(mnist_dataframe.index))\nmnist_dataframe.head()\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      0\n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      ...\n      775\n      776\n      777\n      778\n      779\n      780\n      781\n      782\n      783\n      784\n    \n  \n  \n    \n      5456\n      8\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      934\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      2662\n      8\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      9385\n      4\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      157\n      8\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n  \n\n5 rows × 785 columns\n\n第一列中包含类别标签。其余列中包含特征值，每个像素对应一个特征值，有 28×28=784 个像素值，其中大部分像素值都为零；您也许需要花一分钟时间来确认它们不全部为零。\n\n这些样本都是分辨率相对较低、对比度相对较高的手写数字图片。0-9 这十个数字中的每个可能出现的数字均由唯一的类别标签表示。因此，这是一个具有 10 个类别的多类别分类问题。\n现在，我们解析一下标签和特征，并查看几个样本。注意 loc 的使用，借助 loc，我们能够基于原来的位置抽出各列，因为此数据集中没有标题行。\ndef parse_labels_and_features(dataset):\n  &quot;&quot;&quot;Extracts labels and features.\n  \n  This is a good place to scale or transform the features if needed.\n  \n  Args:\n    dataset: A Pandas `Dataframe`, containing the label on the first column and\n      monochrome pixel values on the remaining columns, in row major order.\n  Returns:\n    A `tuple` `(labels, features)`:\n      labels: A Pandas `Series`.\n      features: A Pandas `DataFrame`.\n  &quot;&quot;&quot;\n  labels = dataset[0]\n \n  # DataFrame.loc index ranges are inclusive at both ends.\n  features = dataset.loc[:,1:784]\n  # Scale the data to [0, 1] by dividing out the max value, 255.\n  features = features / 255\n \n  return labels, features\ntraining_targets, training_examples = parse_labels_and_features(mnist_dataframe[:7500])\ntraining_examples.describe()\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      10\n      ...\n      775\n      776\n      777\n      778\n      779\n      780\n      781\n      782\n      783\n      784\n    \n  \n  \n    \n      count\n      7500.0\n      7500.0\n      7500.0\n      7500.0\n      7500.0\n      7500.0\n      7500.0\n      7500.0\n      7500.0\n      7500.0\n      ...\n      7500.0\n      7500.0\n      7500.0\n      7500.0\n      7500.0\n      7500.0\n      7500.0\n      7500.0\n      7500.0\n      7500.0\n    \n    \n      mean\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      std\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      min\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      25%\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      50%\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      75%\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      max\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      1.0\n      1.0\n      0.8\n      0.2\n      1.0\n      0.2\n      0.0\n      0.0\n      0.0\n      0.0\n    \n  \n\n8 rows × 784 columns\n\nvalidation_targets, validation_examples = parse_labels_and_features(mnist_dataframe[7500:10000])\nvalidation_examples.describe()\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      10\n      ...\n      775\n      776\n      777\n      778\n      779\n      780\n      781\n      782\n      783\n      784\n    \n  \n  \n    \n      count\n      2500.0\n      2500.0\n      2500.0\n      2500.0\n      2500.0\n      2500.0\n      2500.0\n      2500.0\n      2500.0\n      2500.0\n      ...\n      2500.0\n      2500.0\n      2500.0\n      2500.0\n      2500.0\n      2500.0\n      2500.0\n      2500.0\n      2500.0\n      2500.0\n    \n    \n      mean\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      std\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      min\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      25%\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      50%\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      75%\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      max\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      1.0\n      0.8\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n  \n\n8 rows × 784 columns\n\n显示一个随机样本及其对应的标签。\nrand_example = np.random.choice(training_examples.index)\n_, ax = plt.subplots()\nax.matshow(training_examples.loc[rand_example].values.reshape(28, 28))\nax.set_title(&quot;Label: %i&quot; % training_targets.loc[rand_example])\nax.grid(False)\n\n为 MNIST 构建线性模型\n首先，我们创建一个基准模型，作为比较对象。LinearClassifier 可提供一组 k 类一对多分类器，每个类别（共 k 个）对应一个分类器。\n您会发现，除了报告准确率和绘制对数损失函数随时间变化情况的曲线图之外，我们还展示了一个混淆矩阵。混淆矩阵会显示错误分类为其他类别的类别。哪些数字相互之间容易混淆？\n另请注意，我们会使用 log_loss 函数跟踪模型的错误。不应将此函数与用于训练的 LinearClassifier 内部损失函数相混淆。\ndef construct_feature_columns():\n  &quot;&quot;&quot;Construct the TensorFlow Feature Columns.\n \n  Returns:\n    A set of feature columns\n  &quot;&quot;&quot; \n  \n  # There are 784 pixels in each image. \n  return set([tf.feature_column.numeric_column(&#039;pixels&#039;, shape=784)])\n在本次练习中，我们会对训练和预测使用单独的输入函数，并将这些函数分别嵌套在 create_training_input_fn() 和 create_predict_input_fn() 中，这样一来，我们就可以调用这些函数，以返回相应的 _input_fn，并将其传递到 .train() 和 .predict() 调用。\ndef create_training_input_fn(features, labels, batch_size, num_epochs=None, shuffle=True):\n  &quot;&quot;&quot;A custom input_fn for sending MNIST data to the estimator for training.\n \n  Args:\n    features: The training features.\n    labels: The training labels.\n    batch_size: Batch size to use during training.\n \n  Returns:\n    A function that returns batches of training features and labels during\n    training.\n  &quot;&quot;&quot;\n  def _input_fn(num_epochs=None, shuffle=True):\n    # Input pipelines are reset with each call to .train(). To ensure model\n    # gets a good sampling of data, even when number of steps is small, we \n    # shuffle all the data before creating the Dataset object\n    idx = np.random.permutation(features.index)\n    raw_features = {&quot;pixels&quot;:features.reindex(idx)}\n    raw_targets = np.array(labels[idx])\n   \n    ds = Dataset.from_tensor_slices((raw_features,raw_targets)) # warning: 2GB limit\n    ds = ds.batch(batch_size).repeat(num_epochs)\n    \n    if shuffle:\n      ds = ds.shuffle(10000)\n    \n    # Return the next batch of data.\n    feature_batch, label_batch = ds.make_one_shot_iterator().get_next()\n    return feature_batch, label_batch\n \n  return _input_fn\ndef create_predict_input_fn(features, labels, batch_size):\n  &quot;&quot;&quot;A custom input_fn for sending mnist data to the estimator for predictions.\n \n  Args:\n    features: The features to base predictions on.\n    labels: The labels of the prediction examples.\n \n  Returns:\n    A function that returns features and labels for predictions.\n  &quot;&quot;&quot;\n  def _input_fn():\n    raw_features = {&quot;pixels&quot;: features.values}\n    raw_targets = np.array(labels)\n    \n    ds = Dataset.from_tensor_slices((raw_features, raw_targets)) # warning: 2GB limit\n    ds = ds.batch(batch_size)\n    \n        \n    # Return the next batch of data.\n    feature_batch, label_batch = ds.make_one_shot_iterator().get_next()\n    return feature_batch, label_batch\n \n  return _input_fn\ndef train_linear_classification_model(\n    learning_rate,\n    steps,\n    batch_size,\n    training_examples,\n    training_targets,\n    validation_examples,\n    validation_targets):\n  &quot;&quot;&quot;Trains a linear classification model for the MNIST digits dataset.\n  \n  In addition to training, this function also prints training progress information,\n  a plot of the training and validation loss over time, and a confusion\n  matrix.\n  \n  Args:\n    learning_rate: A `float`, the learning rate to use.\n    steps: A non-zero `int`, the total number of training steps. A training step\n      consists of a forward and backward pass using a single batch.\n    batch_size: A non-zero `int`, the batch size.\n    training_examples: A `DataFrame` containing the training features.\n    training_targets: A `DataFrame` containing the training labels.\n    validation_examples: A `DataFrame` containing the validation features.\n    validation_targets: A `DataFrame` containing the validation labels.\n      \n  Returns:\n    The trained `LinearClassifier` object.\n  &quot;&quot;&quot;\n \n  periods = 10\n \n  steps_per_period = steps / periods  \n  # Create the input functions.\n  predict_training_input_fn = create_predict_input_fn(\n    training_examples, training_targets, batch_size)\n  predict_validation_input_fn = create_predict_input_fn(\n    validation_examples, validation_targets, batch_size)\n  training_input_fn = create_training_input_fn(\n    training_examples, training_targets, batch_size)\n  \n  # Create a LinearClassifier object.\n  my_optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate)\n  my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)\n  classifier = tf.estimator.LinearClassifier(\n      feature_columns=construct_feature_columns(),\n      n_classes=10,\n      optimizer=my_optimizer,\n      config=tf.estimator.RunConfig(keep_checkpoint_max=1)\n  )\n \n  # Train the model, but do so inside a loop so that we can periodically assess\n  # loss metrics.\n  print(&quot;Training model...&quot;)\n  print(&quot;LogLoss error (on validation data):&quot;)\n  training_errors = []\n  validation_errors = []\n  for period in range (0, periods):\n    # Train the model, starting from the prior state.\n    classifier.train(\n        input_fn=training_input_fn,\n        steps=steps_per_period\n    )\n  \n    # Take a break and compute probabilities.\n    training_predictions = list(classifier.predict(input_fn=predict_training_input_fn))\n    training_probabilities = np.array([item[&#039;probabilities&#039;] for item in training_predictions])\n    training_pred_class_id = np.array([item[&#039;class_ids&#039;][0] for item in training_predictions])\n    training_pred_one_hot = tf.keras.utils.to_categorical(training_pred_class_id,10)\n        \n    validation_predictions = list(classifier.predict(input_fn=predict_validation_input_fn))\n    validation_probabilities = np.array([item[&#039;probabilities&#039;] for item in validation_predictions])    \n    validation_pred_class_id = np.array([item[&#039;class_ids&#039;][0] for item in validation_predictions])\n    validation_pred_one_hot = tf.keras.utils.to_categorical(validation_pred_class_id,10)    \n    \n    # Compute training and validation errors.\n    training_log_loss = metrics.log_loss(training_targets, training_pred_one_hot)\n    validation_log_loss = metrics.log_loss(validation_targets, validation_pred_one_hot)\n    # Occasionally print the current loss.\n    print(&quot;  period %02d : %0.2f&quot; % (period, validation_log_loss))\n    # Add the loss metrics from this period to our list.\n    training_errors.append(training_log_loss)\n    validation_errors.append(validation_log_loss)\n  print(&quot;Model training finished.&quot;)\n  # Remove event files to save disk space.\n  _ = map(os.remove, glob.glob(os.path.join(classifier.model_dir, &#039;events.out.tfevents*&#039;)))\n  \n  # Calculate final predictions (not probabilities, as above).\n  final_predictions = classifier.predict(input_fn=predict_validation_input_fn)\n  final_predictions = np.array([item[&#039;class_ids&#039;][0] for item in final_predictions])\n  \n  \n  accuracy = metrics.accuracy_score(validation_targets, final_predictions)\n  print(&quot;Final accuracy (on validation data): %0.2f&quot; % accuracy)\n \n  # Output a graph of loss metrics over periods.\n  plt.ylabel(&quot;LogLoss&quot;)\n  plt.xlabel(&quot;Periods&quot;)\n  plt.title(&quot;LogLoss vs. Periods&quot;)\n  plt.plot(training_errors, label=&quot;training&quot;)\n  plt.plot(validation_errors, label=&quot;validation&quot;)\n  plt.legend()\n  plt.show()\n  \n  # Output a plot of the confusion matrix.\n  cm = metrics.confusion_matrix(validation_targets, final_predictions)\n  # Normalize the confusion matrix by row (i.e by the number of samples\n  # in each class).\n  cm_normalized = cm.astype(&quot;float&quot;) / cm.sum(axis=1)[:, np.newaxis]\n  ax = sns.heatmap(cm_normalized, cmap=&quot;bone_r&quot;)\n  ax.set_aspect(1)\n  plt.title(&quot;Confusion matrix&quot;)\n  plt.ylabel(&quot;True label&quot;)\n  plt.xlabel(&quot;Predicted label&quot;)\n  plt.show()\n \n  return classifier\n_ = train_linear_classification_model(\n    learning_rate=0.03,\n    steps=1000,\n    batch_size=30,\n    training_examples=training_examples,\n    training_targets=training_targets,\n    validation_examples=validation_examples,\n    validation_targets=validation_targets)\nTraining model...\nLogLoss error (on validation data):\n  period 00 : 4.39\n  period 01 : 4.01\n  period 02 : 3.77\n  period 03 : 3.84\n  period 04 : 3.70\n  period 05 : 3.59\n  period 06 : 3.65\n  period 07 : 3.54\n  period 08 : 3.50\n  period 09 : 3.55\nModel training finished.\nFinal accuracy (on validation data): 0.90\n\n\n\n使用神经网络替换线性分类器\n使用 DNNClassifier 替换上面的 LinearClassifier，并查找可实现 0.95 或更高准确率的参数组合。\n您可能希望尝试 Dropout 等其他正则化方法。这些额外的正则化方法已记录在 DNNClassifier 类的注释中。\n除了神经网络专用配置（例如隐藏单元的超参数）之外，以下代码与原始的 LinearClassifer 训练代码几乎完全相同。\ndef train_nn_classification_model(\n    learning_rate,\n    steps,\n    batch_size,\n    hidden_units,\n    training_examples,\n    training_targets,\n    validation_examples,\n    validation_targets):\n  &quot;&quot;&quot;Trains a neural network classification model for the MNIST digits dataset.\n  \n  In addition to training, this function also prints training progress information,\n  a plot of the training and validation loss over time, as well as a confusion\n  matrix.\n  \n  Args:\n    learning_rate: A `float`, the learning rate to use.\n    steps: A non-zero `int`, the total number of training steps. A training step\n      consists of a forward and backward pass using a single batch.\n    batch_size: A non-zero `int`, the batch size.\n    hidden_units: A `list` of int values, specifying the number of neurons in each layer.\n    training_examples: A `DataFrame` containing the training features.\n    training_targets: A `DataFrame` containing the training labels.\n    validation_examples: A `DataFrame` containing the validation features.\n    validation_targets: A `DataFrame` containing the validation labels.\n      \n  Returns:\n    The trained `DNNClassifier` object.\n  &quot;&quot;&quot;\n \n  periods = 10\n  # Caution: input pipelines are reset with each call to train. \n  # If the number of steps is small, your model may never see most of the data.  \n  # So with multiple `.train` calls like this you may want to control the length \n  # of training with num_epochs passed to the input_fn. Or, you can do a really-big shuffle, \n  # or since it&#039;s in-memory data, shuffle all the data in the `input_fn`.\n  steps_per_period = steps / periods  \n    \n  # Create the input functions.\n  predict_training_input_fn = create_predict_input_fn(\n    training_examples, training_targets, batch_size)\n  predict_validation_input_fn = create_predict_input_fn(\n    validation_examples, validation_targets, batch_size)\n  training_input_fn = create_training_input_fn(\n    training_examples, training_targets, batch_size)\n  \n  # Create feature columns.\n  feature_columns = [tf.feature_column.numeric_column(&#039;pixels&#039;, shape=784)]\n \n  # Create a DNNClassifier object.\n  my_optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate)\n  my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)\n  classifier = tf.estimator.DNNClassifier(\n      feature_columns=feature_columns,\n      n_classes=10,\n      hidden_units=hidden_units,\n      optimizer=my_optimizer,\n      config=tf.contrib.learn.RunConfig(keep_checkpoint_max=1)\n  )\n \n  # Train the model, but do so inside a loop so that we can periodically assess\n  # loss metrics.\n  print(&quot;Training model...&quot;)\n  print(&quot;LogLoss error (on validation data):&quot;)\n  training_errors = []\n  validation_errors = []\n  for period in range (0, periods):\n    # Train the model, starting from the prior state.\n    classifier.train(\n        input_fn=training_input_fn,\n        steps=steps_per_period\n    )\n  \n    # Take a break and compute probabilities.\n    training_predictions = list(classifier.predict(input_fn=predict_training_input_fn))\n    training_probabilities = np.array([item[&#039;probabilities&#039;] for item in training_predictions])\n    training_pred_class_id = np.array([item[&#039;class_ids&#039;][0] for item in training_predictions])\n    training_pred_one_hot = tf.keras.utils.to_categorical(training_pred_class_id,10)\n        \n    validation_predictions = list(classifier.predict(input_fn=predict_validation_input_fn))\n    validation_probabilities = np.array([item[&#039;probabilities&#039;] for item in validation_predictions])    \n    validation_pred_class_id = np.array([item[&#039;class_ids&#039;][0] for item in validation_predictions])\n    validation_pred_one_hot = tf.keras.utils.to_categorical(validation_pred_class_id,10)    \n    \n    # Compute training and validation errors.\n    training_log_loss = metrics.log_loss(training_targets, training_pred_one_hot)\n    validation_log_loss = metrics.log_loss(validation_targets, validation_pred_one_hot)\n    # Occasionally print the current loss.\n    print(&quot;  period %02d : %0.2f&quot; % (period, validation_log_loss))\n    # Add the loss metrics from this period to our list.\n    training_errors.append(training_log_loss)\n    validation_errors.append(validation_log_loss)\n  print(&quot;Model training finished.&quot;)\n  # Remove event files to save disk space.\n  _ = map(os.remove, glob.glob(os.path.join(classifier.model_dir, &#039;events.out.tfevents*&#039;)))\n  \n  # Calculate final predictions (not probabilities, as above).\n  final_predictions = classifier.predict(input_fn=predict_validation_input_fn)\n  final_predictions = np.array([item[&#039;class_ids&#039;][0] for item in final_predictions])\n  \n  \n  accuracy = metrics.accuracy_score(validation_targets, final_predictions)\n  print(&quot;Final accuracy (on validation data): %0.2f&quot; % accuracy)\n \n  # Output a graph of loss metrics over periods.\n  plt.ylabel(&quot;LogLoss&quot;)\n  plt.xlabel(&quot;Periods&quot;)\n  plt.title(&quot;LogLoss vs. Periods&quot;)\n  plt.plot(training_errors, label=&quot;training&quot;)\n  plt.plot(validation_errors, label=&quot;validation&quot;)\n  plt.legend()\n  plt.show()\n  \n  # Output a plot of the confusion matrix.\n  cm = metrics.confusion_matrix(validation_targets, final_predictions)\n  # Normalize the confusion matrix by row (i.e by the number of samples\n  # in each class).\n  cm_normalized = cm.astype(&quot;float&quot;) / cm.sum(axis=1)[:, np.newaxis]\n  ax = sns.heatmap(cm_normalized, cmap=&quot;bone_r&quot;)\n  ax.set_aspect(1)\n  plt.title(&quot;Confusion matrix&quot;)\n  plt.ylabel(&quot;True label&quot;)\n  plt.xlabel(&quot;Predicted label&quot;)\n  plt.show()\n \n  return classifier\nclassifier = train_nn_classification_model(\n    learning_rate=0.05,\n    steps=1000,\n    batch_size=30,\n    hidden_units=[100, 100],\n    training_examples=training_examples,\n    training_targets=training_targets,\n    validation_examples=validation_examples,\n    validation_targets=validation_targets)\nTraining model...\nLogLoss error (on validation data):\n  period 00 : 4.01\n  period 01 : 3.45\n  period 02 : 3.01\n  period 03 : 3.19\n  period 04 : 2.62\n  period 05 : 2.29\n  period 06 : 2.17\n  period 07 : 2.11\n  period 08 : 2.06\n  period 09 : 2.00\nModel training finished.\nFinal accuracy (on validation data): 0.94\n\n\n\n接下来，我们来验证测试集的准确率。\nmnist_test_dataframe = pd.read_csv(\n  &quot;download.mlcc.google.cn/mledu-datasets/mnist_test.csv&quot;,\n  sep=&quot;,&quot;,\n  header=None)\n \ntest_targets, test_examples = parse_labels_and_features(mnist_test_dataframe)\ntest_examples.describe()\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      1\n      2\n      3\n      4\n      5\n      6\n      7\n      8\n      9\n      10\n      ...\n      775\n      776\n      777\n      778\n      779\n      780\n      781\n      782\n      783\n      784\n    \n  \n  \n    \n      count\n      10000.0\n      10000.0\n      10000.0\n      10000.0\n      10000.0\n      10000.0\n      10000.0\n      10000.0\n      10000.0\n      10000.0\n      ...\n      10000.0\n      10000.0\n      10000.0\n      10000.0\n      10000.0\n      10000.0\n      10000.0\n      10000.0\n      10000.0\n      10000.0\n    \n    \n      mean\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      std\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      min\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      25%\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      50%\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      75%\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n    \n      max\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      ...\n      1.0\n      1.0\n      0.6\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n      0.0\n    \n  \n\n8 rows × 784 columns\n\npredict_test_input_fn = create_predict_input_fn(\n    test_examples, test_targets, batch_size=100)\n \ntest_predictions = classifier.predict(input_fn=predict_test_input_fn)\ntest_predictions = np.array([item[&#039;class_ids&#039;][0] for item in test_predictions])\n  \naccuracy = metrics.accuracy_score(test_targets, test_predictions)\nprint(&quot;Accuracy on test data: %0.2f&quot; % accuracy)\nAccuracy on test data: 0.95\n\n可视化第一个隐藏层的权重。\n我们来花几分钟时间看看模型的 weights_ 属性，以深入探索我们的神经网络，并了解它学到了哪些规律。\n模型的输入层有 784 个权重，对应于 28×28 像素输入图片。第一个隐藏层将有 784×N 个权重，其中 N 指的是该层中的节点数。我们可以将这些权重重新变回 28×28 像素的图片，具体方法是将 N 个 1×784 权重数组变形为 N 个 28×28 大小数组。\n运行以下单元格，绘制权重曲线图。请注意，此单元格要求名为 “classifier” 的 DNNClassifier 已经过训练。\nprint(classifier.get_variable_names())\n \nweights0 = classifier.get_variable_value(&quot;dnn/hiddenlayer_0/kernel&quot;)\n \nprint(&quot;weights0 shape:&quot;, weights0.shape)\n \nnum_nodes = weights0.shape[1]\nnum_rows = int(math.ceil(num_nodes / 10.0))\nfig, axes = plt.subplots(num_rows, 10, figsize=(20, 2 * num_rows))\nfor coef, ax in zip(weights0.T, axes.ravel()):\n    # Weights in coef is reshaped from 1x784 to 28x28.\n    ax.matshow(coef.reshape(28, 28), cmap=plt.cm.pink)\n    ax.set_xticks(())\n    ax.set_yticks(())\n \nplt.show()\n[&#039;dnn/hiddenlayer_0/bias&#039;, &#039;dnn/hiddenlayer_0/bias/t_0/Adagrad&#039;, &#039;dnn/hiddenlayer_0/kernel&#039;, &#039;dnn/hiddenlayer_0/kernel/t_0/Adagrad&#039;, &#039;dnn/hiddenlayer_1/bias&#039;, &#039;dnn/hiddenlayer_1/bias/t_0/Adagrad&#039;, &#039;dnn/hiddenlayer_1/kernel&#039;, &#039;dnn/hiddenlayer_1/kernel/t_0/Adagrad&#039;, &#039;dnn/logits/bias&#039;, &#039;dnn/logits/bias/t_0/Adagrad&#039;, &#039;dnn/logits/kernel&#039;, &#039;dnn/logits/kernel/t_0/Adagrad&#039;, &#039;global_step&#039;]\nweights0 shape: (784, 100)\n\n\n神经网络的第一个隐藏层应该会对一些级别特别低的特征进行建模，因此可视化权重可能只显示一些模糊的区域，也可能只显示数字的某几个部分。此外，您可能还会看到一些基本上是噪点（这些噪点要么不收敛，要么被更高的层忽略）的神经元。\n在迭代不同的次数后停止训练并查看效果，可能会发现有趣的结果。\n分别用 10、100 和 1000 步训练分类器。然后重新运行此可视化。\n您看到不同级别的收敛之间有哪些直观上的差异？\nclassifier = train_nn_classification_model(\n    learning_rate=0.05,\n    steps=10,\n    batch_size=30,\n    hidden_units=[100, 100],\n    training_examples=training_examples,\n    training_targets=training_targets,\n    validation_examples=validation_examples,\n    validation_targets=validation_targets)\nTraining model...\nLogLoss error (on validation data):\n  period 00 : 29.18\n  period 01 : 28.72\n  period 02 : 21.72\n  period 03 : 27.76\n  period 04 : 21.00\n  period 05 : 22.31\n  period 06 : 17.28\n  period 07 : 15.51\n  period 08 : 20.03\n  period 09 : 13.90\nModel training finished.\nFinal accuracy (on validation data): 0.60\n\n\n\nprint(classifier.get_variable_names())\n \nweights0 = classifier.get_variable_value(&quot;dnn/hiddenlayer_0/kernel&quot;)\n \nprint(&quot;weights0 shape:&quot;, weights0.shape)\n \nnum_nodes = weights0.shape[1]\nnum_rows = int(math.ceil(num_nodes / 10.0))\nfig, axes = plt.subplots(num_rows, 10, figsize=(20, 2 * num_rows))\nfor coef, ax in zip(weights0.T, axes.ravel()):\n    # Weights in coef is reshaped from 1x784 to 28x28.\n    ax.matshow(coef.reshape(28, 28), cmap=plt.cm.pink)\n    ax.set_xticks(())\n    ax.set_yticks(())\n \nplt.show()\n[&#039;dnn/hiddenlayer_0/bias&#039;, &#039;dnn/hiddenlayer_0/bias/t_0/Adagrad&#039;, &#039;dnn/hiddenlayer_0/kernel&#039;, &#039;dnn/hiddenlayer_0/kernel/t_0/Adagrad&#039;, &#039;dnn/hiddenlayer_1/bias&#039;, &#039;dnn/hiddenlayer_1/bias/t_0/Adagrad&#039;, &#039;dnn/hiddenlayer_1/kernel&#039;, &#039;dnn/hiddenlayer_1/kernel/t_0/Adagrad&#039;, &#039;dnn/logits/bias&#039;, &#039;dnn/logits/bias/t_0/Adagrad&#039;, &#039;dnn/logits/kernel&#039;, &#039;dnn/logits/kernel/t_0/Adagrad&#039;, &#039;global_step&#039;]\nweights0 shape: (784, 100)\n\n\nclassifier = train_nn_classification_model(\n    learning_rate=0.05,\n    steps=100,\n    batch_size=30,\n    hidden_units=[100, 100],\n    training_examples=training_examples,\n    training_targets=training_targets,\n    validation_examples=validation_examples,\n    validation_targets=validation_targets)\nTraining model...\nLogLoss error (on validation data):\n  period 00 : 18.72\n  period 01 : 10.71\n  period 02 : 7.58\n  period 03 : 8.70\n  period 04 : 5.79\n  period 05 : 5.21\n  period 06 : 5.40\n  period 07 : 6.05\n  period 08 : 5.75\n  period 09 : 3.95\nModel training finished.\nFinal accuracy (on validation data): 0.89\n\n\n\nprint(classifier.get_variable_names())\n \nweights0 = classifier.get_variable_value(&quot;dnn/hiddenlayer_0/kernel&quot;)\n \nprint(&quot;weights0 shape:&quot;, weights0.shape)\n \nnum_nodes = weights0.shape[1]\nnum_rows = int(math.ceil(num_nodes / 10.0))\nfig, axes = plt.subplots(num_rows, 10, figsize=(20, 2 * num_rows))\nfor coef, ax in zip(weights0.T, axes.ravel()):\n    # Weights in coef is reshaped from 1x784 to 28x28.\n    ax.matshow(coef.reshape(28, 28), cmap=plt.cm.pink)\n    ax.set_xticks(())\n    ax.set_yticks(())\n \nplt.show()\n[&#039;dnn/hiddenlayer_0/bias&#039;, &#039;dnn/hiddenlayer_0/bias/t_0/Adagrad&#039;, &#039;dnn/hiddenlayer_0/kernel&#039;, &#039;dnn/hiddenlayer_0/kernel/t_0/Adagrad&#039;, &#039;dnn/hiddenlayer_1/bias&#039;, &#039;dnn/hiddenlayer_1/bias/t_0/Adagrad&#039;, &#039;dnn/hiddenlayer_1/kernel&#039;, &#039;dnn/hiddenlayer_1/kernel/t_0/Adagrad&#039;, &#039;dnn/logits/bias&#039;, &#039;dnn/logits/bias/t_0/Adagrad&#039;, &#039;dnn/logits/kernel&#039;, &#039;dnn/logits/kernel/t_0/Adagrad&#039;, &#039;global_step&#039;]\nweights0 shape: (784, 100)\n\n"},"Tensorflow/tensorflow学习前的准备工作":{"slug":"Tensorflow/tensorflow学习前的准备工作","filePath":"Tensorflow/tensorflow学习前的准备工作.md","title":"TensorFlow学习前的准备工作","links":[],"tags":["Tensorflow"],"content":"工欲善其事必先利其器，在学习tensor flow之前需要先学会使用一些工具，首先是jupyter，这之后关于tensor flow的blog也都会写成jupyter格式的。\nJupyter\nJupyter Notebook 的本质是一个 Web 应用程序，便于创建和共享文学化程序文档，支持实时代码，数学方程，可视化和 markdown。 用途包括：数据清理和转换，数值模拟，统计建模，机器学习等等。\n官网：Jupyter\nInstalling Jupyter with pip\nAs an existing or experienced Python user, you may wish to install Jupyter using Python’s package manager, pip, instead of Anaconda.\r\n\r\nIf you have Python 3 installed (which is recommended):\r\n\r\npython3 -m pip install --upgrade pip\r\npython3 -m pip install jupyter\r\nIf you have Python 2 installed:\r\n\r\npython -m pip install --upgrade pip\r\npython -m pip install jupyter\r\nCongratulations, you have installed Jupyter Notebook! To run the notebook, run the following command at the Terminal (Mac/Linux) or Command Prompt (Windows):\r\n\r\nrun: jupyter notebook\n\n安装成功后执行 jupyter notebook 后会打开一个web，通过网页就可以执行python程序。\n还可以把ipynb文件转换为html，md，pdf等格式\nipython nbconvert --to markdown  filename.ipynb\r\nipython nbconvert --to html  filename.ipynb\n\nipynb转换为html、md、pdf等格式，还有另一种更简单的方法：在jupyter notebook中，选择File→Download as，直接选择需要转换的格式就可以了。需要注意的是，转换为pdf格式之前，同样要保证已经安装了xelatex。\n基础知识\n基础数学\n代数\r\n变量、系数和函数\r\n线性方程式，例如 \r\n对数和对数方程式，例如 \r\nS 型函数\r\n线性代数\r\n张量和张量等级\r\n矩阵乘法\r\n三角学\r\nTanh（作为激活函数进行讲解，无需提前掌握相关知识）\r\n统计信息\r\n均值、中间值、离群值和标准偏差\r\n能够读懂直方图\r\n微积分（可选，适合高级主题）\r\n导数概念（您不必真正计算导数）\r\n梯度或斜率\r\n偏导数（与梯度紧密相关）\r\n链式法则（带您全面了解用于训练神经网络的反向传播算法）\n\n基础 Python\nPython 教程中介绍了以下 Python 基础知识：\r\n\r\n定义和调用函数：使用位置和关键字参数\r\n\r\n字典、列表、集合（创建、访问和迭代）\r\n\r\nfor 循环：包含多个迭代器变量的 for 循环（例如 for a, b in [(1,2), (3,4)]）\r\n\r\nif/else 条件块和条件表达式\r\n\r\n字符串格式（例如 &#039;%.2f&#039; % 3.14）\r\n\r\n变量、赋值、基本数据类型（int、float、bool、str）\r\n\r\npass 语句\n\nPython 库\n机器学习速成课程代码示例使用了第三方库提供的以下功能。无需提前熟悉这些库；您可以在需要时查询相关内容。\r\n\r\nMatplotlib（适合数据可视化）\r\npyplot 模块\r\ncm 模块\r\ngridspec 模块\r\nSeaborn（适合热图）\r\nheatmap 函数\r\nPandas（适合数据处理）\r\nDataFrame 类\r\nNumPy（适合低阶数学运算）\r\nlinspace 函数\r\nrandom 函数\r\narray 函数\r\narange 函数\r\nscikit-learn（适合评估指标）\r\nmetrics 模块\n\nBash shell\n知道命令行，会敲命令。\nPandas\n最后要知道Pandas库中DataFrame的数据结构，看另一篇博客，Pandas简介。\n以上都不会也没有关系，毕竟tensor flow是给学龄前儿童玩耍的。\nTensorFlow\n官网\r\n这里提供了非常详细的中文教程\n我没有搭建实体环境，是通过Colaboratory来实验代码。\nColaboratory\nColaboratory是Google的一个研究项目，旨在提供开发者一个云端训练神经网络的工具。它是Jupyter一个笔记本环境，不用做任何配置，完全运行在云端。Colaboratory存储在Google Drive中，可以进行共享。Colaboratory向开发者提供了免费的Tesla K80 GPU使用。\n实用的键盘快捷键\n\n\n**⌘/Ctrl+m,b：**在当前选择的单元格下方创建一个空白代码单元格\n\n\n**⌘/Ctrl+m,i：**中断单元格的运行\n\n\n**⌘/Ctrl+m,h：**显示所有键盘快捷键列表\n\n\n要查看关于任何 TensorFlow API 方法的文档，请将光标放置在其左括号的正后方，然后按 Tab 键：\n\n\n"},"Tensorflow/保存与加载Tensorflow模型":{"slug":"Tensorflow/保存与加载Tensorflow模型","filePath":"Tensorflow/保存与加载Tensorflow模型.md","title":"保存与加载Tensorflow模型","links":[],"tags":[],"content":"保存与加载模型\n安装tensorflow-datasets，导入依赖项：\n%pip install tensorflow-datasets\nimport tensorflow_datasets as tfds\n \nimport tensorflow as tf\ntfds.disable_progress_bar()\ntf.__version__\n&#039;2.3.0&#039;\n\nmirrored_strategy = tf.distribute.MirroredStrategy()\nWARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\nINFO:tensorflow:Using MirroredStrategy with devices (&#039;/job:localhost/replica:0/task:0/device:CPU:0&#039;,)\n\n创建一个分发变量和图形的策略\ntf.distribute.MirroredStrategy 策略是如何运作的？\n所有变量和模型图都复制在副本上。\n输入都均匀分布在副本中。\n每个副本在收到输入后计算输入的损失和梯度。\n通过求和，每一个副本上的梯度都能同步。\n同步后，每个副本上的复制的变量都可以同样更新。\n\ndef get_data():\n  datasets, ds_info = tfds.load(name=&#039;mnist&#039;, with_info=True, as_supervised=True)\n  mnist_train, mnist_test = datasets[&#039;train&#039;], datasets[&#039;test&#039;]\n \n  BUFFER_SIZE = 10000\n \n  BATCH_SIZE_PER_REPLICA = 64\n  BATCH_SIZE = BATCH_SIZE_PER_REPLICA * mirrored_strategy.num_replicas_in_sync\n \n  def scale(image, label):\n    image = tf.cast(image, tf.float32)\n    image /= 255\n \n    return image, label\n \n  train_dataset = mnist_train.map(scale).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n  eval_dataset = mnist_test.map(scale).batch(BATCH_SIZE)\n \n  return train_dataset, eval_dataset\ndef get_model():\n    with mirrored_strategy.scope():\n        model = tf.keras.Sequential([\n            tf.keras.layers.Conv2D(32, 3, activation=&#039;relu&#039;, input_shape=(28, 28, 1)),\n            tf.keras.layers.MaxPool2D(),\n            tf.keras.layers.Flatten(),\n            tf.keras.layers.Dense(64, activation=&#039;relu&#039;),\n            tf.keras.layers.Dense(10)\n        ])\n        \n        model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n                     optimizer=tf.keras.optimizers.Adam(),\n                     metrics=[&#039;accuracy&#039;])\n        return model\n训练模型\nmodel = get_model()\ntrain_dataset, eval_dataset = get_data()\n\u001b[1mDownloading and preparing dataset mnist/3.0.1 (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\v-xujwan\\tensorflow_datasets\\mnist\\3.0.1...\u001b[0m\nShuffling and writing examples to C:\\Users\\v-xujwan\\tensorflow_datasets\\mnist\\3.0.1.incompleteI371SH\\mnist-train.tfrecord\nShuffling and writing examples to C:\\Users\\v-xujwan\\tensorflow_datasets\\mnist\\3.0.1.incompleteI371SH\\mnist-test.tfrecord\n\u001b[1mDataset mnist downloaded and prepared to C:\\Users\\v-xujwan\\tensorflow_datasets\\mnist\\3.0.1. Subsequent calls will reuse this data.\u001b[0m\n\nmodel.fit(train_dataset, epochs=2)\nEpoch 1/2\nWARNING:tensorflow:From D:\\Anaconda3\\envs\\py36_tf2\\lib\\site-packages\\tensorflow\\python\\data\\ops\\multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.data.Iterator.get_next_as_optional()` instead.\n\n\nWARNING:tensorflow:From D:\\Anaconda3\\envs\\py36_tf2\\lib\\site-packages\\tensorflow\\python\\data\\ops\\multi_device_iterator_ops.py:601: get_next_as_optional (from tensorflow.python.data.ops.iterator_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse `tf.data.Iterator.get_next_as_optional()` instead.\n\n\n938/938 [==============================] - 18s 19ms/step - loss: 0.2089 - accuracy: 0.9389\nEpoch 2/2\n938/938 [==============================] - 19s 20ms/step - loss: 0.0689 - accuracy: 0.97980s - los\n\n&lt;tensorflow.python.keras.callbacks.History at 0x2b4c939e278&gt;\n\n保存并加载模型\n现在有了一个简单的模型可以使用，让我们看一下保存/加载API。有两套可用的API：\n高级的keras model.save和tf.keras.models.load_model\n低级的tf.saved_model.save和tf.saved_model.load\n\n使用keras API\nkeras_model_path = &quot;./tmp/keras_save&quot;\nmodel.save(keras_model_path)\nWARNING:tensorflow:From D:\\Anaconda3\\envs\\py36_tf2\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\nInstructions for updating:\nThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\n\n\nWARNING:tensorflow:From D:\\Anaconda3\\envs\\py36_tf2\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\nInstructions for updating:\nThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\n\n\nWARNING:tensorflow:From D:\\Anaconda3\\envs\\py36_tf2\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\nInstructions for updating:\nThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\n\n\nWARNING:tensorflow:From D:\\Anaconda3\\envs\\py36_tf2\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\nInstructions for updating:\nThis property should not be used in TensorFlow 2.0, as updates are applied automatically.\n\n\nINFO:tensorflow:Assets written to: ./tmp/keras_save\\assets\n\n\nINFO:tensorflow:Assets written to: ./tmp/keras_save\\assets\n\n模型保存成功，看一下保存的文件\n└───tmp\n    └───keras_save\n        ├───assets\n        └───variables\n            └───variables.data-00000-of-00001\n            └───variables.index\n        └───saved_model.pb\n\n接着还原模型\nrestored_keras_model = tf.keras.models.load_model(keras_model_path)\n还原后的模型可以继续训练\nrestored_keras_model.fit(train_dataset, epochs=2)\nEpoch 1/2\n938/938 [==============================] - 16s 17ms/step - loss: 0.0494 - accuracy: 0.09890s - loss: 0.0493 - accuracy: 0.09\nEpoch 2/2\n938/938 [==============================] - 16s 17ms/step - loss: 0.0353 - accuracy: 0.0989\n\n&lt;tensorflow.python.keras.callbacks.History at 0x2b4c5b5a128&gt;\n\n现在加载模型并使用进行训练tf.distribute.Strategy\nanother_strategy = tf.distribute.OneDeviceStrategy(&quot;/cpu:0&quot;)\nwith another_strategy.scope():\n  restored_keras_model_ds = tf.keras.models.load_model(keras_model_path)\n  restored_keras_model_ds.fit(train_dataset, epochs=2)\nEpoch 1/2\n938/938 [==============================] - 16s 17ms/step - loss: 0.0501 - accuracy: 0.0990\nEpoch 2/2\n938/938 [==============================] - 16s 17ms/step - loss: 0.0354 - accuracy: 0.0989\n\nrestored_keras_model_ds.predict\n&lt;tensorflow.python.keras.engine.sequential.Sequential at 0x2b4c75ea2b0&gt;\n\n使用tf.saved_model API\n现在使用低级的api，保存方法和keras类似\nmodel = get_model()\nsaved_model_path = &quot;./tmp/tf_save&quot;\ntf.saved_model.save(model, saved_model_path)\nINFO:tensorflow:Assets written to: ./tmp/tf_save\\assets\n\n\nINFO:tensorflow:Assets written to: ./tmp/tf_save\\assets\n\n可以使用进行加载tf.saved_model.load()。但是，由于它是一个较低级别的API（因此具有更广泛的用例范围），因此它不会返回Keras模型。相反，它返回一个对象，该对象包含可用于进行推断的函数。例如：\n还可以以分布式方式加载和进行推断：\nanother_strategy = tf.distribute.MirroredStrategy()\nwith another_strategy.scope():\n  loaded = tf.saved_model.load(saved_model_path)\n  inference_func = loaded.signatures[DEFAULT_FUNCTION_KEY]\n \n  dist_predict_dataset = another_strategy.experimental_distribute_dataset(\n      predict_dataset)\n \n  # Calling the function in a distributed manner\n  for batch in dist_predict_dataset:\n    another_strategy.run(inference_func,args=(batch,))\nWARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n\n\nWARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n\n\nINFO:tensorflow:Using MirroredStrategy with devices (&#039;/job:localhost/replica:0/task:0/device:CPU:0&#039;,)\n\n\nINFO:tensorflow:Using MirroredStrategy with devices (&#039;/job:localhost/replica:0/task:0/device:CPU:0&#039;,)\n\n\nWARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `experimental_run_v2` inside a tf.function to get the best performance.\n\n\nWARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `experimental_run_v2` inside a tf.function to get the best performance.\n\n\nWARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `experimental_run_v2` inside a tf.function to get the best performance.\n\n\nWARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `experimental_run_v2` inside a tf.function to get the best performance.\n\n\nWARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `experimental_run_v2` inside a tf.function to get the best performance.\n\n\nWARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `experimental_run_v2` inside a tf.function to get the best performance.\n\n\nWARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `experimental_run_v2` inside a tf.function to get the best performance.\n\n\nWARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `experimental_run_v2` inside a tf.function to get the best performance.\n\n\nWARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `experimental_run_v2` inside a tf.function to get the best performance.\n\n\nWARNING:tensorflow:Using MirroredStrategy eagerly has significant overhead currently. We will be working on improving this in the future, but for now please wrap `call_for_each_replica` or `experimental_run` or `experimental_run_v2` inside a tf.function to get the best performance.\n\n保存检查点\n检查站捕获所有参数（的精确值tf.Variable由模型中使用的对象）。检查点不包含由模型所定义的计算的任何描述，因此通常仅当将使用保存的参数值源代码可用有用。\n在另一方面中SavedModel格式包括由除了参数值（检查点）模型中定义的计算的序列化描述。这种格式的模型是独立于创建模型的源代码。因此，它们适用于通过TensorFlow部署服务，TensorFlow精简版，TensorFlow.js，或在其他编程语言（的C，C ++，JAVA，围棋，防锈，C＃等TensorFlow API）的程序。\n本指南涵盖API进行写入和读出检查站。\n建立\nclass Net(tf.keras.Model):\n    # 一个简单的线性模型\n    \n    def __init__(self):\n        super(Net, self).__init__()\n        self.l1 = tf.keras.layers.Dense(5)\n    \n    def call(self, x):\n        return self.l1(x)\nnet = Net()\nnet.save_weights(&#039;./tmp/easy_checkpoint&#039;)\n写检查站\n一个TensorFlow模型的持久状态被存储在tf.Variable对象。这些可以直接构造，但通常通过高级API等生成tf.keras.layers或tf.keras.Model 。\n管理变量最简单的方法是将其安装到Python对象，然后引用这些对象。\n的子类tf.train.Checkpoint ， tf.keras.layers.Layer和tf.keras.Model自动跟踪分配给它们的属性变量。下面的例子构造了一个简单的线性模型，然后写入其中包含所有模型的变量值的检查站。\n您可以轻松地保存模型检查点与Model.save_weights\n手动检查点\ndef toy_dataset():\n    inputs = tf.range(10.)[:, None]\n    labels = inputs * 5. + tf.range(5.)[None, :]\n    return tf.data.Dataset.from_tensor_slices(dict(x=inputs, y=labels)).repeat().batch(2)\ndef train_step(net, example, optimizer):\n    with tf.GradientTape() as tape:\n        output = net(example[&#039;x&#039;])\n        loss = tf.reduce_mean(tf.abs(output - example[&#039;y&#039;]))\n    variables = net.trainable_variables\n    gradients = tape.gradient(loss, variables)\n    optimizer.apply_gradients(zip(gradients, variables))\n    return loss\n创建检查点的对象\n手动进行检查点，您将需要一个tf.train.Checkpoint对象。凡检查点你想要的对象被设置为对象的属性。\n一个tf.train.CheckpointManager也可用于管理多个检查点有帮助。\nopt = tf.keras.optimizers.Adam(0.1)\ndataset = toy_dataset()\niterator = iter(dataset)\nckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer=opt, net=net, iterator=iterator)\nmanager = tf.train.CheckpointManager(ckpt, &#039;./tmp/tf_ckpts&#039;, max_to_keep=3)\n训练和保存检查点模型\n下面的训练循环创建模型的实例和优化的，然后收集他们入tf.train.Checkpoint对象。它在循环中调用数据的每批训练步骤，并定期检查点写入到磁盘。\ndef train_and_checkpoint(net, manager):\n    ckpt.restore(manager.latest_checkpoint)\n    if manager.latest_checkpoint:\n        print(&quot;恢复点：{}&quot;.format(manager.latest_checkpoint))\n    else:\n        print(&quot;开始初始化&quot;)\n        \n    for _ in range(50):\n        example = next(iterator)\n        loss = train_step(net, example, opt)\n        ckpt.step.assign_add(1)\n        if int(ckpt.step) % 10 == 0:\n          save_path = manager.save()\n          print(&quot;保存检查点 {}: {}&quot;.format(int(ckpt.step), save_path))\n          print(&quot;loss {:1.2f}&quot;.format(loss.numpy()))\ntrain_and_checkpoint(net, manager)\n开始初始化\n保存检查点 10: ./tmp/tf_ckpts\\ckpt-1\nloss 29.78\n保存检查点 20: ./tmp/tf_ckpts\\ckpt-2\nloss 23.19\n保存检查点 30: ./tmp/tf_ckpts\\ckpt-3\nloss 16.63\n保存检查点 40: ./tmp/tf_ckpts\\ckpt-4\nloss 10.17\n保存检查点 50: ./tmp/tf_ckpts\\ckpt-5\nloss 4.09\n\n恢复和继续训练\nopt = tf.keras.optimizers.Adam(0.1)\ndataset = toy_dataset()\niterator = iter(dataset)\nckpt = tf.train.Checkpoint(step=tf.Variable(1), optimizer=opt, net=net, iterator=iterator)\nmanager = tf.train.CheckpointManager(ckpt, &#039;./tmp/tf_ckpts&#039;, max_to_keep=3)\ntrain_and_checkpoint(net, manager)\n恢复点：./tmp/tf_ckpts\\ckpt-10\n保存检查点 110: ./tmp/tf_ckpts\\ckpt-11\nloss 0.27\n保存检查点 120: ./tmp/tf_ckpts\\ckpt-12\nloss 0.20\n保存检查点 130: ./tmp/tf_ckpts\\ckpt-13\nloss 0.16\n保存检查点 140: ./tmp/tf_ckpts\\ckpt-14\nloss 0.21\n保存检查点 150: ./tmp/tf_ckpts\\ckpt-15\nloss 0.20\n\nprint(manager.checkpoints)\n[&#039;./tmp/tf_ckpts\\\\ckpt-13&#039;, &#039;./tmp/tf_ckpts\\\\ckpt-14&#039;, &#039;./tmp/tf_ckpts\\\\ckpt-15&#039;]\n\n这些路径，如’./tf_ckpts/ckpt-10’ ，不是磁盘上的文件。相反，它们是一个前缀index文件和包含可变值的一个或多个数据文件。这些前缀在单个组合在一起checkpoint文件（ ‘./tf_ckpts/checkpoint’ ），其中CheckpointManager保存其状态。\n手动检查\ntf.train.list_variables列出了检查点键和变量的形状在一个检查点。检查点键是显示在以上图上的路径。\ntf.train.list_variables(tf.train.latest_checkpoint(&#039;./tmp/tf_ckpts&#039;))\n[(&#039;_CHECKPOINTABLE_OBJECT_GRAPH&#039;, []),\n (&#039;iterator/.ATTRIBUTES/ITERATOR_STATE&#039;, [1]),\n (&#039;net/l1/bias/.ATTRIBUTES/VARIABLE_VALUE&#039;, [5]),\n (&#039;net/l1/bias/.OPTIMIZER_SLOT/optimizer/m/.ATTRIBUTES/VARIABLE_VALUE&#039;, [5]),\n (&#039;net/l1/bias/.OPTIMIZER_SLOT/optimizer/v/.ATTRIBUTES/VARIABLE_VALUE&#039;, [5]),\n (&#039;net/l1/kernel/.ATTRIBUTES/VARIABLE_VALUE&#039;, [1, 5]),\n (&#039;net/l1/kernel/.OPTIMIZER_SLOT/optimizer/m/.ATTRIBUTES/VARIABLE_VALUE&#039;,\n  [1, 5]),\n (&#039;net/l1/kernel/.OPTIMIZER_SLOT/optimizer/v/.ATTRIBUTES/VARIABLE_VALUE&#039;,\n  [1, 5]),\n (&#039;optimizer/beta_1/.ATTRIBUTES/VARIABLE_VALUE&#039;, []),\n (&#039;optimizer/beta_2/.ATTRIBUTES/VARIABLE_VALUE&#039;, []),\n (&#039;optimizer/decay/.ATTRIBUTES/VARIABLE_VALUE&#039;, []),\n (&#039;optimizer/iter/.ATTRIBUTES/VARIABLE_VALUE&#039;, []),\n (&#039;optimizer/learning_rate/.ATTRIBUTES/VARIABLE_VALUE&#039;, []),\n (&#039;save_counter/.ATTRIBUTES/VARIABLE_VALUE&#039;, []),\n (&#039;step/.ATTRIBUTES/VARIABLE_VALUE&#039;, [])]\n\n保存与估计基于对象的检查站\n通过默认保存变量名，而不是在前面的章节中描述的对象图检查点估计。 tf.train.Checkpoint将接受基于域名的检查点，但移动估计的模型以外的部位时，变量名可以更改model_fn 。保存基于对象的检查站，使得它更容易培养的估算内部模型，然后外面用它之一。\nimport tensorflow.compat.v1 as tf_compat\ndef model_fn(features, labels, mode):\n    net = Net()\n    opt = tf.keras.optimizers.Adam(0.1)\n    ckpt = tf.train.Checkpoint(step=tf_compat.train.get_global_step(),\n                              optimizers=opt, net=net)\n    with tf.GradientTape()as tape:\n        output = net(features[&#039;x&#039;])\n        loss = tf.reduce_mean(tf.abs(output - features[&#039;y&#039;]))\n    variables = net.trainable_variables\n    gradients = tape.gradient(loss, variables)\n    return tf.estimator.EstimatorSpec(\n        mode,\n        loss=loss,\n        train_op=tf.group(opt.apply_gradients(zip(gradients, variables)),\n                          ckpt.step.assign_add(1)),scaffold=tf_compat.train.Scaffold(saver=ckpt))\ntf.keras.backend.clear_session()\nest = tf.estimator.Estimator(model_fn, &#039;./tmp/tf_estimator_example/&#039;)\nest.train(toy_dataset, steps=10)\nINFO:tensorflow:Using default config.\n\n\nINFO:tensorflow:Using default config.\n\n\nINFO:tensorflow:Using config: {&#039;_model_dir&#039;: &#039;./tmp/tf_estimator_example/&#039;, &#039;_tf_random_seed&#039;: None, &#039;_save_summary_steps&#039;: 100, &#039;_save_checkpoints_steps&#039;: None, &#039;_save_checkpoints_secs&#039;: 600, &#039;_session_config&#039;: allow_soft_placement: true\ngraph_options {\n  rewrite_options {\n    meta_optimizer_iterations: ONE\n  }\n}\n, &#039;_keep_checkpoint_max&#039;: 5, &#039;_keep_checkpoint_every_n_hours&#039;: 10000, &#039;_log_step_count_steps&#039;: 100, &#039;_train_distribute&#039;: None, &#039;_device_fn&#039;: None, &#039;_protocol&#039;: None, &#039;_eval_distribute&#039;: None, &#039;_experimental_distribute&#039;: None, &#039;_experimental_max_worker_delay_secs&#039;: None, &#039;_session_creation_timeout_secs&#039;: 7200, &#039;_service&#039;: None, &#039;_cluster_spec&#039;: ClusterSpec({}), &#039;_task_type&#039;: &#039;worker&#039;, &#039;_task_id&#039;: 0, &#039;_global_id_in_cluster&#039;: 0, &#039;_master&#039;: &#039;&#039;, &#039;_evaluation_master&#039;: &#039;&#039;, &#039;_is_chief&#039;: True, &#039;_num_ps_replicas&#039;: 0, &#039;_num_worker_replicas&#039;: 1}\n\n\nINFO:tensorflow:Using config: {&#039;_model_dir&#039;: &#039;./tmp/tf_estimator_example/&#039;, &#039;_tf_random_seed&#039;: None, &#039;_save_summary_steps&#039;: 100, &#039;_save_checkpoints_steps&#039;: None, &#039;_save_checkpoints_secs&#039;: 600, &#039;_session_config&#039;: allow_soft_placement: true\ngraph_options {\n  rewrite_options {\n    meta_optimizer_iterations: ONE\n  }\n}\n, &#039;_keep_checkpoint_max&#039;: 5, &#039;_keep_checkpoint_every_n_hours&#039;: 10000, &#039;_log_step_count_steps&#039;: 100, &#039;_train_distribute&#039;: None, &#039;_device_fn&#039;: None, &#039;_protocol&#039;: None, &#039;_eval_distribute&#039;: None, &#039;_experimental_distribute&#039;: None, &#039;_experimental_max_worker_delay_secs&#039;: None, &#039;_session_creation_timeout_secs&#039;: 7200, &#039;_service&#039;: None, &#039;_cluster_spec&#039;: ClusterSpec({}), &#039;_task_type&#039;: &#039;worker&#039;, &#039;_task_id&#039;: 0, &#039;_global_id_in_cluster&#039;: 0, &#039;_master&#039;: &#039;&#039;, &#039;_evaluation_master&#039;: &#039;&#039;, &#039;_is_chief&#039;: True, &#039;_num_ps_replicas&#039;: 0, &#039;_num_worker_replicas&#039;: 1}\n\n\nWARNING:tensorflow:From D:\\Anaconda3\\envs\\py36_tf2\\lib\\site-packages\\tensorflow\\python\\training\\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n\n\nWARNING:tensorflow:From D:\\Anaconda3\\envs\\py36_tf2\\lib\\site-packages\\tensorflow\\python\\training\\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n\n\nINFO:tensorflow:Calling model_fn.\n\n\nINFO:tensorflow:Calling model_fn.\n\n\nINFO:tensorflow:Done calling model_fn.\n\n\nINFO:tensorflow:Done calling model_fn.\n\n\nINFO:tensorflow:Create CheckpointSaverHook.\n\n\nINFO:tensorflow:Create CheckpointSaverHook.\n\n\nINFO:tensorflow:Graph was finalized.\n\n\nINFO:tensorflow:Graph was finalized.\n\n\nINFO:tensorflow:Running local_init_op.\n\n\nINFO:tensorflow:Running local_init_op.\n\n\nINFO:tensorflow:Done running local_init_op.\n\n\nINFO:tensorflow:Done running local_init_op.\n\n\nINFO:tensorflow:Calling checkpoint listeners before saving checkpoint 0...\n\n\nINFO:tensorflow:Calling checkpoint listeners before saving checkpoint 0...\n\n\nINFO:tensorflow:Saving checkpoints for 0 into ./tmp/tf_estimator_example/model.ckpt.\n\n\nINFO:tensorflow:Saving checkpoints for 0 into ./tmp/tf_estimator_example/model.ckpt.\n\n\nINFO:tensorflow:Calling checkpoint listeners after saving checkpoint 0...\n\n\nINFO:tensorflow:Calling checkpoint listeners after saving checkpoint 0...\n\n\nINFO:tensorflow:loss = 4.505075, step = 1\n\n\nINFO:tensorflow:loss = 4.505075, step = 1\n\n\nINFO:tensorflow:Calling checkpoint listeners before saving checkpoint 10...\n\n\nINFO:tensorflow:Calling checkpoint listeners before saving checkpoint 10...\n\n\nINFO:tensorflow:Saving checkpoints for 10 into ./tmp/tf_estimator_example/model.ckpt.\n\n\nINFO:tensorflow:Saving checkpoints for 10 into ./tmp/tf_estimator_example/model.ckpt.\n\n\nINFO:tensorflow:Calling checkpoint listeners after saving checkpoint 10...\n\n\nINFO:tensorflow:Calling checkpoint listeners after saving checkpoint 10...\n\n\nINFO:tensorflow:Loss for final step: 36.96539.\n\n\nINFO:tensorflow:Loss for final step: 36.96539.\n\n\n&lt;tensorflow_estimator.python.estimator.estimator.EstimatorV2 at 0x2b4ccb5f588&gt;\n\ntf.train.latest_checkpoint(&#039;./tmp/tf_estimator_example&#039;)\n&#039;./tmp/tf_estimator_example\\\\model.ckpt-10&#039;\n\ntf.train.Checkpoint则可以从其加载估计的检查站model_dir 。\nopt = tf.keras.optimizers.Adam(0.1)\nnet = Net()\nckpt = tf.train.Checkpoint(step=tf.Variable(1, dtype=tf.int64), optimizer=opt, net=net)\nckpt.restore(tf.train.latest_checkpoint(&#039;./tmp/tf_estimator_example/&#039;))\nckpt.step.numpy()\n10\n"},"WEB3/1.-什么是以太坊？":{"slug":"WEB3/1.-什么是以太坊？","filePath":"WEB3/1. 什么是以太坊？.md","title":"1. 什么是以太坊？","links":[],"tags":["WEB3"],"content":"什么是以太坊？\n比特币等加密货币赋予了全球范围内任何人自由转账的能力。以太坊同样具备这一特性，不仅如此，它还能运行代码，让人们得以创建应用程序和组织。以太坊兼具弹性与灵活性：任何计算机程序均可在其上运行。\nEthereum 是一个去中心化的区块链网络与软件开发平台，由加密货币以太币（ETH）提供动力支持。\n以太坊是 DeFi、NFT、游戏、去中心化社交媒体以及稳定币等数千种加密货币和应用程序的汇聚之地。\n以太坊是一个开放的公共区块链，由软件开发人员 Vitalik Buterin 与一小队联合创始人在 2015 年 7 月共同推出。\n以太坊背后的理念颇为简洁。比特币实现了数字现金的发送与接收功能，而以太坊则在此基础上进一步拓展，引入了名为 “智能合约” 的开源程序。\n智能合约 赋予了任何人创建自身数字资产与去中心化应用程序（dapp）的能力，且这些应用可在全球范围内全天候运行。与银行、公司或其他机构不同，只要拥有互联网连接，任何人都能使用智能合约。\n自 2015 年起，以太坊已逐步发展成为一个繁荣的数字资产生态系统，其中涵盖了 稳定币、非同质化代币（NFT）以及治理代币，同时还孕育了用于去中心化金融（DeFi）、艺术品与收藏品、游戏以及去中心化社交媒体的庞大 dapp 世界。\n这个生态系统被统称为 “web3”，代表着 以所有权为核心的互联网 第三阶段。\n如今，全球数以百万计的人使用以太坊，他们持有价值数十亿美元的资产，每年发送和接收的金额高达数万亿美元——而这一切都无需依赖银行。\n所有这一切的核心，是以太坊的原生加密货币 以太币（ETH），这是一种驱动整个网络的新型数字货币。\n什么是以太坊网络？\n您可以将以太坊网络视作 一个全球性的数字基础设施，人人皆可使用，却无人能够滥用。\n该网络由遍布全球 的数千台独立计算机（即节点）构成。这些节点由普通人运营，它们相互协作，为全球各地的任何人提供金融服务和数字应用程序。\n与机构拥有的传统网络相比，以太坊网络具备 三大关键优势：抗审查性、增强的安全性以及更高的可靠性。\nCensorship resistant  抗审查\n传统应用程序和金融服务往往依赖于银行或公司，这些机构有权决定阻止访问或冻结账户。然而，以太坊上的 dapp 却具有抗审查性。\n这是因为以太坊的节点网络会毫无差别地记录每一笔交易，并且这一规则已深深嵌入代码之中。\nHighly secure  高度安全\n当下，许多应用程序都托管在 AWS 等云服务提供商处，容易遭受攻击和下架。但以太坊上的 dapp 却由网络自身提供安全保障。每个节点都存储并同步以太坊的完整状态，包括所有合约。\n倘若有人试图更改合约，网络会予以拒绝，因为这与它们的记录不符。要关闭一个应用程序，攻击者需掌控整个网络，这不仅需要耗费数十亿美元，而且协调难度极大。\nDurable and reliable  耐用可靠\n云托管平台的宕机可能导致应用程序下线，但以太坊的设计确保了近乎完美的正常运行时间。即便部分节点因软件漏洞、政府打击、自然灾害或战争等原因下线，网络仍能持续运行。\n每天，数百万用户在以太坊上使用数千个 dapp。尽管高需求可能会致使交易费用上涨，但这也恰恰体现了以太坊网络的强大实力，它始终将安全性、去中心化以及随时可用的保障置于首位。\n以太坊扩展（第 2 层）\n为提升以太坊的容量，不同团队创建了在以太坊上运行的二层（L2）网络。L2 犹如快速通道，使交易速度更快、成本更低——有时平均成本甚至不到一美分。\n一些备受欢迎的 L2，如 Optimism、Arbitrum、ZKSync 和 Base，如今每年处理的交易数量多达数百万笔，交易价值高达数十亿美元。\n什么是以太币（ETH）\n以太币（ETH）是以太坊的原生加密货币。\n这是一种新型的 数字货币，只需几美分，便能在几秒内将 ETH 发送给世界任何地方的任何人。但 ETH 的用途远不止于支付。它在维护以太坊网络运行方面发挥着至关重要的作用。\n当你使用以太坊进行转账、收藏艺术品或开发新的 dapp 时，你需要支付一小笔 ETH 交易费（或称 Gas 费）。这笔费用有助于防止垃圾邮件，并奖励那些处理交易的验证者。\n这些 验证者通过名为 “质押” 的系统来保障以太坊网络的安全。他们通过锁定自己的 ETH，获得处理交易的权利。作为回报，他们会获得 ETH 奖励。这使得以太坊拥有了一个由用户驱动、而非公司驱动的自给自足的经济体系。\n与许多传统货币不同，ETH 的稀缺性会随时间推移而加剧。每当有人使用以太坊时，一小部分 ETH 就会被销毁，从而永久性地从供应中移除。在交易繁忙的日子里，销毁的 ETH 数量会超过生成的 ETH 数量，这使得 ETH 处于通缩状态，其价值也会随时间推移而上升。以太坊的使用量越大，销毁的 ETH 数量也就越多。\n正因如此，许多人将 ETH 视为一种投资，并选择持有、质押或借出它来增加自己的储蓄。\n以太坊如何运作？\n以太坊 于 2015 年推出 时，采用了一种名为工作量证明的系统。\n这种机制由比特币首创，所有计算机都通过该机制来协商谁拥有什么。计算机会耗费大量能源来尝试解题。2022 年，以太坊升级到名为 “权益证明” 的新系统 ，能源效率提高了 99%。验证者无需再进行复杂的数学运算，而是将 ETH 锁定为保证金，以获得处理交易的权利。获胜者将有权提议一个包含即将到来交易的区块，并获得新的 ETH。\n如果他们操作正确，就能获得 ETH 奖励；如果他们作弊，就会损失部分权益。\n当你在以太坊上向朋友发送 10 美元的稳定币时：\n\n打开钱包，添加账户地址和金额，然后点击发送。\n您的钱包签署付款并将其广播至网络。\n付款在公共队列（mempool）中等待，直至被区块提议者选中。\n区块提议者将其添加到下一个交易区块中，广播该区块，并赚取费用。\n稳定币合约将 10 美元从您转移至您的朋友，两个钱包均进行更新。\n全球验证者网络仔细检查并证明更改的有效性。\n\n当你在以太坊上铸造价值 5 美元的收藏品时：\n\n您将钱包连接到 dapp 并选择要铸造的物品。\n您确认购买；钱包签署并广播该交易。\n铸币请求加入内存池并由验证器添加到区块中。\nNFT 智能合约将您的钱包记录为新主人。\n几秒钟后，您的新收藏品就会出现在您的钱包中。\n\n这一切都得益于智能合约的强大力量；这些开源程序存在于以太坊上，全天候运行，任何人、在任何地方都能访问。\n每笔交易、更新和操作都会在数千个独立节点之间同步。这赋予了以太坊可靠性、透明度和抗审查性。\n以太坊的用途是什么？\n人们借助以太坊，能够完成许多以往难以实现的事情。\n肯尼亚的农民无需向银行申请，即可获得农作物的自动化保险。像 Visa 这样的企业能够从一开始就推出全球通用的新型支付系统。像 联合国 这样的全球组织可以向难民提供援助，从而节省数百万美元的银行手续费。\n这些 dapps 和资产使用开源代码在以太坊上运行，无法受到限制、审查或关闭。\n以下是目前不同群体的使用方式：\nConsumers  消费者\n数百万用户每天都在使用以太坊上的 DApp 来转移资金、交易和持有数字资产。与传统应用程序不同，DApp 无需注册姓名、等待银行审批或提供个人数据。\n只需一个钱包和互联网连接，您就可以：\n\n无需银行账户或信用记录即可获得金融服务\n拥有不可复制或没收的数字收藏品、艺术品和资产\n使用您的钱包而非电子邮件登录 dapps — 无需密码，无需个人信息\n参与全球社区，您可以无边界地投票、贡献和赚钱\n\nBusinesses &amp; developers  企业和开发商\n\n从第一天起就推出内置全球支付系统的 dapp\n部署防篡改合约，自动执行协议\n创建任何人都可以构建并推动价值的金融产品\n例如，PayPal 在以太坊上推出了自己的稳定币 PYUSD。这表明，即便是全球最大的支付公司，也看到了以太坊开放性和可编程性的优势。\n\nGovernments  政府\n各国政府也开始探索以太坊能够实现的功能。\n\n完全透明地 将公共资金和福利直接分配 给公民\n颁发可验证、可跨境转移的数字身份证 或记录\n为投票、土地所有权和登记建立防篡改的公共基础设施\n\n在另一个案例中，乌克兰数字化转型部使用以太坊分发战时援助。资金通过开放的智能合约直接发送给公民和非政府组织，在危机期间提供了透明度、速度和问责制。\n如何开始使用以太坊\n开始使用以太坊比你想象的要容易。您无需获得许可，无需银行账户，甚至无需身份证件。您只需要一台设备和网络连接即可开启使用之旅。\n对于个人\nZerion、Rainbow 和 Coinbase Wallet 等热门钱包均免费且易于使用。设置好钱包后，您可以：\n\n在交易所或直接在钱包中购买少量 ETH\n使用 ETH 支付交易费用，例如发送代币或收集 NFT\n探索 Zora、Uniswap 或 Farcaster 等 dapp——无需新的登录或批准\n\n随着越来越多的人每天依赖网络，这些优先事项将有助于确保以太坊的安全性、可扩展性和用户友好性。\n这些 dapp 可在浏览器中运行，并立即与你的钱包交互。几分钟内即可开启以太坊使用之旅。\nFor developers  对于开发人员\n以太坊是开发者的乐园。无需许可、审批，甚至无需真金白银，即可开始构建。\n以太坊开发者文档 将指导您完成从编写第一个智能合约到在 Sepolia 等测试网络上部署的所有内容。\n您可以使用 Hardhat、Foundry 和 Ethers.js 等工具构建全栈 dapp，或者尝试使用 thirdweb 或 Moralis 等低代码平台。\n一切都是开源且可组合的，因此您可以重新混合和构建现有内容，无需征求许可。\nUse Ethereum in business  在商业中使用以太坊\n企业已经在使用以太坊来支持新的基础设施。\n许多企业开始使用像 Optimism 和 Base 这样的 L2 网络来支持高容量用例。这些网络提供更低的费用和更快的速度，同时仍然受益于以太坊的安全性并消除了交易对手风险。\n推出模块化忠诚度计划，以提高客户保留率并降低第三方成本\n将票证、优惠券或证书等资产代币化，以降低欺诈和转售风险\n实现即时全球支付，降低交易费用并开拓新市场\n例如，2025 年，Shopify 在 Base 上推出，允许消费者向全球数百万商家消费稳定币。\n以太坊和比特币有什么区别？\n比特币和以太坊是世界上最大的两种加密货币。\n它们都允许你无需银行即可汇款，都采用区块链技术，并且都 对任何人开放。但相似之处也仅限于此。\n比特币就像数字黄金。\n比特币的供应量固定为 2100 万枚，专注于点对点支付，并且其脚本语言基础，限制了其构建功能。这种简洁性是设计使然，因为比特币优先考虑可预测性、持久性和长期安全性，而非灵活性。\n以太坊采取了更广泛的方法。\n它不仅仅是货币，更是可编程的基础设施。以太坊不仅支持价值的发送和接收，还允许开发者构建完整的应用程序。您已经看到了它的实际应用：从借贷市场和稳定币，到收藏品、社交媒体和实时支付——所有这些都由智能合约驱动，并由 ETH 保障安全。\n网络达成共识的方式也不同。\n比特币依靠矿工来保障网络安全。这些强大的计算机相互竞争，破解复杂的难题，最终获胜者可以将下一个交易区块添加到区块链中，并获得比特币作为奖励。这个过程被称为挖矿，会消耗大量的电力。\n以太坊过去也采用这种机制。但在 2022 年，它从工作量证明（PoW）过渡到权益证明（PoS）。如今，交易由锁定 ETH 作为抵押品的验证者确认。诚实的验证者获得 ETH 奖励，而不诚实的验证者则会损失部分权益。这一转变使以太坊的能源效率提高了 99.988% 以上，同时又不牺牲安全性和去中心化程度。\n供应的处理方式也有所不同。\n比特币的供应量是固定的，永远只有 2100 万枚。而以太坊的供应量是动态的。它会发行新的 ETH 来奖励验证者，同时每笔交易都会销毁一部分 ETH。这意味着 以太坊不可能 “无限量地印制 ETH”。\n发行率受质押 ETH 数量限制。随着质押 ETH 数量的增加，个人奖励会减少，从而形成自然平衡。这种设计确保了未来安全预算的可持续性，而无需仅仅依赖交易费用。\n简而言之，比特币是一种传递价值的工具，而以太坊是一个构建价值的平台。\n以太坊何时推出？谁创立了它？现在谁在运营它？\n从一开始，以太坊就被设计为由社区来运行。\n2013 年，Vitalik Buterin 发布了一份白皮书，提出了一种新型区块链，用于货币和任何人都可以使用的应用程序。这个想法很快就得到了广泛关注。到 2014 年，Gavin Wood 和 Joseph Lubin 等联合创始人加入了这一努力，团队通过最早的加密众筹活动之一筹集资金。以太坊于 2015 年 7 月正式推出。\n以太坊历史上的关键时刻\n\n2013 年：19 岁的 Vitalik Buterin 发布以太坊白皮书\n2014 年：以太坊基金会成立并发起众筹活动\n2015 年：开发人员通过 Frontier 版本启动以太坊网络\n2016 年：智能合约漏洞导致 The DAO 损失 6000 万美元（360 万 ETH），引发区块链分叉\n2020 年：信标链启动，开启权益证明（PoS）转型\n2021 年：伦敦 升级通过 EIP-1559 开始消耗 Gas 费\n2022 年：Merge 将用质押取代挖矿，从而减少 99% 的能源消耗\n2025 年：Pectra 升级改进了智能钱包支持和 L2 兼容性\n\n如今，没有任何一个人或一家公司运营以太坊。\n该网络由广泛的贡献者维护：\n\n编写并提出升级建议的开发人员\n为分布式物理基础设施做出贡献的节点运营商\n验证交易的权益持有者\n构建工具和文化的社区成员\n你 通过网络使用\n\n没有首席执行官、董事会或中央权威机构。 以太坊基金会仍然资助研发，但生态系统依靠开放参与运行。\n变更是通过以太坊改进提案（EIP）提出的，经过公开讨论，并且只有 在更广泛的社区支持的情况下 才会被采纳。这使得以太坊的变化速度比初创公司慢，而且关闭或接管也更加困难。\n2025 年以太坊路线图是什么？\n以太坊并不遵循固定的路线图，而是遵循共同的愿景。网络升级以 EIP 的形式进行，并由世界各地的贡献者公开开发。没有中央团队来决定升级内容，只有人们根据用户需求构建他们认为有用的功能。Pectra 是 2025 年 5 月推出的最新升级版本。此次升级改进了钱包功能，为质押者提供了更大的灵活性，并使 dapp 更容易在 Layer2 上运行。目标是在不影响安全性和去中心化性的前提下提高易用性。\n展望未来，以太坊的优先事项包括：\n\n让核心协议及其 L2 更快、更便宜\n改善用户和开发者的体验\n\n随着越来越多的人每天依赖网络，这些优先事项将有助于确保以太坊的安全性、可扩展性和用户友好性。\n如果你想引领以太坊的发展方向，那就参与进来吧。你不需要任何许可，只需要有为这个新的数字经济做出贡献的愿望。"},"WEB3/10.-什么是以太坊的隐私？":{"slug":"WEB3/10.-什么是以太坊的隐私？","filePath":"WEB3/10. 什么是以太坊的隐私？.md","title":"10. 什么是以太坊的隐私？","links":[],"tags":["WEB3"],"content":"隐私：以太坊生态的基石与挑战\n隐私不仅关乎个人安全，更是自由的基石以及去中心化的关键保障。它赋予人们自由表达、交易和组织社群的强大能力。然而，和所有区块链一样，以太坊的公共账本设计给隐私保护带来了诸多挑战。\n以太坊的透明性设计\n以太坊的设计本质上是高度透明的，链上的所有操作对任何人而言都清晰可见。尽管以太坊通过将活动与公钥相关联，而非直接关联真实身份，提供了一定程度的假名保护，但活动模式仍可能被深入分析，进而泄露敏感信息并识别出用户身份。\n在以太坊中构建隐私保护工具，对于个人、组织和机构的安全交互至关重要，同时能有效限制不必要的信息暴露。这不仅显著增强了生态系统的安全性，还使其能够适用于更为广泛的用例场景。\n写入隐私\n默认情况下，以太坊上的每一笔交易都是公开且永久留存的，无论是发送 ETH、注册 ENS 域名、收集 POAP 还是交易 NFT。日常操作如支付、投票或身份验证等，都可能不经意间将信息泄露给非预期的第三方。以下是一些能够提高操作私密性的工具和技术：\n混合协议（混合器）\n混合器通过将多个用户的交易汇聚到一个共享“池”中，随后允许用户将资金提现到新地址，从而巧妙地切断了发送者和接收者之间的直接联系。由于存款和提现操作相互混杂，观察者很难将它们准确关联起来。\n示例：PrivacyPools、Tornado Cash\n屏蔽池\n屏蔽池与混合器原理类似，但功能更为强大，它允许用户在池内私密地持有和转移资金。它不仅隐藏了充值和提现之间的关联，还借助零知识证明等先进技术持续维护隐私状态。这使得构建隐私转账、隐私余额等高级功能成为可能。\n示例：Railgun、Aztec、Nightfall\n隐身地址\n隐身地址为每个发送者提供唯一且一次性的地址，只有接收者能够打开。每次交易都发送到全新的地址，因此无人能够追踪所有付款的最终归属。这极大地保护了付款历史记录的私密性。\n示例：UmbraCash、FluidKey\n其他用例\n其他探索私人写入的项目包括 PlasmaFold（专注于私人支付）以及 MACI 和 Semaphore（致力于私人投票）等系统。\n这些工具极大地扩展了在以太坊上进行私密写入的选项，但每种工具都各有优缺点。有些方法仍处于实验阶段，有些会增加成本或复杂性，而有些工具（如混合器）可能因使用方式面临法律或监管审查。\n读取隐私\n读取以太坊上的信息（如钱包余额）通常需要借助钱包提供商、节点提供商或区块浏览器等服务。由于依赖这些服务来读取区块链数据，它们也能够看到您的请求以及相关元数据（如 IP 地址或位置信息）。若持续查看同一账户，这些信息可能被拼凑起来，进而将身份与活动关联起来。\n运行自己的以太坊节点可以防止这种情况发生，但对于大多数用户（尤其是移动设备用户）来说，存储和同步完整区块链的成本高昂且不切实际。\n一些探索私人读取的项目包括：\n\n私人信息检索（PIR）：能够在获取数据的同时不透露具体查找内容。\nzkID：利用零知识证明进行私人身份检查。\nvOPRF：在 Web3 环境中以假名方式使用 Web2 帐户。\nvFHE：基于加密数据进行计算。\nMachinaIO：在隐藏程序详细信息的同时保持其功能完整性。\n\n隐私证明\n隐私保护证明是以太坊上极具价值的工具，它能够在不泄露不必要细节的情况下证明某事的真实性。例如：\n\n无需透露完整出生日期，即可证明您已年满 18 岁。\n无需展示整个钱包，就能证明对 NFT 或代币的所有权。\n证明会员资格、奖励或投票资格，而无需暴露其他个人数据。\n\n大多数此类工具依赖于零知识证明等先进的加密技术，但面临的挑战在于如何使它们足够高效以在日常设备上流畅运行、可移植到任何平台并且确保安全性。\n一些探索隐私证明的项目包括：\n\n客户端证明（ZK 证明系统）：Client Side Proving\n网络上任何数据的真实性证明：TLSNotary\n移动客户端证明：Mopro\n私人证明委托（避免信任假设的委托框架）：Private Proof Delegation\n用于私人和可验证计算的语言：Noir\n\n隐私术语表\n匿名：与数据中永久删除的所有标识符进行交互，使得信息无法追溯到个人。\n加密：对数据进行加密处理的过程，确保只有拥有正确密钥的人才能读取数据。\n完全同态加密（FHE）：一种能够直接对加密数据执行计算而无需解密的先进方法。了解更多\n不可区分的混淆（iO）：一种使程序或数据难以理解但仍可保持其可用性的隐私技术。了解更多\n多方计算（MPC）：允许多方共同计算结果，同时不暴露各自私人输入的先进方法。了解更多\n可编程加密：一种灵活、规则驱动的加密方式，可在软件中定制，以精确控制数据共享、验证或披露的方式和时间。\n假名：使用唯一代码或数字（如以太坊地址）来代替个人标识符。\n选择性披露：仅共享所需内容的能力（例如，证明您拥有 NFT，而无需透露整个钱包的历史记录）。\n不可链接性：确保区块链上的单独操作无法被绑定到同一个地址。\n可验证性：确保其他人能够确认声明的真实性，例如验证以太坊上的交易或证明。\n可验证委托：将任务（例如生成证明）分配给另一方（例如，使用服务器为移动钱包进行高强度加密），同时仍能够验证该任务是否正确完成。\n零知识证明（ZKP）：一种加密协议，允许某人在不泄露底层数据的情况下证明信息的真实性。\nZK Rollup：一种可扩展系统，可在链下批量处理交易，并在链上提交有效性证明——默认情况下并非隐私保护型，但它们通过降低成本为高效的隐私系统（如屏蔽池）的实现提供了可能。\n资源\n\n以太坊隐私管家（PSE）：以太坊基金会旗下的一个研发实验室，专注于生态系统的隐私研究与发展。\nWeb3PrivacyNow：一个由人员、项目和相关组织组成的网络，致力于保护和促进在线人权。\nWalletBeat：一个以太坊钱包评级网站，旨在提供钱包的综合列表、其功能、实践以及对某些标准的支持情况。\nZk-kit：一组可以在不同项目和零知识协议中重复使用的库（算法、实用函数和数据结构）。\n隐私应用程序：发现在以太坊上运行的精选隐私应用程序列表。\n"},"WEB3/2.-什么是-Web3？":{"slug":"WEB3/2.-什么是-Web3？","filePath":"WEB3/2. 什么是 Web3？.md","title":"2. 什么是 Web3？","links":[],"tags":["WEB3"],"content":"什么是 Web3？\nWeb3 简介\n中心化网络已助力数十亿人接入互联网，并构建起稳定、可靠的基础设施。然而，与此同时，少数中心化巨头几乎垄断了互联网，甚至能够为所欲为。\nWeb3 则是摆脱这一困境的解决方案。与传统互联网由科技巨头垄断不同，Web3 采用去中心化模式，由所有用户共同构建、运营和拥有，将权力赋予个人而非公司。在深入探讨 Web3 之前，我们先来了解一下互联网是如何发展到当前这一阶段的。\n早期的网络\n大多数人认为互联网是现代生活不可或缺的支柱，仿佛自诞生起就一直存在。但实际上，我们如今所熟知的互联网与最初设想有很大差异。为便于理解，可将互联网短暂的历史划分为两个时期——Web 1.0 和 Web 2.0。\nWeb 1.0：只读时代（1990 - 2004）\n1989 年，在日内瓦的欧洲核子研究中心，Tim Berners - Lee 致力于开发将成为万维网的协议。他的构想是什么？创建一种开放、去中心化的协议，实现地球上任何角落的信息共享。\nBerners - Lee 创造的第一个万维网雏形，即如今所称的“Web 1.0”，大约形成于 1990 年至 2004 年间。Web 1.0 主要是由公司拥有的静态网站，用户之间几乎零互动，个人极少创造内容，因此被称为只读网络。\nWeb 2.0：能读能写时代（2004 年 - 至今）\n随着社交媒体平台的兴起，Web 2.0 时代于 2004 年开启。网络不再局限于只读，而是演变为读写网络。互联网公司除向用户提供内容外，还开始提供平台供用户分享生产的内容，并参与用户间的交互。随着上网人数不断增加，少数互联网巨头开始掌控网络上的海量流量和价值。Web 2.0 还催生了广告驱动的盈利模式。尽管用户可以创作内容，但他们并不拥有内容，也无法通过内容变现获益。\nWeb 3.0：能读 - 能写 - 能拥有时代\n2014 年以太坊推出后不久，以太坊联合创始人 Gavin Wood 就提出了“Web 3.0”的概念。Gavin 针对许多早期加密技术采用者面临的难题，即互联网需要过多的信任，给出了一种解决方案。也就是说，如今人们熟知和使用的大部分网络服务都依赖于对少数私人公司的信任，期望他们能以公众的最佳利益行事。\n什么是 Web3？\nWeb3 已成为一个涵盖广泛的术语，代表着一个全新的、更优的互联网愿景。Web3 的核心是通过区块链、加密货币和非同质化代币，以所有权的形式将权力归还给用户。Twitter 上 2020 年的一篇帖子一针见血地指出：Web1 是只读的，Web2 能读/能写，未来的 Web3 能读/能写/能拥有。\nWeb3 的核心思想\n尽管难以对 Web3 作出严格定义，但它是在几项核心原则的指引下构建的。\n\nWeb3 是去中心化的：大部分互联网并非由中心化实体控制和拥有，而是由构建者和用户分配所有权。\nWeb3 无需许可：每个人都有平等参与 Web3 的权限，无人被排除在外。\nWeb3 具有原生支付功能：它使用加密货币进行线上消费和汇款，而非依赖传统银行或第三方支付机构过时的基础设施。\nWeb3 去信任：它通过激励措施和经济机制运转，而非依赖受信任的第三方。\n\n为什么 Web3 很重要？\n尽管 Web3 的卓越特性相互关联、难以简单归类，但为便于理解，我们还是尝试将其分开阐述。\n所有权\nWeb3 以前所未有的方式赋予你数字资产的所有权。例如，假设你正在玩一个 Web2 游戏。若你购买游戏内物品，它会直接与你的账户绑定。若游戏创建者删除你的账户，你将丢失这些物品。或者若你停止玩游戏，你将失去投资在游戏内物品上的价值。\nWeb3 允许通过非同质化代币（NFT）直接拥有所有权。任何人，甚至是游戏创作者，都无权剥夺你的所有权。而且，若你停止玩这个游戏，你可以在公开市场上出售或交易你的游戏内物品并收回其价值。\n抗审查\n平台和内容创作者之间的权力关系严重失衡。\nOnlyFans 是一个由用户生产内容的成人网站，拥有 100 多万内容创作者，其中许多人将其作为主要收入来源。2021 年 8 月，OnlyFans 宣布了禁止色情内容的计划。这一公告在平台创作者中引发了愤怒，他们感觉在帮助创建平台后却被剥夺了收入。在遭遇强烈反对后，该决定很快被推翻。尽管创作者赢得了这场战斗，但它凸显了 Web 2.0 创作者面临的困境：若离开一个平台，就会失去在平台上积攒的声誉和关注。\n在 Web3 中，你的数据存储在区块链上。当你决定离开一个平台时，你可以带走你的声誉，将其带入另一个更符合你价值观的平台。\nWeb 2.0 要求内容创作者信任平台不会更改规则，而抗审查则是 Web3 平台的原生特性。\n去中心化自治组织（DAO）\n在 Web3 中，除了拥有你的数据外，通过使用类似于公司股票的代币，你还可以作为集体中的一员拥有这个平台。去中心化自治组织让你能够协调平台的分散化所有权，以及对平台的未来做出决策。\n去中心化自治组织在技术上被定义为事先商定的智能合约，可以自动做出关于资源池（代币）的去中心化决策。拥有代币的用户对资源的使用方式进行投票，代码自动执行投票结果。\n然而，许多 Web3 社区都被定义为去中心化自治组织。这些社区都通过代码实现了不同程度的去中心化和自动化。目前，我们仍在探索去中心化自治组织的定义以及它们在未来将如何发展。\n身份\n通常，你需要为使用的每个平台创建一个账户。例如，你可能拥有 Twitter 账户、YouTube 账户和 Reddit 账户。若你想更改显示名称或个人资料图片，必须在每个账户中分别操作。在某些情况下，你可以使用社交媒体账户登录，但这会带来一个常见问题——审查。只需轻轻一点，这些平台就可以封锁你的整个线上生活。更糟糕的是，许多平台要求你提供个人识别信息才能创建账户。\nWeb3 允许你使用以太坊地址和以太坊域名服务（ENS）配置文件控制你的数字身份，从而解决了这些问题。使用以太坊地址可以提供跨平台单点登录，这种登录方式安全、抗审查且匿名。\n原生支付功能\nWeb2 的支付基础设施依赖于银行和第三方支付机构，这使得没有银行账户或碰巧生活在某些“不佳”国家/地区的人被排除在外。Web3 使用诸如以太币之类的代币直接在浏览器中汇款，无需受信任的第三方。\nWeb3 的局限性\n尽管现有形式的 Web3 有诸多优点，但这一生态系统仍需克服许多限制才能蓬勃发展。\n可访问性\n如今，人人都可以使用重要的 Web3 功能，例如使用以太坊登录，且无需任何费用。但是，相对较高的交易成本仍然让许多人望而却步。由于高昂的交易费用，Web3 不太可能在不太富裕的发展中国家得到广泛应用。在以太坊，正在通过路线图和二层网络扩容解决方案解决这些难题。技术已现成可用，但我们需要提高技术在二层网络的采用程度，才能让每个人都能使用 Web3。\n用户体验\n目前，使用 Web3 的技术门槛过高。用户必须了解安全问题，通晓复杂的技术文档并浏览晦涩的用户界面。钱包提供商正在努力解决这个问题，但在 Web3 大规模采用之前还有很长的路要走。\n教育\nWeb3 引入了新的范式，这些范式要求学习不同于 Web2.0 上使用的心理模型。随着 Web1.0 在 90 年代后期兴起，类似的教育活动也随之出现。万维网的支持者使用一系列教育技术来教育公众，从简单的比喻（信息高速公路、浏览器、网上冲浪）到电视播放等不一而足。Web3 并不难，但却与众不同。让 Web2 用户了解这些 Web3 范式的教育计划对其成功至关重要。\n中心化基础设施\nWeb3 生态系统尚显年轻，且正在迅速发展。因此，它目前主要依赖中心化基础设施（GitHub、Twitter、Discord 等）。许多 Web3 公司争先恐后填补这些空白，但构建高质量、可靠的基础设施需要时间。\n去中心化的未来\nWeb3 是一个年轻且不断发展的生态系统。Gavin Wood 在 2014 年创造了这个术语，但其中许多想法直到最近才成为现实。仅在去年，我们就经历了人们对加密货币的兴趣大增、二层网络扩容解决方案的改进、新治理形式的大规模实验以及数字身份的革命。\n我们才刚刚开始用 Web3 创建更美好的互联网，但随着我们持续改进它的基础设施，互联网的未来看起来一片光明。"},"WEB3/3.-什么是以太坊钱包？":{"slug":"WEB3/3.-什么是以太坊钱包？","filePath":"WEB3/3. 什么是以太坊钱包？.md","title":"3. 什么是以太坊钱包？","links":[],"tags":["WEB3"],"content":"什么是以太坊钱包？\n以太坊钱包是一款能够让你对账户进行全面控制的应用程序。就如同你日常使用的物理钱包一样，它涵盖了验证身份以及处理资产所需的所有关键信息。借助钱包，你可以轻松登录各类应用程序、随时查看账户余额、发送交易以及完成身份验证等操作。\n钱包是大多数人管理数字资产与身份的核心工具。\n钱包也是你与以太坊账户进行交互的关键媒介。这意味着，你可以根据自身需求随时更换钱包提供商。而且，许多钱包应用都支持同时管理多个以太坊账户，极大地提升了使用的便捷性。\n需要明确的是，钱包提供商并不会保管你的资金。他们仅仅为你提供一个查看以太坊上资产的窗口，以及管理这些资产的实用工具。\n钱包、账户、密钥和地址\n理解这些关键术语之间的差异至关重要。\n\n以太坊账户是一对密钥的组合。其中一个密钥用于创建你可以自由共享的地址，另一个密钥则需要严格保密，因为它用于签名操作。通过结合使用这两个密钥，你便能够持有资产并开展交易。\n以太坊账户拥有一个独特的地址，就如同收件箱拥有电子邮件地址一样。这个地址用于准确识别你的数字资产。\n钱包是一种能够让你与密钥和账户进行互动的工具。通过钱包，你可以方便地查看账户余额、发送交易等。\n\n绝大多数钱包都具备为你生成以太坊地址的功能，因此你无需在下载钱包之前预先生成一个地址。\n钱包种类\n有多种方式可以与你的账户进行连接和交互：\n\n物理硬件钱包：这是一种能够在线下安全保存加密资产的设备，其安全性极高。\n手机应用程序：让你能够随时随地访问自己的资金，便捷性十足。\n浏览器钱包：是一种可以直接在浏览器中与账户进行交互的网络应用程序。\n浏览器扩展钱包：是一种需要下载安装的扩展程序，通过它，你可以借助浏览器与账户以及各类应用程序进行交互。\n桌面应用程序：如果你更倾向于在 MacOS、Windows 或 Linux 系统上管理资金，那么桌面应用程序将是你的理想选择。\n"},"WEB3/4.-什么是智能合约-and-x3f;":{"slug":"WEB3/4.-什么是智能合约-and-x3f;","filePath":"WEB3/4. 什么是智能合约&#x3f;.md","title":"4. 什么是智能合约&#x3f;","links":[],"tags":["WEB3"],"content":"智能合约简介\n智能合约（Smart Contract）是以太坊应用层的基石。它是存储在区块链上的一段计算机程序，遵循“如果……那么……”（IFTTT）逻辑，能够在满足特定条件时自动执行。\n一旦部署到区块链上，智能合约将不可更改、自动执行、公开透明，从而在无需中介的情况下实现安全可信的交易与协作。\n\n一、起源与理念\n“智能合约”一词由密码学家 Nick Szabo 于 1994 年提出。\n他在论文《Smart Contracts: Building Blocks for Digital Free Markets》中设想了一个无需信任中介的数字市场，其中交易规则可以通过加密手段自动执行。\nSzabo 用一个形象的比喻描述了这种概念：\n\n“智能合约就像一台数字自动售货机。\n投入硬币（输入条件），机器自动吐出商品（输出结果），无需中间人介入。”\n\n以太坊创始人 Vitalik Buterin 将这一理念真正落地，通过区块链技术，使“代码即法律”（Code is Law）成为现实。\n\n二、什么是智能合约？\n从技术角度看，智能合约是运行在区块链虚拟机（如 EVM）上的自动化程序。\n从法律或经济角度看，它是一个由代码定义的协议，用于在各方之间执行条款、交换价值或转移资产。\n简单来说：\n\n智能合约是一段自执行的程序，确保合约条款在没有第三方的情况下自动履行。\n\n在以太坊上，智能合约通常使用 Solidity 或 Vyper 等语言编写，并部署在链上特定地址。\n部署后，任何人都可以调用其函数、查看代码状态或参与执行。\n\n三、传统合约中的信任问题\n传统合约的最大问题之一是——信任成本高。\n例如：\n\nAlice 和 Bob 打赌 10 元，谁先骑车到终点谁就赢。\n比赛结束后，Alice 明显获胜，但 Bob 拒绝支付赌注，声称 Alice 作弊。\n\n即使协议条件已满足，执行仍依赖对方的主观意愿和诚信。这意味着合约履行需要法律、法院或第三方机构介入。\n而在智能合约中，这类问题被消除：\n\n条件判断、资产托管与转移都由代码自动执行；\n不可抵赖，也无法事后篡改；\n信任转移到区块链系统本身。\n\n\n四、用自动售货机类比智能合约\n智能合约最经典的类比是自动售货机：\n\n选择一款商品\n查看价格\n投入金额\n机器验证付款\n自动出货\n\n如果输入条件（付款金额、商品选择）不满足，机器不会出货。\n\n智能合约与之类似：\n当输入（区块链事件）满足条件，合约自动执行输出（资产转移、状态更新）。\n\n\n五、智能合约的核心特性\n1. 自动执行\n智能合约在触发条件满足时自动运行，不依赖人工干预。\n例如，你可以编写一个托管资金的合约，允许孩子在 18 岁生日后提取资金，提前尝试将被拒绝。\n2. 确定性与可预测性\n智能合约是确定性代码：\n在相同条件下，执行结果永远相同。\n这避免了传统合约因人为解释差异导致的不一致和争议。\n3. 公开透明\n所有合约代码和执行记录都存储在公共区块链上，\n任何人都可以验证资产流向或执行逻辑，无需信任单方。\n4. 匿名与隐私保护\n以太坊账户基于地址而非身份，因此用户可以在公开系统中保持匿名性。\n尽管交易记录可追踪，但参与者的真实身份不会被直接暴露。\n5. 不可篡改\n一旦部署，合约代码与状态将永久存在于区块链中。\n这既确保了安全性，也意味着合约中的错误将难以修复，因此部署前的审计尤为重要。\n\n六、智能合约的常见应用\n智能合约几乎能实现任何可编程逻辑，以下是一些典型用例：\n💰 去中心化金融（DeFi）\n\n去中心化交易所（如 Uniswap）\n借贷平台（如 Aave、Compound）\n稳定币与收益农场\n\n🖼️ 数字资产与NFT\n\nNFT 铸造与交易\n数字艺术作品版税分配\n虚拟土地与元宇宙资产\n\n👥 去中心化自治组织（DAO）\n\n智能投票系统\n成员治理规则\n自动分红与提案执行\n\n🏷️ 供应链与溯源\n\n货物追踪与自动结算\n物流透明化与防伪验证\n\n⚙️ 其他应用\n\n保险自动理赔\n数字身份管理\n游戏内资产与奖励系统\n\n\n七、智能合约的局限与挑战\n虽然智能合约具备强大的自动化能力，但仍存在一些挑战：\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n问题描述代码漏洞风险合约一旦部署即不可修改，错误可能导致资金损失（如 DAO 攻击事件）法律认可度有限在许多国家，智能合约的法律效力仍不明确隐私不足所有执行数据公开上链，难以实现机密交易Gas 成本高合约执行依赖区块链计算资源，成本随网络拥堵而波动\n\n八、总结\n智能合约让“信任转移到代码”成为可能。\n它的出现让区块链不再只是价值转移的账本，而成为可编程的去中心化计算平台。\n从自动售货机到 DeFi 协议，从 NFT 到 DAO，\n智能合约正推动我们迈向一个“代码定义规则、共识构建信任”的全新时代。\n\n\n延伸阅读\n\nNick Szabo: Smart Contracts: Building Blocks for Digital Markets (1996)\nEthereum 官方文档：What is a Smart Contract?\nSolidity 官方文档：Solidity by Example\n\n"},"WEB3/5.-什么是燃料费？":{"slug":"WEB3/5.-什么是燃料费？","filePath":"WEB3/5. 什么是燃料费？.md","title":"5. 什么是燃料费？","links":[],"tags":["WEB3"],"content":"什么是燃料费？\n网络费用\n在以太坊生态中，网络费用被称作“燃料”。它就像以太坊的“动力源”，为整个网络运转提供能量。\n概览\n\n以太坊上的每一笔交易，都需要支付少量的处理费用。\n这类费用被统一称为“燃料”费。\n燃料费并非固定不变，而是会随着网络拥塞状况发生波动。\n\n什么是燃料费？\n可以把以太坊想象成一个大型的计算机网络，人们在这个网络上能够执行发送信息、运行程序等各种任务。就像在现实世界中完成任务需要能量一样，在以太坊中完成这些任务同样需要能量。\n在以太坊里，每个计算操作都设定了相应的“燃料”价格。燃料费指的是在交易过程中所执行操作的总费用。当用户发送交易或者运行智能合约时，必须支付燃料费，交易才能被处理。\n如何支付较少燃料费？\n虽然以太坊上较高的燃料费有时难以避免，但我们可以采用以下策略来降低费用：\n安排交易时间\n就如同错峰出行能够避开拥堵，且出行成本更低一样，在北美地区的睡眠时间使用以太坊进行交易，往往能享受到更优惠的费用。\n等待费用下降\n以太坊的燃料价格会根据网络拥塞程度，每12秒上下波动一次。当燃料价格较高时，只需在交易前等待几分钟，就有可能看到需要支付的费用显著下降。\n使用二层网络\n二层网络链是在以太坊基础上构建的，它具有费用更低、可处理交易数量更多的特点。对于那些无需在以太坊主网进行的交易，二层网络是节省费用的不错选择。\n是什么造成了高昂的燃料费？\n当以太坊上的计算量（燃料）超过特定阈值时，燃料费就会开始上涨。超过阈值的燃料越多，燃料费增长的速度就越快。\n较高的燃料费可能由以下因素导致：热门去中心化应用程序（dapp）或非同质化代币、去中心化交易所周期性增加的交易量，以及高峰时段海量用户的活动等。\n在部署智能合约前，以太坊上的开发者应当谨慎优化其智能合约的用法。因为如果许多人都在使用一个编写拙劣的智能合约，就会消耗更多的燃料，甚至可能无意中造成网络拥塞。\n为何需要燃料？\n燃料是保障以太坊安全和处理交易的关键要素，它在多个方面发挥着重要作用：\n\n燃料能够帮助以太坊防范女巫攻击，阻止恶意行为者通过欺诈活动压垮网络。\n由于计算需要耗费燃料，进行昂贵的交易在经济上对无意间或恶意对以太坊进行垃圾邮件攻击的行为者来说是不划算的。\n对任何时间可执行的计算量设定硬性限制，能够避免以太坊被压垮，有助于确保网络始终可访问。\n\n如何计算燃料？\n支付的总燃料费由以下几个部分组成：\n\n基础费：由网络设定，是进行交易必须支付的费用。\n优先费：可选择支付的小费，目的是激励节点运营者优先处理你的交易。\n使用的燃料单位：还记得我们说过燃料代表计算吗？复杂操作（例如与智能合约交互）比简单操作（例如发送交易）会使用更多的燃料。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n交易类型使用的燃料单位发送以太币21,000发送 ERC-20 代币65,000转移非同质化代币84,904在 Uniswap 兑换代币184,523\n燃料费计算公式为：使用的燃料单位 *（基础费 + 优先费）。大多数钱包会自动计算燃料用量，并以更直观的方式展示给用户。\n常见问题\n谁获得了我的交易的燃料费？\n大部分燃料费（基础费）会被协议销毁（烧毁）。如果交易中包含优先费，这部分费用会付给提交你的交易的验证者。\n我是否需要用以太币支付燃料费用？\n是的。以太坊上的所有燃料费都必须使用原生货币以太币来支付。\n什么是 gwei？\n在大多数钱包或燃料追踪器中，你会看到燃料价格用“gwei”来计量。\nGwei 只是以太币的一个较小计量单位，类似于美分与美元的关系，不同之处在于 1 以太币等于 10 亿 gwei。在涉及极少量以太币时，使用 Gwei 作为计量单位会更加方便。"},"WEB3/6.-什么是桥梁？":{"slug":"WEB3/6.-什么是桥梁？","filePath":"WEB3/6. 什么是桥梁？.md","title":"6. 什么是桥梁？","links":[],"tags":["WEB3"],"content":"什么是桥梁？\n区块链桥\nWeb3 已发展成一个由一层网络区块链与二层网络扩展解决方案构成的生态系统，每个方案都有独特功能与权衡。随着区块链协议数量增多，跨链转移资产的需求也日益增长。为满足这一需求，我们需要桥接。\n区块链桥如同现实世界中的桥梁。现实桥梁连接两个物理位置，区块链桥则连接两个区块链生态系统。链桥能够传输信息与资产，促进区块链间的通信。\n什么是桥梁？\n区块链桥和现实桥梁类似。现实桥梁连接两个物理地点，区块链桥连接两个区块链生态系统。链桥传输信息与资产，推动区块链间的交流。\n下面看个例子：\n你来自美国，计划去欧洲旅行。你有美元，但消费需用欧元。要将美元兑换成欧元，可使用货币兑换服务并支付少量费用。\n若想在区块链间进行类似兑换，比如用以太坊主网上的以太币兑换 Arbitrum 上的以太币，该怎么办呢？如同兑换欧元，我们需要一种机制将以太币从以太坊转移到 Arbitrum。桥梁使这种交易成为可能。例如，Arbitrum 有一个原生桥梁，可将以太币从主网转移至 Arbitrum。\n我们为什么需要桥梁？\n所有区块链都有局限性。以太坊要实现扩容以满足需求，需进行卷叠。而像 Solana 和 Avalanche 这样的一层网络，采用不同设计以实现更高吞吐量，但牺牲了去中心化程度。\n然而，所有区块链开发都在孤立环境中进行，规则和共识机制各异。这意味着它们无法原生通信，代币也无法在区块链间自由流动。\n桥梁的作用就是连接区块链，实现信息和代币在它们之间的传输。\n链桥的作用：\n\n跨链传输资产与信息。\n让去中心化应用程序利用各区块链优势，增强功能（协议有更多创新设计空间）。\n使用户能访问新平台，利用不同链的优势。\n促进不同区块链生态系统的开发人员协作，为用户构建新平台。\n\n桥梁用例\n以下是使用桥梁的一些场景：\n降低交易费\n假设你在以太坊主网有以太币，想以较低交易费探索不同去中心化应用程序。将以太币从主网桥接到以太坊二层网络卷叠，就能享受更低交易费用。\n其他区块链上的去中心化应用程序\n若你一直在以太坊主网用 Aave 提供 USDT，但 Polygon 上用 Aave 提供 USDT 的利率可能更高。\n探索区块链生态系统\n若你在以太坊主网有以太币，想探索山寨币一层网络，尝试其原生应用程序。可使用桥梁将以太币从以太坊主网转移到山寨币一层网络。\n拥有原生加密资产\n若你只有以太坊网络资产，但想拥有原生比特币，可先兑换得到以太坊上的 BTC - WBTC（Wrapped Bitcoin）。不过，包装比特币是以太坊网络的原生 ERC - 20 代币，是比特币的以太坊版本，并非比特币区块链上的原始资产。然后通过跨链桥，将资产从以太坊网络跨到比特币网络，即把 WBTC 转换为原生 BTC。或者，你拥有比特币，想在以太坊的去中心化金融协议中使用它。\n桥的类型\n桥梁设计和复杂程度多样。一般来说，桥梁分为两类：需信任桥梁和去信任桥梁。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n需信任桥梁去信任桥梁需信任桥梁依赖中心实体或系统运作。去信任桥梁借助智能合约和算法运行。对资金保管和桥梁安全性有信任假设，用户大多依赖桥梁运营商声誉。这种桥梁无需信任，其安全性与底层区块链安全性相同。用户需放弃对加密资产的控制。借助智能合约，去信任链桥让用户始终掌控资金。\n简而言之，需信任桥梁有信任假设，去信任桥梁对信任依赖极小，不会在基础域外产生新信任假设。相关术语解释如下：\n\n去信任：与底层域安全性等同。如 Arjun Bhuptani 在本文中所述。\n信任假设：通过在系统中添加外部验证者摆脱底层域安全性，从加密经济学角度，这会降低安全性。\n\n为更好理解两种方法的主要区别，举个例子：\n想象你在机场安检站，有两种检查站：\n\n手动检查站：由工作人员手动检查机票和身份证明所有详细信息，再交出登机牌。\n自助值机：由机器操作，输入航班详细信息，检查无误后收到登机牌。\n\n手动检查点类似需信任模式，依赖第三方（工作人员）操作。作为用户，你相信工作人员会正确决策并合理使用私人信息。\n自助值机类似去信任模式，无需操作员，利用技术操作。用户始终控制数据，无需信任第三方处理私人信息。\n许多桥梁解决方案采用介于两者之间的模式，去信任程度不同。\n使用链桥\n使用链桥可将资产转移到不同区块链。以下资源可助你找到并使用链桥：\n\nL2BEAT 链桥摘要 &amp; L2BEAT 链桥风险分析：全面汇总各类链桥，包含市场份额、链桥类型和目的地区块链等详细信息。L2BEAT 还对链桥进行风险分析，助用户选择链桥时做出明智决策。\nDefiLlama 链桥摘要：跨以太坊网络的链桥交易量摘要。\n\n使用桥梁的风险\n桥梁处于开发早期，最佳桥梁设计可能尚未发现。与任何类型桥梁互动都有风险：\n\n智能合约风险：代码错误可能导致用户资金损失。\n技术风险：软件故障、代码错误、人为错误、垃圾邮件和恶意攻击可能扰乱用户操作。\n\n此外，需信任桥梁因增加信任假设，带来额外风险，如：\n\n审查风险：桥梁运营商理论上可阻止用户用桥梁转移资产。\n保管风险：桥梁运营商可能串通盗取用户资金。\n\n用户资金面临风险的情况包括：\n\n智能合约存在错误。\n用户操作失误。\n底层区块链被非法侵入。\n桥梁运营商对需信任桥梁有恶意。\n桥梁被非法侵入。\n\n最近一次黑客攻击是 Solana 的虫洞桥，在攻击期间被窃取 12 万包装以太币（3.25 亿美元）。区块链中许多顶级黑客攻击都涉及桥梁。\n桥梁对用户加入以太坊二层网络，甚至探索不同生态系统至关重要。但鉴于与桥梁交互有风险，用户必须了解桥梁的权衡取舍。以下是一些确保跨链安全的策略。\n延伸阅读\n\nEIP - 5164：跨链执行 - 2022 年 6 月 18 日 - Brendan Asselstine\n二层网络桥梁风险框架 - 2022 年 7 月 5 日 - Bartek Kiepuszewski\n“为什么未来将出现多链，而不会是跨链。” - 2022 年 1 月 8 日 - Vitalik Buterin\n利用共享安全实现安全的跨链互操作性：Lagrange 状态委员会及其扩展 - 2024 年 6 月 12 日 - Emmanuel Awosika\nRollup 互操作性解决方案的现状 - 2024 年 6 月 20 日 - Alex Hook\n"},"WEB3/7.-什么是零知识证明？":{"slug":"WEB3/7.-什么是零知识证明？","filePath":"WEB3/7. 什么是零知识证明？.md","title":"7. 什么是零知识证明？","links":[],"tags":["WEB3"],"content":"6. 什么是零知识证明？\n零知识证明是一种能够在不披露声明本身内容的情况下，验证声明有效性的方法。其中，“证明者”是试图证明声明的一方，“验证者”则负责验证声明的真实性。\n零知识证明最早在1985年的一篇论文《交互式证明系统的知识复杂度》中被提出，该论文给出了至今仍被广泛使用的零知识证明定义：\n零知识协议是一种方法，借助此方法，一方（证明者）能够向另一方（验证者）证明某个声明是真实的，并且除了该声明真实这一信息外，不会透露任何额外信息。\n多年来，零知识证明不断发展，目前已在现实世界中得到广泛应用。\n我们为什么需要零知识证明？\n零知识证明是应用密码学领域的一项重大突破，有望显著提高个人信息的安全性。以向服务提供商证明一项声明（如“我是X国的公民”）为例，通常需要提供“证据”来支持该声明，例如国民护照或驾驶证。\n然而，这种方法存在明显问题，主要是缺乏隐私保护。与第三方服务商共享的个人身份信息（PII）存储在中心化数据库中，这些数据库极易受到黑客攻击。随着身份盗用问题日益严峻，人们迫切需要采用更能保护隐私的方式来分享敏感信息。\n零知识证明通过在不透露相关信息的情况下证明声明的有效性，有效解决了这一问题。零知识协议以声明（称为“证人”）作为输入，然后生成一个关于其有效性的简要证明。这种证明能够有力地保证一项声明是真实的，同时不会透露创建声明时所使用的信息。\n回到前面的例子，你只需提供零知识证明即可证明自己的公民身份声明。验证者只需检查证明中的一些属性是否属实，从而确认背后声明的真实性。\n零知识证明的用例\n匿名支付\n信用卡支付通常对多方可见，包括支付服务提供商、银行和其他相关方（如政府部门）。金融监管有助于识别非法活动，但同时也损害了普通民众的隐私。\n加密货币旨在为用户提供一种私密的、点对点的交易手段。然而，大部分加密货币的交易在区块链上是公开可见的。虽然用户身份通常是匿名的，但也可以主动与现实世界的身份相关联（例如在Twitter或GitHub的个人资料上包含以太坊地址），或者通过一些基本的链上或脱链数据分析来关联现实世界身份。\n有一些专门为完全匿名交易设计的“隐私币”。注重隐私的区块链，如Zcash和Monero，会屏蔽交易细节，包括发送人和接收人的地址、资产类型、数量和交易时间线。\n通过在协议中加入零知识技术，注重隐私的区块链网络允许节点在无需访问交易数据的情况下验证交易。\n零知识证明也用于在公共区块链上对交易进行匿名处理。例如，Tornado Cash是一个去中心化的非托管服务，允许用户在以太坊上进行私密交易。Tornado Cash使用零知识证明来模糊化交易细节，保障金融隐私。不幸的是，由于这些是“主动选择加入”的隐私工具，它们与非法活动存在关联。为了克服这一问题，隐私最终必须成为公开区块链的默认设置。\n身份保护\n目前的身份管理系统使个人信息面临风险。零知识证明能够帮助个人验证身份，同时保护敏感细节。\n零知识证明在去中心化身份中特别有用。去中心化身份（又称“自主身份”）让个人能够控制私人身份信息的访问。在不透露税号或护照细节的情况下，证明你的公民身份就是利用零知识技术实现去中心化身份的一个典型例子。\n认证\n使用在线服务需要证明身份和访问这些平台的权限。这通常需要提供个人信息，如姓名、邮箱地址、出生日期等。用户可能需要记住长长的密码，同时也面临无法访问的风险。\n然而，零知识证明可以简化平台和用户的认证流程。一旦通过公开输入（如证明用户是平台成员的数据）和私密输入（如用户详细信息）生成零知识证明，用户就能在需要访问服务时，使用该证明来认证身份。这改善了用户体验，同时组织也无需存储大量用户信息。\n可验证计算\n可验证计算是零知识技术用于改进区块链设计的另一种应用。可验证计算允许我们将计算外包给另一个实体，同时保留可验证的计算结果。该实体将计算结果和验证程序被正确执行的证据一起提交。\n可验证计算对于在不降低安全性的前提下提高区块链上的处理速度至关重要。要理解这一点，需要了解拟议的以太坊扩容解决方案之间的差异。\n链上扩容方案，如分片，需要对区块链的基础层进行大量修改。然而，这种方法非常复杂，如果在实现过程中出错，则会危及以太坊的安全模型。\n脱链扩容解决方案无需重新设计以太坊的核心协议。相反，它们依靠外包计算模型来提高以太坊基础层的吞吐量。\n以下是实际运作过程：\n\n以太坊不处理每一个交易，而是将执行放到一个单独的链上。\n在处理完交易后，另一条链返回结果，然后应用到以太坊的状态上。\n\n这样做的好处是，以太坊无需进行任何执行，只需将外包计算的结果应用到其状态中。这减少了网络拥堵，也提高了交易速度（脱链协议为快速执行进行了优化）。\n链上需要一种方法来验证脱链交易，而无需重新执行它们，否则脱链执行就失去了其价值。\n这就是可验证计算发挥作用的地方。当一个节点在以太坊外执行交易时，它提交一个零知识证明来证明脱链执行的正确性。这个证明（称为有效性证明）保证了一个交易是有效的，允许以太坊把结果应用到状态中 — 无需等待任何人提出异议。\n零知识卷叠和 Validium 是两个脱链扩容的解决方案，它们使用有效性证明来提供安全的可扩容性。这些协议执行了数以千计的脱链交易，并提交了用于以太坊验证的证明。一旦证明被验证，这些结果就会被立即应用，允许以太坊在不增加基础层计算量的情况下处理更多的交易。\n减少链上投票中的贿赂和串通\n区块链投票方案具有诸多有利特点：它们是完全可审计的、安全抗攻击、抗审查以及不受地域限制。但即使是链上投票方案也难以避免串通问题。\n串通的定义是“通过欺骗、诈骗和误导他人来协调，达到限制公开竞争的目的”，串通的形式可能是恶意行为者通过行贿来影响投票。例如，Alice可能收到Bob的贿赂，让她在选票上投给选项B，即使她更倾向于选项A。\n贿赂和串通限制了任何以投票作为信号机制的过程的有效性（特别是在用户可以证明他们如何投票的情况下）。这可能会产生重大影响，特别是在通过投票来分配稀缺资源的情况下。\n例如，二次融资机制根据捐款来衡量对不同公益项目中某些选项的偏好。每笔捐款都算作对某一特定项目的“投票”，获得更多投票的项目将从匹配资金池中获得更多资金。\n使用链上投票使得二次融资容易受到串通的影响：区块链交易是公开的，所以行贿者可以检查受贿者的链上活动，看他们是如何“投票”的。这种方式使得二次融资不再是一个根据社区的集体偏好来分配资金的有效手段。\n幸运的是，更新的解决方案，例如MACI（最低限度的反串通基础设施），正在使用零知识证明来让链上投票（如二次融资机制）能够抵制贿赂和串通。MACI是一个智能合约和脚本的集合，它允许中央管理员（称为“协调员”）汇总票数和统计结果，无需暴露每个人是如何投票的。即便如此，仍然可以验证投票是否被正确计数，或确认某个人参加了该轮的投票。\nMACI是如何与零知识证明协作的？\n一开始，协调员在以太坊部署MACI合约，之后用户就可以注册投票（通过把它们的公钥注册到智能合约上）。用户通过发送经公钥加密后的信息给智能合约来完成投票（一个有效投票必须使用与用户身份关联的最新公钥来签名，另外还有其他标准）。然后，协调员在投票期结束后处理所有的信息，统计票数，在链上验证结果。\n在MACI中，零知识证明用来确保计算的正确性，避免协调员错误地处理投票和汇总结果。这是通过要求协调员生成零知识简洁非交互式知识论证证明来实现的，验证a)所有的信息都被正确处理；b)最后的结果与所有有效投票总和对应。\n因此，即便没有分享每个用户的投票明细（通常如此），MACI也能保证计算过程中计算结果的完整性。这一特点有助于减少基本的串通方案的有效性。通过之前Bob贿赂Alice来给某个选项投票的例子，我们可以探索这种可能性：\n\nAlice通过发送她的公钥到智能合约来注册投票。\nAlice同意投票给选项B，以此来换取Bob的贿赂。\nAlice投票给选项B。\nAlice偷偷发送一个加密交易来更改与她身份关联的公钥。\nAlice发送另一个（加密的）消息到智能合约，使用新的公钥给选项A投票。\nAlice向Bob展示交易，显示她已经投票给了选项B（投票是无效的，因为最初的那个公钥在系统中已经不再跟Alice的身份关联）。\n在处理消息时，协调员跳过Alice投给选项B的选票，只把选项A的投票纳入计数。因此，Bob串通Alice和操纵链上投票的意图失败了。\n\n使用最低限度的反串通基础设施时，确实需要相信协调员不会与行贿者串通或者试图自己来贿赂投票者。协调员可以解密用户消息（创建证明的时候需要），以确保它们可以准确验证每个人是如何投票的。\n在协调员保持诚实的情况下，MACI代表了一个强大的工具，保证了链上投票的神圣性。这解释了它为何在二次融资应用中如此受欢迎（例如 clr.fund），二次融资严重依赖于每个人投票选择的完整性。\n零知识证明如何运作？\n零知识证明允许你证实一个声明是否真实，无需分享声明的内容以及你是如何发现其真实性的。为了做到这点，零知识证明依靠算法获取一些数据作为输入，然后输出“真”或“假”。\n零证明协议需要满足以下标准：\n\n完备性：如果输入是有效的，那么零知识协议应该始终返回“真”。因此，如果所依据的声明是真的，同时证明者和验证者都诚实，那么证明就能被接受。\n可靠性：如果输入是无效的，那么理论上不可能欺骗零知识协议输出“真”。因此，一个说谎的证明者无法忽悠一个诚实的验证者相信一个无效的声明是有效的（小概率事件除外）。\n零知识：验证者除了知道声明的有效性或虚假性之外，其余一概不知（他们对声明内容“一无所知”）。这项要求也阻止了验证者从证明中推导出原始输入（声明的内容）。\n\n零知识证明的基本形式由三个要素组成：证人、挑战和回复。\n\n证人：通过零知识证明，证明者想要证明对一些隐藏信息的了解。这些秘密信息就是证明的“证人”，证明者基于对证人的了解设立了一组问题，这些问题只有了解信息的一方才能回答出来。因此，证明者随机选择一个问题来发起证明，计算问题答案，然后发送给验证者。\n挑战：验证者从问题集里随机挑选另外一个问题，然后让证明者来回答。\n回复：证明者接收问题，计算答案，然后把答案发送给验证者。证明者的回复让验证者可以检验证明者是否真的可以接触到证人。为了确保证明者并不是偶然盲目猜对答案的，验证者会再选择一个问题来提问。通过多次重复这个过程，证明者造假的可能性会极大的降低，直到最终验证者满意。\n\n以上描述了“交互式零知识证明”的结构。早期的零知识协议使用交互式证明，验证一个声明的有效性需要证明者和验证者之间来回多次沟通。\n一个比较好的展示交互式证明是如何运转的例子是Jean-Jacques Quisquater非常有名的阿里巴巴山洞故事。在故事里，Peggy（证明者）想要向Victor（验证者）证明她知道打开魔法之门的暗号，但她又没有透露暗号是什么。\n非交互式零知识证明\n虽然是革命性的，但互动式证明的作用有限，因为它要求双方同时在线，反复互动。即使验证者相信证明者是诚实的，该证明也不能用于独立验证（计算一个新的证明需要证明者和验证者之间一个新的信息集）。\n为了解决这个问题，Manuel Blum、Paul Feldman和Silvio Micali提出了第一个非交互式零知识证明，证明者和验证者将有一个共享的密钥。这能让证明者表明他们对某些信息（即证人）的了解，而不提供信息本身。\n与交互式证明不同的是，非互动式证明只需要参与者（证明者和验证者）之间进行一轮沟通。证明者把秘密信息输入到一个特殊的算法中，然后计算出一个零知识证明。这个证明会被发送给验证者，验证者使用另一个算法来检验证明者是否知道秘密信息。\n非交互式证明减少了证明者和验证者之间的沟通，让零知识证明更高效。另外，一旦生成了证明，任何人（可访问共享密钥和验证算法）都可以对其进行验证。\n零知识证明的类型\n零知识简洁非交互式知识论证 (ZK-SNARK)\nZK-SNARK是零知识简洁非交互式知识论证的缩写。零知识简洁非交互式知识论证协议具有以下特点：\n\n零知识：验证者可以验证声明的完整性，而不需要知道关于该声明的任何其他信息。验证者对声明的唯一了解就是它是真还是假。\n简洁：零知识证明比证人小，可以快速验证。\n非交互式：该证明是非交互式的，因为证明者和验证者之间只需要交互一次，不像交互式证明需要多轮的沟通。\n论证：该证明满足“可靠性”要求，因此基本不可能作弊。\n（的）知识：如果不能获取秘密信息（证人），就无法构建零知识证明。证明者在没有证人的情况下计算一个有效的零知识证明是非常困难的，即便不是不可能。\n\n前面提到的“共享密钥”是指证明者和验证者同意在生成和验证证明时使用的公共参数。生成公共参数（统称为通用参考编码 (CRS)）是一个敏感的操作，因为它在协议安全性方面非常重要。如果用于生成通用参考编码的熵（随机数）落入到不诚实的证明者手上，他们就能计算虚假证明。\n[多方计算 (MPC)]是一种在生成公共参数时降低风险的方法。多个参与方参与到信任设置仪式，每个人提供一些随机数值来生成通用参考编码。只要有一个诚实的参与方销毁了他们那部分的熵，零知识简洁非交互式知识论证协议就能保持计算的可靠性。\n信任设置要求用户信任参数生成的参与者。然而，ZK-STARK的发展使其能在非信任的设置下证明协议。\n零知识可扩容透明知识论证 (ZK-STARK)\nZK-STARK是零知识可扩容透明知识论证的缩写。ZK-STARK类似于ZK-SNARK，除了它是：\n\n可扩容：当证人的规模比较大时，ZK-STARK生成和验证证明比零知识简洁非交互式知识论证要快。随着证人规模的增加，简洁的非交互式知识论证证明的证明和验证时间只会略微增加（简洁的非交互式知识论证证明和验证时间与证人规模呈线性增长）。\n透明：ZK-STARK依赖于可公开验证的随机数来生成用于证明和验证的公共参数，而不是信任设置。因此，它比零知识简洁非交互式知识论证更加透明。\n\nZK-STARK会产生比零知识简洁非交互式知识论证更大的证明，这意味着它通常具有更高的验证开销。然而，在有些情况下（如证明大型数据集），ZK-STARK可能比零知识简洁非交互式知识论证更具成本效益。\n使用零知识证明的缺点\n硬件成本\n生成零知识证明涉及到非常复杂的计算，最好在专用机器上运行。由于这类机器价格昂贵，普通人通常买不起。此外，想要使用零知识技术的应用必须考虑硬件成本 — 这可能会增加终端用户的费用。\n证明验证费用\n验证证明也需要复杂的计算，这增加了在应用中实施零知识技术的成本。这种成本在证明计算方面尤其突出。例如，零知识卷叠在以太坊上支付约500,000单位燃料来验证一个零知识简洁非交互式知识论证证明，零知识可扩容透明知识论证则需要更高的费用。\n信任假设\n在零知识简洁非交互式知识论证中，公共参考字符串（公共参数）只生成一次，然后可供零知识协议的参与方重复使用。公共参数由可信设置仪式创建，其参与者假定是诚实的。\n但是并没有什么方法能让用户评估参与者是否诚实，用户必须相信开发者的话。零知识可扩容透明知识论证不存在信任假设，因为生成字符串所使用的随机数是可公开验证的。与此同时，研究人员正在研究零知识简洁非交互式知识论证的非可信设置，以增加证明机制的安全性。\n量子计算的威胁\n零知识简洁非交互式知识论证使用椭圆曲线加密法进行加密。虽然椭圆曲线离散对数问题目前被认为是难以破解的，但在未来，量子计算机的发展可能会打破这种安全模式。\n由于仅使用抗碰撞的哈希函数确保安全性，零知识可扩容透明知识论证被认为不会受到量子计算的威胁。与椭圆曲线加密法中使用的公私密钥对不同，抗碰撞哈希算法更难被量子计算算法破解。\n延伸阅读\n\n零知识证明用例概述 — Privacy and Scaling Explorations Team\nSNARKs与STARKs与递归SNARK — Alchemy Overviews\n零知识证明：改善区块链上的隐私 — Dmitry Lavrenov\nzk-SNARK — 一个现实的零知识例子和深入研究 — Adam Luciano\nZK-STARK — 创建可验证的信任，即使面对量子计算机 — Adam Luciano\n关于如何让zk-SNARK成为可能的大致介绍 — Vitalik Buterin\n为什么零知识证明 (ZKP) 是自主主权身份的颠覆者 — Franklin Ohaegbulam\n"},"WEB3/8.-什么是以太坊节点？":{"slug":"WEB3/8.-什么是以太坊节点？","filePath":"WEB3/8. 什么是以太坊节点？.md","title":"8. 什么是以太坊节点？","links":[],"tags":["WEB3"],"content":"运行你自己的以太坊节点\n\n“Don’t trust, verify.” —— 不要信任，亲自验证。\n\n运行属于你自己的以太坊节点，是实现完全控制与自主权的最佳方式。\n它不仅能帮助你安全地与网络交互，还能增强整个以太坊生态系统的去中心化和抗审查性。\n\n一、为什么要运行自己的节点？\n运行节点意味着：\n\n你直接连接到以太坊网络，而非依赖第三方服务；\n你自己验证区块与交易的真实性；\n你在帮助整个网络维持安全性与去中心化。\n\n换句话说，运行节点让你成为真正的以太坊公民。\n不是被动使用者，而是积极参与者。\n\n二、什么是以太坊节点？\n以太坊节点（Node）是运行**客户端软件（Client）**的计算机。\n它的主要职责包括：\n\n\n下载和存储区块链数据\n节点会保存区块链上所有区块和交易的副本。\n\n\n验证区块和交易\n每当新区块产生，节点都会独立验证其有效性。\n\n\n广播信息\n节点将新的交易和区块在网络中转发给其他节点，保持全网同步。\n\n\n提供接口\n节点允许钱包、DApp 或 CLI 工具通过 RPC（如 JSON-RPC）与以太坊网络通信。\n\n\n简而言之：\n\n节点是以太坊的基础设施，它们共同维护着去中心化的共识与数据完整性。\n\n\n三、节点类型\n以太坊节点有不同类型，取决于存储和验证的深度：\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n节点类型描述硬盘需求验证方式全节点（Full Node）保存完整的区块链数据并验证所有交易高（~1TB 及以上）全部验证轻节点（Light Node）仅保存区块头，通过全节点请求具体数据低（~几GB）依赖全节点验证归档节点（Archive Node）保存区块链的完整历史状态（适合开发或分析）极高（~15TB+）全部验证验证者节点（Validator Node）在 PoS 下参与共识、打包区块、质押 ETH中等主动验证与提议\n\n🚀 普通用户推荐运行 全节点或轻节点，\n开发者或分析师可以选择 归档节点，\n想参与出块的用户可以运行 验证者节点。\n\n\n四、运行以太坊节点的方式\n1. 运行客户端软件\n客户端（Client）是节点的核心软件，负责连接网络、验证区块和处理请求。\n以太坊是多客户端设计，不依赖单一实现，常见的有：\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n客户端名称开发语言特点GethGo官方最成熟的客户端，社区最广泛使用NethermindC#性能优秀，支持 Windows 环境BesuJava企业级，兼容以太坊主网与联盟链ErigonGo高性能轻量化实现，适合运行归档节点LodestarTypeScript支持共识层（信标链）客户端\n\n📘 建议：\n对大多数用户来说，Geth 是最易上手且文档最丰富的选择。\n\n\n2. 使用合适的硬件\n以太坊节点并不需要昂贵的设备。\n你可以在普通消费级电脑上运行，也可以使用专用硬件（如小型服务器或树莓派）来保证稳定性。\n推荐配置：\n\nCPU：4 核及以上\n内存：16 GB+\n存储：1 TB SSD（归档节点需更大）\n网络：稳定宽带（上/下行带宽 ≥ 10 Mbps）\n系统：Linux / macOS / Windows\n\n\n⚙️ 提示：SSD 对节点性能影响极大，建议避免使用机械硬盘。\n\n\n3. 在线运行与同步\n运行节点其实很简单：\n\n一台连接互联网的计算机，持续运行客户端软件即可。\n\n当节点首次启动时，它会从其他节点下载区块链数据（同步阶段），\n之后只需保持在线，它会自动接收和验证新区块。\n如果节点暂时离线，重新上线后会自动从最新区块同步，直到赶上主网。\n\n五、谁应该运行节点？\n答案是：所有人！\n运行节点并非权益证明验证者（Staker）的专属行为。\n任何人都可以运行节点，甚至无需质押以太币。\n虽然非验证节点不会获得区块奖励，但运行节点仍然带来许多重要好处：\n✅ 好处一：增强隐私\n使用自己的节点意味着钱包（如 MetaMask）直接向你的节点发出请求，\n无需将交易或地址数据暴露给第三方 RPC 服务（如 Infura 或 Alchemy）。\n✅ 好处二：安全可信\n你不再依赖别人告诉你“区块链的状态”，\n而是自己验证每笔交易和每个区块的真实性。\n✅ 好处三：抗审查\n当更多人运行节点，网络越分散，就越难被中心化机构控制或审查。\n✅ 好处四：促进去中心化\n运行节点意味着你为以太坊网络贡献资源与可靠性。\n更多的节点 = 更强的网络。\n\n六、如何运行自己的节点（简要指南）\n以下是使用 Geth 运行全节点的基本流程：\n# 1. 安装 Geth\nsudo apt install software-properties-common\nsudo add-apt-repository -y ppa:ethereum/ethereum\nsudo apt update\nsudo apt install geth\n \n# 2. 启动节点同步主网\ngeth --http --http.addr 0.0.0.0 --http.vhosts=&quot;*&quot; --http.api eth,net,web3\n \n# 3. 查看同步状态\ngeth attach\n&gt; eth.syncing\n \n# 4. 停止节点\nsudo systemctl stop geth\n\n运行后，你的节点将自动连接到以太坊主网，并同步区块数据。\n完成同步后，你就拥有了一份最新的以太坊账本副本。\n\n\n七、总结：成为真正的以太坊\n运行节点的意义，远不止技术层面。\n它代表着一种理念——去中心化、自主、信任最小化。\n\n不要依赖第三方来告诉你区块链的状态，\n自己运行节点，自己验证真相。\n\n通过运行节点，你不仅获得了对数据的完全控制，\n也在默默地守护着以太坊这张全球信任网络。\n\n\n🧭 延伸阅读\n\nEthereum.org 官方节点运行指南\nGeth 官方文档\nNethermind Client\nBeacon Chain &amp; Staking 介绍\n\n"},"WEB3/9.-什么是安全的以太坊措施？":{"slug":"WEB3/9.-什么是安全的以太坊措施？","filePath":"WEB3/9. 什么是安全的以太坊措施？.md","title":"9. 什么是安全的以太坊措施？","links":[],"tags":["WEB3"],"content":"以太坊安全指南：保护你的数字资产\n随着加密货币的日益普及，黑客和诈骗者的威胁也在不断增加。本文将介绍一些降低风险的最佳实践，帮助你安全地使用以太坊。\n重要提示：ethereum.org 团队绝不会主动联系你。 请不要回复任何声称来自以太坊官方支持的邮件。\n加密货币安全基础\n提升你的知识水平\n对加密货币运作机制的误解可能导致严重错误。例如，有人可能冒充客服，声称可以帮你找回丢失的以太币，以换取你的私钥。这实际上是在利用你对以太坊去中心化特性的不了解。深入理解以太坊的工作原理是一项重要的投资。\n钱包安全\n保护你的私钥\n切勿与任何人分享你的私钥！\n私钥是访问你以太坊钱包的密码，是防止他人盗取你资产的唯一屏障。\n不要截图助记词或私钥\n截图并存储助记词或私钥可能导致它们被同步到云端，从而被黑客获取。这是常见的攻击手段之一。\n使用硬件钱包\n硬件钱包提供离线存储，是最安全的私钥保管方式。私钥永远不会接触互联网，完全保存在本地设备中。\n即使你的电脑被控制，离线存储的私钥也能大大降低被攻击的风险。\n推荐硬件钱包：\n\nLedger\nTrezor\n\n发送交易前仔细核对\n将加密货币发送到错误的地址是一个常见错误。以太坊上的交易是不可逆的。 除非你能联系到地址所有者并说服他们退还资金，否则你将无法找回。\n在发送交易前，务必确认接收地址的准确性。与智能合约交互时，在签名前检查交易信息是一个好习惯。\n设置智能合约支出限额\n与智能合约交互时，不要设置无限支出限额。这可能导致智能合约清空你的钱包。建议仅设置交易所需的金额。\n许多以太坊钱包提供限额保护功能，防止账户被掏空。\n常见诈骗类型\n完全避免诈骗是不可能的，但了解常见手段可以降低风险。记住以下原则：\n\n保持怀疑态度\n没有人会免费或打折提供以太币\n不要向任何人透露你的私钥或个人信息\n\n赠品诈骗\n这是最常见的加密货币骗局之一。诈骗者声称，如果你向指定地址发送以太币，将收到双倍返还，也称为“买一送一”骗局。\n这些骗局通常设置时间限制，制造紧迫感。\n社交媒体黑客攻击\n2020年7月，多名知名人士和组织的Twitter账户被黑。黑客利用这些账户发布比特币赠品活动，成功骗取了11个比特币（截至2021年9月，价值约50万美元）。\n名人赠品骗局\n这种骗局录制名人的视频采访或会议，然后在YouTube上直播，伪装成名人在推广加密货币赠品活动。\n除了Vitalik Buterin，Elon Musk和Charles Hoskinson等知名人士也常被利用。直播中加入名人元素使骗局看起来更可信。\n赠品活动通常是骗局。向这些账户转账将导致资金永久损失。\n技术支持诈骗\n诈骗者冒充加密货币钱包、交易所或区块链的支持人员，利用用户对技术的不熟悉进行诈骗。\n在Discord等平台上，诈骗者会搜索寻求支持的用户，然后冒充支持人员提供帮助，试图获取私钥或诱导转账。\n记住：\n\n不要分享私钥、助记词或密码\n不允许任何人远程访问你的电脑\n仅通过官方渠道沟通\n\n“以太坊2”代币骗局\n在以太坊合并期间，诈骗者利用“以太坊2”这一模糊术语，诱使用户将以太币兑换为不存在的“以太坊2”代币。\n实际上，合并并未产生任何新的合法代币。从工作量证明切换到权益证明时，无需对账户进行任何以太币相关操作。\n注意：一些衍生代币（如Rocket Pool的rETH、Lido的stETH、Coinbase的ETH2）可能代表质押的以太币，但无需“迁移”。\n网络钓鱼诈骗\n网络钓鱼邮件要求用户点击链接，跳转到仿冒网站，输入助记词、重置密码或发送以太币。有些邮件还可能包含恶意软件，感染你的电脑。\n安全提示：\n\n不要点击来自不明地址的链接或附件\n不要透露个人信息或密码\n删除可疑邮件\n\n加密货币交易经纪人诈骗\n诈骗者冒充专业交易员，声称可以代你投资加密货币。收到资金后，他们可能诱使你投入更多资金，或直接消失。\n这些骗子常利用YouTube上的虚假账户进行对话，增加可信度，但点赞可能来自机器人账户。\n不要让互联网上的陌生人代你投资，否则将失去你的加密货币。\n加密货币矿池骗局\n自2022年9月起，以太坊挖矿已不再可行。然而，矿池骗局仍然存在。诈骗者声称你可以通过加入以太坊矿池获得丰厚回报，诱使你投入更多资金，最终将所有资金转移到未知地址。\n记住：\n\n对声称能帮你用加密货币赚钱的人保持警惕\n了解质押、流动性池等投资方式\n这种计划即使存在也很少合法，若合法会广为人知\n\n空投骗局\n诈骗者创建项目，向你的钱包空投资产（如NFT或其他代币），然后引导你访问诈骗网站领取。领取时，网站会要求你用以太坊钱包登录并“批准”交易，实际上是将你的公钥和私钥发送给诈骗者。\n网络安全基础\n使用强密码\n超过80%的账户被黑是由于密码薄弱或被盗。使用长字符、数字和符号组合的密码可以增强账户安全。\n常见错误：\n\n使用常见或关联的单词组合（如”CuteFluffyKittens!“）容易被字典攻击破解\n使用母亲婚前姓氏、孩子或宠物名字、出生日期等个人信息容易被社会工程学攻击\n\n强密码示例： ymv*azu.EAC8eyp8umf\n设置强密码的方法：\n\n尽可能使用长密码\n混合大小写字母、数字和符号\n避免使用个人信息\n不要使用常见单词\n\n使用独立密码\n数据泄露中泄露的强密码不再安全。你可以在Have I Been Pwned检查账户是否被泄露。如果是，立即更改密码。为每个账户设置独立密码，可以降低一个密码泄露导致所有账户被攻破的风险。\n使用密码管理器\n记住每个账户的唯一强密码不现实。密码管理器提供安全的加密存储，通过一个强主密码访问。它们还能在注册新服务时生成强密码，并提醒你数据泄露情况。\n推荐密码管理器：\n\nBitwarden\nKeePass\n1Password\n查看其他推荐的密码管理器\n\n启用双重身份验证（2FA）\n双重身份验证为在线账户增加额外的安全层。最常见的2FA是通过身份验证应用（如Google Authenticator或Authy）生成的6位数随机代码（TOTP）。\n安全密钥\n安全密钥是更高级、更安全的2FA方式。它们是物理硬件设备，工作原理类似于身份验证应用。使用安全密钥是最安全的2FA方法，许多采用FIDO U2F标准。了解更多关于FIDO U2F。\n卸载不必要的浏览器扩展\n浏览器扩展（如Chrome扩展或Firefox插件）可以增强功能，但也带来风险。大多数扩展默认请求“读取和更改网站数据”的权限，几乎可以对你的数据为所欲为。Chrome扩展自动更新，旧版安全的扩展可能在更新后被植入恶意代码。\n安全建议：\n\n仅安装来自可信来源的扩展\n删除不使用的扩展\n本地安装Chrome扩展以停止自动更新（高级用户）\n\n延伸阅读\n网络安全\n\n多达300万台设备被带有恶意软件的Chrome和Edge插件感染 - Dan Goodin\n如何创建一个不会忘记的强密码 - AVG\n什么是安全密钥？ - Coinbase\n\n加密货币安全\n\n保护自己和资金 - MyCrypto\n常见加密通信软件中的安全问题 - Salus\n任何人都适用的安全指南 - MyCrypto\n加密货币安全：密码和身份验证 - Andreas M. Antonopoulos\n\n防诈骗指南\n\n指南：如何识别诈骗代币\n保持安全：常见骗局 - MyCrypto\n避免骗局 - Bitcoin.org\n关于常见加密货币网络钓鱼电子邮件和消息的Twitter线程 - Taylor Monahan\n\n通过遵循这些安全实践，你可以显著降低在以太坊生态系统中遭遇风险的可能性。安全是一个持续的过程，保持警惕和不断学习是保护你的数字资产的关键。"},"WEB3/Web3-合约事件机制（Event-Log-System）":{"slug":"WEB3/Web3-合约事件机制（Event-Log-System）","filePath":"WEB3/Web3 合约事件机制（Event Log System）.md","title":"Web3 合约事件机制（Event Log System）","links":[],"tags":["WEB3"],"content":"Web3 合约事件机制（Event Log System）\nWeb3 合约事件机制（Event Log System） 是智能合约与外部世界（如 Python 客户端）进行通信的主要方式。接下来，我们从 Solidity 到 Python，逐步深入探讨这一机制。\n一、Web3 事件机制与传统消息系统的差异\n在开始之前，有必要明确一点：你已经敏锐地抓住了 Web3 事件机制与传统消息系统（如 MQ、WebSocket、Pub/Sub）之间的根本差异。你的理解是正确的，大多数“监听函数”实际上是通过 轮询（polling）或订阅区块事件 来实现的，这在区块链这种去中心化架构下几乎是唯一可行的方式。\n但别担心，这种轮询并不意味着“浪费大量计算”。接下来，我们将详细解释这一点。\n\n二、什么是 event？\n在 Solidity 中，event 是一种 链上日志（log） 机制，它既不是函数，也不是状态。其主要作用在于：\n\n在交易执行时，向链上写入结构化日志，供外部程序（如 DApp 或 Python 脚本）监听和读取。\n\n\n三、ERC20 的标准事件\n让我们先来看 ERC20 代币合约中的两个标准事件：\nevent Transfer(address indexed from, address indexed to, uint256 value);\nevent Approval(address indexed owner, address indexed spender, uint256 value);\n这两个事件具有固定的含义：\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n事件名触发时机含义Transfer每次执行 transfer() 或 transferFrom() 时从一个地址转出代币到另一个地址Approval执行 approve() 时授权另一个地址可以代为支配你的代币\n\n四、事件是如何触发的？\n在 Solidity 中，当执行 emit 语句时，会在链上产生一个“事件日志记录”：\nemit Transfer(msg.sender, to, amount);\n这一行代码不会修改合约状态，而是向 EVM 发出指令：\n\n“请把一条 Transfer 类型的日志，记录到交易回执（receipt）的 logs 字段中。”\n\nEVM 执行完交易后：\n\n区块中会存储这笔交易；\n交易执行结果（receipt）中会包含这条日志；\n节点会为这类日志生成索引（基于 topic）。\n\n\n五、事件在以太坊节点中的存储方式\n每个事件会被记录成一个结构体，例如：\n{\n  &quot;address&quot;: &quot;0xTokenContractAddress&quot;,\n  &quot;topics&quot;: [\n      &quot;0xddf252ad...（事件签名哈希）&quot;,\n      &quot;0x0000...from&quot;,\n      &quot;0x0000...to&quot;\n  ],\n  &quot;data&quot;: &quot;0x...value编码&quot;,\n  &quot;blockNumber&quot;: 1234567,\n  &quot;transactionHash&quot;: &quot;0xabc...&quot;,\n  &quot;logIndex&quot;: 3\n}\n其中：\n\ntopics 是可索引的字段（对应 event 中加了 indexed 的参数）；\ndata 是非索引字段；\naddress 指明事件来自哪个合约；\n所有这些信息被存入区块的 receipts trie。\n\n\n六、为什么事件监听是“轮询式”的？\n在区块链中，没有中心服务器，也没有人能“主动通知”你。所有节点都是被动地等待新区块的产生。因此，外部程序（如你的 Python 服务）想知道“有没有新的 Transfer 事件”，只能通过以下方式：\n\n获取最新区块号\n检查其中的日志（logs）\n看是否匹配我感兴趣的合约和事件主题（topics）\n\n这本质上就是一种“轮询新区块”的行为。但别担心，这并不意味着“浪费算力”。\n\n七、轮询并不等于“无效计算”\n你担心的“浪费算力”问题，实际上主要取决于 监听方式的实现。现代 Web3 节点和 API 提供了几种优化手段：\n🔹1. 节点本身会索引 logs\n当你调用：\ntoken.events.Transfer.create_filter(fromBlock=&#039;latest&#039;)\n或\nw3.eth.get_logs({...})\n节点不会重新执行交易或扫描全链，而是从自己的 日志索引数据库（log index） 中筛选出匹配的事件。这只是数据库查询，而非计算执行。\n换句话说：\n\n监听日志消耗的资源更多是 I/O（查询），不是 CPU 计算。\n\n🔹2. 使用 WebSocket 订阅（实时推送）\n以太坊节点支持 eth_subscribe WebSocket 接口，允许你订阅事件：\nfrom web3 import Web3\n \nw3 = Web3(Web3.WebsocketProvider(&quot;wss://mainnet.infura.io/ws/v3/你的API_KEY&quot;))\n \ndef handle_event(event):\n    print(f&quot;Transfer: {event[&#039;args&#039;][&#039;from&#039;]} -&gt; {event[&#039;args&#039;][&#039;to&#039;]} {event[&#039;args&#039;][&#039;value&#039;]}&quot;)\n \ntoken = w3.eth.contract(address=address, abi=abi)\nevent_filter = token.events.Transfer.create_filter(fromBlock=&#039;latest&#039;)\n \nwhile True:\n    for e in event_filter.get_new_entries():\n        handle_event(e)\n这底层使用的是：\n{&quot;method&quot;: &quot;eth_subscribe&quot;, &quot;params&quot;: [&quot;logs&quot;, {...}]}\n这样做的效果是：\n\n你不需要主动轮询；\n节点在有新日志时会通过 WebSocket 主动“推”给你；\n你只需保持一个持续的连接。\n\n👉 这种方式的资源消耗几乎可以忽略（主要是保持长连接）。\n🔹3. 大型节点服务商做了事件缓存\n像 Infura、Alchemy、QuickNode 等节点服务提供商，都会在后台维护高效的事件索引数据库。因此，当你监听代币事件时：\n\n你的查询请求不会直达以太坊底层；\n而是命中这些云端的缓存索引；\n响应延迟低、资源消耗小。\n\n\n八、为什么不能“反向通知”？\n你可能会问：既然是事件，为什么不能让合约主动通知我？原因在于：\n\n智能合约没有“外部网络访问”权限。\n\nEVM 在设计上是完全封闭的：\n\n不能发 HTTP 请求；\n不能写文件；\n不能向外推送数据。\n\n因此，合约只能“写日志”，不能“发消息”。外部程序（Python、前端、服务端）需要主动去链上读取这些日志。\n\n九、Python 中如何监听这些事件？\n现在，让我们具体看看如何在 Python 中监听这些事件。\n1️⃣ 连接节点\nfrom web3 import Web3\n \n# 连接以太坊节点（可用Infura或本地节点）\nw3 = Web3(Web3.HTTPProvider(&quot;mainnet.infura.io/v3/你的API_KEY&quot;))\n2️⃣ 定义合约对象\nabi = [...]  # ERC20 合约 ABI\ncontract_address = &quot;0xYourTokenContractAddress&quot;\n \ntoken = w3.eth.contract(address=contract_address, abi=abi)\n3️⃣ 查询历史事件（离线）\n# 获取最近10000个区块的 Transfer 事件\nevents = token.events.Transfer.create_filter(\n    from_block=w3.eth.block_number - 10,\n    to_block=&#039;latest&#039;\n).get_all_entries()\n \nfor e in events:\n    print(e[&#039;args&#039;][&#039;from&#039;], e[&#039;args&#039;][&#039;to&#039;], e[&#039;args&#039;][&#039;value&#039;])\n4️⃣ 实时监听事件（在线监听）\nimport time\n \ntransfer_filter = token.events.Transfer.create_filter(from_block=&#039;latest&#039;)\n \nprint(&quot;Listening for Transfer events...&quot;)\n \nwhile True:\n    for event in transfer_filter.get_new_entries():\n        print(f&quot;📦 {event[&quot;event&quot;]}: {event[&#039;args&#039;][&#039;from&#039;]} -&gt; {event[&#039;args&#039;][&#039;to&#039;]} amount={event[&#039;args&#039;][&#039;value&#039;]}&quot;)\n    time.sleep(2)\n每当有人调用合约的 transfer() 函数，并在区块链上成功打包时，这个监听脚本就会立刻打印出这笔转账信息。\n\n十、轮询优化建议（如果你要自己实现）\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n场景建议方式效果你只关心实时事件✅ 用 WebSocket eth_subscribe几乎零轮询，节点推送你要批量查询历史✅ 用 eth_getLogs 一次批量查节点内部索引查询你要监控多个代币✅ 建立多个 Filter（或统一 Filter topics）减少重复扫描你要在自己节点上跑✅ 使用 --log-bloom-filter（Geth 参数）加速事件检索\n\n十一、总结一张图（事件流转机制）\n\n\n十二、总结一句话\n\n区块链世界没有“消息推送”，\n事件是写在区块里的“日志”，\n监听是从这些日志中筛选出你关心的部分。\n现代节点实现已经让这个过程几乎不消耗无效计算。\n\n完整的监听事件脚本\n离线获取事件\n# 获取最近10个区块的 Transfer 事件\nimport json\nfrom web3 import Web3\n \ninfura_url = f&quot;mainnet.infura.io/v3/{YOUR_INFURA_API_KEY}&quot;\nw3 = Web3(Web3.HTTPProvider(infura_url))\n \nabi_string = &#039;[{&quot;constant&quot;:true,&quot;inputs&quot;:[],&quot;name&quot;:&quot;name&quot;,&quot;outputs&quot;:[{&quot;name&quot;:&quot;&quot;,&quot;type&quot;:&quot;string&quot;}],&quot;payable&quot;:false,&quot;stateMutability&quot;:&quot;view&quot;,&quot;type&quot;:&quot;function&quot;},{&quot;constant&quot;:false,&quot;inputs&quot;:[{&quot;name&quot;:&quot;_upgradedAddress&quot;,&quot;type&quot;:&quot;address&quot;}],&quot;name&quot;:&quot;deprecate&quot;,&quot;outputs&quot;:[],&quot;payable&quot;:false,&quot;stateMutability&quot;:&quot;nonpayable&quot;,&quot;type&quot;:&quot;function&quot;},{&quot;constant&quot;:false,&quot;inputs&quot;:[{&quot;name&quot;:&quot;_spender&quot;,&quot;type&quot;:&quot;address&quot;},{&quot;name&quot;:&quot;_value&quot;,&quot;type&quot;:&quot;uint256&quot;}],&quot;name&quot;:&quot;approve&quot;,&quot;outputs&quot;:[],&quot;payable&quot;:false,&quot;stateMutability&quot;:&quot;nonpayable&quot;,&quot;type&quot;:&quot;function&quot;},{&quot;constant&quot;:true,&quot;inputs&quot;:[],&quot;name&quot;:&quot;deprecated&quot;,&quot;outputs&quot;:[{&quot;name&quot;:&quot;&quot;,&quot;type&quot;:&quot;bool&quot;}],&quot;payable&quot;:false,&quot;stateMutability&quot;:&quot;view&quot;,&quot;type&quot;:&quot;function&quot;},{&quot;constant&quot;:false,&quot;inputs&quot;:[{&quot;name&quot;:&quot;_evilUser&quot;,&quot;type&quot;:&quot;address&quot;}],&quot;name&quot;:&quot;addBlackList&quot;,&quot;outputs&quot;:[],&quot;payable&quot;:false,&quot;stateMutability&quot;:&quot;nonpayable&quot;,&quot;type&quot;:&quot;function&quot;},{&quot;constant&quot;:true,&quot;inputs&quot;:[],&quot;name&quot;:&quot;totalSupply&quot;,&quot;outputs&quot;:[{&quot;name&quot;:&quot;&quot;,&quot;type&quot;:&quot;uint256&quot;}],&quot;payable&quot;:false,&quot;stateMutability&quot;:&quot;view&quot;,&quot;type&quot;:&quot;function&quot;},{&quot;constant&quot;:false,&quot;inputs&quot;:[{&quot;name&quot;:&quot;_from&quot;,&quot;type&quot;:&quot;address&quot;},{&quot;name&quot;:&quot;_to&quot;,&quot;type&quot;:&quot;address&quot;},{&quot;name&quot;:&quot;_value&quot;,&quot;type&quot;:&quot;uint256&quot;}],&quot;name&quot;:&quot;transferFrom&quot;,&quot;outputs&quot;:[],&quot;payable&quot;:false,&quot;stateMutability&quot;:&quot;nonpayable&quot;,&quot;type&quot;:&quot;function&quot;},{&quot;constant&quot;:true,&quot;inputs&quot;:[],&quot;name&quot;:&quot;upgradedAddress&quot;,&quot;outputs&quot;:[{&quot;name&quot;:&quot;&quot;,&quot;type&quot;:&quot;address&quot;}],&quot;payable&quot;:false,&quot;stateMutability&quot;:&quot;view&quot;,&quot;type&quot;:&quot;function&quot;},{&quot;constant&quot;:true,&quot;inputs&quot;:[{&quot;name&quot;:&quot;&quot;,&quot;type&quot;:&quot;address&quot;}],&quot;name&quot;:&quot;balances&quot;,&quot;outputs&quot;:[{&quot;name&quot;:&quot;&quot;,&quot;type&quot;:&quot;uint256&quot;}],&quot;payable&quot;:false,&quot;stateMutability&quot;:&quot;view&quot;,&quot;type&quot;:&quot;function&quot;},{&quot;constant&quot;:true,&quot;inputs&quot;:[],&quot;name&quot;:&quot;decimals&quot;,&quot;outputs&quot;:[{&quot;name&quot;:&quot;&quot;,&quot;type&quot;:&quot;uint256&quot;}],&quot;payable&quot;:false,&quot;stateMutability&quot;:&quot;view&quot;,&quot;type&quot;:&quot;function&quot;},{&quot;constant&quot;:true,&quot;inputs&quot;:[],&quot;name&quot;:&quot;maximumFee&quot;,&quot;outputs&quot;:[{&quot;name&quot;:&quot;&quot;,&quot;type&quot;:&quot;uint256&quot;}],&quot;payable&quot;:false,&quot;stateMutability&quot;:&quot;view&quot;,&quot;type&quot;:&quot;function&quot;},{&quot;constant&quot;:true,&quot;inputs&quot;:[],&quot;name&quot;:&quot;_totalSupply&quot;,&quot;outputs&quot;:[{&quot;name&quot;:&quot;&quot;,&quot;type&quot;:&quot;uint256&quot;}],&quot;payable&quot;:false,&quot;stateMutability&quot;:&quot;view&quot;,&quot;type&quot;:&quot;function&quot;},{&quot;constant&quot;:false,&quot;inputs&quot;:[],&quot;name&quot;:&quot;unpause&quot;,&quot;outputs&quot;:[],&quot;payable&quot;:false,&quot;stateMutability&quot;:&quot;nonpayable&quot;,&quot;type&quot;:&quot;function&quot;},{&quot;constant&quot;:true,&quot;inputs&quot;:[{&quot;name&quot;:&quot;_maker&quot;,&quot;type&quot;:&quot;address&quot;}],&quot;name&quot;:&quot;getBlackListStatus&quot;,&quot;outputs&quot;:[{&quot;name&quot;:&quot;&quot;,&quot;type&quot;:&quot;bool&quot;}],&quot;payable&quot;:false,&quot;stateMutability&quot;:&quot;view&quot;,&quot;type&quot;:&quot;function&quot;},{&quot;constant&quot;:true,&quot;inputs&quot;:[{&quot;name&quot;:&quot;&quot;,&quot;type&quot;:&quot;address&quot;},{&quot;name&quot;:&quot;&quot;,&quot;type&quot;:&quot;address&quot;}],&quot;name&quot;:&quot;allowed&quot;,&quot;outputs&quot;:[{&quot;name&quot;:&quot;&quot;,&quot;type&quot;:&quot;uint256&quot;}],&quot;payable&quot;:false,&quot;stateMutability&quot;:&quot;view&quot;,&quot;type&quot;:&quot;function&quot;},{&quot;constant&quot;:true,&quot;inputs&quot;:[],&quot;name&quot;:&quot;paused&quot;,&quot;outputs&quot;:[{&quot;name&quot;:&quot;&quot;,&quot;type&quot;:&quot;bool&quot;}],&quot;payable&quot;:false,&quot;stateMutability&quot;:&quot;view&quot;,&quot;type&quot;:&quot;function&quot;},{&quot;constant&quot;:true,&quot;inputs&quot;:[{&quot;name&quot;:&quot;who&quot;,&quot;type&quot;:&quot;address&quot;}],&quot;name&quot;:&quot;balanceOf&quot;,&quot;outputs&quot;:[{&quot;name&quot;:&quot;&quot;,&quot;type&quot;:&quot;uint256&quot;}],&quot;payable&quot;:false,&quot;stateMutability&quot;:&quot;view&quot;,&quot;type&quot;:&quot;function&quot;},{&quot;constant&quot;:false,&quot;inputs&quot;:[],&quot;name&quot;:&quot;pause&quot;,&quot;outputs&quot;:[],&quot;payable&quot;:false,&quot;stateMutability&quot;:&quot;nonpayable&quot;,&quot;type&quot;:&quot;function&quot;},{&quot;constant&quot;:true,&quot;inputs&quot;:[],&quot;name&quot;:&quot;getOwner&quot;,&quot;outputs&quot;:[{&quot;name&quot;:&quot;&quot;,&quot;type&quot;:&quot;address&quot;}],&quot;payable&quot;:false,&quot;stateMutability&quot;:&quot;view&quot;,&quot;type&quot;:&quot;function&quot;},{&quot;constant&quot;:true,&quot;inputs&quot;:[],&quot;name&quot;:&quot;owner&quot;,&quot;outputs&quot;:[{&quot;name&quot;:&quot;&quot;,&quot;type&quot;:&quot;address&quot;}],&quot;payable&quot;:false,&quot;stateMutability&quot;:&quot;view&quot;,&quot;type&quot;:&quot;function&quot;},{&quot;constant&quot;:true,&quot;inputs&quot;:[],&quot;name&quot;:&quot;symbol&quot;,&quot;outputs&quot;:[{&quot;name&quot;:&quot;&quot;,&quot;type&quot;:&quot;string&quot;}],&quot;payable&quot;:false,&quot;stateMutability&quot;:&quot;view&quot;,&quot;type&quot;:&quot;function&quot;},{&quot;constant&quot;:false,&quot;inputs&quot;:[{&quot;name&quot;:&quot;_to&quot;,&quot;type&quot;:&quot;address&quot;},{&quot;name&quot;:&quot;_value&quot;,&quot;type&quot;:&quot;uint256&quot;}],&quot;name&quot;:&quot;transfer&quot;,&quot;outputs&quot;:[],&quot;payable&quot;:false,&quot;stateMutability&quot;:&quot;nonpayable&quot;,&quot;type&quot;:&quot;function&quot;},{&quot;constant&quot;:false,&quot;inputs&quot;:[{&quot;name&quot;:&quot;newBasisPoints&quot;,&quot;type&quot;:&quot;uint256&quot;},{&quot;name&quot;:&quot;newMaxFee&quot;,&quot;type&quot;:&quot;uint256&quot;}],&quot;name&quot;:&quot;setParams&quot;,&quot;outputs&quot;:[],&quot;payable&quot;:false,&quot;stateMutability&quot;:&quot;nonpayable&quot;,&quot;type&quot;:&quot;function&quot;},{&quot;constant&quot;:false,&quot;inputs&quot;:[{&quot;name&quot;:&quot;amount&quot;,&quot;type&quot;:&quot;uint256&quot;}],&quot;name&quot;:&quot;issue&quot;,&quot;outputs&quot;:[],&quot;payable&quot;:false,&quot;stateMutability&quot;:&quot;nonpayable&quot;,&quot;type&quot;:&quot;function&quot;},{&quot;constant&quot;:false,&quot;inputs&quot;:[{&quot;name&quot;:&quot;amount&quot;,&quot;type&quot;:&quot;uint256&quot;}],&quot;name&quot;:&quot;redeem&quot;,&quot;outputs&quot;:[],&quot;payable&quot;:false,&quot;stateMutability&quot;:&quot;nonpayable&quot;,&quot;type&quot;:&quot;function&quot;},{&quot;constant&quot;:true,&quot;inputs&quot;:[{&quot;name&quot;:&quot;_owner&quot;,&quot;type&quot;:&quot;address&quot;},{&quot;name&quot;:&quot;_spender&quot;,&quot;type&quot;:&quot;address&quot;}],&quot;name&quot;:&quot;allowance&quot;,&quot;outputs&quot;:[{&quot;name&quot;:&quot;remaining&quot;,&quot;type&quot;:&quot;uint256&quot;}],&quot;payable&quot;:false,&quot;stateMutability&quot;:&quot;view&quot;,&quot;type&quot;:&quot;function&quot;},{&quot;constant&quot;:true,&quot;inputs&quot;:[],&quot;name&quot;:&quot;basisPointsRate&quot;,&quot;outputs&quot;:[{&quot;name&quot;:&quot;&quot;,&quot;type&quot;:&quot;uint256&quot;}],&quot;payable&quot;:false,&quot;stateMutability&quot;:&quot;view&quot;,&quot;type&quot;:&quot;function&quot;},{&quot;constant&quot;:true,&quot;inputs&quot;:[{&quot;name&quot;:&quot;&quot;,&quot;type&quot;:&quot;address&quot;}],&quot;name&quot;:&quot;isBlackListed&quot;,&quot;outputs&quot;:[{&quot;name&quot;:&quot;&quot;,&quot;type&quot;:&quot;bool&quot;}],&quot;payable&quot;:false,&quot;stateMutability&quot;:&quot;view&quot;,&quot;type&quot;:&quot;function&quot;},{&quot;constant&quot;:false,&quot;inputs&quot;:[{&quot;name&quot;:&quot;_clearedUser&quot;,&quot;type&quot;:&quot;address&quot;}],&quot;name&quot;:&quot;removeBlackList&quot;,&quot;outputs&quot;:[],&quot;payable&quot;:false,&quot;stateMutability&quot;:&quot;nonpayable&quot;,&quot;type&quot;:&quot;function&quot;},{&quot;constant&quot;:true,&quot;inputs&quot;:[],&quot;name&quot;:&quot;MAX_UINT&quot;,&quot;outputs&quot;:[{&quot;name&quot;:&quot;&quot;,&quot;type&quot;:&quot;uint256&quot;}],&quot;payable&quot;:false,&quot;stateMutability&quot;:&quot;view&quot;,&quot;type&quot;:&quot;function&quot;},{&quot;constant&quot;:false,&quot;inputs&quot;:[{&quot;name&quot;:&quot;newOwner&quot;,&quot;type&quot;:&quot;address&quot;}],&quot;name&quot;:&quot;transferOwnership&quot;,&quot;outputs&quot;:[],&quot;payable&quot;:false,&quot;stateMutability&quot;:&quot;nonpayable&quot;,&quot;type&quot;:&quot;function&quot;},{&quot;constant&quot;:false,&quot;inputs&quot;:[{&quot;name&quot;:&quot;_blackListedUser&quot;,&quot;type&quot;:&quot;address&quot;}],&quot;name&quot;:&quot;destroyBlackFunds&quot;,&quot;outputs&quot;:[],&quot;payable&quot;:false,&quot;stateMutability&quot;:&quot;nonpayable&quot;,&quot;type&quot;:&quot;function&quot;},{&quot;inputs&quot;:[{&quot;name&quot;:&quot;_initialSupply&quot;,&quot;type&quot;:&quot;uint256&quot;},{&quot;name&quot;:&quot;_name&quot;,&quot;type&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;_symbol&quot;,&quot;type&quot;:&quot;string&quot;},{&quot;name&quot;:&quot;_decimals&quot;,&quot;type&quot;:&quot;uint256&quot;}],&quot;payable&quot;:false,&quot;stateMutability&quot;:&quot;nonpayable&quot;,&quot;type&quot;:&quot;constructor&quot;},{&quot;anonymous&quot;:false,&quot;inputs&quot;:[{&quot;indexed&quot;:false,&quot;name&quot;:&quot;amount&quot;,&quot;type&quot;:&quot;uint256&quot;}],&quot;name&quot;:&quot;Issue&quot;,&quot;type&quot;:&quot;event&quot;},{&quot;anonymous&quot;:false,&quot;inputs&quot;:[{&quot;indexed&quot;:false,&quot;name&quot;:&quot;amount&quot;,&quot;type&quot;:&quot;uint256&quot;}],&quot;name&quot;:&quot;Redeem&quot;,&quot;type&quot;:&quot;event&quot;},{&quot;anonymous&quot;:false,&quot;inputs&quot;:[{&quot;indexed&quot;:false,&quot;name&quot;:&quot;newAddress&quot;,&quot;type&quot;:&quot;address&quot;}],&quot;name&quot;:&quot;Deprecate&quot;,&quot;type&quot;:&quot;event&quot;},{&quot;anonymous&quot;:false,&quot;inputs&quot;:[{&quot;indexed&quot;:false,&quot;name&quot;:&quot;feeBasisPoints&quot;,&quot;type&quot;:&quot;uint256&quot;},{&quot;indexed&quot;:false,&quot;name&quot;:&quot;maxFee&quot;,&quot;type&quot;:&quot;uint256&quot;}],&quot;name&quot;:&quot;Params&quot;,&quot;type&quot;:&quot;event&quot;},{&quot;anonymous&quot;:false,&quot;inputs&quot;:[{&quot;indexed&quot;:false,&quot;name&quot;:&quot;_blackListedUser&quot;,&quot;type&quot;:&quot;address&quot;},{&quot;indexed&quot;:false,&quot;name&quot;:&quot;_balance&quot;,&quot;type&quot;:&quot;uint256&quot;}],&quot;name&quot;:&quot;DestroyedBlackFunds&quot;,&quot;type&quot;:&quot;event&quot;},{&quot;anonymous&quot;:false,&quot;inputs&quot;:[{&quot;indexed&quot;:false,&quot;name&quot;:&quot;_user&quot;,&quot;type&quot;:&quot;address&quot;}],&quot;name&quot;:&quot;AddedBlackList&quot;,&quot;type&quot;:&quot;event&quot;},{&quot;anonymous&quot;:false,&quot;inputs&quot;:[{&quot;indexed&quot;:false,&quot;name&quot;:&quot;_user&quot;,&quot;type&quot;:&quot;address&quot;}],&quot;name&quot;:&quot;RemovedBlackList&quot;,&quot;type&quot;:&quot;event&quot;},{&quot;anonymous&quot;:false,&quot;inputs&quot;:[{&quot;indexed&quot;:true,&quot;name&quot;:&quot;owner&quot;,&quot;type&quot;:&quot;address&quot;},{&quot;indexed&quot;:true,&quot;name&quot;:&quot;spender&quot;,&quot;type&quot;:&quot;address&quot;},{&quot;indexed&quot;:false,&quot;name&quot;:&quot;value&quot;,&quot;type&quot;:&quot;uint256&quot;}],&quot;name&quot;:&quot;Approval&quot;,&quot;type&quot;:&quot;event&quot;},{&quot;anonymous&quot;:false,&quot;inputs&quot;:[{&quot;indexed&quot;:true,&quot;name&quot;:&quot;from&quot;,&quot;type&quot;:&quot;address&quot;},{&quot;indexed&quot;:true,&quot;name&quot;:&quot;to&quot;,&quot;type&quot;:&quot;address&quot;},{&quot;indexed&quot;:false,&quot;name&quot;:&quot;value&quot;,&quot;type&quot;:&quot;uint256&quot;}],&quot;name&quot;:&quot;Transfer&quot;,&quot;type&quot;:&quot;event&quot;},{&quot;anonymous&quot;:false,&quot;inputs&quot;:[],&quot;name&quot;:&quot;Pause&quot;,&quot;type&quot;:&quot;event&quot;},{&quot;anonymous&quot;:false,&quot;inputs&quot;:[],&quot;name&quot;:&quot;Unpause&quot;,&quot;type&quot;:&quot;event&quot;}]&#039;\nabi = json.loads(abi_string)\ntoken = w3.eth.contract(address=&#039;0xdAC17F958D2ee523a2206206994597C13D831ec7&#039;, abi=abi)\n \nevents = token.events.Transfer.create_filter(\n    from_block=w3.eth.block_number - 10,\n    to_block=&#039;latest&#039;\n).get_all_entries()\n \nfor e in events:\n    print(e[&#039;args&#039;][&#039;from&#039;], e[&#039;args&#039;][&#039;to&#039;], e[&#039;args&#039;][&#039;value&#039;])\n通过http实时监听事件\nimport time\n \ntransfer_filter = token.events.Transfer.create_filter(from_block=&#039;latest&#039;)\n \nprint(&quot;Listening for Transfer events...&quot;)\n \nwhile True:\n    for event in transfer_filter.get_new_entries():\n        print(f&quot;📦 {event[&quot;event&quot;]}: {event[&#039;args&#039;][&#039;from&#039;]} -&gt; {event[&#039;args&#039;][&#039;to&#039;]} amount={event[&#039;args&#039;][&#039;value&#039;]}&quot;)\n    time.sleep(2)\n使用 WebSocket 订阅事件的完整示例\nimport asyncio\nfrom web3 import AsyncWeb3, WebSocketProvider\nfrom eth_abi.abi import decode\n \nSOCKET_URL = f&quot;wss://mainnet.infura.io/ws/v3/{YOUR_INFURA_API_KEY}&quot;\nCONTRACT_ADDRESS = &quot;0xdAC17F958D2ee523a2206206994597C13D831ec7&quot;\n \nasync def subscribe_to_transfer_events():\n    async with AsyncWeb3(WebSocketProvider(SOCKET_URL)) as w3:\n        transfer_event_topic = w3.keccak(text=&quot;Transfer(address,address,uint256)&quot;)\n        filter_params = {\n            &quot;address&quot;: CONTRACT_ADDRESS,\n            &quot;topics&quot;: [transfer_event_topic],\n        }\n        subscription_id = await w3.eth.subscribe(&quot;logs&quot;, filter_params)\n        print(f&quot;Subscribing to transfer events for WETH at {subscription_id}&quot;)\n \n        async for payload in w3.socket.process_subscriptions():\n            result = payload[&quot;result&quot;]\n \n            from_addr = decode([&quot;address&quot;], result[&quot;topics&quot;][1])[0]\n            to_addr = decode([&quot;address&quot;], result[&quot;topics&quot;][2])[0]\n            amount = decode([&quot;uint256&quot;], result[&quot;data&quot;])[0]\n            print(f&quot;{w3.from_wei(amount, &#039;ether&#039;)} WETH from {from_addr} to {to_addr}&quot;)\n \nasyncio.run(subscribe_to_transfer_events())"},"WEB3/Web3.py学习笔记":{"slug":"WEB3/Web3.py学习笔记","filePath":"WEB3/Web3.py学习笔记.md","title":"Web3.py学习笔记","links":[],"tags":[],"content":"Web3.py 简介 面向 Python 开发者的以太坊\n今天，我将带领大家开启 Web3.py 库的使用之旅。倘若你是一名 Python 开发者，那么 Web3.py 将是你与以太坊区块链进行交互的不二之选。在接下来这个由 6 部分构成的系列教程中，我会循序渐进地为你展示如何运用 Web3.py 与以太坊区块链展开交互。\n什么是 Web3.py？\n本视频作为系列教程的第一部分（共六部分），将首先对 Web3.py 库进行整体概述，随后演示如何查看以太坊账户的余额。\n利用以太坊开发区块链应用程序，涉及多个不同方面：\n\n开发智能合约：运用 Solidity 编程语言编写可在区块链上运行的程序。\n开发与区块链交互的客户端：编写用于从区块链读取和写入数据的代码（涵盖与智能合约的交互）。\n\nWeb3.py 能够助力你完成第二项任务，即开发与以太坊区块链交互的客户端。这里所说的“客户端”，并非特指面向用户的应用程序（如 Web 应用程序），而是指通过读取区块链信息、写入新的交易数据，或者借助智能合约执行业务逻辑，来与区块链进行交互的“客户端”。鉴于你使用 Python 编写代码，这个“客户端”可能是一个抓取区块链数据的脚本，也可能是执行智能合约函数的服务器进程。Web3.py 是一个库集合，借助它，你可以执行以下操作：创建以太坊交易、从智能合约读取和写入数据、创建智能合约等等！\n下面，让我们一同看看如何使用 Web3.py 与以太坊进行交互（如下图所示）：\n\nWeb3.py 通过 JSON RPC （远程过程调用）协议与以太坊区块链实现通信。以太坊是一个点对点的节点网络，它将所有数据分布在网络中的每个节点上。也就是说，网络上的每个节点都保存着网络上所有代码和数据的副本。Web3.py 允许我们使用 JSON RPC 代表整个网络向单个以太坊节点发出请求。如此一来，我们便能够通过单个节点读取和写入网络数据。这有点类似于向 Web 服务器上的 JSON API 发出 HTTP 请求。\n依赖项\n让我们着手安装并配置使用 Web3.py 进行开发所需的各项依赖。\nPython 环境\n首先，请确保你的系统中已安装合适版本的 Python。在本系列教程中，我使用的是 3.6.7 版本。为了更好地管理开发环境，建议你使用环境管理器，如 Python 自带的 venv 或 pyenv，你可以从官方渠道下载它们。\n要检查你的机器上是否已安装 Python，可以在终端中输入以下命令：\npython --version\n在继续后续操作前，请务必确认已安装 Python 3 版本。如果你的系统中安装了多个 Python 版本，并且需要使用 python3 命令来访问 Python 3 版本，那么在本系列教程的所有 Python 终端命令中，都请使用 python3 替代 python。\n接下来，使用 venv 或 pyenv 创建一个虚拟环境，用于安装后续的 Python 依赖项。\nWeb3.py 库安装\n在终端中，你可以使用 pip 工具来安装 Web3.py 库，命令如下：\npip install web3\nInfura RPC URL 配置\n为了通过 JSON RPC 连接到以太坊主网节点，我们需要获取访问以太坊节点的途径。有几种方式可以实现这一目标。其一，你可以使用 Geth 或 Parity 自行运行一个以太坊节点，但这需要你下载大量的区块链数据并保持节点的同步状态。如果你曾尝试过这种方式，就会知道这其中的难度和复杂性。\n更为便捷的方式是，使用 Infura 提供的服务来访问以太坊节点，而无需自己运行节点。Infura 是一项免费的服务，它提供了远程的以太坊节点。你只需注册 Infura 账号，获取 API 密钥以及要连接的网络的 RPC URL 即可。\n注册完成后，你的 Infura RPC URL 格式应如下所示：\nmainnet.infura.io/v3/YOUR_INFURA_API_KEY_GOES_HERE\n\n检查账户余额\n当所有依赖项都安装并配置完成后，你就可以开始使用 Web3.py 进行开发了！首先，在终端中启动 Python shell，命令如下：\n现在，你已经成功打开了 Python shell！在 Python shell 中，你可以按照以下方式导入 Web3.py 库：\nfrom web3 import Web3\n \ninfura_url = f&quot;mainnet.infura.io/v3/{YOUR_INFURA_API_KEY}&quot;\nweb3 = Web3(Web3.HTTPProvider(infura_url))\n现在，您已经拥有一个实时的 Web3 连接，可以与以太坊主网通信了！我们可以像这样确保连接成功：\nweb3.is_connected()\nTrue\n\n如果连接有效，则返回 true。接下来，我们可以像这样检查最新的区块号：\nweb3.eth.block_number\n23644784\n\n这是最新的区块号！现在让我们检查一下这个账户的余额： 0x90e63c3d53E0Ea496845b7a03ec7548B70014A91 。我们可以通过使用 web3.eth.get_balance() 检查余额来了解这个账户持有多少以太币。\n首先，让我们将地址分配给一个变量：\naccount = &quot;0x388C818CA8B9251b393131C08a736A67ccB19297&quot;\nbalance = web3.eth.get_balance(account)\nweb3.from_wei(balance, &quot;ether&quot;)\nDecimal(&#039;19.647278378122494711&#039;)\n\n这就是账户余额！不过，需要注意的是，这个账户余额是以 wei 表示的，wei 是 Ether 的一个分支，类似于一分钱。你必须用 10 ** 18 除以 wei 才能将其转换为 Ether。\n我还要提一下，通读 Web3.py 文档可以让你全面了解该库的功能。我强烈建议你浏览一下，即使你并不完全了解它的功能。你可以在这里找到 Web3.py 的完整文档：\nweb3py.readthedocs.io/en/stable/\n此外，请注意，Web3.py 正在积极开发中。您可以访问 Web3.py 的 GitHub 仓库来跟踪其进展，也可以阅读代码以更好地理解该库本身。您可以在此处找到 GitHub 仓库：\ngithub.com/ethereum/Web3.py/\n使用 Web3.py 从智能合约读取数据\n欢迎来到系列教程的第二集。在本集中，我将为大家详细介绍如何使用 Web3.py 从以太坊区块链读取智能合约数据。\n接下来，我们继续导入所需的库，并设置 web3 连接：\nimport json\nfrom web3 import Web3\n \ninfura_url = f&quot;mainnet.infura.io/v3/{YOUR_INFURA_API_KEY}&quot;\nweb3 = Web3(Web3.HTTPProvider(infura_url))\n为了使用 Web3.py 从智能合约中读取数据，我们需要以下两个关键要素：\n\n目标智能合约的 Python 表示\n调用智能合约函数以读取数据的方法\n\n我们可以通过 web3.eth.Contract() 函数获取以太坊智能合约的 Python 表示。该函数需要两个核心参数：智能合约的 ABI（应用二进制接口）和智能合约地址。\n智能合约 ABI（Application Binary Interface）是一个 JSON 数组，它详细描述了特定智能合约的接口信息和工作方式。以下是一个 ABI 示例：\nabi = [{&quot;constant&quot;:true,&quot;inputs&quot;:[],.....],&quot;name&quot;:&quot;Transfer&quot;,&quot;type&quot;:&quot;event&quot;}]\n（注：这是一个完整的 ABI 数组示例，结构较为复杂。此示例展示的是 OmiseGo 代币的 ABI，该代币实现了 ERC - 20 代币标准。若您对 ERC - 20 标准不熟悉，可参考相关视频进行了解。您可以在 Etherscan 上查询该代币的详细信息，包括其 ABI 和地址。在本示例的后续部分，我们将使用此智能合约 ABI。）\n在此，我将存储来自以太坊主网的 OMG 代币地址：\naddress = &quot;0xd26114cd6EE289AccF82350c8d8487fedB8A0C07&quot;\n现在，我们已经获取了 ABI 和地址这两个必要值，接下来就可以创建 OMG 代币智能合约的完整 Python 表示，代码如下：\naddress = &quot;0xd26114cd6EE289AccF82350c8d8487fedB8A0C07&quot;\nabi = json.loads(&#039;[{&quot;constant&quot;:true,&quot;inputs&quot;:[],&quot;name&quot;:&quot;mintingFinished&quot;,&quot;outputs&quot;:[{&quot;name&quot;:&quot;&quot;,&quot;type&quot;:&quot;bool&quot;}],&quot;payable&quot;:false,&quot;type&quot;:&quot;function&quot;},{&quot;constant&quot;:true,&quot;inputs&quot;:[],&quot;name&quot;:&quot;name&quot;,&quot;outputs&quot;:[{&quot;name&quot;:&quot;&quot;,&quot;type&quot;:&quot;string&quot;}],&quot;payable&quot;:false,&quot;type&quot;:&quot;function&quot;},{&quot;constant&quot;:false,&quot;inputs&quot;:[{&quot;name&quot;:&quot;_spender&quot;,&quot;type&quot;:&quot;address&quot;},{&quot;name&quot;:&quot;_value&quot;,&quot;type&quot;:&quot;uint256&quot;}],&quot;name&quot;:&quot;approve&quot;,&quot;outputs&quot;:[],&quot;payable&quot;:false,&quot;type&quot;:&quot;function&quot;},{&quot;constant&quot;:true,&quot;inputs&quot;:[],&quot;name&quot;:&quot;totalSupply&quot;,&quot;outputs&quot;:[{&quot;name&quot;:&quot;&quot;,&quot;type&quot;:&quot;uint256&quot;}],&quot;payable&quot;:false,&quot;type&quot;:&quot;function&quot;},{&quot;constant&quot;:false,&quot;inputs&quot;:[{&quot;name&quot;:&quot;_from&quot;,&quot;type&quot;:&quot;address&quot;},{&quot;name&quot;:&quot;_to&quot;,&quot;type&quot;:&quot;address&quot;},{&quot;name&quot;:&quot;_value&quot;,&quot;type&quot;:&quot;uint256&quot;}],&quot;name&quot;:&quot;transferFrom&quot;,&quot;outputs&quot;:[],&quot;payable&quot;:false,&quot;type&quot;:&quot;function&quot;},{&quot;constant&quot;:true,&quot;inputs&quot;:[],&quot;name&quot;:&quot;decimals&quot;,&quot;outputs&quot;:[{&quot;name&quot;:&quot;&quot;,&quot;type&quot;:&quot;uint256&quot;}],&quot;payable&quot;:false,&quot;type&quot;:&quot;function&quot;},{&quot;constant&quot;:false,&quot;inputs&quot;:[],&quot;name&quot;:&quot;unpause&quot;,&quot;outputs&quot;:[{&quot;name&quot;:&quot;&quot;,&quot;type&quot;:&quot;bool&quot;}],&quot;payable&quot;:false,&quot;type&quot;:&quot;function&quot;},{&quot;constant&quot;:false,&quot;inputs&quot;:[{&quot;name&quot;:&quot;_to&quot;,&quot;type&quot;:&quot;address&quot;},{&quot;name&quot;:&quot;_amount&quot;,&quot;type&quot;:&quot;uint256&quot;}],&quot;name&quot;:&quot;mint&quot;,&quot;outputs&quot;:[{&quot;name&quot;:&quot;&quot;,&quot;type&quot;:&quot;bool&quot;}],&quot;payable&quot;:false,&quot;type&quot;:&quot;function&quot;},{&quot;constant&quot;:true,&quot;inputs&quot;:[],&quot;name&quot;:&quot;paused&quot;,&quot;outputs&quot;:[{&quot;name&quot;:&quot;&quot;,&quot;type&quot;:&quot;bool&quot;}],&quot;payable&quot;:false,&quot;type&quot;:&quot;function&quot;},{&quot;constant&quot;:true,&quot;inputs&quot;:[{&quot;name&quot;:&quot;_owner&quot;,&quot;type&quot;:&quot;address&quot;}],&quot;name&quot;:&quot;balanceOf&quot;,&quot;outputs&quot;:[{&quot;name&quot;:&quot;balance&quot;,&quot;type&quot;:&quot;uint256&quot;}],&quot;payable&quot;:false,&quot;type&quot;:&quot;function&quot;},{&quot;constant&quot;:false,&quot;inputs&quot;:[],&quot;name&quot;:&quot;finishMinting&quot;,&quot;outputs&quot;:[{&quot;name&quot;:&quot;&quot;,&quot;type&quot;:&quot;bool&quot;}],&quot;payable&quot;:false,&quot;type&quot;:&quot;function&quot;},{&quot;constant&quot;:false,&quot;inputs&quot;:[],&quot;name&quot;:&quot;pause&quot;,&quot;outputs&quot;:[{&quot;name&quot;:&quot;&quot;,&quot;type&quot;:&quot;bool&quot;}],&quot;payable&quot;:false,&quot;type&quot;:&quot;function&quot;},{&quot;constant&quot;:true,&quot;inputs&quot;:[],&quot;name&quot;:&quot;owner&quot;,&quot;outputs&quot;:[{&quot;name&quot;:&quot;&quot;,&quot;type&quot;:&quot;address&quot;}],&quot;payable&quot;:false,&quot;type&quot;:&quot;function&quot;},{&quot;constant&quot;:true,&quot;inputs&quot;:[],&quot;name&quot;:&quot;symbol&quot;,&quot;outputs&quot;:[{&quot;name&quot;:&quot;&quot;,&quot;type&quot;:&quot;string&quot;}],&quot;payable&quot;:false,&quot;type&quot;:&quot;function&quot;},{&quot;constant&quot;:false,&quot;inputs&quot;:[{&quot;name&quot;:&quot;_to&quot;,&quot;type&quot;:&quot;address&quot;},{&quot;name&quot;:&quot;_value&quot;,&quot;type&quot;:&quot;uint256&quot;}],&quot;name&quot;:&quot;transfer&quot;,&quot;outputs&quot;:[],&quot;payable&quot;:false,&quot;type&quot;:&quot;function&quot;},{&quot;constant&quot;:false,&quot;inputs&quot;:[{&quot;name&quot;:&quot;_to&quot;,&quot;type&quot;:&quot;address&quot;},{&quot;name&quot;:&quot;_amount&quot;,&quot;type&quot;:&quot;uint256&quot;},{&quot;name&quot;:&quot;_releaseTime&quot;,&quot;type&quot;:&quot;uint256&quot;}],&quot;name&quot;:&quot;mintTimelocked&quot;,&quot;outputs&quot;:[{&quot;name&quot;:&quot;&quot;,&quot;type&quot;:&quot;address&quot;}],&quot;payable&quot;:false,&quot;type&quot;:&quot;function&quot;},{&quot;constant&quot;:true,&quot;inputs&quot;:[{&quot;name&quot;:&quot;_owner&quot;,&quot;type&quot;:&quot;address&quot;},{&quot;name&quot;:&quot;_spender&quot;,&quot;type&quot;:&quot;address&quot;}],&quot;name&quot;:&quot;allowance&quot;,&quot;outputs&quot;:[{&quot;name&quot;:&quot;remaining&quot;,&quot;type&quot;:&quot;uint256&quot;}],&quot;payable&quot;:false,&quot;type&quot;:&quot;function&quot;},{&quot;constant&quot;:false,&quot;inputs&quot;:[{&quot;name&quot;:&quot;newOwner&quot;,&quot;type&quot;:&quot;address&quot;}],&quot;name&quot;:&quot;transferOwnership&quot;,&quot;outputs&quot;:[],&quot;payable&quot;:false,&quot;type&quot;:&quot;function&quot;},{&quot;anonymous&quot;:false,&quot;inputs&quot;:[{&quot;indexed&quot;:true,&quot;name&quot;:&quot;to&quot;,&quot;type&quot;:&quot;address&quot;},{&quot;indexed&quot;:false,&quot;name&quot;:&quot;value&quot;,&quot;type&quot;:&quot;uint256&quot;}],&quot;name&quot;:&quot;Mint&quot;,&quot;type&quot;:&quot;event&quot;},{&quot;anonymous&quot;:false,&quot;inputs&quot;:[],&quot;name&quot;:&quot;MintFinished&quot;,&quot;type&quot;:&quot;event&quot;},{&quot;anonymous&quot;:false,&quot;inputs&quot;:[],&quot;name&quot;:&quot;Pause&quot;,&quot;type&quot;:&quot;event&quot;},{&quot;anonymous&quot;:false,&quot;inputs&quot;:[],&quot;name&quot;:&quot;Unpause&quot;,&quot;type&quot;:&quot;event&quot;},{&quot;anonymous&quot;:false,&quot;inputs&quot;:[{&quot;indexed&quot;:true,&quot;name&quot;:&quot;owner&quot;,&quot;type&quot;:&quot;address&quot;},{&quot;indexed&quot;:true,&quot;name&quot;:&quot;spender&quot;,&quot;type&quot;:&quot;address&quot;},{&quot;indexed&quot;:false,&quot;name&quot;:&quot;value&quot;,&quot;type&quot;:&quot;uint256&quot;}],&quot;name&quot;:&quot;Approval&quot;,&quot;type&quot;:&quot;event&quot;},{&quot;anonymous&quot;:false,&quot;inputs&quot;:[{&quot;indexed&quot;:true,&quot;name&quot;:&quot;from&quot;,&quot;type&quot;:&quot;address&quot;},{&quot;indexed&quot;:true,&quot;name&quot;:&quot;to&quot;,&quot;type&quot;:&quot;address&quot;},{&quot;indexed&quot;:false,&quot;name&quot;:&quot;value&quot;,&quot;type&quot;:&quot;uint256&quot;}],&quot;name&quot;:&quot;Transfer&quot;,&quot;type&quot;:&quot;event&quot;}]&#039;)\ncontract = web3.eth.contract(address=address, abi=abi)\ncontract\n&lt;web3._utils.datatypes.Contract at 0x1097afb30&gt;\n\n既然本课的第一部分已经完成，接下来我们需要完成第二部分：通过调用智能合约的函数来读取数据。所有智能合约函数都列在指定的 Web3 合约的 contract.functions 命名空间下。例如，如果合约实现了 myFunction()，我们可以调用 contract.functions.myFunction()。\n太棒了！理论上，我们可以调用智能合约实现的任何函数。但是，如何才能知道它具体实现了哪些函数呢？首先，我们可以将 contract.functions 记录到控制台，查看返回的内容（你可以在上面的视频中看到我是如何操作的）。不过，由于这个智能合约实现了 ERC - 20 标准，我们已经知道它实现了多个函数，例如 totalSupply()、name()、symbol() 和 balanceOf()。我们可以分别读取这些函数的值，代码如下：\n首先，所有 OMG 代币的总供应量：\ntotalSupply = contract.functions.totalSupply().call()\ntotalSupply\n140245398245132780789239631\n\nweb3.from_wei(totalSupply, &#039;ether&#039;)\nDecimal(&#039;140245398.245132780789239631&#039;)\n\n二、OMG 代币名称：\ncontract.functions.name().call()\n&#039;OMGToken&#039;\n\n三、OMG 代币的符号：\ncontract.functions.symbol().call()\n&#039;OMG&#039;\n\n最后，我们可以查看指定账户的余额。我在 Etherscan 上查找了一位 OMG 持有者，找到了这个地址0xd26114cd6EE289AccF82350c8d8487fedB8A0C07 。我们可以像这样查看这个账户的余额：\nbalance = contract.functions.balanceOf(&#039;0xd26114cd6EE289AccF82350c8d8487fedB8A0C07&#039;).call()\nweb3.from_wei(balance, &#039;ether&#039;)\nDecimal(&#039;35983.547643640001789023&#039;)\n\n就是这样！使用 Web3.py 从智能合约读取数据就是这么简单。\n发送以太坊交易\n欢迎来到由 6 部分组成的教程系列的第三部分。在本视频中，我将向大家详细介绍如何使用 Web3.py 在以太坊区块链上创建交易。本课程适合从初学者到高级用户的各类人群，我将从基础概念开始，逐步深入讲解。在学习的过程中，你不仅能够掌握 Web3.py 的使用方法，还能深入理解以太坊区块链上交易运作的基础知识。\n每当你在以太坊区块链上创建一笔交易时，实际上就是在向区块链写入数据并更新其状态。创建交易的方式有很多种，例如将以太币从一个账户发送到另一个账户、调用能够写入数据的智能合约函数，以及将智能合约部署到区块链上。通过使用 Web3.py 库来执行这些操作，并仔细观察每个步骤的工作原理，我们能够更加深入地理解这些概念。\n为了将交易广播到网络，有多种方法可供选择。在本教程中，为了保护私钥这一极其敏感的数据，我将使用个人开发的区块链进行演示。我强烈推荐大家使用 Foundry + Anvil 组合作为你的以太坊开发个人区块链。Foundry 是 Rust 生态下的一个强大的智能合约开发工具链，而 Anvil 则是它提供的本地 EVM 模拟器／节点。该工具在 Windows、Mac 和 Linux 系统上均可使用，并且提供了桌面应用程序和命令行工具两种形式，方便你根据自己的需求进行选择。\n以下是安装和启动 Anvil 的命令：\ncurl -L foundry.paradigm.xyz | bash\nanvil\n您可以在此处[github.com/foundry-rs/foundry]找到适合您操作系统的最新版本。下载存档包后，解压安装程序并执行设置步骤。安装完成后，每次打开它时都应该看到此屏幕：\n\n\n                             _   _\n                            (_) | |\n      __ _   _ __   __   __  _  | |\n     / _` | | &#039;_ \\  \\ \\ / / | | | |\n    | (_| | | | | |  \\ V /  | | | |\n     \\__,_| |_| |_|   \\_/   |_| |_|\n\n    1.4.3-stable (fa9f934bda 2025-10-22T05:33:20.255411000Z)\n    github.com/foundry-rs/foundry\n\nAvailable Accounts\n==================\n\n(0) 0xf39Fd6e51aad88F6F4ce6aB8827279cffFb92266 (10000.000000000000000000 ETH)\n......\n\nPrivate Keys\n==================\n\n(0) 0xac0974bec39a17e36ba4a6b4d238ff944bacb478cbed5efcae784d7bf4f2ff80\n......\n\nWallet\n==================\n......\n\n0\n\nListening on 127.0.0.1:8545\n\n太棒了！🎉现在您已经拥有一个正在运行的个人区块链网络！\n现在让我们代码连接到 anvil，而不是 Infura 节点：\nfrom web3 import Web3\n \nanvil_url = &quot;http://127.0.0.1:8545&quot;\nweb3 = Web3(Web3.HTTPProvider(anvil_url))\nweb3.is_connected()\nTrue\n\n现在从 anvil 复制前两个帐户并将它们分配给如下变量：\naccount_1 = web3.eth.accounts[0]\naccount_2 = web3.eth.accounts[1]\nprint(&quot;account_1 余额：&quot;, web3.eth.get_balance(account_1))\nprint(&quot;account_2 余额：&quot;,web3.eth.get_balance(account_2))\naccount_1 余额： 10000000000000000000000\naccount_2 余额： 10000000000000000000000\n\n接下来，从列表中获取第一个帐户的私钥。前往 anvil 的终端显示， 找到 Private Keys。复制私钥后，将其赋值给一个变量，如下所示：\nprivate_key = &#039;0xac0974...&#039;  # 替换为 account_1 的私钥\n现在让我们构建一个以太坊交易，将以太币（ETH，以太坊的加密货币）从 account_1 发送到 account_2。我们可以像这样构建一个交易字典，该字典包含将广播到区块链的所有必要信息：\nnonce = web3.eth.get_transaction_count(account_1)\n \ntx = {\n    &#039;nonce&#039;: nonce,\n    &#039;to&#039;: account_2,\n    &#039;value&#039;: web3.to_wei(1, &#039;ether&#039;),\n    &#039;gas&#039;: 2000000,\n    &#039;gasPrice&#039;: web3.to_wei(&#039;50&#039;, &#039;gwei&#039;),\n}\n \nsigned_tx = web3.eth.account.sign_transaction(tx, private_key)\ntx_hash = web3.eth.send_raw_transaction(signed_tx.raw_transaction)\nweb3.to_hex(tx_hash)\n&#039;0x007a4876a06e0e2b22f83f05e5a5b92d0000d99f5b0fed5e64197620c81adc53&#039;\n\n代码解释：\n\n\n获取 Nonce\nnonce = web3.eth.get_transaction_count(account_1)\n\nNonce 是账户发出的交易计数（从 0 开始）。\n它是一个必填字段，用于防止双重支付（double-spending）和交易重放攻击（replay attacks）。\n每次交易被确认后，nonce 会递增。\n\n\n\n构建交易对象\ntx = {\n    &#039;nonce&#039;: nonce,\n    &#039;to&#039;: account_2,\n    &#039;value&#039;: web3.to_wei(1, &#039;ether&#039;),\n    &#039;gas&#039;: 2000000,\n    &#039;gasPrice&#039;: web3.to_wei(&#039;50&#039;, &#039;gwei&#039;),\n}\n\nnonce：当前交易的 nonce（必须与账户的当前交易计数匹配）。\nto：接收 ETH 的目标地址（account_2）。\nvalue：要发送的 ETH 金额，使用 web3.to_wei() 转换单位（这里发送 1 ETH）。\ngas： gas 限制（即最多消耗的 gas 数量），这里设为 2,000,000（足够完成普通 ETH 转账）。\ngasPrice：每单位 gas 的价格（50 Gwei），影响交易被打包的速度（价格越高，矿工越优先打包）。\n\n\n\n补充说明：\n\n\ngas 和 gasPrice\n\ngas 是交易执行的计算资源上限，如果交易消耗的 gas 超过这个值，交易会失败并回滚。\ngasPrice 是你愿意为每单位 gas 支付的费用（1 Gwei = 10⁹ Wei，1 ETH = 10¹⁸ Wei）。\n你可以用 web3.eth.gas_price 获取当前推荐的 gas 价格（动态变化）。\n\n\n\n签名交易\n上述代码只是构建了交易，尚未签名。要广播交易，还需要用私钥对交易进行签名：\n  signed_tx = web3.eth.account.sign_transaction(tx, private_key)\n  tx_hash = web3.eth.send_raw_transaction(signed_tx.raw_transaction)\n（注：实际使用时请妥善保管私钥，不要硬编码在代码中！）\n\n\n动态 Gas 价格（可选）\n如果你想优化手续费，可以动态获取当前 gas 价格：\ngas_price = web3.eth.gas_price  # 自动获取网络推荐的 gas 价格\ntx[&#039;gasPrice&#039;] = gas_price\n\n\n关键点总结：\n\nNonce 确保交易顺序正确且防止重放。\nvalue 必须用 web3.to_wei() 转换单位（ETH → Wei）。\nGas 相关参数 影响交易执行成本和速度。\n实际使用时，务必通过安全方式管理私钥（如环境变量或密钥管理服务）。\n\n最后我们观察一下余额的变化\nprint(&quot;account_1 余额：&quot;, web3.eth.get_balance(account_1))\nprint(&quot;account_2 余额：&quot;,web3.eth.get_balance(account_2))\naccount_1 余额： 9998998950000000000000\naccount_2 余额： 10001000000000000000000\n\n耶！你已经成功使用 Python 将 Ether 从一个账户发送到另一个账户了！\n使用 Web3.py 调用智能合约函数\n准备工作：部署智能合约\n在调用智能合约函数之前，我们需要先将智能合约部署到区块链网络。这里我们使用 Remix IDE 进行快速部署（无需本地安装开发环境）。\n1. 使用 Remix 部署智能合约\n\n访问 Remix IDE\n创建一个新文件 Greeter.sol，粘贴以下简单合约代码：\n\n// SPDX-License-Identifier: GPL-3.0\npragma solidity &gt;=0.8.2 &lt;0.9.0;\n \ncontract Greeter {\n    string public greeting;\n \n    constructor() {\n        greeting = &quot;Hello&quot;;\n    }\n \n    function set_greeting(string memory _greeting) public {\n        greeting = _greeting;\n    }\n \n    function greet() public view returns (string memory) {\n        return greeting;\n    }\n}\n\n\n编译合约：\n\n点击左侧”Solidity编译器”标签\n确保编译器版本与合约声明匹配（^0.8.0）\n点击”Compile Greeter.sol”\n\n\n\n部署合约：\n\n切换到”部署和运行交易”标签\n在”Environment”下拉菜单中选择”Injected Provider - DEV “Foundry Provider”（测试用）\n点击”Deploy”按钮\n部署成功后，你会在”Deployed Contracts”部分看到你的合约\n\n\n\n\n2. 获取合约ABI和地址\n部署后，你需要记录：\n\n合约地址：在Remix的”Deployed Contracts”部分显示\n合约ABI：在编译后，点击”ABI”按钮复制JSON格式的ABI\n\n编译后的 ABI 如下所示（您可以复制它以方便使用）：\nabi = [\n\t\t\t{\n\t\t\t\t&quot;inputs&quot;: [],\n\t\t\t\t&quot;stateMutability&quot;: &quot;nonpayable&quot;,\n\t\t\t\t&quot;type&quot;: &quot;constructor&quot;\n\t\t\t},\n\t\t\t{\n\t\t\t\t&quot;inputs&quot;: [],\n\t\t\t\t&quot;name&quot;: &quot;greet&quot;,\n\t\t\t\t&quot;outputs&quot;: [\n\t\t\t\t\t{\n\t\t\t\t\t\t&quot;internalType&quot;: &quot;string&quot;,\n\t\t\t\t\t\t&quot;name&quot;: &quot;&quot;,\n\t\t\t\t\t\t&quot;type&quot;: &quot;string&quot;\n\t\t\t\t\t}\n\t\t\t\t],\n\t\t\t\t&quot;stateMutability&quot;: &quot;view&quot;,\n\t\t\t\t&quot;type&quot;: &quot;function&quot;\n\t\t\t},\n\t\t\t{\n\t\t\t\t&quot;inputs&quot;: [],\n\t\t\t\t&quot;name&quot;: &quot;greeting&quot;,\n\t\t\t\t&quot;outputs&quot;: [\n\t\t\t\t\t{\n\t\t\t\t\t\t&quot;internalType&quot;: &quot;string&quot;,\n\t\t\t\t\t\t&quot;name&quot;: &quot;&quot;,\n\t\t\t\t\t\t&quot;type&quot;: &quot;string&quot;\n\t\t\t\t\t}\n\t\t\t\t],\n\t\t\t\t&quot;stateMutability&quot;: &quot;view&quot;,\n\t\t\t\t&quot;type&quot;: &quot;function&quot;\n\t\t\t},\n\t\t\t{\n\t\t\t\t&quot;inputs&quot;: [\n\t\t\t\t\t{\n\t\t\t\t\t\t&quot;internalType&quot;: &quot;string&quot;,\n\t\t\t\t\t\t&quot;name&quot;: &quot;_greeting&quot;,\n\t\t\t\t\t\t&quot;type&quot;: &quot;string&quot;\n\t\t\t\t\t}\n\t\t\t\t],\n\t\t\t\t&quot;name&quot;: &quot;set_greeting&quot;,\n\t\t\t\t&quot;outputs&quot;: [],\n\t\t\t\t&quot;stateMutability&quot;: &quot;nonpayable&quot;,\n\t\t\t\t&quot;type&quot;: &quot;function&quot;\n\t\t\t}\n\t\t]\n请注意，设置了一个“默认”帐户，我们将在上面的示例中使用它与区块链进行交互。\nimport json\nfrom web3 import Web3\n \n# Set up web3 connection with anvil\nanvil_url = &quot;http://127.0.0.1:8545&quot;\nweb3 = Web3(Web3.HTTPProvider(anvil_url))\n \nweb3.eth.default_account = web3.eth.accounts[0]\n现在像这样存储智能合约地址（注意它必须是校验和格式to_checksum_address）：\ncontract_address = &quot;0xe7f1725E7734CE288F8367e1Bb143E90bb3F0512&quot;\ncontract_address = web3.to_checksum_address(contract_address)\ncontract_address\n&#039;0xe7f1725E7734CE288F8367e1Bb143E90bb3F0512&#039;\n\n接下来，我们将像这样初始化合约：\ncontract = web3.eth.contract(address=contract_address, abi=abi)\n接下来，我们可以像这样读取默认问候语：\ncontract.functions.greet().call()\n&#039;Hello&#039;\n\n最后，我们可以通过调用 set() 函数来设置新的问候语，如下所示：\n# Set a new greeting\ntx_hash = contract.functions.set_greeting(&#039;nihao&#039;).transact()\n# Wait for transaction to be mined\nweb3.eth.wait_for_transaction_receipt(tx_hash)\n# Display the new greeting value\ncontract.functions.greet().call()\n&#039;nihao&#039;\n\n使用 Web3.py 部署智能合约\n智能合约是部署在以太坊网络上的自执行程序。部署前需完成以下环境配置：\n环境准备\n安装 Solidity 编译器支持库：\npip install py-solc-x\n安装指定版本的 Solidity 编译器（示例使用 v0.8.30）：\nfrom solcx import install_solc, set_solc_version\ninstall_solc(version=&#039;0.8.30&#039;)\nset_solc_version(&#039;0.8.30&#039;)\n\n强调：set_solc_version 函数确保您使用的是正确的编译器版本。\n\n您现在应该准备好编译和部署contract。\n以下示例贯穿以下步骤：\n\n将 Solidity 合约编译为字节码和 ABI\n初始化合约实例\n使用 Contract 实例部署合约并发起交易\n使用 Contract 实例与合约函数进行交互\n\nfrom web3 import Web3\nfrom solcx import compile_source\n \n# Solidity source code\ncompiled_sol = compile_source(\n&#039;&#039;&#039;\n \n// SPDX-License-Identifier: GPL-3.0\npragma solidity &gt;=0.8.2 &lt;0.9.0;\n \ncontract Greeter {\n    string public greeting;\n \n    constructor() {\n        greeting = &quot;Hello&quot;;\n    }\n \n    function set_greeting(string memory _greeting) public {\n        greeting = _greeting;\n    }\n \n    function greet() public view returns (string memory) {\n        return greeting;\n    }\n}\n&#039;&#039;&#039;,\n    output_values=[&#039;abi&#039;, &#039;bin&#039;]\n)\n \n# retrieve the contract interface\ncontract_id, contract_interface = compiled_sol.popitem()\n# get bytecode / bin\nbytecode = contract_interface[&#039;bin&#039;]\n# get abi\nabi = contract_interface[&#039;abi&#039;]\n# web3.py instance\nw3 = Web3(Web3.EthereumTesterProvider())\n# set pre-funded account as sender\nw3.eth.default_account = w3.eth.accounts[0]\nGreeter = w3.eth.contract(abi=abi, bytecode=bytecode)\n# Submit the transaction that deploys the contract\ntx_hash = Greeter.constructor().transact()\n \n# Wait for the transaction to be mined, and get the transaction receipt\ntx_receipt = w3.eth.wait_for_transaction_receipt(tx_hash)\ngreeter = w3.eth.contract(\n    address=tx_receipt.contractAddress,\n    abi=abi\n)\ngreeter.functions.greet().call()\n&#039;Hello&#039;\n\ntx_hash = greeter.functions.set_greeting(&#039;Nihao&#039;).transact()\ntx_receipt = w3.eth.wait_for_transaction_receipt(tx_hash)\ngreeter.functions.greet().call()\n&#039;Nihao&#039;\n\n使用 Web3.py 检查区块\n如何使用 Web3.py 检查以太坊区块链的区块。分析以太坊区块链的历史记录时，检查区块通常很有用。Web3.py 有很多功能可以帮助我们做到这一点。例如，我们可以在 Etherscan 上构建类似这样的区块历史记录功能：\n\n让我们开始使用 Web3.py 提供的一些功能。这个设置比之前的课程简单得多。我们将重新连接到主网来检查那里的区块：\nfrom web3 import Web3\n \n# Fill in your infura API key here\ninfura_url = f&quot;mainnet.infura.io/v3/{YOUR_INFURA_API_KEY}&quot;\nweb3 = Web3(Web3.HTTPProvider(infura_url))\nweb3.is_connected()\nTrue\n\n首先，我们可以像这样获取最新的区块号：\nweb3.eth.block_number\n23644813\n\n我们还可以像这样获取最新区块的所有数据：\nweb3.eth.get_block(&#039;latest&#039;)\nAttributeDict({&#039;baseFeePerGas&#039;: 98176494,\n &#039;blobGasUsed&#039;: 393216,\n &#039;difficulty&#039;: 0,\n &#039;excessBlobGas&#039;: 393216,\n &#039;extraData&#039;: HexBytes(&#039;0xe29ca82051756173617220287175617361722e77696e2920e29ca8&#039;),\n &#039;gasLimit&#039;: 45043772,\n &#039;gasUsed&#039;: 39905046,\n &#039;hash&#039;: HexBytes(&#039;0x0789473cf3dfd0b0bb9a2e62b00a8615ce4c122677bd0d1f3fec99bd93460a41&#039;),\n &#039;logsBloom&#039;: HexBytes(&#039;0xffa7fbf2cde519db30f5a58df663a27197e4bc62afe1adfc3bffd9fcfd3fc2d37f9eb25de7ad74fcff574bffa97fed99ffef9fffbdfe6e37b3df23eeeafd8ab7ffbebbfcb77efdbe7cb556fdfb7aaafbfefef4b3efff9ff6db87fe7dbbe8fcde7da678edebaccfebb97fd79dea5cdffbaf21fb7db7973fe453826e7f1b3fcfda3ad7ab0e1fd3af3768ffa5aa1795d152ff0536ebe7f575ef977abdffbf9edeffe31fff67dceffadc87ff67eefedd37fa3a38fdefb0d7bf9dd3fefa7bff177ff57f2ffa2ef3fb5975d7936c7f6bf7bdfbffdf1eefdcfcecdd7cdf161ef5ad6fdef1b3bfede33ab76d29f6e7ede17dec7ffddcb670d59eeef6cdddf773e2cb79f7&#039;),\n &#039;miner&#039;: &#039;0x396343362be2A4dA1cE0C1C210945346fb82Aa49&#039;,\n &#039;mixHash&#039;: HexBytes(&#039;0xed43e9a0a48a5fa125681adb03a6c4e3590e58b11986d1c3ed691974a1c6dee9&#039;),\n &#039;nonce&#039;: HexBytes(&#039;0x0000000000000000&#039;),\n &#039;number&#039;: 23644813,\n &#039;parentBeaconBlockRoot&#039;: HexBytes(&#039;0x1c365e94c3fd9e7abdd86bdf69b83507dccaa1bebf88ffae504239002798e91f&#039;),\n &#039;parentHash&#039;: HexBytes(&#039;0xbab3a1dd833a5d9ea01cb52dd7a88e03e3c9f626ac7c4c93f3be242f3b673957&#039;),\n &#039;receiptsRoot&#039;: HexBytes(&#039;0xca0ae41cd8279c83023294d125cf8f672ac16e82d7aaa241676518ee08d4ef84&#039;),\n &#039;requestsHash&#039;: HexBytes(&#039;0xe3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855&#039;),\n &#039;sha3Uncles&#039;: HexBytes(&#039;0x1dcc4de8dec75d7aab85b567b6ccd41ad312451b948a7413f0a142fd40d49347&#039;),\n &#039;size&#039;: 101261,\n &#039;stateRoot&#039;: HexBytes(&#039;0x663860cf65ecdb107449940ce3b0353fd58eeef37df8f18b86579323b8ff246a&#039;),\n &#039;timestamp&#039;: 1761278495,\n &#039;transactions&#039;: [HexBytes(&#039;0x64b4a2bb72487e2ed563bbbccc844caf4a12e2ce3dfd9d98d49d74ab7d50103c&#039;), ......],\n &#039;transactionsRoot&#039;: HexBytes(&#039;0x3397fe7a221a5beaa73c7faab3631020fb78e60b1b55cbc62937e38583efabe0&#039;),\n &#039;uncles&#039;: [],\n &#039;withdrawals&#039;: [AttributeDict({&#039;address&#039;: &#039;0x0b26C05866e6353E46f4A7e2d10Cb42d4B583E57&#039;,\n   &#039;amount&#039;: 109998072,\n   &#039;index&#039;: 105757627,\n   &#039;validatorIndex&#039;: 2021458})......],\n &#039;withdrawalsRoot&#039;: HexBytes(&#039;0xe6c70f7dbbfd8647c8e7ba2a2031d37ca9bca09bee039f72e176f5a632f72756&#039;)})\n\n下面是以太坊（Ethereum）区块链上的区块信息的解释：\n\n🔷 一、基本区块信息（Header）\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n字段值说明number23644435区块高度（Block Number），这是第 23,644,435 个区块。hash0x69eb...7667当前区块的哈希值（SHA3），唯一标识这个区块。parentHash0x2690...07c3上一个区块的哈希，用于链接到前一个区块，形成链式结构。timestamp1761273923时间戳（Unix 时间），对应 UTC 时间：2025-10-24 10:45:23（与当前时间接近，合理）。miner0x4838...f97出块矿工/验证者的地址（PoS 时代实际是提议者 validator）。difficulty0在以太坊转向 PoS（The Merge）后，难度为 0，不再有意义。nonce0x0000000000000000PoW 非ces，PoS 中固定为 0。mixHash0x44e2...06d6PoW 相关字段，现在无实际作用。sha3Uncles0x1dcc...9347叔区块哈希，此处为空列表，所以是空哈希。extraData0x546974616e...附加数据，十六进制解码后为 ASCII：“Tian (titanbuilder.xyz)“，可能是矿池/客户端标识。\n\n🔷 二、Gas 相关参数\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n字段值说明gasLimit45,000,000当前区块允许的最大 Gas 总量。gasUsed8,951,783本区块中所有交易实际消耗的 Gas 总和，约为容量的 20%。baseFeePerGas316,260,888 wei (~0.316 gwei)EIP-1559 引入的基础 Gas 费率（每单位 Gas 的最低费用）。用户需额外支付 priorityFee 给矿工。\n\n🔷 三、状态与默克尔根（Merkle Roots）\n这些是用于验证完整性和一致性的加密哈希：\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n字段说明stateRoot状态树根哈希，表示该区块执行后的全局账户状态。transactionsRoot交易树根哈希，包含本区块所有交易。receiptsRoot交易收据树根哈希，记录每笔交易执行结果（日志、状态等）。logsBloomBloom Filter，快速判断某日志是否存在于收据中。withdrawalsRoot提款操作的默克尔根（见下文）。\n\n🔷 四、EIP-4844 Blob 相关字段（Proto-Danksharding）\n这是 EIP-4844（Blob Transactions） 引入的新特性，支持 Layer2 扩展。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n字段值说明blobGasUsed786,432本区块使用的 blob gas（约 768 KB）。每个 blob 最大 128KB，最多可容纳 6 个。excessBlobGas1,572,864超出目标 blob gas 的累计量，影响下个区块的 blobFee。requestsHash0xe3b0...b855请求列表哈希（目前为空，即 keccak256(空)），未来用于 P2P 交易广播优化。\n\n✅ 这表明该区块包含了 Blob 类型的交易（常见于 Optimism、Arbitrum 等 L2 提交数据）。\n\n\n🔷 五、交易信息\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n字段值说明transactions列表（共 128 条）包含 128 笔交易的哈希（Tx Hash），可通过 eth.getTransaction(tx_hash) 查看详情。transactionsRoot0x7c69...1145所有交易构成的 Merkle 树根。\n\n🔷 六、提款（Withdrawals）——上海升级后功能\n以太坊在 上海升级（Shanghai Upgrade） 后支持质押 ETH 提款。此区块包含 17 笔提款。\n&#039;withdrawals&#039;: [\n  AttributeDict({\n    &#039;address&#039;: &#039;0x210b3CB99FA1De0A64085Fa80E18c22fe4722a1b&#039;,\n    &#039;amount&#039;: 18081897,         # 单位：Gwei\n    &#039;index&#039;: 105751579,\n    &#039;validatorIndex&#039;: 2014771\n  }),\n  ...\n]\n解读：\n\nvalidatorIndex: 质押验证者的编号。\naddress: 提款接收地址（即提款地址）。\namount: 提款金额，单位是 Gwei（1 ETH = 10^9 Gwei）。\n示例：第一笔提款 18,081,897 Gwei ≈ 0.01808 ETH\n\n\n📌 注意：多个提款来自同一地址（0x210...a1b, 0xB23...C93），可能是同一批验证者或同一质押服务商（如 Lido、Coinbase）的操作。\n\n\n🔷 七、其他字段\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n字段说明size46,134 字节，约 45KB，表示整个区块序列化后的大小。uncles空列表 []，表示本区块没有引用任何叔区块（在 PoS 中已废弃）。parentBeaconBlockRoot指向共识层（Beacon Chain）的父区块根，体现 PoS 信标链与执行层的耦合。\n\n如果我们要构建一个像上图所示的 Etherscan 那样的区块历史记录功能，我们需要获取链中最新区块的列表。我们可以先获取最新的区块，然后倒数，直到找到链中的最后 10 个区块。我们可以使用如下的 for 循环来实现：\nlatest = web3.eth.block_number\nfor i in range(0, 10):\n  print(web3.eth.get_block(latest - i))\nAttributeDict({&#039;baseFeePerGas&#039;: 107648489, &#039;blobGasUsed&#039;: 393216, &#039;difficulty&#039;: 0, &#039;excessBlobGas&#039;: 0, &#039;extraData&#039;: HexBytes......})\n......\n\nWeb3.py 还有另一个很棒的功能，可以让你检查特定区块中包含的交易。我们可以这样做：\nhash = &#039;0x66b3fd79a49dafe44507763e9b6739aa0810de2c15590ac22b5e2f0a3f502073&#039;\nweb3.eth.get_transaction_by_block(hash, 2)\nAttributeDict({&#039;blockHash&#039;: HexBytes(&#039;0x66b3fd79a49dafe44507763e9b6739aa0810de2c15590ac22b5e2f0a3f502073&#039;),\n &#039;blockNumber&#039;: 5855123,\n &#039;from&#039;: &#039;0xF7848Afd92397fbA7b08915639faE4f29F16e4cf&#039;,\n &#039;gas&#039;: 66666,\n &#039;gasPrice&#039;: 66000000000,\n &#039;hash&#039;: HexBytes(&#039;0x2d14bc5f3ff31f312297715e73d7a04c72ae93f4466b881f2a2780b41b6e3c4f&#039;),\n &#039;input&#039;: HexBytes(&#039;0xa9059cbb00000000000000000000000099fe5d6383289cdd56e54fc0baf7f67c957a88880000000000000000000000000000000000000000000001144b9d7142dc2b0000&#039;),\n &#039;nonce&#039;: 1,\n &#039;to&#039;: &#039;0x8912358D977e123b51EcAd1fFA0cC4A7e32FF774&#039;,\n &#039;transactionIndex&#039;: 2,\n &#039;value&#039;: 0,\n &#039;type&#039;: 0,\n &#039;chainId&#039;: 1,\n &#039;v&#039;: 38,\n &#039;r&#039;: HexBytes(&#039;0xb96c587cb0ddcb147b04a72a857431660d52ad0dcb49b136652e07beee2da66a&#039;),\n &#039;s&#039;: HexBytes(&#039;0x4e1d1547b99c8c338483d1ff7521e575547f5dde85b990ebe8e6cbbf21763067&#039;)})\n\n就是这样！使用 Web3.py 检查区块就是这么简单。"},"WEB3/以太坊开发者指南（一）：开启区块链探险之旅":{"slug":"WEB3/以太坊开发者指南（一）：开启区块链探险之旅","filePath":"WEB3/以太坊开发者指南（一）：开启区块链探险之旅.md","title":"以太坊开发者指南（一）：开启区块链探险之旅","links":[],"tags":["WEB3"],"content":"以太坊开发者指南：开启区块链探险之旅\n嘿！你听说过以太坊的大名了吧？这可是科技圈的“魔法森林”。如果你已经准备好跳进这个既神秘又充满机遇的“兔子洞”，那恭喜你，旅程马上开始！\n这篇文章就像你的“冒险地图”——我们先来唠唠区块链的那些事儿，再手把手教你和以太坊节点打交道：读取区块、查看账户、发送交易，全程带你上手。过程中你还会体会到，传统的应用开发和去中心化世界到底差多远。\n放心，我们不会让你被技术术语吓跑。咱们要用的 Python 工具只是思想的“信使”，不是数学考试题。就算你不是 Python 高手，也一样能轻松跟上节奏。\n\n前置假设：装备检查\n在正式出发前，咱们先确认装备齐全：\n\n✅ 你能打开终端，内心还有点小激动。\n✅ 你写过几行 Python，知道缩进不只是美学。\n✅ 你装了 Python 3.7+，最好有个虚拟环境——给代码一个专属“安全小窝”。\n✅ 你知道 pip 是啥，它不是零食。\n\n就算你都没准备好，也完全没关系——就当读一本轻松的技术冒险小说嘛。\n\n🪙 区块链是啥？通俗讲讲\n要说以太坊的心脏，那就是“区块链”。想象一条由无数闪亮珍珠组成的项链，每颗珍珠就是一个区块。\n在 JSON 格式里，它看上去大概是这样👇\n{\n  &quot;number&quot;: 1234567,\n  &quot;hash&quot;: &quot;0xabc123...&quot;,\n  &quot;parentHash&quot;: &quot;0xdef456...&quot;,\n  &quot;miner&quot;: &quot;0xa1b2c3...&quot;,\n  &quot;transactions&quot;: [...]\n}\n每个区块都礼貌地指向前一个区块，就像一串牵手成功的积木。\n没有任何“中心大佬”在发号施令，一切靠节点之间的“默契协作”。他们验证、同步、竞争、打包交易，就像一群全网最忙碌的“区块工人”。\n当你想转点以太币给朋友时，交易会广播到整个网络，然后大家一起决定：“这笔交易靠不靠谱？”\n\n💡 小知识：以太坊的原生货币叫 Ether（以太币），区块链上记账的那份余额，就是全球唯一的“官方账本”。\n\n\n🔮 新范式登场\n去中心化开发，就像从“独轮车”升级到“飞行滑板”——概念新、速度快、自由多。\n在 Python 世界里，我们的得力助手叫 web3.py。它就像一个万能翻译官，帮你和以太坊节点愉快聊天。\n\n顺带说下，“以太坊节点”和“客户端”几乎是同义词。无论名字咋变，它们都是那个运行软件、同步区块、广播交易的“小家伙”。\n\nweb3.py 提供多种连接方式（IPC、HTTP、WebSocket），你只需选一个，让它知道该用哪种“姿势”打招呼。\n配置好后，你就能像这样和区块链说话👇\n# 读取最新区块\nw3.eth.get_block(&#039;latest&#039;)\n \n# 发起一笔交易\nw3.eth.send_transaction({&#039;from&#039;: ..., &#039;to&#039;: ..., &#039;value&#039;: ...})\n这波操作，就像让 Python 成为了一个“以太坊通灵师”。\n\n安装流程：一分钟起飞\n别怕，这次的安装比做早饭还快。我们只在命令行和 Python 解释器里折腾，不需要建项目文件夹。\n\n提醒：命令行里 $ 开头的符号是提示，不要真的打进去哟。\n\n安装 IPython，让你的交互更丝滑：\n$ pip install ipython\n安装 web3：\n$ pip install web3\n要玩模拟节点？再加个测试环境：\n$ pip install &quot;web3[tester]&quot;\n然后运行：\n$ ipython\n此刻，你就进入了一个“代码沙盒”，能直接和 Python对话。准备好了吗？那就导入主角：\nfrom web3 import Web3\n\nWeb3 模块：开发者的“瑞士军刀”\nWeb3 模块不只是“开门钥匙”，更是“修理工具包”。\n💱 单位转换魔法\n在以太坊的世界里，钱多得吓人——不仅有 Ether，还有 Gwei、Wei……\n1 Ether = 1,000,000,000,000,000,000 Wei\n想在它们之间变换？简单：\nWeb3.to_wei(1, &#039;ether&#039;)\n# 输出：1000000000000000000\n \nWeb3.from_wei(500000000, &#039;gwei&#039;)\n# 输出：Decimal(&#039;0.5&#039;)\n\n小贴士：Wei 是最小单位，18位小数的存在。换算时就像看天文数字，数零数到怀疑人生。\n\n其他神奇工具\nWeb3 还自带各种“辅助神器”：\n\nto_hex()：把数据转成十六进制，让信息更“潮”。\nis_address()：检查一个字符串是不是有效地址。\nkeccak()：给数据盖个加密印章。\n\n在 IPython 里输入 Web3. 然后敲两下 Tab，就能打开一整个“工具菜单”，爽得像拆福袋。\n\n与链对话：让代码“通灵”\n接下来，咱们要让 web3.py 真正连上以太坊节点。三种通信法宝：\n\nIPC：本地最稳，适合高玩。\nHTTP：最通用，就像慢速邮递。\nWebSocket：实时互动，像打电话。\n\n不过最推荐的，是为新手量身打造的第四种方式：\n⚡ EthereumTesterProvider：测试网里的“神奇传送门”\n这是一个本地模拟节点，内置假币 + 宽松权限，简直是“以太坊游乐园”。\nfrom eth_tester import EthereumTester\nw3 = Web3(Web3.EthereumTesterProvider())\n搞定！不需要下载、同步、等区块——你已经在链上“冲浪”了。\n\n模拟链生存指南\n健康检查\nw3.is_connected()\n如果报错，八成是你忘了加括号，就像忘带钥匙一样尴尬。\n账户预览\nw3.eth.accounts\n# 输出：[&#039;0x7E5F...&#039;, &#039;0x2B5A...&#039;, ...]\n每个账户自带 100 万测试以太币，堪称“开发者特权账户”。\nw3.from_wei(w3.eth.get_balance(w3.eth.accounts[0]), &#039;ether&#039;)\n# 输出：Decimal(&#039;1000000&#039;)\n别高兴太早，这钱花完也不能提。\n区块数据\nw3.eth.get_block(&#039;latest&#039;)\nAttributeDict({&#039;number&#039;: 0,\n &#039;hash&#039;: HexBytes(&#039;0xc25f4a44ff5e59c66931161bd15abff10600c9c11a7f7bdbe9f87461095c1dce&#039;),\n &#039;parentHash&#039;: HexBytes(&#039;0x0000000000000000000000000000000000000000000000000000000000000000&#039;),\n &#039;nonce&#039;: HexBytes(&#039;0x0000000000000000&#039;),\n &#039;sha3Uncles&#039;: HexBytes(&#039;0x1dcc4de8dec75d7aab85b567b6ccd41ad312451b948a7413f0a142fd40d49347&#039;),\n &#039;logsBloom&#039;: HexBytes(&#039;0x00&#039;),\n &#039;transactionsRoot&#039;: HexBytes(&#039;0x56e81f171bcc55a6ff8345e692c0f86e5b48e01b996cadc001622fb5e363b421&#039;),\n &#039;receiptsRoot&#039;: HexBytes(&#039;0x56e81f171bcc55a6ff8345e692c0f86e5b48e01b996cadc001622fb5e363b421&#039;),\n &#039;stateRoot&#039;: HexBytes(&#039;0xf1588db9a9f1ed91effabdec31f93cb4212b008c8b8ba047fd55fabebf6fd727&#039;),\n &#039;miner&#039;: &#039;0x0000000000000000000000000000000000000000&#039;,\n &#039;difficulty&#039;: 0,\n &#039;totalDifficulty&#039;: 0,\n &#039;mixHash&#039;: HexBytes(&#039;0x0000000000000000000000000000000000000000000000000000000000000000&#039;),\n &#039;size&#039;: 616,\n &#039;extraData&#039;: HexBytes(&#039;0x0000000000000000000000000000000000000000000000000000000000000000&#039;),\n &#039;gasLimit&#039;: 30029122,\n &#039;gasUsed&#039;: 0,\n &#039;timestamp&#039;: 1761019059,\n &#039;transactions&#039;: [],\n &#039;uncles&#039;: [],\n &#039;baseFeePerGas&#039;: 1000000000,\n &#039;withdrawals&#039;: [],\n &#039;withdrawalsRoot&#039;: HexBytes(&#039;0x56e81f171bcc55a6ff8345e692c0f86e5b48e01b996cadc001622fb5e363b421&#039;),\n &#039;parentBeaconBlockRoot&#039;: HexBytes(&#039;0x56e81f171bcc55a6ff8345e692c0f86e5b48e01b996cadc001622fb5e363b421&#039;),\n &#039;blobGasUsed&#039;: 0,\n &#039;excessBlobGas&#039;: 0,\n &#039;requestsHash&#039;: HexBytes(&#039;0xe3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855&#039;)})\n\n区块 0 是“创世区块”，区块链的“出生证明”。没有交易、没有叔叔块，干净得像新安装的系统。\n模拟转账\ntx_hash = w3.eth.send_transaction({\n   &#039;from&#039;: w3.eth.accounts[0],\n   &#039;to&#039;: w3.eth.accounts[1],\n   &#039;value&#039;: w3.to_wei(3, &#039;ether&#039;)\n})\n交易流程像极了发快递：\n\n提交交易：先发出交易，保留交易哈希（相当于快递单号）。\n等待打包：在真实网络得等几秒到几分钟，但模拟环境里——叮！立刻打包进新块！\n查收结果：用 w3.eth.get_transaction(tx_hash) 看交易详情，确认它被塞进1号块的第0笔交易（transactionIndex: 0）。\n\n最后验证“快递是否送达”：\n# 账户0余额：原本100万 - 3 = 999997 ether（注意单位转换后的精度）\nw3.eth.get_balance(w3.eth.accounts[0])\n# 输出：999996999979000000000000（实际是999997 ether，因为wei单位有18位小数）\n \n# 账户1余额：原本100万 + 3 = 1000003 ether\nw3.eth.get_balance(w3.eth.accounts[1])\n# 输出：1000003000000000000000000（实际是1000003 ether）\n看，三笔测试以太币就这么“丝滑”地从账户0溜达到账户1啦！现在是不是觉得，在模拟链上“冲浪”比想象中简单多了？\n\n🎬 尾声：勇者启程\n恭喜！你刚刚用 Python 在模拟以太坊上完成了第一次交易。\n从打开终端那一刻起，你已经不只是“代码搬砖工”，而是“链上探险家”。\n接下来，你可以：\n\n深入研究合约交互；\n用 web3.py 构建 DApp；\n甚至部署你自己的“迷你宇宙”。\n\n以太坊世界的大门已经敞开——快去创造点“链”上奇迹吧！🚀"},"WEB3/以太坊开发者指南（三）：智能合约详解":{"slug":"WEB3/以太坊开发者指南（三）：智能合约详解","filePath":"WEB3/以太坊开发者指南（三）：智能合约详解.md","title":"以太坊开发者指南（三）：智能合约详解","links":[],"tags":["WEB3"],"content":"以太坊开发者指南（三）：智能合约详解\n欢迎阅读本系列的第三部分！在第一部分中，我们介绍了区块链的基础知识；第二部分则深入探讨了以太坊账户及其如何参与网络活动。本部分将在此基础上，引入智能合约这一核心概念。\n智能合约简介\n以太坊区块链承载着巨大的价值。在之前的文章中，我们讨论了以太币如何通过交易在用户之间流转。然而，该网络的能力远不止于此，它能够实现更为复杂的交互，而这些复杂用例正是通过智能合约得以实现的。\n首先，让我们给出一个简单的定义：智能合约是部署到区块链上的代码（即计算机程序）。其流行语中的“合约”一词，体现了这些程序的相对永久性，因为它们决定了资产的转移方式；而“智能”则彰显了它们的可编程性。为简洁起见，我们通常将其简称为“合约”，本文也将沿用这一称呼。\n可以将合约和个人账户视为该系统中的两类参与者。合约能够通过编程指令与区块链进行交互，其方式与个人账户极为相似：它们可以发送和接收以太币，或与其他合约进行交互。此外，合约还能更进一步，管理一些内部状态——我们将在后续部分深入探讨这一概念。\n注意：多年来，个人账户的描述方式多种多样。外部拥有账户（EOA）是以太坊白皮书中定义的原始术语，您很可能会再次遇到这个缩写。\n合约可以根据您的需求进行复杂或简单的设置。您可以将其开放给所有人使用，也可以限制仅允许您的账户使用，或者要求用户拥有一定余额或特定资产的所有权才能进行交互。无论哪种方式，只要您的合约部署到以太坊主网，其代码都是公开的！\n公开源代码？\n在这一范式中，公开源代码是一项基本要求。用户（或其他合约）可能会利用您的合约来转移实际价值。他们需要能够相信您的合约能够按照您所宣称的方式运行，而要做到这一点，他们需要能够自行阅读您的合约代码。\n实际上，大多数用户并不会阅读他们所交互的每个智能合约的源代码。但如果源代码没有经过验证（例如，在Sourcify或Etherscan上）和由行业资深人士审查（例如，进行审计），大多数用户都会避免与该合约进行交互。\n考虑另一种情况：如果合约是一个黑匣子，那么就没有什么能够阻止恶意行为者发布看似无害但实际上却赋予他们转移用户资产能力的合约。如今，恶意行为者确实可以部署这样的合约，并试图通过社会工程学手段引诱用户，但钱包界面通常会在代码未经验证时警告用户，并提醒他们谨慎操作。\n我的商业模式怎么样？\n您是否好奇，如果所有智能合约代码都是开源的，该如何保持竞争优势？公有链确实迫使您在这方面发挥创造力。\n不过，这并不一定意味着限制会更加严格。由于每个合约都是开源的，您可以构建一个平台，供其他人在其上进行开发，或者在其他人已经完成的基础上进行进一步开发。例如，Safe是一个开源多重签名钱包，其丰富的辅助金融工具生态系统正在围绕它构建。任何人都可以构建与Safe兼容的产品，而无需获得Safe团队的任何许可。\n开源许可证也千差万别。业内另一家知名企业Uniswap推出了一款具有独特延时许可证的产品。其代码作为开源软件可立即使用，但商业复用期限为两年。这一领域必将继续探索创新的许可模式。\n合同是什么样的？\n以太坊智能合约可以用多种专门为此目的而创建的编程语言编写。每种语言都有其优缺点，但任何一种都可以满足需求；最终，代码只需编译为EVM（以太坊虚拟机）可以读取的字节码即可。流行的语言选项包括Solidity、Vyper以及新加入的Fe。\n每种语言都值得一看，但下面是一个用最成熟的语言Solidity编写的“Hello, World”风格的示例。此示例合约名为Billboard，它存储一条消息，并包含一个用于更新该消息的函数。正如所写，任何人都可以不受限制地更新该消息。\n您可以想象，一个网站可能会显示存储的所有消息，并提供一个输入框，让用户输入新消息以替换当前消息。智能合约与其用户界面的结合，就是所谓的去中心化应用程序，简称“dapp”。\n// SPDX-License-Identifier: MIT\npragma solidity 0.8.17;\n \ncontract Billboard {\n    string public message;\n    \n    constructor(string memory _message) {\n        message = _message;\n    }\n    \n    function writeBillboard(string memory _message) public {\n        message = _message;\n    }\n}\n如果您熟悉面向对象编程，那么合约看起来和类的概念非常相似。实际上，当合约被部署时，一个实例就会被提供给所有人使用——这类似于“单例”类。\n对于所有用户来说，已部署的合约在任何给定区块都具有特定的状态。换句话说，公告牌上的message对每个人来说都是相同的。合约的状态可以追踪各种事物；例如，在代币合约中，状态可能包括谁拥有多少资产。\n在Solidity合约中，constructor方法仅在合约首次部署时执行一次。继续类比，constructor可能会让您想起Python类中的__init__方法或其他语言中类似的初始化方法。因此，部署此合约的任何人都可以决定起始广告牌消息。\n您可能已经注意到，Solidity的语法与JavaScript类似，包括使用驼峰命名法、分号和内联注释。此外，它们之间也存在一些显著的差异：类型系统、编译器版本声明和新的关键字。希望这个例子足够简单，能够传达相关概念，但该语言的复杂性超出了本文的讨论范围。您可以在文末找到更多学习资源的链接。\n合约如何上链？\n在本系列的前面部分，您可能还记得读到过，更改以太坊区块链状态的唯一方法是通过交易。对于部署新合约来说，这同样适用。\n在编写合约时，开发人员会频繁地编译代码以进行手动或自动测试。每次编译的输出就是合约的字节码。\n要部署合约，只需发送一笔交易，并在交易的data字段中包含该合约的字节码，并省略to地址。EVM将处理剩下的事情。一旦交易被打包到区块中，交易收据中就会包含已部署合约的地址，以便与合约进行交互。\ntx = {\n    &quot;from&quot;: your_account,\n    &quot;data&quot;: &quot;0x60abc...&quot;,\n    ...\n} \n \nw3.eth.send_transaction(tx)\n像web3.py这样的工具提供了更人性化的方式来实现这一点。编译的另一个输出是合约的ABI和一些额外的元数据。\n注意：ABI代表应用程序二进制接口（Application Binary Interface）。简单来说，ABI是一个机器可读的数据块，它描述了合约如何与用户交互——哪些函数可用以及预期的数据类型。它是一些JSON数据，您可以将其传递给以太坊库（例如web3.py、ethers.js等），以便它能够提供人性化的界面。现在您明白这个名字的含义了吗？ABI传达了应用程序字节码的接口。\n一旦web3.py知道了合约的ABI和字节码（或者，如果已经部署，则是合约地址），该库就可以为您提供与合约交互的更直观的界面。\n# Instantiate a contract factory:\nBillboard = w3.eth.contract(abi=abi, bytecode=bytecode)\n \n# Deploy an instance of the contract:\ntx_hash = Billboard.constructor(&quot;eth very wow&quot;).transact()\n \n# Wait for the transaction to be included and get the receipt:\ntx_receipt = w3.eth.wait_for_transaction_receipt(tx_hash)\n \n# Retrieve the deployed contract instance:\nbillboard = w3.eth.contract(\n    address=tx_receipt.contractAddress,\n    abi=abi\n)\n \n# Interact with the contract instance:\nbillboard.functions.message().call()\n# eth very wow\n \nbillboard.functions.writeBillboard(&quot;sneks everywhere&quot;).transact()\n \nbillboard.functions.message().call()\n# sneks everywhere\n合同期限与数字资产管理的编程基础\n合同的有效期如何界定？\n在数字资产管理的语境下，合同的灵活性得益于区块链的编程能力。实体资产通过代币化上链已成为现实，但这一过程涉及复杂的挑战。\n数字资产标准的演进\n多年来，数字资产领域逐步形成了一系列标准化协议，为复杂合约的设计奠定了基础。其中：\n\nERC-20：定义了同质化代币（Fungible Tokens）的标准，适用于可互换资产（如稳定币）。\nERC-721：开创了非同质化代币（NFT）的先河，每个代币具有唯一性，适用于数字艺术品或收藏品。\n\n术语解析：\n\nFungible（同质化）：代币之间无差异，可自由交换（如100个ERC-20代币无顺序之分）。\nNon-Fungible（非同质化）：每个代币具有独特属性，身份至关重要（如NFT的唯一所有权）。\n\n标准背后的技术逻辑\n\nERC-20：通过智能合约维护一个公共地址列表，记录每个地址的代币余额（整数形式）。\nERC-721：在ERC-20基础上引入唯一代币ID和元数据，支持个性化属性。\n\n标准化效应：\n模块化标准的普及使得开发者能快速组合创新，例如将ERC-20与ERC-721结合，创造兼具流通性与独特性的资产。\n\n智能合约的进阶实践：继承与交互\n代码复用：通过继承简化开发\n借鉴面向对象编程的继承概念，开发者可复用已验证的合约模板。例如，OpenZeppelin提供的标准合约库允许直接继承功能：\nimport &quot;@openzeppelin/contracts/token/ERC20/ERC20.sol&quot;;\n \ncontract MyToken is ERC20 {\n    constructor() ERC20(&quot;ExampleToken&quot;, &quot;XMPL&quot;) {}\n}\n优势：\n\n无需重复实现基础逻辑（如转账、余额查询）。\n专注定制化功能开发，提升效率。\n\n合约交互与工厂模式\n智能合约不仅能调用其他合约，还可作为“工厂”动态部署新合约，或作为“代理”管理多个合约实例。这些模式为复杂应用（如去中心化交易所）提供了扩展性。\n\n可升级性与开发工具链\n可升级合约的设计模式\n尽管区块链强调不可变性，但通过代理模式等设计，合约可实现功能迭代。需权衡安全性与灵活性，选择适合项目需求的方案。\n开发者资源推荐\n\n\n入门工具：\n\nRemix：浏览器内集成开发环境，支持快速原型设计。\nSolidity示例库：按主题分类的代码片段，辅助学习。\nOpenZeppelin合约向导：可视化生成代币合约框架，支持一键导入Remix。\n\n\n\n进阶学习：\n\nCryptoZombies：游戏化Solidity教程，适合初学者。\nSpeedrun Ethereum：结合JavaScript前端的实战挑战。\nPatrick Collins课程：涵盖Python/JavaScript的全面视频教程。\n\n\n\n问题排查：\n\n以太坊Stack Exchange：社区驱动的问答平台，解决常见难题。\n\n\n\n\n从Remix到专业框架：开发环境升级\nRemix的局限性\nRemix适合快速验证概念，但在团队协作、自动化测试和规模化开发中显得不足。此时需转向专业框架：\n\nEthereum.org推荐框架：如Hardhat、Foundry等，支持多语言和复杂工作流。\nApe框架（Python生态）：为Python开发者量身定制的智能合约工具链。\n\n\nApe框架深度解析：Python开发者的福音\n核心特性\n\n插件架构：通过插件扩展编译器、节点连接和代码分析功能。\nWeb3.py集成：基于EF Python团队维护的库，确保兼容性。\n多语言支持：与Hardhat等框架类似，但专注于Python生态。\n\n为什么选择Ape？\n\n非Web场景适用：即使无需前端界面，Ape仍可高效部署和管理合约。\nUI开发灵活性：可为Ape部署的合约构建独立UI，不受工具限制。\n\n实战：通过WETH合约学习Ape\n目标：理解WETH（Wrapped Ether）合约机制，掌握Ape的三种使用模式（控制台、测试、脚本）。\n先决条件：\n\n基础Python环境（推荐虚拟环境）。\n对以太坊概念（如Gas、交易）有初步了解。\n\n操作步骤：\n\n初始化项目：\nmkdir ape-weth-demo &amp;&amp; cd ape-weth-demo\npip install eth-ape\nape init\n\n项目结构：\n\n生成contracts、scripts、tests目录及配置文件。\n保持默认目录布局，避免移动文件导致路径错误。\n\n\n\n后续步骤：\n\n在contracts目录编写WETH合约逻辑。\n使用scripts部署合约，tests验证功能。\n\n参考资源：\n\n示例代码库：[GitHub链接]\nApe官方文档：[Ape Framework Docs]\n\n通过Ape框架，Python开发者能以更低的门槛参与智能合约开发，同时保持代码的可维护性和扩展性。\n编译与部署WETH合约：Ape框架实战指南\n1. 获取WETH9合约代码\n首先从Etherscan获取经过验证的WETH9合约源码：\n\n搜索”etherscan weth”，进入合约页面\n在”Contract”选项卡复制完整源代码\n在项目contracts/目录创建WETH9.sol文件并粘贴代码\n\n版本说明：\nWETH采用开尔文版本控制（从9开始递减），这种设计旨在鼓励充分测试后再确定最终版本。当前主流使用的WETH9已通过长期验证。\n2. 配置Ape编译环境\n首次编译问题处理：\n$ ape compile\n# 报错：No compilers detected for .sol\n解决方案：\n修改ape-config.yaml添加Solidity插件：\nplugins:\n  - name: solidity\n    version: 0.8.30  # 推荐使用最新稳定版\n安装插件并重新编译：\n$ ape plugins install .\n$ ape compile\n \n100%|███████████████████████████████████████████████████████████████████████████████████| 35.7M/35.7M [00:46&lt;00:00, 768kiB/s]\nINFO:     Compiling using Solidity compiler &#039;0.8.30+commit.73712a01&#039;.\nInput:\n        contracts/WETH9.sol\nSUCCESS:  &#039;local project&#039; compiled.\n \n# 成功输出：Compiling using Solidity compiler &#039;0.8.30...&#039;\n编译结果：\n生成.build/目录，包含合约ABI和字节码等元数据（开发者无需手动处理这些文件）。\n3. Ape控制台深度探索\n启动交互式控制台：\n$ ape console\n核心功能演示：\n① 区块链状态查询：\n# 查看连接的网络\nchain  # 输出: &lt;ChainManager (id=1337)&gt;\n \n# 获取客户端版本\nchain.provider.web3.client_version  # 输出: &#039;EthereumTester/0.9.1b1...&#039;\n \n# 查询最新区块\nchain.blocks.height  # 输出: 0 (测试网初始状态)\nchain.blocks.head   # 输出: 创世区块详情\n② 账户管理操作：\n# 查看测试账户列表\naccounts.test_accounts  # 输出: [0x1e59..., 0xc89D...]\n \n# 账户间转账\nacct1 = accounts.test_accounts[0]\nacct2 = accounts.test_accounts[1]\nacct1.transfer(acct2, 100)  # 转账100wei\n \n# 验证余额\nacct2.balance  # 输出: 1000000000000000000000100 (初始1e21 + 100)\n③ 项目合约访问：\n# 获取项目合约列表\nproject.contracts  # 输出: &lt;Contracts /path/to/contracts&gt;\n \n# 加载WETH9合约（自动触发编译）\nweth = project.contracts.get(name=&quot;WETH9&quot;)\n4. WETH合约核心机制解析\n代币化原理：\nWETH9实现了ERC-20标准的以太币包装方案，核心功能包括：\n\n存款：用户发送ETH到合约，获得等量WETH\n提款：销毁WETH代币，提取等值ETH\n零费用兑换：除Gas费外无额外成本\n\n典型应用场景：\nDeFi协议通过统一ERC-20接口简化开发，例如：\n\n在Uniswap提供WETH/DAI流动性\n在Aave抵押WETH借款\n在MakerDAO生成DAI稳定币\n\n5. 插件生态扩展\n可用插件查询：\n$ ape plugins list -a\n# 显示支持Hardhat/Foundry等网络插件\n推荐插件组合：\n\nape-hardhat：增强调试和追踪功能\nape-foundry：集成Foundry测试工具链\nape-etherscan：直接验证已部署合约\n\n6. 后续部署准备\n在进入部署阶段前，建议：\n\n配置网络参数（ape-config.yaml）：\n\ndefault_ecosystem: ethereum\nnetworks:\n  local:\n    default: true\n    cmd_settings:\n      gas_limit: 8000000\n\n编写部署脚本（scripts/deploy_weth.py）：\n\n\nfrom ape import accounts, project\n \ndef run():\n    deployer = accounts.test_accounts[0]\n    weth = deployer.deploy(project.contracts.WETH9)\n    print(f&quot;WETH deployed at: {weth.address}&quot;)\n实践建议：\n\n先在本地测试网验证合约行为\n使用ape test运行单元测试\n部署到主网前通过ape-etherscan验证源码\n"},"WEB3/以太坊开发者指南（二）：开启交易的钥匙":{"slug":"WEB3/以太坊开发者指南（二）：开启交易的钥匙","filePath":"WEB3/以太坊开发者指南（二）：开启交易的钥匙.md","title":"以太坊开发者指南（二）：开启交易的钥匙","links":[],"tags":["WEB3"],"content":"以太坊开发者指南（二）：开启交易的钥匙\n欢迎回归！在本系列的第一部分中，我们通过与模拟以太坊网络的交互，深入探讨了诸多核心概念。现在，您应该对以下基本问题有了初步的认识：\n\n什么是区块链，区块中包含哪些内容？\n什么因素使得以太坊实现去中心化？\n什么是以太（Ether），为何它是网络不可或缺的组成部分？\n\n在本文中，我们将基于这些基础知识，进一步探讨它们对开发者的实际影响。如果您错过了第一部分或需要回顾，请务必返回查阅。\n下一步探索：账户\n我们将从账户入手，深入剖析如何与以太坊网络进行交互。值得注意的是，以太坊账户与Web 2.0时代的账户存在显著差异。\n注： “Web 2.0”一词最初用于描述引入用户生成内容（如社交媒体和博客）的互联网时代。而以太坊及其他去中心化技术则被视为下一代互联网——Web 3.0的基石。Web3的缩写已被Web3.js、web3.py等库及生态系统中的其他部分广泛采用。\nWeb2与Web3的对比\n在当今的互联网环境中，账户几乎无处不在。从社交媒体应用到新闻网站，从快递服务到零售商和航空公司，无一不要求用户创建账户。然而，这些账户均存储在公司的服务器上，用户必须遵守相应的条款和条件、隐私政策及安全措施。更糟糕的是，用户账户可能因公司决策而被冻结、删除、审查或修改。\nWeb3则带来了账户管理的全新范式：您，且仅您，拥有对以太坊账户的绝对控制权。创建账户时，无需任何公司授权，且账户可随您在不同应用间自由迁移。事实上，创建以太坊账户根本无需与以太坊区块链进行交互。接下来，让我们通过实践来验证这一点。\n注意： 本练习仅供学习之用。除非您充分了解安全隐患，否则请勿在账户中存储实际资产。某些错误可能无法挽回！更多背景信息将在后续介绍。\n创建账户\n账户生成\nw3 = Web3()\nacct = w3.eth.account.create()\nprint(acct.address)  # 输出示例：&#039;0xE51212f7D68f95600b502409E0f6ceD19c7a8ff8&#039;\nprint(acct.key)      # 输出示例：HexBytes(&#039;0x1b8ef920a15719317a4123ab7b8821c7a77d0d2641aa5cc74c77160617015787&#039;)\n操作如此简单！无需注册，也无需与区块链或任何服务器进行交互。事实上，即使您完全断开网络连接，依然可以创建有效的以太坊账户。\n在上述代码中，您会发现一个账户由两部分组成：一个公共地址和一个私钥。简而言之，私钥是账户的“密码”，而公共地址则是从私钥派生出来的、可共享的账号标识。如代码示例所示，两者通常均以十六进制数字表示。\n注意： 虽然以太坊用户和应用程序开发者无需完全掌握账户生成过程的工作原理，但如果您对此感兴趣，请参阅我之前的帖子：《以太坊201：助记符》和《以太坊201：HD钱包》。\n使用账户\n一个关键要点是：影响区块链更改的唯一途径是通过交易，且每笔交易都必须由账户签名。账户可发起多种交易，如转移以太币、部署智能合约或与合约交互以执行铸造新代币等操作。接下来，我们将简要探讨这三种交易类型。\n转移以太币\n回想一下，EthereumTesterProvider提供了一个包含账户和虚假以太币的测试环境。让我们先查看一些测试账户及其余额：\nw3 = Web3(Web3.EthereumTesterProvider())\nprint(w3.eth.accounts)  # 输出测试账户列表\nacct_one = w3.eth.accounts[0]\nprint(w3.eth.get_balance(acct_one))  # 输出账户余额\n接下来，我们创建一个新账户：\nacct_two = w3.eth.account.create()\nprint(acct_two.address, acct_two.key)  # 输出新账户的地址和私钥\n然后，我们将一些虚假以太币发送到新账户：\ntx_hash = w3.eth.send_transaction({\n    &#039;from&#039;: acct_one,\n    &#039;to&#039;: acct_two.address,\n    &#039;value&#039;: Web3.to_wei(1, &#039;ether&#039;)\n})\n这笔交易将立即执行，但一些关键细节被隐藏了。web3.py足够智能，它知道EthereumTesterProvider正在管理acct_one，并且我们正在使用测试环境。为了方便起见，acct_one处于“解锁”状态，这意味着该账户的交易默认会被批准（签名）。\n那么，如果不是来自解锁账户，交易会是什么样子呢？为了解答这个问题，让我们从acct_two（一个不受EthereumTesterProvider管理的账户）发送一些以太币。这个更手动的过程需要三个步骤：1）指定交易详细信息，2）签署交易，然后3）将交易广播到网络。\ntx = {\n    &#039;to&#039;: acct_one,\n    &#039;value&#039;: 10000000,\n    &#039;gas&#039;: 21000,\n    &#039;gasPrice&#039;: w3.eth.get_block(&#039;pending&#039;)[&#039;baseFeePerGas&#039;],\n    &#039;nonce&#039;: 1\n}\nsigned = w3.eth.account.sign_transaction(tx, acct_two.key)\ntx_hash = w3.eth.send_raw_transaction(signed.raw_transaction)\n让我们分解一下这个过程。第一步，定义一个包含所需交易字段的Python字典。我们在第一部分中简要了解了gas（交易费），但nonce可能是一个新概念。在以太坊中，nonce只是账户的交易计数。以太坊协议会跟踪这个值，以防止双花。\n由于这是acct_two发起的第一笔交易，其nonce值为零。如果您提供错误的值，则结果为无效交易，并会被web3.py拒绝：\nValidationError: Invalid transaction nonce: Expected 1, but got 4\n\n请注意，从acct_one发送交易时仍然需要一个随机数，但EthereumTesterProvider会跟踪管理账户的交易计数，并将适当的随机数添加到新交易中。\n您可能注意到的另一个细节是tx中缺少from值。在这种情况下，sign_transaction方法可以根据发送者的私钥推断出其地址。同样，公共地址可以通过私钥推导出来，但私钥无法通过公共地址进行逆向工程。\n最后，“原始”交易只是以字节形式表示的交易数据和签名。在底层，send_transaction执行与send_raw_transaction相同的编码。\n部署智能合约\n智能合约的完整介绍将在单独的博客文章中详细阐述，但这是一个很好的机会来提前了解一下。与智能合约的交互看起来与标准交易非常相似。\n简而言之，智能合约是运行在以太坊区块链上的程序，任何人都可以使用。当您准备部署智能合约时，您需要将代码编译为字节码，并将其作为data值包含在交易中：\nbytecode = &quot;6080604052348015610...36f6c63430006010033&quot;\n \ntx = {\n   &#039;data&#039;: bytecode,\n   &#039;value&#039;: 0,\n   &#039;nonce&#039;: 0,\n   # ...\n}\n除了需要更多gas之外，合约部署交易的唯一区别是没有to字段。其余流程与标准的ETH转账相同。\n与智能合约交互\n使用已部署的合约只是同一交易格式的另一种变体。在这种情况下，to值指向已部署合约的地址，而data值将根据正在执行的合约方法的输入而变化。\n请注意，web3.py等工具提供了用于部署和与合约交互的更直观的界面：\n# 与已存在合约交互:\nmyContract = web3.eth.contract(address=address, abi=abi)\ntwentyone = myContract.functions.multiply7(3).call()\n    \n# 部署新合约:\nExample = w3.eth.contract(abi=abi, bytecode=bytecode)\ntx_hash = Example.constructor().transact()\n签名消息\n交易是影响区块链状态的唯一途径，但并非账户的唯一用途。仅仅证明某个账户的所有权本身就很有用。\n例如，以太坊市场OpenSea允许您通过使用您的账户签名消息来竞标待售物品。只有当拍卖到期或卖家接受您的出价时，才会进行实际交易。同样，该应用程序在向您显示一些账户详细信息之前，也会使用签名消息作为一种身份验证形式。\n与交易不同，签名消息无需任何成本。它们不会广播到网络，也不会被打包到区块中。消息只是用私钥签名的一段数据。正如您所料，发送者的私钥是隐藏的，但接收者可以通过数学方法证明发送者的公开地址。换句话说，消息发送者无法被伪造。\n注意： “链上”和“链下”这两个术语是指数据是否位于以太坊区块链上。例如，智能合约状态在链上管理，但消息签名在链下进行。\n我们将在以后的文章中深入探讨消息签名，但这里有一些伪代码可以让您了解工作流程：\n# 1. 编写消息\nmsg = &quot;amanaplanacanalpanama&quot;\n \n# 2. 使用账户的私钥签名\npk = b&quot;...&quot;\nsigned_message = sign_message(message=msg, private_key=pk)\n \n# 3. 发送`signed_message`\n \n# 4. 消息接收者解码发送者的公共地址\nsender = decode_message_sender(msg, signed_message.signature)\nprint(sender)\n# 输出示例：&#039;0x5ce9454...b9aB12E&#039;\nWeb3账户对开发者的影响\n我们可以非常轻松地创建以太坊账户：离线且独立于任何应用程序。这些账户可用于签署消息或发送各种类型的交易。这对应用程序开发者意味着什么呢？\n永久密码与无恢复服务\n现实是残酷的：基本账户类型没有密码恢复服务。如果您丢失了私钥和恢复短语，那么您只能与这个账户说再见了。这才是真正所有权的双刃剑。应用程序开发者有道德义务引导以太坊新手了解这一现实。\n注： 社交恢复钱包可能会改善用户体验，“账户抽象化”是一项相关的研究工作——关于此主题，请另文探讨。\n入职挑战\n向新用户介绍以太坊并非易事。正如您所了解的，许多范式转变并非显而易见。您可能需要引导尚未拥有以太坊账户的访客，或者没有以太币支付交易费的用户。所需的教育材料数量取决于您的受众，但如果您能够优雅地引导新用户，整个生态系统都将受益。\n账户管理功能较少\n鉴于用户在您的应用程序之外创建账户，您可能会发现您的用例几乎不需要或根本不需要账户管理功能。\n新的商业模式\n数据挖掘不会消失，但这种新的账户所有权模式为Web 2.0模式提供了一种健康的替代方案。在Web 2.0模式中，公司拥有用户的所有数据，并将其出售给出价最高的人。而以太坊的智能合约平台则提供了一个全新的激励机制。\n新的软件架构\n在定义您的商业模式时，一个有趣的问题是如何处理链上和链下的数据。正如我们所讨论的，消息签名不需要链上交互。此外，您也可以使用私有数据库存储部分数据，并使用以太坊区块链存储其他数据或功能。有很多权衡需要考虑：可用性、成本、透明度、去中心化、隐私等等。\n您可以创建的账户数量没有限制，您可以自由地将同一个账户用于多个应用，也可以为每个应用创建一个新账户。这正是公链被称为“无需许可”的原因之一：您和网络之间没有守门人。无需等待任何人授予您创建网络的权限。\n当您准备好继续前进时，第三部分将介绍系统中的下一个参与者：智能合约。"},"WEB3/在以太坊上编写自己的加密货币（如何构建-ERC-20-代币）":{"slug":"WEB3/在以太坊上编写自己的加密货币（如何构建-ERC-20-代币）","filePath":"WEB3/在以太坊上编写自己的加密货币（如何构建 ERC-20 代币）.md","title":"在以太坊上编写自己的加密货币（如何构建 ERC-20 代币）","links":[],"tags":["WEB3"],"content":"在以太坊上编写自己的加密货币：构建 ERC-20 代币\n以太坊及整个区块链生态系统，广泛采用 ERC20 代币开展各类商业活动。任何人都有机会创建代币，并将其作为加密货币使用。在本教程中，我们将借助 Python 和 Solidity 来部署 ERC20 代币。即便你对这些语言并不熟悉，也能轻松跟上教程的步伐。\nERC-20 代币标准\n什么是代币？\n在以太坊的世界里，代币的用途极为广泛，几乎可以代表任何事物：\n\n在线平台信誉积分：例如在特定社区中，用户通过完成任务或参与互动积累的积分。\n游戏角色技能：游戏内角色所具备的独特技能，以代币形式体现其价值。\n金融资产：类似公司股份的资产，在区块链上实现数字化流转。\n法定货币：如与美元等法定货币等值的代币。\n实物资产：像一盎司黄金这样的实物，通过代币化实现便捷交易。\n更多可能：代币的潜力无限，还可代表其他各种价值。\n\n以太坊的这一强大特性，需要强有力的标准来规范，而 ERC-20 标准正是为此而生。它为开发者构建可与其他产品和服务互操作的代币应用程序提供了规范。此外，ERC-20 标准还为以太币赋予了附加功能。\n什么是 ERC-20？\nERC-20 定义了同质化代币的标准。所谓同质化，即每个代币在类型和价值上完全相同。例如，一个 ERC-20 代币就如同以太币一样，任何时候，一个代币的价值与其他代币始终保持一致。\n什么是 ERC-20 代币？\n以太坊区块链赋予用户创建自有加密货币或代币的能力，这些代币可使用以太坊区块链的原生加密货币——以太币进行购买。ERC-20 仅仅是一个标准，它明确了这些代币的行为规范，确保它们能够与其他平台（如加密货币交易所）实现无缝兼容。\n要理解 ERC-20 代币的运作机制，我们首先需要了解以太坊区块链的运作原理。\nERC20 的全称为“以太坊征求意见稿 20”。征求意见稿是社交团体对想法进行同行评审的一种方式。以太坊通过鼓励人们提出想法并征求他人意见，持续优化其生态系统。这与以太坊改进提案（EIP）略有不同，尽管两者存在差异，但有时人们会互换使用这两个术语。\n当人们提及 ERC20 时，实际上指的是第 20 个以太坊征求意见稿，它由 Fabian Vogelsteller 和 Vitalik Buterin 共同创建。\n此次征求意见稿为在以太坊平台上创建“代币”提供了标准化途径。所有代币都必须遵循这一标准，以便平台和工程师能够轻松使用，而无需进行重复开发。\n想象一下，如果每个代币都采用独特的功能来实现代币转移，那么围绕这些代币构建协议将变得极为复杂。而 ERC20 代币则如同可互换的部件，通过 Solidity 实现的函数实现统一操作。\n这些 ERC20 代币遵循相同的模式，具备相同的功能列表，这些功能在本质上是一致的。其中一些关键功能包括：\n\ntransfer：实现代币在所有者之间的转移。\nbalanceOf：用于查看特定地址持有的代币数量。\ntransferFrom：允许非所有者将代币从一个地址转移到另一个地址。\napprove：批准合约调用 transferFrom 函数。\n\n在最简单的形式上，ERC20 代币是一种合约，用于追踪区块链上每个地址所拥有的某个价值单位的数量。它们通过 Solidity 中的 mappings 来实现这一功能。\n简单来说，ERC20 是代表代币的智能合约。\nERC20 代币的典型示例\nERC20 代币的一些典型示例包括 LINK、AAVE、USDT 和 DAI。你会发现，某些代币（如 LINK 代币）实际上是 ERC20 的更高级形式。特别是 LINK，它是一种 ERC677 代币，增加了与预言机配合使用的额外功能，但它仍然向后兼容 ERC20 代币，并具备 ERC20 的所有功能。\n另一个流行的代币标准是 ERC777，它相较于 ERC20 有一些显著的质量改进，并且同样向后兼容 ERC20。\n制作 ERC20 代币的用途\nERC20 代币的用途多种多样，以下列举一些最常见的用途：\n1. 治理\n治理代币允许用户对协议的未来发展方向进行投票，并持有相应的权益。例如，UNI、AAVE 和 CRV 就是典型的治理代币。这些代币可以“质押”到平台中，用于投票并提出新的发展方向，供协议参考和决策。\n2. 确保网络安全\n网络或协议通常会跨多条区块链运行，而整个协议的安全性不应仅仅依赖于单一的底层区块链。协议通常需要与对协议更有意义的底层资产进行交易，以将资产与网络无关的其他资产的市场波动相隔离。\n如果协议依赖于底层链，那么当底层市场波动性恶化时，网络的安全性可能会受到威胁。\n3. 合成资产和稳定币\n如果你拥有某种喂价机制，就可以轻松地创建带有抵押品支持的合成资产。这是一种接触和交易区块链生态系统之外资产的有效方式。这正是 Synthetix 协议的工作原理，它将 Chainlink 喂价机制与 SNX 质押的抵押品相结合，让 DeFi（去中心化金融）投资者能够接触到传统金融世界。\n同样地，稳定币也是一种合成资产，只不过它们代表的是像美元这样的“稳定”资产。Tether、USDC 和 Dai 就是稳定币的典型例子。\n4. 其他用途\n在思考为什么要创建 ERC20 代币时，创造力是唯一的限制。我们已经看到一些协议使用 ERC20 代币作为底层抵押资产进行质押，为使用协议的用户提供奖励（参见收益耕作），围绕代币创建经济体系，以及实现更多创新应用。\nERC-20 规范深度解析：智能合约接口与函数实现\nERC-20 规范作为以太坊生态中最广泛采用的代币标准，其核心在于定义了智能合约必须遵循的接口规范。这一标准不仅明确了智能合约的基本结构，还详细规定了代币合约必须实现的功能类型，同时提供了一系列虽非强制但极具实用价值的推荐功能。尤为关键的是，ERC-20 规范强制要求代币合约实现特定事件，如 transfer 事件，这些事件如同智能合约的“广播系统”，允许外部应用订阅并响应代币交易等关键行为。通过这一机制，开发者可以轻松构建出能够实时感知代币交易的应用，极大提升了区块链应用的交互性和实用性。\nERC-20 核心接口与事件机制\nERC-20 规范的核心在于其定义的接口和事件。智能合约必须实现这些接口以确保与钱包、交易所等外部系统的兼容性。同时，规范要求合约在特定操作发生时触发相应事件，如 transfer 事件在代币转移时触发。这种设计模式不仅提高了系统的透明度，还为外部监控和审计提供了便利。\n方法\nfunction name() public view returns (string)\nfunction symbol() public view returns (string)\nfunction decimals() public view returns (uint8)\nfunction totalSupply() public view returns (uint256)\nfunction balanceOf(address _owner) public view returns (uint256 balance)\nfunction transfer(address _to, uint256 _value) public returns (bool success)\nfunction transferFrom(address _from, address _to, uint256 _value) public returns (bool success)\nfunction approve(address _spender, uint256 _value) public returns (bool success)\nfunction allowance(address _owner, address _spender) public view returns (uint256 remaining)\n事件\nevent Transfer(address indexed _from, address indexed _to, uint256 _value)\nevent Approval(address indexed _owner, address indexed _spender, uint256 _value)\ntransfer 函数：代币转移的核心实现\ntransfer 函数是 ERC-20 代币合约中最为关键的函数之一，它负责管理用户之间的代币转移。以下是一个符合 ERC-20 标准的 transfer 函数示例实现，该函数详细展示了代币转移的全过程：\ncontract ERC20Token {\n    // 存储每个地址的代币余额\n    mapping(address =&gt; uint256) public balanceOf;\n    \n    // Transfer 事件定义\n    event Transfer(address indexed from, address indexed to, uint256 value);\n \n    /**\n     * @dev 将代币从调用者的账户转移到指定地址\n     * @param _to 接收代币的地址\n     * @param _value 要转移的代币数量\n     * @return 成功返回 true\n     */\n    function transfer(address _to, uint256 _value) public returns (bool success) {\n        // 检查发送者是否有足够的余额\n        require(balanceOf[msg.sender] &gt;= _value, &quot;Insufficient balance&quot;);\n        \n        // 更新余额：从发送者账户扣除，向接收者账户增加\n        balanceOf[msg.sender] -= _value;\n        balanceOf[_to] += _value;\n        \n        // 触发 Transfer 事件，通知外部系统代币已转移\n        emit Transfer(msg.sender, _to, _value);\n        \n        return true;\n    }\n}\ntransfer 函数实现要点解析\n\n存在性验证：函数必须存在于合约中，这是 ERC-20 标准的基本要求。\n参数正确性：函数接受两个参数——接收者地址 _to 和转移数量 _value，这两个参数的类型和顺序必须严格符合规范。\n余额检查：在执行转移前，函数会检查发送者的余额是否足够。如果余额不足，交易将被回滚，并返回错误信息“Insufficient balance”。\n余额更新：函数通过修改 balanceOf 映射来更新发送者和接收者的余额，确保账本的准确性。\n事件触发：函数在完成余额更新后，会触发 Transfer 事件。这一事件可以被外部系统（如钱包、交易所）订阅，从而实现实时监控和通知功能。\n返回值：函数返回 true 表示转移成功，这是 ERC-20 标准规定的返回值。\n\n通过这一精心设计的函数实现，ERC-20 代币合约能够确保代币转移的安全、可靠和透明，为以太坊生态中的代币经济提供了坚实的基础。\n如何创建 ERC20\n我们只需要合约的应用程序二进制接口 (ABI) 来创造一个 ERC-20 代币界面。 下面我们将使用一个简化的应用程序二进制接口，让例子变得更为简单。\n首先，请确保你已安装 Web3.py Python 库：\n\npip install web3\n\n然后用anvil创建一个本地以太坊节点用来开发测试。\n安装 foundryup：\n\ncurl -L foundry.paradigm.xyz | bash\n\n运行anvil：\n\nanvil\n\nfrom web3 import Web3\n \nanvil_url = &quot;http://127.0.0.1:8545&quot;\nw3 = Web3(Web3.HTTPProvider(anvil_url))\nw3.is_connected()\nTrue\n创建两个测试账户\nfrom eth_account import Account\n \n# 生成两个本地账户（随机私钥）\nac1 = Account.create()\nac2 = Account.create()\n# 用默认账户给新账户转账一些以太币，以便支付部署合约的燃气费。\n \nfunder = w3.eth.accounts[0]\ntx = {\n    &#039;from&#039;: funder,\n    &#039;to&#039;: ac1.address,\n    &#039;value&#039;: w3.to_wei(1, &#039;ether&#039;),\n    &#039;gas&#039;: 21000,\n    &#039;nonce&#039;: w3.eth.get_transaction_count(funder),\n}\ntx_hash = w3.eth.send_transaction(tx)\nw3.eth.wait_for_transaction_receipt(tx_hash)\nw3.eth.get_balance(ac1.address)\n1000000000000000000\n\n部署合约\n先写一个简单的ERC20合约，然后部署它。\n创建一个MyToken.sol文件，内容如下：\npragma solidity ^0.8.0;\n \nimport &quot;@openzeppelin/contracts/token/ERC20/ERC20.sol&quot;;\n \ncontract MyToken is ERC20 {\n    constructor() public ERC20(&quot;MyToken&quot;, &quot;MT&quot;) {\n        _mint(msg.sender, 1000000000000000000000000);\n    }\n}\nopenzeppelin-contracts仓库中包含ERC20合约。从github上克隆该仓库。\n用python的solcx库构建智能合约\n首先先引入solcx库，并编译合约文件\nfrom solcx import install_solc, set_solc_version\ninstall_solc(version=&#039;0.8.30&#039;)\nset_solc_version(&#039;0.8.30&#039;)\nfrom solcx import compile_files\ncompiled_sol = compile_files([\n    &quot;contracts/MyToken.sol&quot;\n], import_remappings=[&quot;@openzeppelin=~/code/web3/openzeppelin-contracts&quot;])\ncontract_id, contract_interface = compiled_sol.popitem()\n# get bytecode / bin\nbytecode = contract_interface[&#039;bin&#039;]\n# get abi\nabi = contract_interface[&#039;abi&#039;]\n编译完成后，我们可以使用Web3.py来部署合约。用ac1作为我们的代币的创建者。\n他会拥有_mint函数中写的1000000000000000000000000\nMyTokenContract = w3.eth.contract(abi=abi, bytecode=bytecode)\n \n# 构造部署交易\nconstruct_txn = MyTokenContract.constructor().build_transaction({\n    &#039;from&#039;: ac1.address,\n    &#039;nonce&#039;: w3.eth.get_transaction_count(ac1.address),\n    &#039;gasPrice&#039;: w3.to_wei(&#039;2&#039;, &#039;gwei&#039;),\n    &#039;chainId&#039;: w3.eth.chain_id,\n})\n \n# 估算 gas 并补上\nconstruct_txn[&#039;gas&#039;] = w3.eth.estimate_gas(construct_txn)\n# 签名 + 发送\nsigned_txn = w3.eth.account.sign_transaction(construct_txn, private_key=ac1.key)\ntx_hash = w3.eth.send_raw_transaction(signed_txn.raw_transaction)\ntx_receipt = w3.eth.wait_for_transaction_receipt(tx_hash)\n这样合约就部署到链上了，我们可以使用合约地址和ABI与之交互。\nmy_token = w3.eth.contract(\n    address=tx_receipt.contractAddress,\n    abi=abi\n)\n# 查询我们的代币名称\nmy_token.functions.name().call()\n&#039;MyToken&#039;\n\n# 查询我们的发行量\nmy_token.functions.totalSupply().call()\n1000000000000000000000000\n\nprint(f&quot;账户1的余额： {my_token.functions.balanceOf(ac1.address).call()}&quot;)\nprint(f&quot;账户2的余额： {my_token.functions.balanceOf(ac2.address).call()}&quot;)\n账户1的余额： 1000000000000000000000000\n账户2的余额： 0\n\n用账户1给账户2转1个代币\n# 1. 构建交易\ntx = my_token.functions.transfer(ac2.address, 1).build_transaction({\n    &#039;from&#039;: ac1.address,\n    &#039;nonce&#039;: w3.eth.get_transaction_count(ac1.address),\n    &#039;gasPrice&#039;: w3.to_wei(&#039;2&#039;, &#039;gwei&#039;),\n    &#039;gas&#039;: 100000,  # 足够高的 gas limit\n})\n2. 签名交易\nsigned_tx = w3.eth.account.sign_transaction(tx, private_key=ac1.key)\n3. 发送交易\ntx_hash = w3.eth.send_raw_transaction(signed_tx.raw_transaction)\n4. 等待交易确认\ntx_receipt = w3.eth.wait_for_transaction_receipt(tx_hash)\ntx_receipt\n    AttributeDict({&#039;type&#039;: 0,\n     &#039;status&#039;: 1,\n     &#039;cumulativeGasUsed&#039;: 52117,\n     &#039;logs&#039;: [AttributeDict({&#039;address&#039;: &#039;0x11416eE8d5D8Fb1dC1d3a1DfD719935AA638Dd9c&#039;,\n       &#039;topics&#039;: [HexBytes(&#039;0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef&#039;),\n        HexBytes(&#039;0x000000000000000000000000948ff0e70a5f90db5b8bbc4815c1fc089add01b7&#039;),\n        HexBytes(&#039;0x0000000000000000000000001e062370e60f8fe748180e3a838f454e66496e3e&#039;)],\n       &#039;data&#039;: HexBytes(&#039;0x0000000000000000000000000000000000000000000000000000000000000001&#039;),\n       &#039;blockHash&#039;: HexBytes(&#039;0xbe1c47de397b7a1ca0e57dfaf66ccc60643aea85544ff7bf8a64963857708a05&#039;),\n       &#039;blockNumber&#039;: 9,\n       &#039;blockTimestamp&#039;: &#039;0x68ff2dfd&#039;,\n       &#039;transactionHash&#039;: HexBytes(&#039;0x8c418450045ca3f5147b76b9dd402eb70245f0da13b00e0a50a86bc55ddc3596&#039;),\n       &#039;transactionIndex&#039;: 0,\n       &#039;logIndex&#039;: 0,\n       &#039;removed&#039;: False})],\n     &#039;logsBloom&#039;: HexBytes(&#039;0x00000000000000000000000000000000000000000000000000000000000000010000000000000000008000000000000000000000000000000000000000000000000000000000000000000008000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000010000000000020000000000000000000000000001000000000000000000000000000000000000000040000000000200000000000000000000000000000000000000001000000000002000000000000000000000000000000000800000000000000000000000000000000000000000000000000000002000000000000000000000000000000&#039;),\n     &#039;transactionHash&#039;: HexBytes(&#039;0x8c418450045ca3f5147b76b9dd402eb70245f0da13b00e0a50a86bc55ddc3596&#039;),\n     &#039;transactionIndex&#039;: 0,\n     &#039;blockHash&#039;: HexBytes(&#039;0xbe1c47de397b7a1ca0e57dfaf66ccc60643aea85544ff7bf8a64963857708a05&#039;),\n     &#039;blockNumber&#039;: 9,\n     &#039;gasUsed&#039;: 52117,\n     &#039;effectiveGasPrice&#039;: 2000000000,\n     &#039;blobGasPrice&#039;: 1,\n     &#039;from&#039;: &#039;0x948ff0E70a5F90dB5b8BBC4815C1Fc089add01B7&#039;,\n     &#039;to&#039;: &#039;0x11416eE8d5D8Fb1dC1d3a1DfD719935AA638Dd9c&#039;,\n     &#039;contractAddress&#039;: None})\n\nprint(f&quot;账户1的余额： {my_token.functions.balanceOf(ac1.address).call()}&quot;)\nprint(f&quot;账户2的余额： {my_token.functions.balanceOf(ac2.address).call()}&quot;)\n账户1的余额： 999999999999999999999999\n账户2的余额： 1\n"},"index":{"slug":"index","filePath":"index.md","title":"欢迎来到我的博客","links":[],"tags":[],"content":"——探索 [Web3] × [AI] 的无限可能"},"python笔记/Pandas-简介":{"slug":"python笔记/Pandas-简介","filePath":"python笔记/Pandas 简介.md","title":"Pandas 简介","links":[],"tags":["机器学习","Pandas"],"content":"这个是学习tensorflow前的准备，\npandas 是一种列存数据分析 API。它是用于处理和分析输入数据的强大工具，很多机器学习框架都支持将 pandas 数据结构作为输入。 虽然全方位介绍 pandas API 会占据很长篇幅，但它的核心概念非常简单，我们会在下文中进行说明。有关更完整的参考，请访问 pandas 文档网站，其中包含丰富的文档和教程资源。\n基本概念\n导入pandas 并输出版本\nfrom __future__ import print_function\nimport pandas as pd\nprint(pd.__version__)\n\n0.23.4\npandas中的主要数据结构被时限为一下两类：\nDataFrame： 一个关系型数据表格，其中包含多行和已命名的列，就像excel一样\nSeries：它是单独的一列，DataFrame中包含一个或多个Series，每个Series都有一个名称。就像我们写个表格在第一列写上每一行代表什么一样。\n\n数据框架是用于数据操控的一种常用抽象实现形式，spark中的rdd，数据库中的table 类似。\n创建Series的一种方法是构建Series对象。列入：\npd.Series([&#039;Beijing&#039;, &#039;Shanghai&#039;, &#039;Shenzhen&#039;])\n你可以将映射string列名称的dict传递到它们各自的Series，从而创建DataFrame对象。如果Series在长度上不一致，系统会用特殊的NA值填充缺失的值。\ncity_names = pd.Series([&#039;Beijing&#039;, &#039;Shanghai&#039;, &#039;Shenzhen&#039;])\npopulation = pd.Series([21534678, 23541023, 120456])\n \ndata = pd.DataFrame({&#039;City name&#039;: city_names, &#039;Population&#039;: population})\nprint(data)\n  City name  Population\n0   Beijing    21534678\n1  Shanghai    23541023\n2  Shenzhen      120456\n\n大多数情况下，我们需要把整个文件加载到DataFrame中，下面我们加载一个包含加利福尼亚州住房的数据文件。并创建特征定义，通过head方法浏览DataFrame前几个纪录\ncalifornia_housing_dataframe = pd.read_csv(&quot;download.mlcc.google.cn/mledu-datasets/california_housing_train.csv&quot;, sep=&quot;,&quot;)\ncalifornia_housing_dataframe.describe()\nprint(california_housing_dataframe.head())\n   longitude  latitude         ...          median_income  median_house_value\n0    -114.31     34.19         ...                 1.4936             66900.0\n1    -114.47     34.40         ...                 1.8200             80100.0\n2    -114.56     33.69         ...                 1.6509             85700.0\n3    -114.57     33.64         ...                 3.1917             73400.0\n4    -114.57     33.57         ...                 1.9250             65500.0\n\n[5 rows x 9 columns]\n\npandas的另一个强大的功能是绘图制表，借助DataFrame.hist，可以快速了解一个列中值的分布。pandas使用的画图库是matplotlib所以我们也可以使用这个库中的方法来操作图表。\nimport matplotlib.pyplot as plt\nhist = california_housing_dataframe.hist(&#039;housing_median_age&#039;)\nplt.show()\n\n访问数据\n可以使用 dict 或list 的方法来访问DataFrame数据\ncities = pd.DataFrame({&#039;City name&#039;: city_names, &#039;Population&#039;: population})\nprint(type(cities[&#039;City name&#039;]))\nprint(cities[&#039;City name&#039;])\n \nprint(type(cities[&#039;City name&#039;][1]))\nprint(cities[&#039;City name&#039;][1])\n \nprint(type(cities[0:2]))\nprint(cities[0:2])\n&lt;class &#039;pandas.core.series.Series&#039;&gt;\n0     Beijing\n1    Shanghai\n2    Shenzhen\nName: City name, dtype: object\n\n&lt;class &#039;str&#039;&gt;\nShanghai\n\n&lt;class &#039;pandas.core.frame.DataFrame&#039;&gt;\n  City name  Population\n0   Beijing    21534678\n1  Shanghai    23541023\n\n操控数据\n可以向series应用Python的基本用算指令。\npopulation / 1000\n0    21534.678\n1    23541.023\n2      120.456\n\nNumPy是一个用于科学计算的常用工具包。pandas series可作用大多数NumPy函数的参数。\nimport numpy as np\nnp.log(population)\n\n0    13.655892\n1    13.831172\n2    13.092314\ndtype: float64\n\n对于更加复杂的单列转换，可以使用Series.apply。像Python映射函数一样，Series.apply将以参数形式接受lambda函数，而该函数会应用与每个值，下面的例子是创建一个population是否超过一定数值的series。\nprint(population.apply(lambda val: val &gt; 1000000))\n \n0     True\n1     True\n2    False\ndtype: bool\nDataFrames的修改方式也非常简单。例如，一下代码向现有的DataFrame添加了两个Series。\ncities[&#039;Area square miles&#039;] = pd.Series([98.87, 176.53, 46.92])  # 随便写的数\ncities[&#039;Population density&#039;] = cities[&#039;Population&#039;] / cities[&#039;Area square miles&#039;]\nprint(cities)\n\n  City name         ...          Population density\n0   Beijing         ...               217808.010519\n1  Shanghai         ...               133354.234408\n2  Shenzhen         ...                 2567.263427\n\n练习1\n通过添加一个新的布尔值列，修改cities表格\n城市以sh开头\n城市面积大于50 （上面数都是我随便写的）\n\n注意：布尔值 Series 1 辑与时，应使用 &amp;，而不是 and。\ncities[&#039;Is wide and has Sh name&#039;] = (cities[&#039;Area square miles&#039;] &gt; 50) &amp; cities[&#039;City name&#039;].apply(lambda name: name.startswith(&#039;Sh&#039;))\n\nprint(cities)\n\n  City name           ...             Is wide and has Sh name\n0   Beijing           ...                               False\n1  Shanghai           ...                                True\n2  Shenzhen           ...                               False\n\n[3 rows x 5 columns]\n\n索引\nSeries和DataFrame对象也定义了index属性，改属性向每个Series项或DataFrame行赋一个标识符值。默认情况下，在构造时，pandas会赋可反应数据源数据顺序的索引值。索引值在创建后时稳定的；也就是说，他们不会因为数据重新排序而发生改变。\nprint(city_names.index)\nRangeIndex(start=0, stop=3, step=1)\n\nprint(cities.index)\nRangeIndex(start=0, stop=3, step=1)\n\nprint(cities.reindex([2, 0, 1]))\n  City name           ...             Is wide and has Sh name\n2  Shenzhen           ...                               False\n0   Beijing           ...                               False\n1  Shanghai           ...                                True\n\n[3 rows x 5 columns]\n\nprint(cities.reindex(np.random.permutation(cities.index)))\n  City name           ...             Is wide and has Sh name\n0   Beijing           ...                               False\n2  Shanghai           ...                                True\n1  Shenzhen           ...                               False\n\n[3 rows x 5 columns]\n\n练习2\nreindex方法允许使用未包含在原始DataFrame索引值中的索引值。请示一下，看看如果使用此类值会发生什么。\n如果reindex输入数组包含原始DataFrame索引值中没有的值，reindex会为此类“丢失的”索引添加新行，并在所有对应列中填充NaN值\ncities.reindex([0,4,5,2])\n  City name           ...             Is wide and has Sh name\n0   Beijing           ...                               False\n4       NaN           ...                                 NaN\n5       NaN           ...                                 NaN\n2  Shenzhen           ...                               False\n\n[4 rows x 5 columns]\n\n这种行为是可取的，因为索引通常是从实际数据中提取的字符串，在这种情况下，如果容许出现“丢失的”索引，将可以轻松的使用外部列表重建索引，因为我们不必担心将输入清理掉。"},"python笔记/Python-进度条-tqdm":{"slug":"python笔记/Python-进度条-tqdm","filePath":"python笔记/Python 进度条 tqdm.md","title":"Python 进度条 tqdm","links":[],"tags":["python库"],"content":"tqdm是Python中专门用于进度条美化的模块，通过在非while的循环体内嵌入tqdm，可以得到一个能更好展现程序运行过程的提示进度条，本文就将针对tqdm的基本用法进行介绍。\n基本用法\ntqdm()的使用非常简单，只要传入一个迭代器就可以了，例如range()。\nfrom tqdm import tqdm\nimport time\n \nfor c in tqdm([&#039;a&#039;, &#039;b&#039;, &#039;c&#039;, &#039;d&#039;, &#039;e&#039;]):\n    time.sleep(1)\n100%|██████████| 5/5 [00:05&lt;00:00,  1.00s/it]\n\nfor it in tqdm(range(10)):\n    time.sleep(1)\n100%|██████████| 10/10 [00:10&lt;00:00,  1.00s/it]\n\ntqdm 还提供了tqdm(range())的简单版本，trange()\nfrom tqdm import trange\n \nfor i in trange(10):\n    time.sleep(1)\n100%|██████████| 10/10 [00:10&lt;00:00,  1.00s/it]\n\ntqdm为jupyter提供了一个效果更好的进度条，在jupyter里我们可以使用这个效果更好。\nfrom tqdm import tqdm_notebook\n \nfor i in tqdm_notebook(range(100),desc=&#039;demo：&#039;):\n    time.sleep(1)\n\nok，结束。"},"python笔记/Python实现K-均值算法":{"slug":"python笔记/Python实现K-均值算法","filePath":"python笔记/Python实现K-均值算法.md","title":"Python实现K-均值算法","links":[],"tags":["机器学习","python"],"content":"第一个无监督学习算法，K-均值，这是一个非常普及的聚类算法，实现起来也比较简单，学习了Andrew Ng的视频讲解，直接纪录一下重点吧。\n首先训练集合选取了sklearn自带的多类单标签数据集make_blobs\n初始化变量有m:训练集的个数，Feature:训练集的维度，K：要分成几类，u：一个K*Feature维度的数组，储存聚类中心，c:储存每次迭代的分类结果，uDict:储存分类结果的字典\n总结：因为数据量比较少，根据观察畸变函的结果数，基本迭代三次就分类成功了，说明这是一个非常优秀的算法。\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets.samples_generator import make_blobs\nfrom sklearn.cluster import KMeans\nimport random\n \n# 构建单标签数据集\ncenter = [[1,1],[-1,-1],[1,-1]]\ncluster_std = 0.3\nX,labels = make_blobs(n_samples=200, centers=center, n_features=2, cluster_std=cluster_std, random_state=0)\nunique_lables = set(labels)\ncolors=plt.cm.Spectral(np.linspace(0, 1, len(unique_lables)))\nfor k,col in zip(unique_lables, colors):\n    x_k=X[labels==k]\n    plt.plot(x_k[:,0], x_k[:,1], &#039;o&#039;, markerfacecolor=col, markeredgecolor=&quot;k&quot;, markersize=14)\n \n##########\nFeature = 2  # 特征数\nm = 200  # 训练数据个数\n# 初始化K 和 聚类中心u\nK = 3\nu = np.empty([K, Feature])\n \nfor i in range(K):\n    u[i] = random.choice(X)\n \n#  c 储存分类结果和距离\nc = np.zeros([m,2])\n \n# 画出初始聚类中心\nt = np.transpose(u)\nplt.plot(t[0], t[1], &#039;+&#039;, markerfacecolor=&#039;g&#039;, markeredgecolor=&quot;k&quot;, markersize=14)\n \n# 储存分类结果的字典\nuDict = {}\n \n \n# 移动聚类中心\ndef MoveK(c):\n    u = np.empty([K, Feature])\n    for i in range(K):\n        uDict[i] = []\n    for i in range(m):\n        for j in range(K):\n            if(c[i][0] == j):\n                uDict[j].append(X[i])\n \n    for i in range(K):\n        sum = np.zeros([1, Feature])\n        for j in uDict[i]:\n            sum = np.add(sum, j)\n        u[i] = sum/len(uDict[i])\n    return u\n \n \n# 畸变函数 Distortion function\ndef Distortion(u):\n    sum = 0\n    for i in uDict.keys():\n        for j in uDict[i]:\n            dis = np.linalg.norm(j - u[i])\n            sum += dis * dis\n    return sum/m\n \n \n# 开始迭代\nfor t in range(5):\n    # 我希望找到 c[i](代表第i个数据) 距离 u[k]（聚类中心） 最小\n    for i in range(m):\n        flag = True\n        for j in range(K):\n            dis = np.linalg.norm(X[i]-u[j])\n            if(flag or dis &lt; c[i][1]):\n                flag = False\n                c[i][0] = j\n                c[i][1] = dis\n    u = MoveK(c)\n    print(Distortion(u))\n \n# 验证结果\nprint(&quot;my kemans cluster enters:&quot;, u)\nkmeans = KMeans(n_clusters=3, random_state=0).fit(X)\nkmeans_u = kmeans.cluster_centers_\nprint(&quot;sklearn kemans cluster enters:&quot;, kmeans_u)\n \nt = np.transpose(u)\nplt.plot(t[0], t[1], &#039;*&#039;, markerfacecolor=&#039;blue&#039;, markeredgecolor=&quot;k&quot;, markersize=14)\nplt.show()\n执行结果：\n0.8093064467708514\n0.2770795968584342\n0.17288024424551154\n0.17288024424551154\n0.17288024424551154\nmy kemans cluster enters: [[ 0.95712283 -1.02057236]\n[ 1.01281413  1.06595402]\n[-1.03507066 -1.03233287]]\nsklearn kemans cluster enters: [[ 0.95712283 -1.02057236]\n[ 1.01281413  1.06595402]\n[-1.03507066 -1.03233287]]\n\n算法成功的从+号的位置移动到五角星的位置。"},"python笔记/Python实现PCA降维算法":{"slug":"python笔记/Python实现PCA降维算法","filePath":"python笔记/Python实现PCA降维算法.md","title":"Python实现PCA降维算法","links":[],"tags":["机器学习","python"],"content":"这是第二个无监督学习的算法，是一个降维算法，可以把多个特征进行压缩，我在压缩后计算了与原数据的偏差，当我把四个特征压缩为三个时偏差只有0.5%，压缩为一个特征时偏差也只有7%，当只有一个特征时把数据展开也可以轻易的分为三类，所以这是一个非常优秀的算法。\n值得注意的点是在计算奇异矩阵时遇到的问题，首先我们有一个m×n（m个数据，n个特征）的矩阵X，我们希望得到一个m×k的矩阵Z，具体降维过程分三步：\n·第一步：均值归一化，就是把每一个数都减去总数的平均值，得到的一个和平均数差距的新矩阵Xj。\n·第二部：计算协方差矩阵，在这里要注意的时，Xj(i)是一个n×1的矩阵，Xj(i)的转置是一个1×n的矩阵，所以他俩相乘得到一个n×n的矩阵Σ，其实就是的到一个奇异矩阵，因为只有奇异矩阵才可能有特征值。\n·第三部：奇异值分解，计算∑的特征值，使用svd()函数分解出U,S,V三个向量，U也是一个n×n的矩阵，在U中选取k个向量，获得一个n×k的矩阵Ureduce，新的特征矩阵z就等于Ureduce的转置(k×n)乘以X(n×m)结果得到一个k×m的新矩阵\n\n#!/usr/bin/python\n# coding=utf-8\nfrom sklearn.datasets import load_iris\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport numpy as np\nfrom sklearn.cluster import KMeans\n \n \nclass MYPCA:\n    def __init__(self, data, k):\n        self.m = len(data)  # 训练数据个数\n        self.n = len(data[0])  # 现在的特征数\n        self.k = k  # 优化后的特征数\n        self.X = data\n \n    # 第一步是均值归一化。我们需要计算出所有特征的均值\n    def data_preprocess(self):\n        sum = 0\n        for i in self.X:\n            sum += i\n        u = sum/self.m\n        self.newX = np.empty([0, self.n])\n        for i in self.X:\n            self.newX = np.row_stack((self.newX, i - u))\n        return self.newX\n \n    # 第二步计算协方差矩阵 传入均值归一化后的矩阵 Σ=1𝑚Σ(𝑥(𝑖))𝑛𝑖=1(𝑥(𝑖))𝑇\n    def covariance_matrix(self, X):\n        sum = 0\n        for i in X:\n            i = i[np.newaxis, :]\n            sum += np.dot(i.T, i)\n        sigma = sum/self.m\n        return sigma\n \n    # 计算新的特征向量Z\n    def get_z(self, U, X):\n        z = np.empty([self.k, 0])\n        Ureduce = U[...,0:self.k]\n        for i in X:\n            i = i[np.newaxis, :]\n            t = np.dot(Ureduce.T, i.T)\n            z = np.column_stack((z, t))\n        return z\n \n    # 计算训练集误差\n    def error_analysis(self):\n        S = self.S\n        sigmaK = 0\n        sigmaN = 0\n        for i in range(self.n):\n            if i &lt; self.k:\n                sigmaK += S[i]\n            if i &lt; self.n:\n                sigmaN += S[i]\n \n        return 1 - sigmaK/sigmaN\n \n    # 恢复到之前维度\n    def rovecor_dimensional(self):\n        Ureduce = self.U[..., 0:self.k]\n        Xappox = np.dot(Ureduce, self.z)\n        return Xappox\n \n    def train(self):\n        newX = self.data_preprocess()\n        sigma = self.covariance_matrix(newX)\n        self.U, self.S, self.V = np.linalg.svd(sigma)\n        # 这里使用均值归一化后的X和原X对结果没有影响\n        #self.z = self.get_z(self.U, self.X)\n        self.z = self.get_z(self.U, newX)\n        return self.z.T\n \n \n# 构造训练集：引入鸢尾花数据集来作为训练集, 具有四个特征,分三类\niris = load_iris()\ndata = iris.data\ndata = np.array(data[:])\nm = len(data)\n#np.random.shuffle(data)\n \n# 把四个特征压缩为三个\nirispca = MYPCA(data, 3)\nz = irispca.train()\nerror = irispca.error_analysis()\nprint(error)\nx1 = z[:, [0]]\nx2 = z[:, [1]]\nx3 = z[:, [2]]\nfig = plt.figure()\nax = fig.add_subplot(111, projection=&#039;3d&#039;)\nax.scatter(x1, x2, x3, c=&#039;r&#039;, marker=&#039;*&#039;)\nax.set_xlabel(&#039;x1 Label&#039;)\nax.set_ylabel(&#039;x2 Label&#039;)\nax.set_zlabel(&#039;x3 Label&#039;)\nplt.show()\n \n# 把四个特征压缩为一个\nirispca = MYPCA(data, 1)\nz = irispca.train()\nplt.plot(z, &#039;.&#039;)\nerror = irispca.error_analysis()\nprint(error)\n \n# 使用Kmeans的算法验证一下是否还可以正确分类\nkmeans = KMeans(n_clusters=3, random_state=0).fit(z)\nkmeans_u = kmeans.cluster_centers_\nu = np.transpose(kmeans_u)\nplt.plot([m/6, m/2, 5*m/6], u[0], &#039;*&#039;, markerfacecolor=&#039;g&#039;, markeredgecolor=&quot;k&quot;, markersize=14)\nplt.show()\n\n"},"python笔记/Python实现异常检测算法":{"slug":"python笔记/Python实现异常检测算法","filePath":"python笔记/Python实现异常检测算法.md","title":"Python实现异常检测算法","links":[],"tags":["机器学习","python"],"content":"应用高斯分布开发异常检测算法，这个比较简单，高斯分布也叫做正态分布，高中就学过，如果我们的数据符合高斯分布或者比较像高斯分布的时候可以使用这个算法，通过训练集计算高斯分布函数，与交叉验证集比较设置合适的Σ，当测试数据小于Σ时则为异常\n#!/usr/bin/python\n# coding=utf-8\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.io as sio\n \ndata = sio.loadmat(&#039;./data/ex8data1.mat&#039;);\nX = data[&#039;X&#039;] # 训练集\nXval = data[&#039;Xval&#039;] # 交叉验证集\nYval = data[&#039;yval&#039;]\nX1 = X[:, [0]]\nX2 = X[:, [1]]\n \nfig, ax = plt.subplots(figsize=(12,8))\nax.scatter(X1, X2)\n# plt.plot(Xval[:, [0]], Xval[:, [1]], &#039;.&#039;, markerfacecolor=&#039;g&#039;, markeredgecolor=&quot;k&quot;, markersize=14)\nplt.show()\n \n \n# 计算平均数 𝜇 和 方差 𝜎2\ndef aveandvar(x):\n    sum = np.array([0])\n    for i in x: sum = sum + i\n    ave = sum/len(x)\n    sum = 0\n    for i in x: sum += (i - ave)*(i - ave)\n    var = sum/len(x)\n    return ave, var\n \n \n# 计算概率密度p(x); 特征集, 平均值, 平方差\ndef gaussian_distribution(x, u, s):\n    px = []\n    for i in x:\n        p = 1/(np.sqrt((2 * np.pi * s))) * np.exp(-((i - u) * (i - u))/(2 * s))\n        px.append(p)\n    px = np.array(px)\n    return px\n \n# 选择阈值\ndef select_threshold(pval, yval):\n    best_epsilon = 0\n    best_f1 = 0\n    f1 = 0\n \n    step = (pval.max() - pval.min()) / 10000\n \n    for epsilon in np.arange(pval.min(), pval.max(), step):\n        preds = pval &lt; epsilon\n \n        tp = np.sum(np.logical_and(preds == 1, yval == 1)).astype(float)\n        fp = np.sum(np.logical_and(preds == 1, yval == 0)).astype(float)\n        fn = np.sum(np.logical_and(preds == 0, yval == 1)).astype(float)\n \n        precision = tp / (tp + fp)\n        recall = tp / (tp + fn)\n        f1 = (2 * precision * recall) / (precision + recall)\n \n        if f1 &gt; best_f1:\n            best_f1 = f1\n            best_epsilon = epsilon\n \n    return best_epsilon, best_f1\n \n \n# 画一下这两个特征值的高斯曲线 首先要排下序\nX1.T.sort()\nX2.T.sort()\nu,s = aveandvar(X1)\npx1 = gaussian_distribution(X1, u, s)\nu,s = aveandvar(X2)\npx2 = gaussian_distribution(X2, u, s)\n \nplt.subplot(211)\nplt.title(&#039;X1&#039;)\nplt.plot(X1, px1 )\nplt.subplot(212)\nplt.title(&#039;X2&#039;)\nplt.plot(X2, px2 )\nplt.show()\n \n# 计算训练集\nu,s = aveandvar(X)\npx = gaussian_distribution(X, u, s)\n \n# 计算测试集\nu,s = aveandvar(Xval)\ntpx = gaussian_distribution(X, u, s)\n \nepsilon, f1 = select_threshold(tpx, Yval)\nprint(epsilon, f1)\n \n# 标记出异常数据\noutliers = np.where(px &lt; epsilon)\nfig, ax = plt.subplots(figsize=(12,8))\nax.scatter(X[:,0], X[:,1])\nax.scatter(X[outliers[0],0], X[outliers[0],1], s=50, color=&#039;r&#039;, marker=&#039;o&#039;)\nplt.show()\n \n# 调库验证\nfrom scipy import stats\npx = np.zeros((X.shape[0], X.shape[1]))\npx[:,0] = stats.norm(u[0], s[0]).pdf(X[:,0])\npx[:,1] = stats.norm(u[1], s[1]).pdf(X[:,1])\noutliers = np.where(px &lt; epsilon)\nfig, ax = plt.subplots(figsize=(12,8))\nax.scatter(X[:,0], X[:,1])\nax.scatter(X[outliers[0],0], X[outliers[0],1], s=50, color=&#039;r&#039;, marker=&#039;o&#039;)\nplt.show()\n这是我们的训练集合，明显有六个是异常数据\n\n画出连个特征的高斯函数，比较像高斯分布\n\n通过我自己写的高斯密度函数计算，有些过拟合，多拟合到了两个点，不知道为什么。\n\n调用scipy的高斯函数库计算后完美的检测到了异常数据\n"},"python笔记/Python实现梯度下降":{"slug":"python笔记/Python实现梯度下降","filePath":"python笔记/Python实现梯度下降.md","title":"Python实现梯度下降","links":[],"tags":["机器学习","python"],"content":"看了Andrew Ng的关于机器学习中梯度下降的学习，用最简单粗暴的解法实现下\n注意的地方就是$\\theta_0,\\theta_1$是同时更新的，所以用一个临时变量接了下\n收敛条件的判断：可以让函数迭代指定的次数后退出，也可以认为n次迭代的结果和n-1次的结果非常接近时就代表下降到谷底，退出函数\n步数alpha的设置和epsilon的选择，这个例子我尝试步数为0.0025时就会振荡无法收敛，epsilon等于于0.001时有时会产生局部最优解，建议是$10^{-4}$\n明天继续尝试最小二乘法，这个代码写得比较loser，先放这，学完再优化，记录下现在和以后的思考有什么区别\n#!/usr/bin/python\n# coding=utf-8\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport math   # This will import math module\n# 构造训练集\n# x 特征值\n# y 实际结果\nx = np.arange(0, 50, 1)\nm = len(x)\ny = x/2 + np.random.randn(m) -5\n \n# 终止条件\nloop_max = 100000  # 最大迭代次数(防止死循环)\nepsilon = 1e-4   # 精确度\n \nalpha = 0.002  # 步长(注意取值过大会导致振荡即不收敛,过小收敛速度变慢)\ncount = 0  # 循环次数\nfinish = 0  # 终止标志\ntheta = np.random.randn(2)# 初始化theta\n#theta = [0.5,-0.5]\ntemp = np.zeros(2)\nerror = 0\n \nwhile count &lt; loop_max:\n    count+=1\n    sum = np.zeros(2)\n    for i in range(m):\n        sum[0] = sum[0] + (theta[0] + theta[1] * x[i] - y[i])\n    temp0 = theta[0] - alpha * sum[0] / m\n \n    for i in range(m):\n        sum[1] = sum[1]+ (theta[0] + theta[1] * x[i] - y[i]) * x[i]\n    temp1 = theta[1] - alpha * sum[1] / m\n \n    theta[0] = temp0\n    theta[1] = temp1\n \n    # 判断是否已收敛\n    if abs((sum[1]+ sum[0] - error)) &lt; epsilon:\n        finish = 1\n        break\n    else:\n        error = sum[1]+ sum[0]\n    print(&#039;intercept = %s slope = %s&#039; % (theta[0], theta[1]))\n \n \n#slope, intercept, r_value, p_value, slope_std_error = stats.linregress(x, y)\n#print(&#039;intercept = %s slope = %s&#039; % (intercept, slope))\nprint(&#039;loop count = %d\\n&#039; % count, theta)\nplt.plot(x, y, &#039;r*&#039;)\nplt.plot(x, theta[1] * x + theta[0], &#039;g&#039;)\n#plt.plot(x, slope * x + intercept, &#039;b&#039;)\nplt.show()\n偷懒了两天后用normal equation方法实现了，结果和stats.linregress的结果完全一样，注意矩阵需要垂直排列，记录俩函数用来修改矩阵堆叠方式\nvstack()函数 \n函数原型：vstack(tup) ，参数tup可以是元组，列表，或者numpy数组，返回结果为numpy的数组 \n作用：在垂直方向把元素堆叠起来\n&gt;&gt;&gt;import numpy as np\n&gt;&gt;&gt;a=[[1],[2],[3]]\n&gt;&gt;&gt;b=[[1],[2],[3]]\n&gt;&gt;&gt;c=[[1],[2],[3]]\n&gt;&gt;&gt;d=[[1],[2],[3]]\n&gt;&gt;&gt;print(np.vstack((a,b,c,d)))\n[[1]\n [2]\n [3]\n [1]\n [2]\n [3]\n [1]\n [2]\n [3]\n [1]\n [2]\n [3]]\n \n stack函数原型为：stack(arrays, axis=0)\n import numpy as np\na=[[1,2,3],\n   [4,5,6]]\nprint(&quot;列表a如下：&quot;)\nprint(a)\n\nprint(&quot;增加一维，新维度的下标为0&quot;)\nc=np.stack(a,axis=0)\nprint(c)\n\nprint(&quot;增加一维，新维度的下标为1&quot;)\nc=np.stack(a,axis=1)\nprint(c)\n\n输出：\n列表a如下：\n[[1, 2, 3], [4, 5, 6]]\n增加一维，新维度下标为0\n[[1 2 3]\n [4 5 6]]\n增加一维，新维度下标为1\n[[1 4]\n [2 5]\n [3 6]]\n\n Numpy中stack()，hstack()，vstack()函数详解\n#!/usr/bin/python\n# coding=utf-8\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n \n# 构造训练集\n# x 特征值\n# y 实际结果\nx1 = np.arange(0, 50, 1) + np.random.randn(50) -5\nm = len(x1)\nx0 = np.full(m, 1.0)\ny = x1/2 + np.random.randn(m) -5\ntarget_data = np.vstack(y)      # 将结果矩阵修改为垂直方向\nx = np.stack((x0, x1), axis=1)  # 构建X矩阵\n \n#print(x,y)\ntheta = np.dot(np.dot(np.linalg.inv(np.dot(x.T, x)), x.T), target_data)\n \nprint(theta)\nslope, intercept, r_value, p_value, slope_std_error = stats.linregress(x1, y)\nprint(&#039;intercept = %s slope = %s&#039; % (intercept, slope))\n&quot;&quot;&quot;\n得到的结果和stats.linregress函数完全一样，猜测这个函数也是如此实现的\n&quot;&quot;&quot;\nplt.plot(x1, y, &#039;*&#039;)\nplt.plot(x, slope * x + intercept, &#039;b&#039;)\nplt.plot(x, theta[1] * x + theta[0], &#039;r&#039;)\nplt.show()\n\n通过学习别人的代码和修改完成了最终版本，要注意步长和终止条件，步长alpha通过多次尝试最后选取适合值\n#!/usr/bin/python\n# coding=utf-8\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n \n# 构造训练集\n# x 特征值\n# y 实际结果\nx1 = np.arange(0, 50, 1) + np.random.randn(50)\nm = len(x1)\nx0 = np.full(m, 1.0)\nx = np.vstack([x0, x1]).T\ny = x1/2 + np.random.randn(m) -5\n \n# 两种终止条件\nloop_max = 10000  # 最大迭代次数(防止死循环)\nepsilon = 1e-4\n \n# 初始化权值\nnp.random.seed(0)\ntheta = np.random.randn(2)\n \nalpha = 0.002  # 步长(注意取值过大会导致振荡即不收敛,过小收敛速度变慢) 大于0.002会不收敛\nerror = np.zeros(2)\ncount = 0  # 循环次数\n \nwhile count &lt; loop_max:\n    count += 1\n    delta = np.zeros(2)\n    for i in range(m):\n        delta = delta + (np.dot(theta, x[i]) - y[i]) * x[i]/m\n    theta = theta - alpha * delta\n \n    # 判断是否已收敛\n    if np.linalg.norm(theta - error) &lt; epsilon: # np.linalg.norm 求范类：平方和，开方\n        break\n    else:\n        error = theta\n    print(theta)\n \nprint(theta,count)\nslope, intercept, r_value, p_value, slope_std_error = stats.linregress(x1, y)\nprint(&#039;intercept = %s slope = %s&#039; % (intercept, slope))\nplt.plot(x1, y, &#039;g*&#039;)\nplt.plot(x, theta[1] * x + theta[0], &#039;r&#039;)\nplt.plot(x, slope * x + intercept, &#039;b&#039;)\nplt.show()"},"python笔记/Python实现逻辑回归":{"slug":"python笔记/Python实现逻辑回归","filePath":"python笔记/Python实现逻辑回归.md","title":"Python实现逻辑回归","links":[],"tags":["机器学习","python"],"content":"首先总结一下学习，虽然叫回归但是和回归没有任何关系，刚尝试写代码时，思考分类问题陷入了线性回归的思路，纠结了好久，已经求出weights但不会拟合直线，后来用笔画了下立刻明白思考偏离了，所以就算有了电脑还是应该用笔在纸上画一画。\n写代码首先第一步是要知道做什么：我需要画一个直线，直线公式为 θ0 * x0 + θ1 * x1 + θ2 * x2 = 0  其中x0 = 1。想要画出这条直线我需要知道三个θ的值，通过吴大大的机器学习视频我知道的把θ的转置乘以x的带入逻辑函数g(z)就能求出预测函数h(x),然后通过梯度下降的方式更新θ，最终得到θ的近似值。\n#!/usr/bin/python\n# coding=utf-8\nfrom sklearn.datasets import load_iris\nimport matplotlib.pyplot as plt\nimport numpy as np\n \n# 逻辑函数(Logistic function)\ndef gfunc(z):\n    return 1 / (1 + np.exp(-z))\n \n# 构造训练集：引入了鸢尾花数据集来作为训练集\niris = load_iris()\ndata = iris.data\ntarget = iris.target\n \n# 取前一百行的第一列和第三列做特征值\nX = data[0:100, [0, 2]]\ny = target[0:100]\n \n# 画出训练集的散点图\nlabel = np.array(y)\nindex_0 = np.where(label == 0)\nplt.scatter(X[index_0, 0], X[index_0, 1], marker=&#039;x&#039;, color=&#039;b&#039;, label=&#039;0&#039;, s=15)\nindex_1 = np.where(label == 1)\nplt.scatter(X[index_1, 0], X[index_1, 1], marker=&#039;o&#039;, color=&#039;r&#039;, label=&#039;1&#039;, s=15)\nplt.xlabel(&#039;X1&#039;)\nplt.ylabel(&#039;X2&#039;)\nplt.legend(loc=&#039;upper left&#039;)\n \n########################################################\n# 训练集构建完成后判断边界，我猜边界是一条直线\n# 直线的公式：θ0 * x0 + θ1 * x1 + θ2 * x2 = 0  其中x0 = 1\n# 因为这个问题里是一个二维分类，所以边界是有三个θ决定的\n########################################################\n \n# 训练集的个数m\nm = 100\n \n# 重新构建了X向量 加上了x0=1\nx0 = np.full(m, 1.0)\nx0 = np.vstack(x0)\nx = np.column_stack((x0, X))\n \n# 随机设置三个theta值\ntheta = np.random.randn(3)\n \n# 两种终止条件\nloop_max = 10000  # 最大迭代次数(防止死循环)\nepsilon = 1e-3\n \nerror = np.zeros(3)\ncount = 0\nalpha = 0.001  # 步长\n \nwhile count &lt; loop_max:\n    delta = np.zeros(3)\n    for i  in range(m):\n        delta = delta + (gfunc(np.dot(theta, x[i])) - y[i]) * x[i]\n    theta = theta - alpha * delta\n    # 判断是否已收敛\n    if np.linalg.norm(theta - error) &lt; epsilon:\n        finish = 1\n        break\n    else:\n        error = theta\n    count += 1\n \nprint(&quot;The number of iterations = &quot;, count)\nprint(theta)\n \n# x0 = 1\n# 已经求得theta参数，给出x1的值，根据theta计算x2，画出直线\nx1 = np.arange(4, 7.5, 0.5)\nx2 = (- theta[0] - theta[1] * x1) / theta[2]\n \nplt.plot(x1, x2, color=&#039;black&#039;)\n \nplt.show()\n\n后来我通过学习他人的逻辑回归函数，修改步长，观察损失图，发现了些有趣的事，我把代码重构了，更便于可视化\n&gt;逻辑回归源代码&lt;\n首先我把步长设置为0.001，然后画出loss图：\n\n\n0.001的步数大概迭代2500多次达到低谷，从图中中观察到loss损失相当平滑，没有出现震荡\n然后我修改了步数为0.01，只通过800次迭代就下降到低谷，但是出现震荡，如果在线性回归中出现震荡则不会收敛，但是在逻辑回归问题中，尽管出现了震荡，但最终还是收敛。\n\n\n但如果我把步数设置的更大0.02时，就会每1800次后出现震荡的情况，最终无法收敛。\n"},"python笔记/Python虚拟环境的搭建":{"slug":"python笔记/Python虚拟环境的搭建","filePath":"python笔记/Python虚拟环境的搭建.md","title":"Python虚拟环境的搭建","links":[],"tags":[],"content":"我使用的Ubuntu18已经自带了pyhon3.6，现在我想用pip安装一些其它的应用的版本和现有的有冲突，为了防止冲突，我需要另一个python环境。\npython的安装\n这里因为是在Linux系统上，所以使用源码安装。PythonSource下载ptyhon3.8的源码\n解压后进入Python-3.8.1文件夹，执行命令\n$ ./configure\r\nchecking build system type... x86_64-pc-linux-gnu\r\nchecking host system type... x86_64-pc-linux-gnu\r\nchecking for python3.8... no\r\nchecking for python3... python3\r\nchecking for --enable-universalsdk... no\r\nchecking for --with-universal-archs... no\r\nchecking MACHDEP... &quot;linux&quot;\r\nchecking for gcc... no\r\nchecking for cc... no\r\nchecking for cl.exe... no\r\nconfigure: error: in `/home/void/Python-3.8.1&#039;:\r\nconfigure: error: no acceptable C compiler found in $PATH\r\nSee `config.log&#039; for more details\n\n出错了，我们竟然没有gcc，cc和cl也没有。\n安装依赖\n安装python前先安装需要的依赖软件，如果已经安装则不需要再安装\n如果是新安装的系统先更新下apt：\n$ sudo apt-get update\n\n先安装gcc：\n$ sudo apt install gcc\r\n\r\n......\r\n......\r\n\r\n$ gcc --version\r\ngcc (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\n\n安装g++\n$ sudo apt install g++\r\n\r\n......\r\n......\r\n\r\n$ g++ --version\r\ng++ (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\n\n连make也没有，安装make\n$ sudo apt install make\r\n\r\n......\r\n......\r\n\r\n$ make -v\r\nGNU Make 4.1\n\n安装其它依赖：\nsudo apt-get install -y make build-essential libssl-dev zlib1g-dev libbz2-dev libreadline-dev libsqlite3-dev wget curl llvm libncurses5-dev libncursesw5-dev xz-utils tk-dev libffi-dev liblzma-dev python-openssl\n\n构建Python\n$ ./configure\r\n......\r\n......\r\ncreating Modules/Setup.local\r\ncreating Makefile\r\n\r\n$ make\r\n......\r\n......\r\nPython build finished successfully!\r\n\r\n$ sudo make altinstall \r\n......\r\n......\r\nInstalling collected packages: setuptools, pip\r\nSuccessfully installed pip-19.2.3 setuptools-41.2.0\n\nIf you want a release build with all stable optimizations active (PGO, etc),\r\nplease run ./configure —enable-optimizations\n警告 make install 可以覆盖或伪装 python3 二进制文件。因此，建议使用 make altinstall 而不是 make install ，因为后者只安装了 exec_prefix/bin/pythonversion 。\n检查安装是否成功\n$ python3.8 --version\r\nPython 3.8.1\r\n$ pip3.8 --version\r\npip 19.2.3 from /usr/local/lib/python3.8/site-packages/pip (python 3.8)\n\n顺便更新下pip\n$ sudo pip3.8 install --upgrade pip\n\n安装虚拟环境\nvirtualenv是用来创建一个单独的Python运行环境的工具。\n$ sudo pip3.8 install virtualenv\n\n安装virtualenv好后就可以创建一个单独python环境了\n先创建一个存放python环境的文件夹\r\n$ mkdir pyvenv\r\n$ cd pyenv\r\n创建环境\r\n$ sudo virtualenv env\n\n新建一个名为env的虚拟环境，并在当前目录下新建同名文件夹\n虚拟环境的使用\n使用source命令执行虚拟环境目录中bin/activate文件，将激活虚拟环境，命令行前出现（环境名）表示已在虚拟环境中\n激活并使用虚拟环境\n$ source env/bin/activate\r\n(env) []$ deactive\n\n退出环境\r\n执行命令deactivate退出虚拟环境"},"python笔记/简单的微博爬虫":{"slug":"python笔记/简单的微博爬虫","filePath":"python笔记/简单的微博爬虫.md","title":"微博爬虫","links":[],"tags":["python"],"content":"准备写论文了，需要爬取些互联网的数据进行分析下，Pyhton爬虫是一个比较简单的方法，虽然有许多的爬虫框架但是我并不需要太复杂的爬虫，所以决定从零开始写一个简单的爬虫爬一下微博和评论。\n准备工具\n爬虫就是一个分析HTTP请求的工作，首先需要一些抓取http的工具，推荐使用 Fiddler，简单免费。\n还需要浏览器 GoogleChrome\n分析微博网站\n微博有三个入口，分别是\nweibo.com\nweibo.cn\nm.weibo.cn\n\n第一个是PC网页版，后两个是手机版，手机版网站结构相对简单爬取容易，所以选择m.weibo.cn为爬取对象。\n打开浏览器开发者模式，观察Network，在热门频道刷新，发现一个API\n\nm.weibo.cn/api/container/getIndex\n\n这个地址的response就是热门微博的数据，接下来寻找翻页的方法，向下刷新观察发现一个地址在变化。\nm.weibo.cn/api/container/getIndex\n\n\nsince_id这个参数是控制翻页的。\n用同样的方法观察评论页面\nm.weibo.cn/detail/{微博编号}\n\n这个地址就是具体微博的详细，\nm.weibo.cn/comments/hotflow{mblogid}&amp;mid={mblogid}&amp;max_id={max_id}&amp;max_id_type=0\n\nmblogid是微博的编号，max_id控制翻页，前一个request会告诉这个id的数，如果是0则表示最后一页。\n微博页面分析完毕，接下来编写代码模仿浏览器发Http请求就OK了。\n编写代码\n首先我们需要用到urllib这个库来发送请求。\n还需要json化一下数据。\n# -*- coding: utf-8 -*-\n# env python3.7\n \nimport json\nimport time\nimport random\n \nfrom urllib import request\nfrom urllib import parse\n \n# 热门微博的url\nHotWeiBoUrl = &#039;m.weibo.cn/api/container/getIndex{sinceid}&#039;\n# 微博评论的Url\nWeiBoCommentsUrl = &#039;m.weibo.cn/comments/hotflow{mblogid}&amp;mid={mblogid}&amp;max_id={max_id}&amp;max_id_type=0&#039;\n首先访问热门微博页面，解析当前页面的热门微博的response，提取出每个微博的内容和地址。\ndef GetHotWeiBoData(url):\n    scheme_url = []\n    body_text = []\n \n    req = request.Request(url)\n    req.add_header(&#039;User-Agent&#039;, &#039;Mozilla/6.0 (iPhone; CPU iPhone OS 8_0 like Mac OS X) AppleWebKit/536.26 (KHTML, like Gecko) Version/8.0 Mobile/10A5376e Safari/8536.25&#039;)\n    with request.urlopen(req) as f:\n        print(&#039;Status:&#039;, f.status, f.reason)\n        if f.status == 200:\n            data = f.read().decode(&#039;utf-8&#039;)\n            data = json.loads(data)\n            for i in data[&#039;data&#039;][&#039;cards&#039;]:\n                scheme_url.append(i[&#039;scheme&#039;])\n                body_text.append(i[&#039;mblog&#039;][&#039;text&#039;])\n            return body_text, scheme_url\n    return None, None\n获取评论和下一页评论\ndef GetCommentsData(comments_url):\n    text = []\n    req = request.Request(comments_url)\n    req.add_header(&#039;User-Agent&#039;, &#039;Mozilla/6.0 (iPhone; CPU iPhone OS 8_0 like Mac OS X) AppleWebKit/536.26 (KHTML, like Gecko) Version/8.0 Mobile/10A5376e Safari/8536.25&#039;)\n    print(comments_url)\n    with request.urlopen(req) as f:\n        if f.status == 200:\n            data = f.read().decode(&#039;utf-8&#039;)\n            data = json.loads(data)\n            if &quot;data&quot; in data:\n                data = data[&quot;data&quot;]\n                for i in data[&quot;data&quot;]:\n                    text.append(i[&quot;text&quot;])\n                next_id = data[&#039;max_id&#039;]\n                return text, next_id\n    return None, None\n循环执行上面两个函数，就可以了\ndef GetCommentsUrl(url, max_id=0):\n    parsed_tuple = parse.urlparse(url)\n    mblogid = parsed_tuple.path[8:]\n    comments_url = WeiBoCommentsUrl.format(mblogid=mblogid, max_id=max_id)\n    return comments_url, mblogid\n \ndef GetWeiBo(sinceid):\n    data = {}\n    url = HotWeiBoUrl.format(sinceid=0)\n    body_text, scheme_url = GetHotWeiBoData(url)\n \n    for i in range(len(scheme_url)):\n        url = scheme_url[i]\n        body = body_text[i]\n        comments_url, mblogid = GetCommentsUrl(url)\n        comments, next_id = GetCommentsData(comments_url)\n        if comments:\n            data[mblogid] = {}\n            data[mblogid][&#039;comments&#039;] = []\n            data[mblogid][&#039;comments&#039;].extend(comments)\n            data[mblogid][&#039;body&#039;] = body\n \n        # while next_id != 0:\n        #    comments_url, mblogid = GetCommentsUrl(url, next_id)\n        #    comments, next_id = GetCommentsData(comments_url)\n        #    data[mblogid][&#039;comments&#039;].extend(comments)\n \n    return data\n把数据保存起来\ndef SaveData(data):\n    data = json.dumps(data, ensure_ascii=False) + &quot;\\n&quot;\n    WeiBoData.write(data)\n    # 关闭打开的文件\n    # fo.close()\n让爬虫开始工作，为防止被禁，把爬取频率调低点。\n# 打开一个文件 a 已追加的方式打开，编码方式为 utf-8\nWeiBoData = open(&quot;./WeiBoData.txt&quot;, &quot;a&quot;, encoding=&quot;utf-8&quot;)\n \nwhile True:\n    data = GetWeiBo(0)\n    SaveData(data)\n    time.sleep(random.randint(280, 320))\n如果想要爬取多页评论，需要登陆操作，设置 cookie。"},"信息资源管理笔记/信息资源管理导论":{"slug":"信息资源管理笔记/信息资源管理导论","filePath":"信息资源管理笔记/信息资源管理导论.md","title":"信息资源管理导论","links":["信息资源管理笔记/信息资源管理导论"],"tags":["信息资源管理学"],"content":"信息资源管理概论\n1.社会信息化包括哪几个层次\n社会信息化一般包含三个层次\n\n通过自动控制、知识密集而实现的生产工具信息化\n通过对生产行业、部门以至整个国民经济的自动化控制而实现的社会生产力系统信息化\n通过通信系统、咨询产业以及其他设施实现的社会生活信息化。\n\n2.什么是信息资源\n\n侠义的理解，信息资源是指人类社会经济活动中经过加工处理有序化并大量积累起来的有用信息的集合，如科技信息，政策法规信息、社会发展信息、市场信息、金融信息等，都是信息资源的重要构成要素。\n广义的理解，信息资源是人类社会信息活动中积累起来的信息、信息生产者、信息技术等信息活动要素的集合。\n综合侠义和广义两种理解：信息资源是指人类社会信息活动中积累起来的以信息为核心的各种信息活动要素（信息技术，设备，设施，信息生产者等）的集合。\n这里的信息活动围绕信息的搜集，整理和利用而开展的一系列社会经济活动。\n\n3.信息资源有哪些特征\n作为经济资源，它与物质和能源资源一样具有经济资源的一般特征：\n\n作为生产要素的人类需求性\n稀缺性\n使用方向的可选择性，信息资源可以广泛的渗透到经济活动的方方面面。同一信息资源可以作用于不同的作用对象上，并产生不同的作用效果。\n\n与其他资源相比较，本身的特殊性\n\n共享性\n时效性\n生产和使用中的不可分割性\n不同一性\n驾驭性\n\n4.信息资源有什么功能\n\n经济功能：信息作为重要的资源，具有经济功能。信息资源的经济功能表现在多个方面，在经济活动中发挥不同的作用，其中最重要的是它对社会生产力系统的作用功能。\n管理与协调功能：在企业活动中的作用，传递企业系统的运行目的，有效管理“5M”资源，调节和控制物质与能源的数量、方向和速度；传递外界对系统的作用，保持企业系统的内部环境稳定。\n选择与决策功能：信息的选择与决策功能广泛作用人类选择与决策活动的各个环节，并优化起选择与决策行为，实现预期目标。信息的这种功能体现两个方面：没有信息就无任何选择和决策可言：没有信息的反馈，选择和决策就无优化可言。\n研究与开发功能：即在人类科学研究和创新活动中，信息具有活化知识、生产新知识的功能。\n\n5.试述信息资源管理的发展过程\n\n传统管理阶段：以信息资源管理为核心，以图书馆为象征，同时也包含档案管理和其他文献资源管理。\n信息管理阶段：以信息流的控制为核心，以计算机为工具，以自动化信息处理和信息系统建造为主要工作内容。\n信息资源管理阶段：信息资源管理是信息管理从古代到当代的必然延伸，是对信息管理单纯依赖技术的否定。信息作为一种重要的经济资源，是当代社会经济发展的必然结果\n\n6.信息资源管理的目标是什么\n\n总目标，保证信息资源的开发利用在有领导、有组织的统一规划和管理下，协调一致、有条不紊地进行，使各类信息资源以更高的效率、效能和更低的成本在国家社会进步、经济发展、人民物质文化生活水平的提高中充分发挥应有的作用。\n分目标，为保证总目标的实现服务，并受总目标的制约\n（1）信息资源开发分目标：主要根据社会发展的需求来合理组织、规划信息资源的开发，确保相关的潜在信息资源管理能及时、经济地转化为现实信息资源。\n（2）信息资源利用分目标：主要按照社会化、专业化和产业化的原则合理组织信息资源的分配，确保信息资源管理能得到充分有效的利用。\n（3）信息资源管理机制分目标：主要是遵循客观经济规律，建立全科学、合理的信息资源管理机制，完善信息资源开发利用的保障体系。\n\n7.信息资源管理的任务包括哪些方面\n\n制定信息资源的开发战略、规划、方针和政策，使信息资源管理的开发活动在国家统一的指导和管理下有条不紊地进行，满足国民经济和社会发展的总体需求。\n制定信息资源管理的法律、规章和条例，建立信息资源管理的监督和保障体系。\n综合利用经济法律和必要的行政手段协调各部门、各地区和各企业之间的关系，使信息资源的开发利用机构在平等互利的基础上最大限度地实现资源共享。\n加强国家信息基础设施和信息资源管理网络的建设，使信息资源的开发利用活动建立在较高的起点和良好的社会基础上。\n\n8.信息资源管理有何意义\n\n信息资源管理开辟了管理新天地，建立在科学、合理的基础上，突破了传统的管理思想、管理目标和管理方式、开辟了管理新天地\n加强信息资源管理是使信息资源真正得以合理开发和有效利用的必要条件，合理开发信息资源的前提是能够有效的控制其使用，并保证其在成本最低时经济功能最强。\n加强信息资源管理有利于保证信息资源开发利用机构的合法权益，制定并监督实施一系列有利于信息资源开发利用的法律、政策、规章和条例。\n\n9.宏观层次的信息资源管理应遵循哪些原则\n\n信息资源是一种重要的经济资源，要从思想上把它提高到一个战略的认识高度\n信息资源管理是一项复杂的社会系统工程，规模巨大、结构复杂，必须实行分级分类管理\n国家的信息资源管理，主要是确定目标、进行投资决策，并为各级政府业务部门中观层次的信息资源管理提供条件\n大力推广使用现代信息技术，以提高信息资源的开发水平和利用效果\n确定信息资源管理的保密和保存制度，协调与其它国家之间的信息资源交流关系\n\n10.信息资源管理有哪些手段\n\n技术手段，以计算机和通信技术为基础的现代信息系统和信息网络以及与此相适应的信息加工方法，是信息资源管理的主要手段和内容\n经济手段，运用各种经济杠杆和利益诱导作用，促使信息资源开发利用机构从经济利益上关心自己的活动，是一种间接组织和协调信息资源开发利用活动的手段\n法律手段，指用以协调信息资源开发利用活动各种有关的法律规范和总称，与经济手段相比，信息资源管理的法律手段具有普遍的约束性、严格的强制性、相对稳定性和明确的规定性等特点\n行政手段，指凭借国家政权的权威，采取命令、指示等形式来直接控制和管理信息资源及其他相关活动\n\n11.社会信息化\n社会信息化指的是人类社会发展过程中的一种特定现象，在这种现象出现时，人类对信息的依赖程度越来越高，而对物质和依赖程度相对降低。社会信息化不是从来就有的，二十人类社会政治、经济、文化、生活发展到一定历史阶段后的必然产物\n12.经济信息化\n经济信息化是社会信息化的基本表现形式之一。主要指信息要素广泛地渗透到人类社会各种经济活动之中，社会经济的发展主要不是 有赖于物质材料的增加和新能源的开发，而是有赖于信息力量的推动。\n13.信息资源\n信息资源是指人类社会信息活动中积累起来的以信息为核心的各类信息活动要素（信息技术、设备、设施、信息生产者等）的集合。这里的信息活动包括围绕信息的搜集、整理、提供和利用而开展的一系列社会经济活动。\n14.记录型信息资源\n记录型信息资源：包括传统介质（纸张竹帛等）和各种现代介质（磁盘、光盘等）记录和存储的知识信息。它是信息资源存在的基本形式，也是信息资源的主体。\n15.零次信息资源\n这类信息资源是指各种渠道中由人的口头传播的信息。零次文献记录在非正规物理载体上的未经任何加工处理的源信息叫做零次信息，比如书信、论文手稿、笔记、实验记录、会议记录等，这是一种零星的、分散的和无规则的信息。零次信息的载体形式就称之为零次文献，它具有原始性、新颖性、分散性和非检索性等特征。\n16.智力型信息资源\n这类信息资源主要表现为人脑存贮的知识信息，包括人们掌握的诀窍、技能和经验，又称为隐性知识。\n17.简述信息资源的选择与决策功能\n信息的选择与决策功能广泛作用人类选择与决策活动的各个环节，并优化其选择与决策行为，实现预期目标。信息的这种功能体现在两个方面：\n(1) 没有信息就无任何选择和决策可言;\n(2) 没有信息的反馈，选择和决策就无优化可言。\n一个典型的选择（或决策）遵循这样的程序：针对某一目标，考虑所受的条件限制和其他约束，从几种可能的方案中做出一种选择。选择单元中的目标、限制条件、多种方案都必须依赖信息的支持。而当一次选择成功之后，还必须依赖反馈信息不断修正，才能达到选择和决策结果的优化。信息在人类的选择与决策活动中还发挥预见性功能。人类的选择与决策活动实际上就是处在不断利用信息并对未来进行预测之中的。\n18.信息资源管理的基本要素\n管理对象：信息资源；\n实施领域：国际、国家和社会组织；\n实施方法：综合使用各种管理手段，包括技术手段、经济手段、人文手段等；\n信息资源管理的实质，即一种管理思想、管理模式和管理活动；因此也有人认为信息资源管理是国际国家和社会组织机构为达到预定目标，综合运用各种手段，对信息活动中的各要素（信息、人员、设备、资金等）实施组织、规划、协调、配置、控制等全面管理的一种管理模式与管理活动。\n19.简述信息资源管理概念提出的两个背景\n信息资源管理（IRM）概念的提出基于两个背景：\n一方面，是信息管理阶段纯粹的技术手段不能实现对信息的有效控制和利用：另一方面，也是更重要的原因，是当代社会经济发展使得信息成为一种重要的资源，迫切需要从经济的角度思考问题，并对这种资源进行优化配置和管理。\n在第一种背景下，信息资源管理是信息管理从古代到当代的必然延伸，是对信息管理单纯依赖技术的否定。在第二种背景下，信息作为一种重要的经济资源，是当代社会经济发展的必然结果。\n既然是经济资源，它的管理模式就与在公益性信息活动基础上形成的信息管理模式有很大差别，需要从经济角度对其进行管理和优化配置，使其效益达到最大化。\n信息资源的优化配置\n1.什么是信息资源配置\n作为经济发展的基本条件和表现形式，资源优化配置是指为最大限度减少宏观经济浪费和现实社会福利最大话而对现代技术成果与各种投入要素进行的有机组合。\n信息资源的有效配置的含义是为在整个社会资源有效配置条件下对信息产业投入与产出的安排。从横向看，它应该考虑产业配置和行业配置；从纵向上看，他应包括信息的生产、扩散、组织、交换、利用的全过程。\n2.信息资源配置应遵循哪些原则\n\n社会经济福利最大化原则，判断信息资源配置是否有效不能单纯地从某些经济利益主体出发，而必须站在全社会的高度，以社会经济福利最大化为判定标准\n需求导向原则，信息资源不论是在时间上、空间矢量上的配置还是品种类型上的配置，最基本的依据都是用户对信息资源的需求性，用户信息需求的每一变化都会影响到各种资源配置模式的效益，并进而影响到配置模式的调整和选择决策\n公平原则，指人们对某种社会现象的一种道德评价，认为它是否应当如此，是否公平合理。\n市场手段和政府手段互补原则，市场机制和资源配置是同一问题的两个不同侧面。市场供求、价格、竞争、风险机制的充分运作可以有效的调节信息资源在生产、传输、分配和开发利用过程的经济利益和经济关系，以利益驱动构建信息资源配置效率的大厦\n\n3.试述信息资源配置的主要内容\n\n信息资源地横向配置，反应为部门、行业或地区地宏观布局，包括区域配置和行业配置。\n信息资源地纵向配置，是指只有符合资源增值地投资项目，才能吸引资源地流入。\n\n4.网络环境下的信息资源应如何配置\n\n网络信息资源的时间矢量配置，指网络信息资源在时间坐标上地配置。\n网络信息资源的空间矢量配置，指网络信息资源在不同的地区、不同的行业部门之间的分布，实质上是在不同使用方向上的分配。\n网络信息资源的品种类型配置，网络信息资源系统的大小和服务能力的强弱不能简单地看其拷贝数量是否庞大，而应当综合性的以信息资源品种类型的多寡及其网络用户信息需求的满足程度作为主要评判依据。\n\n5.信息资源配置的机制是什么\n\n信息资源的市场配置，指市场通过价格杠杆自动组织信息的生产和消费。信息资源的市场配置是通过市场机制对信息生产的自组织过程实现的。\n信息资源的政府配置，指政府利用政策、法律、税收工具，或通过直接投资和财政补贴来调整信息产出。\n信息资源的产权配置，指通过调整和明晰产权，优化信息资源配置。\n\n6.试述信息资源配置市场失灵的原因\n\n信息的外部效应。信息商品和信息服务既有正的外部效应，也有负的外部效应，当其具有正的外部效应时，信息生产者的边际收益小于边际社会收益，从而导致信息生产不足，信息资源配置无效；当其具有负的外部效应时将会把社会成本强加给市场中那些间接消费者和生产者，从而使信息市场偏离均衡，信息资源配置无效\n信息的公共物品的属性。即信息消费的非排他性、非消耗性所导致的“搭便车”问题，使得信息生产者的成本无法通过市场机制取得有效的补偿，从而导致生产不足，这与信息外部效应所导致的结果是十分类似的\n信息商品的垄断性。信息商品和信息服务的垄断表现在两个方面：一是信息商品和信息服务具有很高的初始成本、较低的边际成本形成自然垄断。二是为了保护信息生产者的合法权益和创造性，需要用法律排除信息商品的共享性而形成垄断。在两种情形下都会出现垄断定价，使得价格高于边际成本，导致效率损失，信息资源配置无效\n信息市场的信息不对称。价格在市场中的重要功能之一是传递资源稀缺程度的信号。但在信息市场中，价格的这种信号功能被大大消弱了，其原因主要在于两个方面：一是信息的效用具有不确定性，它在很大程度上要取决于消费者对信息的偏好和利用能力；二是在空间上和时间上都存在着严重的信息不对称。\n信息活动的非营利性。教育和基础研究这类基础性信息活动，并不是出于商业性目的，因而市场机制无法对其进行调节\n\n7.政府在信息资源配置方面有何作用\n政府配置机制是指政府利用政策、法律、税收工具，或者通过直接投资和财政补贴来调整信息产出，在信息资源配置方式上，政府干预只能作为一种辅助手段，发挥有限的作用。\n\n信息基础设施建设，为信息产业的发展构筑一个操作平台\n公共信息服务，有效的公共信息服务对于提高民众的文化素质、传播知识信息具有重要的作用。\n教育，教育是提高国民素质的根本手段，它决定了一国的科学技术水平，是一个国家社会经济 可持续发展的根本保障。\n科学研究，科学技术水平直接决定一国的技术创新能力，从而决定了一国经济的长期增长速度。科学研究，尤其是基础研究的投入是政府的一项重要职能\n制度建设，包括规范市场、界定产权、完善法律制度，以及调整信息产业结构和信息产业组织，从而在制度上保障市场机制的有效性\n\n8.信息资源产权配置的功能主要体现在哪些方面\n\n相对于无产权或产权不明晰状况而言，设置产权就是配置资源。它可以减少浪费，提高经济效率。\n任何一种稳定的产权格局或结构，都会形成一种资源配置的客观状态。\n产权的变动同时也改变资源配置格局，包括改变资源在不同主体间的配置，资源的流向和流量，资源的分布状况。\n\n9.应从哪些方面考察信息资源配置的有效性\n\n信息生产的有效性，是信息资源配置有效的基本条件。\n各种信息商品的生产比例的有效性，要求不仅用有效的方式生产，而且要求能最好地满足消费者的需求，即生产出来的信息产品能够反映消费者的偏好。\n信息市场与交换的有效性，要求生产的信息商品能够以适当的方式实现价值，这就意味着信息市场必须高度发达。\n\n10.试述我国文献信息资源的共享模式\n\n垂直型（纵向）共享，是指具有隶属关系的某一系统内的各图书情报机构在不同层次之间协同共享信息资源的一种方式，所有下级馆都与中心馆相连接，并利用中心馆的文献信息资源。\n水平型（横向）共享，是指一个地区不同系统、不同专业图书馆之间的资源共享模式。这种模式易于实行，但各文献收藏机构各自有不同的隶属关系，彼此间缺乏合作的强烈动机和有力的协作机制，因而在实践中往往流于空谈\n网络型共享，指全国所有的图书情报机构之间都可直接相互连接，共享信息资源。这在理论上是最理想的模式，但在实践中操作难度也是最大的，在现阶段不可能实现\n\n11.信息资源配置的经济意义\n\n有效配置信息资源有利于更好地满足人类对资源地需求\n有效配置信息资源有利于最大范围内实现资源共享\n有效配置信息资源有利于防止信息资源污染，实现社会可持续发展\n\n12.信息资源的纵向配置\n指的是只有符合资源增值的投资项目，才能吸引资源的流入。必须建立一种有效的资源流动机制，以保证资源能够迅速的从低收益项目流入高收益项目，避免投资的恶性膨胀\n13.信息市场失灵\n信息市场的自组织机制不是万能的，它对信息资源的市场配置存在着一些自身无法克服的缺陷，即市场失灵\n14.信息资源的政府配置\n指政府利用政策、法律、税收工具，或通过直接投资和财政补贴来调整信息产出\n15.信息资源产权配置的功能\n由于信息的消费具有，非消耗性的特点，因此，存在许多搭便车现象。这样会导致市场自动调节失灵，从而引起现象资源配置的低效率。信息资源产权配置是指通过调整和明晰产权，优化信息资源配置\n16.如何衡量信息资源配置的效率\n信息资源配置一方面必须克服从于总体社会资源优化配置和社会福利最大化的宏观目标，另一方面又必须立足于信息生产、信息服务有效的微观基础。衡量信息资源配置效率需要从宏观，中观和微观三种不同的层次来加以考察\n17.网络信息资源的空间矢量配置\n网络信息资源不在同地区、不同部门之间分布，实质上是不同使用方向上的分配\n企业信息资源管理\n1.企业从哪些方面来评价信息\n一. 有价值的信息必须具备的条件\n有价值的信息必须具备具有，及时性、准确性、综合性、获取简易性、经济性等特性。具体的说，应该具备以下条件\n\n能够及时以及适当的方式提供解决问题所需要的依据\n信息符合需求的内容\n信息的可信赖度高\n信息具有综合性\n信息容易获取\n信息费用与目标吻合\n\n二. 信息源的选择评价\n对于通过一般的信息搜集活动得到的信息，首先必须确定它是通过那种信息载体、从什么样的信息源获得的，其次还应评价信息发生的意图和可靠性。\n\n信息发生源\n信息载体\n意图\n可靠性\n\n三. 信息准确度的比较评价\n第一种角度包括以下三种方法：\n\n从不同的信息源获得同一性质的信息，对这种信息进行比较。\n定期地、系统地搜集信息，调查过去同种信息是否出现并和新获取地信息进行比较评价\n从多种信息源搜集、分析同种信息和相关信息，与切题地信息进行比较评价\n\n第二种角度从信息所含的六个要素出发评价信息的准确度\n\n内容\n原因\n时间\n地点\n人\n方法\n途径\n状况\n\n三. 信息的经济性评价\n\n所需信息的存在率的评价， 调查有关的信息源、载体、实物是否存在，如果存在，要用什么方法从何种途径获得\n所需信息的适合率的评价，评价获取的信息与所需的信息内容吻合程度，例如，解决问题的有效程度，为利用该信息而需要加工处理的必要程度等都是评价的尺度\n所需信息的可靠性的评价，对于二次信息和三次信息，应评价其性质、加工深度、是否能获得证明性信息以确认其可靠性。\n\n2.企业信息资源管理的任务是什么\n企业信息资源管理属于微观层次的信息管理范畴，是指企业为达到预期目标，运用现代的管理方法和手段对与企业相关的信息资源和信息活动进行组织，规划，协调和控制，以实现对企业信息资源的合理开发和有效利用。\n企业信息资源管理的任务就是要有效地搜集、获取和处理企业内外信息，最大限度地提高企业信息资源的质量、可用性和价值，并使企业各部门能够共享这些信息资源。信息资源的质量取决于信息的准确性和连续性，信息资源的可用性可以通过信息的获取性和便捷的信息渠道而增强。\n3.企业信息资源管理的主要内容是什么\n\n提高企业全体人员对信息价值的认识并促进企业活动对信息的需求\n促进信息在企业不同部门和不同群体之间的共享\n为企业建立合理的信息结构\n保证信息的相关性和标准化，以最大限度的减少信息的重复现象\n提高信息质量，提高信息安全\n保证信息资源管理系统的可用性和充分运行，以便在任何时候和任何地点均可及时检索到所需的信息\n促进企业员工利用信息\n\n4.试述企业信息资源管理的技术框架\n\nMIS 管理信息系统，传统的MIS设计和实现更注重一个企业、部门内部的信息交流、共享，或者说更注重信息源的加工、处理，而对信息的分析、传递、共享、使用都显得不足，更忽视企业其他非技术要素的影响。由于这些缺陷，使得MIS作为信息管理核心的技术框架正在被人们扬弃。\nIntranet 及其应用，Intranet是由Internet发展而来的，它继承了Internet的标准化技术，在企业内部集成各类信息系统，形成一个统一、开放、标准、简捷的网路平台。它不仅是企业内部信息发布系统，而且是企业内部业务运转系统。它既具有严格的网络安全保障机制，又具有良好的开放性，从而有效地解决了信息系统内部信息的共享的交流问题\nExtranet，是Intranet的延伸和扩展，它把Intranet的构筑技术应用到企业与企业之间的系统中，可为外部用户提供选择性服务\n\n5.试述企业信息资源管理的组织结构\n\n企业信息总监 CIO，是企业信息资源管理的最高负责人\n信息技术人员，包括系统设计员、系统分析员、应用程序员、维护程序员、程序库管理员、系统程序员、数据通信专家、数据库管理员等。主要职责是保证企业Intranet及构建于其上的企业信息资源管理技术装备的建立和正常运转\n信息管理人员，包括用户需求分析员、信息资源调查员、信息资源管理系统运行工艺设计员、信息资源管理系统效果评测员、信息资源安全保护管理员，主要职责是利用各种信息技术和信息资源更好的支持企业的各项活动，促进企业所拥有的各类信息资源的增值\n信息资源管理辅助人员，包括法律专家、经济分析评估员、社会心理专家、公共关系专家、企业管理专家等。其主要职责是协助信息管理人员更好的实施各项信息资源管理工作和活动\n\n6.什么是知识管理\n知识管理是对知识及其创造，收集，组织，传布，利用与宣传等相关过程的系统管理。它要求将个人知识转变为某个组织可以广泛共享与适当利用的团体知识。\n7.企业知识管理的目标是什么\n\n提高企业创新能力\n提高企业反映能力\n提高企业效率\n提高企业员工技能\n实现企业知识资产价值\n\n8.试述企业知识管理的职能\n基本职能：\n\n为联机和非联机知识建立知识地图\n向用户提供知识利用方面的指导和培训，并提供相应的知识检索工具\n监视企业外部知识源\n\n具体来说：\n\n外化 externalization，就是将员工头脑的隐含知识转化为可提供知识共享的企业知识库中的知识。它以外部知识库的形式捕捉知识，并根据分类框架或标准来进行组织。例如镜像系统和数据库。\n内化 internalization，强调从外部知识库中提取知识、进行过滤，从而为知识寻求者提供高度相关的知识，重视发现与特定需求相关的知识结构。在外化过程中，从外部知识库提取知识，并以最合适的方式来进行重新布局或呈现，通过过滤来发现与知识寻求者相关的内容。\n中介 intermediation，指实现知识拥有者与知识寻求着之间有效的知识传递，以使后者获取最切合其需求的知识。\n认知 cognition，是在前三项功能的基础上对知识的运用，是知识管理的终极目标，在现有知识的基础上进行决策。\n评测 measurement，指所有评估、量化和描绘企业知识资源及知识管理业绩的活动。\n\n9.企业知识管理的基本流程包括哪些步骤\n\n制定知识管理战略，根据企业总体战略和目标指定相应的知识管理总体规划，成果是一份企业知识管理战略规划\n确定知识管理重点领域，调查企业关键部门或流程以及高成本区或高潜在收益区的知识需求，成果是一份企业内部优先实行知识管理部门或流程名单\n评估企业知识资源，调查企业内部知识资源的现状、特性、优势和不足，以及企业外部知识网络上知识资源的内容、特性、和可能性，成果是一份企业知识地图\n制定知识管理方案，研究企业推行知识管理可以运用的工具和措施，成果是一份知识管理实施计划书\n实施知识管理方案，知识项目管理，成果是一个成功的知识管理项目\n监督知识利用，对已完成的知识管理项目进行评估，成果是企业知识管理项目评估报告\n\n10.企业知识管理的实现技术主要\n\n网络技术，网络在知识管理技术中居核心地位\n群件技术，是帮助群组协同工作的软件，一般包括，电子邮件、文档管理与工作流应用\n知识库技术，是利用计算机科学中的人工智能或知识工程技术使隐性知识得以显性化，即将隐形知识转化为显性知识的能力\n其它重要技术，Push技术、决策支持系统与专家系统技术、信息检索技术、超文本与电子出版技术\n\n11.知识管理\n是对知识及其创造、收集、组织、传播、利用与宣传等相关过程的系统管理。它要求将个人知识转变为某个组织可以广泛共享与适当利用的团体知识\n12.记录管理\n记录管理又称文书管理，从管理内容上看，记录管理的内容产生与政府内部业务工作之中，一般不包括产生于政府外部但对政府日后的业务活动有影响的信息。记录管理的对象仅限于记录，而不包括生产者、录存设备、录存技术、费用系统活动要素\n13.简述企业信息的特点\n信息的渠道来看，企业不但要搜集、存储和利用外部信息，还要对企业内部信息进行搜集、加工和存储。\n从信息载体来看，既有传统的印刷型，又有磁盘、光盘、微缩胶片、胶卷、网络等新型存储介质，既有文字型，又有实物型；既有书本型，又有报表式；既有正式文献，又有非正式书信。\n从信息内容来看，既有科技信息，又有经济信息、财务信息、市场信息等。各种信息特征不同，性质各异，需要采用不同的整序和存储方法。\n从时效性看，企业信息更要求新颖及时。从需求和利用的角度考察，企业对信息一般要求具体实用。\n企业无论哪一部门，也无论哪一类人员，都要求提供的信息具有很强的针对性，可以直接为决策提供依据。企业无论哪一部门，无论哪一类人员，都要求提供的信息具有很强的针对性，可以直接为决策提供依据。\n企业信息系统必须对输入和存储的信息进行进一步的分析、综合、预测、浓缩、整理，有针对性地向有关人员提供。企业在进行外部宣传和传播信息时，同样要求准确、具体地数据或描述\n14.企业信息资源管理的主要内容是什么\n\n提高企业全体人员对信息价值的认识并促进企业活动对信息的需求\n促进信息在企业不同部门和不同群体之间的共享\n为企业建立合理的信息结构\n保证信息的相关性和标准化，以及最大限度地减少信息重复现象\n提高现象地质量，保证信息地安全\n保证信息资源管理系统地可用性和充分运行，以便在任何时候和地点均可及时检索到所需要地信息\n促进企业员工利用信息。企业信息资源管理是企业管理地重要内容，目的是通过管理、组织和协调企业地信息、信息活动和信息设施来支持企业地各项决策和经营活动\n\n政府信息资源管理\n1.试述政府信息资源管理产生的背景\n作为信息资源管理重要环节的政府信息资源管理，其兴起有这深刻的历史背景，他是社会经济发展的必然产物，是众多内因外因相融合的结果。\n\n信息资源观的转变，人们的信息观念发生了极大的变化，意识到信息是一种重要的资源，但要使他发挥作用，必须对其进行管理。信息需要管理成为信息活动中的共识。政府信息资源是政府活动所涉及的信息资源的集合，包括信息内容资源以及收集、处理、传输、发布、使用、存储信息内容的技术、设备、网络和人等资源。政府信息资源作为国民经济和社会事务状态和特征，是一种特殊资源，拥有特殊性质，因而在人类社会经济发展和政府管理中具有特殊的重要地位，必须加强管理\n信息技术的发展，信息技术的发展为信息资源管理的产生提供了工具和支持手段。政府的特殊性质和地位决定了政府总是重大信息技术的积极倡导者和最直接的受益者，信息技术最大的试验场就是政府信息系资源管理领域\n经济信息化的推进，政府信息资源管理在国民经济信息化中具有极强的带动作用。政府信息资源管理不仅涉及国名经济和社会发展的各个方面，还涉及广阔的地域。从推动国民经济发展的全局来看，政府信息资源管理不仅可以促进国民经济新增长点的形成，而且能够成为带动整个国民经济发展的重要驱动力。经济信息化与政府信息资源管理活动息息相关，是政府信息资源管理不歇的动力\n政府体制改革的需求，政府应用信息技术的目标是提高政府的有效性、效率和劳动产生率扩展为转变政府的职能，从而改善政府为人们所提供的服务。现代信息技术正在成为新的促进政府与人民沟通互动的工具，从而改善政府和人民群众的联系，推动民众参与决策，改变政府的工作方式、决策方式和行为方式\n\n2.试述政府信息资源管理的发展历程\n\n记录管理：文书管理\n信息技术管理：办公自动化技术（OA）、网络技术（局域网技术，广域网技术，互联网技术）、信息资源定位技术（政府资源定位系统 GILS），其它通用技术：信息组织与检索技术、数据库技术、数据仓库和数据挖掘、协同工具技术、知识管理技术\n政府信息资源管理\n电子政务\n\n3.政府信息资源管理有哪几种类型\n\n可以完全对社会公开的信息\n只在指定的系统或部门之间共享的信息\n只在本系统或部门内部共享的信息\n只对某一或某些特定的个体开放的信息\n\n4.政府信息资源管理有哪些特点\n政府信息具有数量大、价值高、储存分散、搜寻难等特点。\n\n政府是国家信息资源地最大拥有者，也是最大的信息生产者、使用者、发布者、政府信息资源地内容涉及政治、经济、科技、军事、文化、等众多领域，因此数量庞大，是信息资源地主要形式\n政府信息不仅数量大，而且常常比一般信息更有价值，直接关系到国民经济和社会发展地状况和水平。\n政府信息资源地特点还体现在其存储分散、搜寻难\n\n5.试分析政府信息资源共享和保密的关系\n从表面上看，公开与保密是一对矛盾，但实质上两者相辅相成、相互促进。公开是为了让公众共享政府信息和接受公众的舆论监督，保密则是在更高的层次上排除障碍，以确保特定对象对政府信息的有效共享和对政府行为的及时监督。\n6.政府信息资源管理有何意义\n\n政府信息资源管理为实现政府信息化保驾护航\n政府信息资源管理与国家安全息息相关\n政府信息资源管理为培养优秀的政务人员创造条件\n\n7.政府信息资源管理应遵循哪些原则\n\n领导必须重视信息工作\n制定详细而又周密的政府信息化以及信息资源开发和利用规划\n建立有效的政府信息资源网络，并对每个重要的信息资源建立管理控制系统，保证信息资源网络产出的可靠性和有效性\n制定政府信息资源管理政策\n\n8.政府信息资源管理的主要任务是什么\n\n增强信息意识，提高政府部门工作人员对信息资源的认识\n提高政府信息资源开发和利用的能力，即政府信息资源的采集、处理、利用和交流的能力\n最大限度地降低政府信息活动的费用，使政府信息的生产最经济，分配最有效，使用最容易和最方便，政府信息的效用得到最大的发挥\n保障国家信息安全\n\n9.试述政府信息资源管理的主要内容\n\n政府信息化政策的制定\n政府信息化发展计划\n政府信息化的项目管理\n政府信息的管理\n政府信息基础设施的管理\n政府信息技术的管理\n政府应用信息系统管理\n政府信息化的人力资源开发\n政府信息资源管理相关的法律和法规\n政府信息的安全管理\n\n10.试述政府信息资源建设的内容\n政府信息资源管理的组织结构，包括两个层次，决策层和执行层\n政府信息资源管理的实施\n\n电子政务：G2G，G2B，G2C\n电子政务的系统构成：网络应用平台子系统，网络安全管理子系统，信息发布子系统，公文运转子系统，经济计划管理子系统，项目管理子系统\n我国电子政务发展对策分析：首先要转变观念，充分认识发展电子政务的重要性和紧迫性，从战略高度重视电子政务的发展\n\n11.试述政府信息化与电子政务的关系\n政府信息化是指用信息技术全面推进政府部门的办公自动化、网络化和电子化以及信息资源共享，利用信息技术、信息资源和信息网络来提高政府的决策水平，机关的工作效率和公务人员的信息能力的进程\n电子政务，即在政府内部行政电子化与自动化的基础上，利用现代计算机和通信技术，建立起网络化的政府信息系统，并通过不同的信息服务设施，为企业，社会组织和公众提供政府信息服务。所以说，政府信息化是一个渐进的过程，它涵盖了电子政务的全部内容，不间断地推进政府组织机构、运作流程的改进，重组公共管理，它的高级阶段和最终目标就是实现电子政府\n12.政府信息化建设的优势与劣势分别是什么\n优势：\n\n从动机来看，政府部门是进行信息化建设的热情倡导者，信息化是政府适应时代发展，与时代发展同步的自身和内在要求\n从资金来看，政府部门解决了信息化建设所需的财政资金问题\n从信息技术的应用来看，政府部门是现代信息技术的积极推进者\n\n劣势：\n\n政府部门习惯的管理模式和思维上的惰性是信息化建设的首要障碍\n对信息技术的应用认识不当\n政府部门信息建设各自为政的作风阻碍信息的共享\n\n13. 我国政府信息化建设\n\n起步阶段（20 世纪 80 年代初 —— 20 世纪 90 年代初）\n推进阶段（20 世纪 90 年代初 —— 20 世纪 90 年代末）\n发展阶段（1999 —— 2001年）\n高速发展阶段（2002 至今）\n\n14.电子政务\n政府机构应用现代信息和通信技术，将管理和服务通过网络技术集成，在Internet上实现政府组织结构和工作流程地优化重组，超越时间、空间和部门分割地限制，全方位地向社会提供优质、规范、透明、符合国际水准地管理和服务，电子政务代表未来政府信息资源管理地发展方向\n15.G2B电子政务\nG2B是指政府通过电子网络进行电子采购与招标，精简管理业务流程，快捷迅速地为企业提供各种服务。G2B主要包括：电子采购与招标、电子税务、电子证照办理、信息咨询服务、中小企业电子服务\n信息政策与法律\n信息政策是据以调控信息生产、交流和利用的措施、规范和准则的集合，它涉及信息产品的生产、分配、交换和消费各个环节。\n1.信息政策包括哪些要素\n\n摩尔提出的信息政策要素：\n\n法规问题\n宏观经济问题\n组织问题\n社会问题\n\n\n希尔提出的要素：\n政府的信息获取，政府信息管理，政府对信息和传播技术的利用，信息技术产业，电信和网络，信息经济，工业农业和商业，科学技术信息，信息产业，图书馆和档案馆，公共部门和私营部门问题，向公众发布官方消息，出版和传媒，跨国数据流，隐私部门和数据保护，社会事件，健康和消费者信息，教育和工作，信息自由与国家安全和犯罪预防，法律方面，知识产权和工业产权，质量和可靠性，信息的生产\n\n2.试述信息政策的体系结构\n三个层次：\n\n产业政策层面，研究信息政策如何规范信息服务部门的发展\n组织政策层面，研究信息政策对机构内信息利用的影响，提高组织内部的效率和竞争力\n社会政策层面，研究人们利用信息的方式\n\n五个因素：\n\n信息技术因素，通信设施，决策支持系统等政策\n信息市场因素，信息市场不仅仅局限于商业信息的交换，而因面向所有的信息交换\n信息管理因素，改进信息管理和信息利用的过程，扩展其范围\n人力资源因素，通过教育和培训，提高公民的信息意识，改进个人获取和处理信息以及利用信息获益的技巧和能力\n信息法规因素，强调立法工具对实现信息政策目标的作用\n\n3.信息法律的主体和客体分别指什么\n信息法律的主体，即权力义务主体是指在信息法律关系中依法享有权力承担义务的人或组织。包括了社会生活中存在的能够、有资格享有权力和承担义务的全部主体形式：\n\n政府部门，包括各政府机构和代行部分政府职能的相关组织\n经济组织\n非营利性组织，包括学校、社会团体和事业机构\n个人\n\n信息法律的客体，是指一定的行为以及在特定环境中的物化的和非物化的财产。包括：\n\n信息资源\n信息技术\n各相关主体的信息行为\n\n4.信息法律与信息政策的区别是什么\n\n在性质上，信息法律是一种法律手段、具有强制性和约束力；而信息政策是一种行政手段，是一种软性的指导性准则或指南\n在作用范围上，信息法律的问题、对象比信息政策更基本，更普遍、时效更长、相对更稳定；信息政策具有波动性和针对性，政策的对象范围和有效时间范围比信息法律要小，更带有阶段性和灵活性\n从二者的关系上，信息政策是信息立法的基础，信息立法是保障信息政策得以贯彻和实施的重要法律手段\n\n5.构建信息法律体系遵循什么原则\n\n充分考虑各种相关权利主体和利益和地位，通过立法规范主体行为，创建一个适于各行行为主体充分发展的政策法律环境\n必须考虑到信息技术和信息产业的未来发展，具有前瞻性\n考虑到信息活动的跨行业、跨地域的特性，将信息立法置于社会结构、产业结构、技术结构以及国际关系的大背景之下通盘考虑，加强信息产业于其它产业，国内法于国际法的协调。\n\n6.信息立法要解决哪些问题\n\n信息资源管理问题，信息资源是一种重要的战略资源，信息资源的开发利用决定着一国科技和经济竞争水平。信息资源主要包括政府信息资源、商用信息资源、公共信息资源三种类型，必须按照资源共享的要求，实现信息资源的合理布局和有效利用\n信息技术和信息产业发展问题，信息产业是未来社会的支柱性产业，通过立法措施，明确信息技术和信息产业在经济与社会发展中的地位和作用，用法律手段来促进信息技术和信息产业的发展，这在世界上许多国家都有成功的先例\n信息市场管理问题，目前，我国信息市场还没有建立起完善的管理规范和有效的运行机制，垄断与无序竞争并存，缺乏必要的游戏规则，迫切要求制定相应的法律法规，促进信息市场走向法制化，逐步建立一个富于竞争的、规范化的信息市场\n知识产权保护问题，现代信息技术的发展，突破了传统的信息存储方式和获取手段，使的以各种形式存储的信息都容易复制和扩散，网络信息版权纠纷、域名纠风等各种新的侵权案件层出不群，使知识产权保护变得更加困难和复杂，因此必须修改和补充知识产权保护的现行法律法规，使其适用于以电子媒介和数字化形成为主的网络信息传递方式\n信息安全与计算机犯罪问题，如何在保证信息系统开放性的前提下，保障计算机信息系统和信息网络的安全、保护国家秘密和个人隐私，已成为信息法所面临的有一难题\n信息利益分配和“信息公平”问题，由于信息将成为未来最重要的资源，信息技术的发展可能造成一种新的社会不公，及信息贫穷和信息富裕的两级分化，因此从调整社会利益分配的角度，信息法律应考虑信息公平问题，防止信息技术的发展造成新的分配不公，妨碍经济和社会的均衡发展\n信息国际化问题，必须以法律形式对信息的跨国流动作为限制性的规定，在保障信息自由的同时确保国家的信息主权\n\n7.试述信息法律体系的结构\n\n信息基本法\n信息法实施细则及补充规定\n信息资源管理法律制度、信息自由与信息安全法律制度、知识产权保护法律制度\n信息市场与信息服务法律制度、信息传输与数据交换法律制度、新闻出版与信息传播法律制度、信息技术和信息产业法律制度、国际信息合作与交流法律制度\n\n8.试述信息法律体系的主要内容\n\n信息资源管理法律制度，包括政府、公益、商用三方面信息资源管理条例\n信息自由与信息安全法律制度，包括信息自由法、个人信息保护条例、计算机犯罪处罚条例、保密法、科学技术保密条例、档案法、科技档案管理条例、广播电台电视台保密规定、网络信息安全法、信息系统安全保护条例、信息加密与解密管理条例、个人隐私保护条例等\n知识产权保护法律制度，包括专利法、专利法实施条例、著作权法，商标法等\n信息市场与信息服务法律制度，信息市场管理条例、信息贸易税收管理条例、信息产品与服务质量管理条例、信息产品价格管理条例、信息咨询业管理条例等\n信息技术与信息产业法律制度，包括信息技术法、信息产业发展法、信息技术标准化管理条例、计算机技术发展条例、信息产业投资管理条例等\n新闻出版与信息传播法律制度，包括新闻法、出版法、广告法、信息传播法、电子出版物管理条例等\n信息传输与数据交换法律制度，包括电信法、邮政法、数据通信法、计算机信息网络国际联网管理规定等\n国际信息合作与交流法律制度，包括涉外信息交流法、关于信息产业利用外资的规定、跨国数据流宣言、跨国数据传输管理规定等\n\n10.帕累托最优\n经济学意义上的效率指地是资源地配置以及达到这样一种状况：无论作何改变都不可能同时使一部分人受益而其他人不受损，也就是说经济运行达到有效时，一部分人进一步改善处境必须以另一些人处境恶化为代价。反之，如果资源配置是低效率的，那么通过改变现有的资源配置，至少一部分人可以提高福利水平，而不减少其他人的福利。经济学家经常将资源地最大效率配置称为“帕累托最优”，并将其作为检验经济总体运行效率与社会福利的一种准则\n11.信息适合率\n获取的信息与所需的内容吻合度，例如，解决问题的有效程度，为利用该信息而需要加工处理的必要程度等都是评价尺度。\n论述题\n1.信息资源的功能\n根据休息资源在社会经济活动中的利用的过程和发挥的特点，我们可以把信息资源的主要功能归纳如下：\n\n经济功能。信息作为最重要的资源，具有经济功能。信息资源的经济功能表现在多个方面，在经济活动中发挥不同的作用，其中最重要的是他对社会生产力系统的作用功能\n管理与协调功能。在企业活动中的作用主要体现在：传递整个系统的运行目的，有效管理资源；调节和控制物质流与能源流的数量、方向和速度；传递外界对系统的作用，保持企业的内部环境稳定。\n选择与决策功能。信息的选择与决策功能广泛作用于人类选择与决策活动的各个环节，并优化起选择与决策行为，实现预期目标。信息的这种功能体现在两个方面：没有信息就无选择和决策可言；没有信息的反馈，选择和决策就无优化可言\n研究与开发功能\n\n2.宏观层次的信息资源管理应该遵循哪些原则\n\n信息资源是一种重要的经济资源，要从思想上把它提高到一个战略的认识高度\n信息资源管理是一项复杂的社会系统工程，规模巨大、结构复杂、必须实行分级分类管理\n大力推广使用现代信息技术，以提高信息资源的开发水平和利用效果\n确定信息资源管理的保密和保存制度，协调与国际间的交流关系\n\n3.信息资源管理手段及作用\n\n技术手段：是指以计算机和通信技术为基础的现代信息系统和信息网络以及与此相适应的信息加工方法，是信息管理的主要手段\n经济手段：运用各种经济杠杆和利用诱导作用，促使信息资源开发利用机构从经济利益上关系自己的活动，是一种间接组织和协调信息资源开发与利用活动的手段。\n信息资源管理的经济手段具有下述功能：\n\n调节功能，包括调节信息资源开发利用各个机构之间、各个环节之间的关系以及调节国家、集体和个人之间的利益关系；\n控制功能，即通过价格、税率、利率等经济杠杆引导各项信息资源开发利用活动向信息资源管理的目标靠拢\n核算功能，即通过价格、税收、工资等经济杠杆核算劳动耗费，比较投入产出，平衡社会需求\n监督功能，即借助会计、统计、审计、银行、监管、稽查等手段，根据法律和规章，对信息资源开发利用机构及其政府、职工和相关企业之间的关系进行监督管理\n\n\n法律手段：指用以协调信息资源开发利用活动的各种有关的法律规范的总称。与经济手段相比，信息资源管理的法律手段具有普遍的约束性、严格的强制性、相对稳定性和明确的规定性等特点\n行政手段：指凭借国家政权的权威，采取命令指示等形式来直接控制和管理信息资源及其相关活动。应该注意以下几个问题：\n\n要明确行政手段的使用范围和条件\n要提高运用行政手段的决策水平\n要注意防止“多头管理”现象的发生\n要注重原则性和灵活性的有机结合\n\n\n\n4.信息资源管理的经济手段\n信息资源管理的经济手段是指运用各种经济杠杆和诱导作用，促使信息资源开发利用机构从经济利益上关心自己的活动，是一钟间接组织和协调信息资源开发与利用活动的手段。信息资源管理的经济手段具有下述功能：\n\n调节功能，包括调节资源开发利用各个机构之间、各个环节之间的关系以及调节国家、集体和个人之间的利益关系\n控制功能，即通过价格、税率、利率等经济杠杆引导各项信息资源开发利用活动向信息资源管理的目标靠拢\n核算功能，即通过价格、税收、工资等经济杠杆核算劳动耗费，比较投入产出，平衡社会需求\n监督功能，即借助会计、统计、审计、银行、监管、稽查等手段，根据法律和规章，对信息资源开发利用机构及其政府、职工和相关企业之间的关系进行监督管理\n\n5.试述信息资源配置的经济意义\n\n有效的信息资源有利于更好地满足人类对资源的需求。有效的信息资源管理有利于最大限度的降低产品成本，合理的资源配置结构不仅有利与提高产品生产及营销管理中信息资源的使用效率，防止信息资源的闲置、浪费和短缺并存的弊端，而且有利于改善新产品成本构成中信息资源与信息资源的管理之间的关系，提高各类生产性资源和非生产性资源的综合使用效果\n有效配置信息资源有利于最大范围内实现资源共享。信息资源具有共享性。信息资源的共享范围直接决定了信息资源开发利用的经济价值。信息资源共享范围的扩大除了取决于政治、经济等环境因素外，主要取决于下列两个因素：\n\n信息资源的量，包括质量和数量。其中优良的质量是促进信息资源共享的内在因素，充足的数量是确保信息资源共享的外在条件\n信息资源扩散程度。信息资源共享以信息资源获取为前提。信息资源扩散程度越大，信息资源共享范围就越广。有效配置信息资源有利于调节和改善上述影响因子\n\n\n有效配置信息资源有利与防止信息资源污染，实现社会可持续发展。有效配置信息资源有利于形成最合理的信息资源开发和利用体系，形成信息资源开发和利用的良性循环。在该体系里，信息资源开发和利用被提到了有效开发和有效利用的高度。零污染的信息是可持续发展社会的重要特征。\n\n6.信息资源配置的主要原则\n[[信息资源管理笔记/信息资源管理导论#span-id信息资源配置应遵循哪些原则2信息资源配置应遵循哪些原则span|2.信息资源配置应遵循哪些原则]]\n7.网络环境下的信息资源配置\n\n网络信息资源的时间矢量配置：指网络信息资源在时间坐标上的配置。这种配置从时态上有过去、现在和将来之分，从时段上又有大小之分和连续与不连续之分。信息资源在时间矢量配置的经济意义是由信息资源内容本身的时效性决定的\n网络信息资源的空间矢量配置：主要考虑以下两个方面，1.充分认识国家经济发展在不同区域、不同行业的不平衡因素，有重点地配置信息资源。2.网络信息资源在空间矢量上的配置既要避免重复以充分发挥网络信息资源共享的优势，又要保持信息资源在不同“节点”上的适度冗余，以方便不同用户的使用\n网络信息资源的品种类型配置：网络信息资源在时间和空间矢量上的配置必然要涉及到信息资源的品种类型。由于对于既定的信息资源系统而言，当冗余信息量趋于零时，该系统必定是不同内容的信息的集合，集合中的每一信息都具有独特的性质，因此，网络信息资源系统规模的大小和服务能力的强弱不能简单地看其信息拷贝数量是否庞大，而应当综合性地以信息资源品种类型地多寡及其对网络用户信息需求地满足程度作为主要评判依据\n\n8.市场地自组织作用（市场机制对信息生产地自组织作用）\n信息资源地市场配置机制是指市场通过价格杠杆自动组织信息地生产和消费。信息资源地市场配置是通过市场机制对信息生产地自组织过程实现地\n\n市场可以减少信息产生地不确定影响，不确定性是信息生产的内在属性，也是制约信息生产的一大因素。市场机制允许多个企业为某一新产品进行竞争性的研究开发，这种做法表面上看会造成一定的资源浪费，但效率往往更高。技术竞争也有利于刺激信息产生效率的提高\n市场能自动的使企业、个人甘冒创新风险，为信息生产提供动力，信息生产的风险是明显存在的，因为信息生产的投资是不可逆的，研究与开发的创新活动不一定成功，一但失败，便会带来巨大的损失，但另一方面也有巨大的吸引力，如果获得成功，就会因此获得巨大的收益。简而言之，信息生产投入、风险的另一端是高收益的诱惑\n市场通过价格信号引导信息产生，市场把信息生产成功与否的裁决交给消费者，这即达到了信息生产服务于消费者的目的，又达到了引导信息产生的目的，消费者需求的变化，常常通过市场价格反映\n市场竞争的压力迫使企业不断创新，在市场体制下，技术水平低，创新能力差的企业会自动被市场淘汰，企业不创新就等于自杀。\n市场制度有助于培育创新的企业家，这里所说的企业家，是创新的组织者。在市场机制下，经过优胜略汰的选择，一些有才能的企业家会脱颖而出。\n\n9.信息资源配置机制的生长功能\n所谓信息资源配置的生长功能，就是充分利用信息资源对物质资源的替代，更多地在产品价值构成中融入技术的成分。一个社会的资源配置，如果不能使已有的技术得到充分的应用和推广，便无法形成良性循环。即一方面信息资源闲置，不能附加于产品之上，产出的价值创造处在较低水平；另一方面，低价值构成的产出不能提供必要的剩余，技术进步受到阻碍，导致经济在低水平上缓慢增进。较高的技术利用率则相反，通过产品附加价值的提高，可以在不影响投资和消费的前提下提供较多的信息开发费用，进一步推动技术进步。这种良性循环机制，无疑是经济发展的引擎。信息资源配置机制的生长功能，实际上是技术进步在生产中的广泛应用\n10.企业知识管理的流程\n企业知识管理的基本流程包括哪些步骤\n11.政府信息资源管理产生的背景\n试述政府信息资源管理产生的背景\n12.信息立法主要解决哪些问题\n信息立法要解决哪些问题\n信息立法"},"信息资源管理笔记/图书馆学":{"slug":"信息资源管理笔记/图书馆学","filePath":"信息资源管理笔记/图书馆学.md","title":"图书馆学","links":["信息资源管理笔记/图书馆学"],"tags":["信息资源管理学"],"content":"图书馆学基础 (P138)\n1.试述数据、信息、知识、文献之间的关系 (P138)\n\n数据是对某种情况的记录，包括数值数据与非数值数据，它是能够被记录和储存在计算机等媒体上的已知事实\n经过加工处理后形成的具有参考价值的数据就转化为信息，信息是具有普遍性的事物运动的状态和方式，是由事物发出的数据、信号中所包含的意义。信息是知识的原料\n知识来源于信息，是被人类系统化后的信息的一部分。知识可以促进信息的再生，新的信息又可以转换为新的知识\n文献是储存与传递知识的载体，它是知识的一种物化形态，是知识范畴中的显性知识部分\n\n2.简述图书馆学研究对象与研究客体的关系 P139\n图书馆学研究对象是图书馆学所要阐释一种社会的本质现象，而研究客体则是与这种本质现象发生密切联系的一个现象系统。\n图书馆学的研究客体是客观知识、知识集合、知识受众以及相互之间的关系。就一门学科而言，其研究客体包含了研究对象，并以研究对象为其客体系统的核心\n3.简述文献单元与知识单元有哪些区别 P141\n文献单元：是专门记录和传递有知识的人工载体单元，如一本书、一篇文章等\n知识单元：是知识系统中有实际意义的基本单位。如一个概念、公式等\n前者具有“硬性”特点，后者具有“软性”特点；前者往往表现为知识体系，后者往往表现为知识点；前者是静止的，后者是衍变的；前者组织方法比较单一，后者组织方法则复杂多样。但文献单元与知识单元又是相互联系的，在许多方面又有着共同的特性\n4.简述图书馆读者权力有哪些 P142\n读者在获取知识活动中享有平等获取知识权，自由选择知识权，知识信息知情权，知识服务保障权，对知识服务机构的批评、建议和监督权等\n5.举出两位图书馆学家，试述其主要贡献 P146\n文献整理编纂家：刘向刘歆父子，郑樵，王云五\n经营服务扩展家：柳诒徽，袁同礼\n学科理论创建家：梁启超，杜定友，刘国钧\n专业人才教育家：韦隶华女士，王重民\n6.图书馆事业的主要特征有哪些 P148\n\n典型的公益性社会事业，由于图书馆属于公共物品，所以它成为社会公益事业的一个重要组成部分。一个国家图书馆事业的繁荣、发展水平，折射出该国家社会福利、公平与正义的发展水平\n突出的文化教育性职能，图书馆既是保存文化、传播文化、融合文化、创新文化的重要工具，同时也是实现自我教育、穷人教育，全面教育、终身教育的重要手段。\n重要的社会公共知识空间，图书馆可以向人们提供社会化的、客观化的大量显性知识媒介，所以本质上它是人们可以自由出入的、开放的一个公共知识空间\n\n7.图书馆事业建设应遵循哪些原则 P148\n\n政府承担投资主体的责任，国家和政府是图书馆事业的投资主体。\n国家力量和社会力量相结合，由于国家和政府的财力优先，社会力量包括基金会、社会团体、企业、个人等。依靠社会积极性举办基层图书馆，是发展图书管事业、适应社会信息需求的重要原则与措施\n全面规划与分工合作相结合，一要统筹安排，合理布局，保持图书馆事业的平衡发展，二要全面进行规划，既要保证重点图书馆的建设，使其在图书馆资源建设上达到先进水平，发挥它们对现代图书馆建设的促进作用和示范作用，又要支持和扶持中小型图书馆与民办图书馆的发展。三要相互协作与协调，加强图书馆业务辅导，以建设起一个分工协作的社会图书馆网络体系\n发展图书馆学教育，加强图书馆学研究，图书馆事业的发展依赖于图书馆学教育与图书馆学理论研究成果的支持与指导。同时，图书馆学理论研究对图书馆事业建设和图书馆实际工作有着积极的推动作用\n\n8.试述图书馆工作标准化的作用与意义 P152\n图书馆工作标准化的作用：\n\n使图书馆工作由繁变简，由杂乱走向统一和规范\n减少重复劳动，节省各种资源，提高图书馆工作质量和社会效益\n确保图书馆工作的成果和用品科学、统一、通用\n加速图书馆工作现代化，充分发挥图书馆现代技术的作用\n提高图书馆工作的计划性和整体性，有利于图书馆事业的整体协调与国际合作\n\n图书馆工作标准化的意义：\n有利于图书馆工作的现代化，有利于图书馆事业的协调与协作，有利于图书馆集中统一的管理\n9.什么是信息资源建设？请说明信息资源建设的具体内涵 P153\n信息资源建设是指人类对处于无序状态的各种媒介信息进行选择、采集、组织和开发等活动，使之形成可资利用的信息资源体系的全过程。信息资源建设的内涵包括三个方面：信息资源建设所针对的是处于无序状态的各种媒介的信息，信息资源建设活动的内容是对信息进行选择、采集、组织和开发，信息资源建设的目的是形成可资利用的信息资源体系。\n信息资源建设的内容：\n\n信息资源体系规划。根据信息资源体系的功能要求，设计信息资源体系的微观结构和宏观结构\n信息资源的选择与采集。根据已经确定的信息资源体系的基本模式，通过各种途径，选择与采集信息资源，建立并充实馆藏，扩大“虚拟馆藏”，是信息资源建设的基础工作\n馆藏数字化与数据库建设。运用计算机和大容量的存储技术、全文扫描技术、多媒体技术，将馆藏中具有独特价值的印刷性文献转化为扫描版全文电子文献，制成光盘或在网上传播，同时开发图书馆的书目资源和特色资源，编制数目资源数据库和特色资源数据库，提供网上利用\n网络信息资源的开发利用，根据用户的需求与信息资源建设的需求，搜索、选择、挖掘因特网中的信息资源，并下载到本馆或本地网络中，建立信息资源导航库以方便用户迅速检索和利用\n信息资源的组织管理，对入藏的文献信息资源进行加工、整序、布局、排列、清点和保护；对数字化信息资源进行整合，将购买的数据库与自建的数据库有机地集成一体，对其内容进行充分地揭示，实现跨库检索，提供一站式服务\n信息资源共建和共享，通过整体规划与图书馆之间地分工协调，建设相对完备地文献信息资源保障体系，形成覆盖面宽，可利用方便快捷地数目信息网络，实现网络公共查询、联机合作编目\n\n10.什么是数字化信息资源？它包括了哪些类型的信息资源 P152\n数字化信息资源是指数字化的形式，将文字、图像、声音、动画等多种形式的信息存储在光、磁盘等非纸质载体中，以光信号、电信号的形式传输，并通过计算机和其他外部设备再现出来的信息资源。它包括网络信息资源和单机信息资源。\n11.信息资源建设政策有哪些基本内容 P153 P156\n信息资源建设政策是人们为实现信息资源建设目标而制定的方针、原则、策略、措施、对策等。\n信息资源建设政策的基本内容包括：\n\n图书馆所服务的地区或机构的基本情况和图书馆任务的分析\n图书馆信息资源现状分析\n信息资源建设目标的确定\n图书馆信息资源结构的规划\n印刷型文献的采访政策\n数字馆藏建设政策\n书刊交换与接受赠书政策\n经费分配政策\n馆藏信息资源管理政策\n\n12.何为参考咨询？参考咨询的基本内容有哪些 P159\n参考咨询是图书馆员对读者在利用文献和寻求知识、情报提供帮助的活动。它以协助检索、解答咨询和专题文献报道等方式向读者提供事实、数据和文献线索。或者说，参考咨询是参考咨询馆员以文献为依据，通过编制资料或利用检索工具等方式，有针对性地为用户揭示，检索和传递知识信息的信息服务工作。\n参考咨询的内容：\n\n答复咨询，就是对读者提出的一般知识性问题如事实、数据等，通过查阅各种载体的工具书及有关的书刊资料等，直接给予答复；或者指引读者自己查阅有关的工具书及其它书刊资料，求的问题的解决。\n书目参考，则是对读者提出的一些研究性问题，通过提供各种形式的专题文献目录索引，供读者查阅所需文献资料，以解决有关课题的咨询\n信息检索，是指将信息按一定方式组织和存储起来，并按需检索有关信息的程序、方法和过程\n情报研究，指根据用户的特定需要，为用户收集、处理、研究和提供情报信息\n用户教育，指为读者熟悉与利用图书馆、向读者普及信息检索知识等提供辅导和培训的活动\n参考服务评价，是指对参考咨询服务工作作出定性或定量的评价，以检查发现参考咨询服务中的问题，促进工作的改进，提高服务的质量\n\n13.参考信息源有哪些类型 P164\n参考信息源可粗略地分为传统参考信息源和电子参考信息源，前者即为印刷型工具书，后者为电子工具书。\n14.书目参考方法主要有哪些 P166\n书目参考工作地立足点是建立在对文献信息地加工上\n粗加工的书目参考服务包括编写题录、新刊目次报道、新书报道、信息摘编等，精加工的专题情报服务包括编制专题书目、专题索引、专题文摘、引文索引、地方文献书目等，综合性的情报研究包括撰写综述、述评和研究报告等。\n15.何为虚拟参考咨询？虚拟参考咨询有什么特点 P167\n虚拟参考咨询又称虚拟参考服务，虚拟参考服务是建立在网络基础上的将用户与专家和学科专门知识联系起来的问答式服务。或者说，虚拟参考咨询是建立在数字通信基础上，通过网络收发电子邮件、网页表单或者使用在线聊天软件、呼叫中心软件、视频会议软件、网络联系中心软件、即时视像软件等，给远程用户提供方便、快捷的信息咨询服务。\n虚拟参考咨询的特点：\n\n咨询参考源多元化\n咨询服务手段自动化\n咨询服务方式多样化\n咨询服务内容信息化\n咨询服务对象广域化\n咨询服务队伍专业化\n咨询服务观念人本化\n\n16.虚拟参考咨询有哪些主要方式 P169\n\nE-mail及Web表单服务\nFAQ服务\n案例库服务\n专题库与特色库服务\nReal-time服务\nBBS服务\n科学导航服务\n电子剪报服务\n电子文献传递服务\n用户教育服务\n\n17.数字图书馆的体系构成有哪些方面 P173 P174\n技术构成：\n\n知识信息的采集和移植\n知识信息的组织\n知识信息的检索\n知识信息的安全\n\n体系构成：\n\n资源加工系统\n储存管理系统\n资源调度系统\n网络运营系统\n\n18.数字图书馆有哪些功能 P174\n\n知识信息的获取与创建\n知识信息的存储与管理\n知识信息的访问与查询\n知识信息的动态与发布\n知识信息的权限管理\n\n19.我国数字图书馆建设面临的问题有哪些 P175\n\n技术发展问题\n知识产权问题\n人才培养问题\n运用模式问题\n\n20.信息资源建设的内容 P154\n\n信息资源体系规划\n信息资源的选择与采集\n馆藏资源数字化与数据库建设\n网络信息资源的开发利用\n信息资源的组织与管理\n信息资源共建与共享\n信息资源建设基本理论与方法研究\n\n21.信息资源建设的原则 P155\n文献资源建设是指依据图书情报机构的服务任务与服务对象以及整个社会的文献情报需求，系统地规划、选择、收集、组织管理文献资源，建立特定功能的文献资源体系的全过程。\n\n实用性原则，指图书馆以及其他类型信息机构要从实际使用出发，规划、选择、收集、整序、组织和管理文献资源，以最大限度地满足社会文献信息需求。它是信息资源建设地基本原则，要求各类型图书馆和其他信息机构根据具体地服务任务和服务对象地需求来进行信息资源建设，建立符合实际使用需要地学习资源体系\n系统性原则，指在学习资源建设中要特别注意宏观和微观地信息系统中各个要素之间地相互关系，注意信息资源系统与环境的联系。他是由科学知识的系统性、信息资源自身的系统性、读者需求的系统性、保存于传递的系统性所决定的\n特色化原则，指一个图书馆的信息资源应该具有自己独特的风格，具有其他图书馆信息资源不同的特点。数字化环境中，信息资源建设特色化包括两个方面：一是馆藏文献特色化建设，二是特色数据库建设。图书馆应该建设具有学科特色、专题特色、地方特色、文献类型特色的信息资源体系\n共建共享性原则，指一个地区、一个系统、一个国家，乃至全球的图书馆之间，图书馆与其他信息机构之间建立广泛的合作关系，科学划分，分工协作，共同建设，相互提供利用，建立相互联系、相互依存的信息资源保障体系。\n\n文献资源建设的四项原则是一个相互联系、不可分割的统一体。实用性是基本原则，系统性和特色化则既要以实用性为前提，又是实用性原则保证。共建共享原则则把信息资源建设的实用性、系统性和特色化从微观邻域带入宏观邻域，丰富了这些原则的内涵，同时也使信息资源建设真正成为一项社会性的事业，并对促进社会的发展和进步发挥重要的作用。\n22.郑樵\n字渔仲，南宋人，根据大量文献资料编辑的《通志》200券是一部综合历史史料而成的通史，其中总天下学术、条其纲目而编就的《二十略》被《四库全书总目》评价为《通志》全帙的菁华所在\n23.柳诒徽\n字翼谋，江苏镇江人开“住馆读书”之先例。规定“有志研究国学之士，经学术家之介绍，视本馆空屋容额，由馆长主任认可者，得住管读书”\n24.袁同礼\n字守和，河北省徐水人。在美国获得图书馆学士学位回国，先后任广东岭南大学图书馆馆长、北京大学图书馆馆长、国立北平图书馆馆长等职。在主持北平图书馆工作期间，访求书籍不遗余力，礼贤下士，造就学者，使北平图书馆的业务工作一跃而居全国之首\n25.刘国钧\n字衡如，出生于江苏南京，先后在金陵大学图书馆、北平图书馆、国立西北图书馆任职或担任馆长。后任教于北京大学图书馆系并担任过系主任。他在图书馆学基础理论、图书分类、图书编目、中国图书史、图书馆工作自动化邻域有着突出的建树\n26.王重民\n字有三，河北高阳县人。在国立北平图书馆任职期间，曾被派到法国、梵蒂冈、英国等地图书馆收集与研究中国流失海外的图书资料，如敦煌遗书、太平天国文献等。他抄录卡片、拍摄微缩胶卷、做提要或札记，成绩斐然，知名海外。1947年向北大校长胡适建议创办了图书馆学专修科，曾延聘名师后来任教或授课，培养了一批图书馆学专业人才\n27.文献采访\n指的是图书馆的采访工作人员通过对馆藏、读者需求和文献源的调查，有目的、有计划和系统地收集馆藏所需地文献信息资源，以满足读者需求地过程。它包括了文献选择和文献采集两个重要环节\n28.书目参考\n是对读者提出地一些研究性问题，如专题性、专门性研究课题等，通过提供各种形式地专题文献目录索引，供读者查阅所需文献资料，以解决有关课题地咨询。由于它不直接提供具体答案，只提供资料线索，作为解决有关问题时参考，所以称为书目参考或称专题咨询\n29.参考咨询\n参考咨询馆员以文献为依据，通过编制资料或利用检索工具等方式，有针对性地为用户揭示、检索和传递知识信息服务工作\n30.数字图书馆\n能够存取海量数字化信息知识地资料库，包括现有馆藏地数字化资源，新采购地封装型电子出版物、全文数据库，通过网络采购，下载的网络电子出版物及其他免费资源等，以及相关的提供远程服务的信息检索系统\n31.个人数字图书馆\n指个人为了读书治学、在自己的计算机上采用免费或基本免费的全文数据库软件，将有关的网上知识信息和自创的数字化知识信息采集、存储，使之成为有组织的知识信息集合，以供个人使用的数字图书馆\n32.数字典藏\n是搜集和存储数字化信息以提供长期保存和使用的、具有存储和检索功能的信息资源系统\n33.简述图书馆管理的内容 P151\n\n图书馆系的决策，包括：图书馆发展方针、政策、战略的决策；各项业务工作的决策；人事制度的决定；财务管理决策等\n图书馆计划，包括：国家图书馆事业发展计划，如图书馆事业总体规划、图书馆网的发展计划、专业人员培养、科学研究与协调发展计划等；个体图书馆发展计划，如图书馆的长期与短期计划、整体发展规划与局部发展计划等\n图书馆组织。包括图书馆机构的组织，图书馆人员的配备，图书馆工作岗位的设立等\n图书馆领导，包括图书馆领导者的素质、领导层的结构配置、领导的方式方法、领导的沟通等\n图书馆控制，包括图书馆控制机制的建立、图书馆控制系统的设置、信息反馈的渠道、图书馆目标的确立、绩效评估标准的制定\n图书馆协调，包括图书馆内部的微观协调、图书馆与环境和其他社会系统的宏观协调\n\n34.文献信息资源建设的原则\n7.图书馆事业建设应遵循哪些原则 P148\n35.咨询的类型及其作用\n联合国工业发展组织，将咨询划分为如下类型：战略咨询、工程咨询、管理咨询、技术咨询。它是信息服务业的一项重要内容，通常是分行业进行的。支持该业的主要是国家或社会的情报研究和统计部门。这些部门设有庞大的情报信息收集网络和处理中心，并在计算机的支持下以出版物、公报、媒体及终端向各有关行业提供信息服务。\n36.图书馆参考咨询的内容\n12.何为参考咨询？参考咨询的基本内容有哪些\n37.图书馆参考咨询的方法\n指个人为了读书治学、在自己的计算机上采用免费或基本免费的全文数据库软件，将有关的网上知识信息和自创的数字化知识信息进行采集、存储，使之成为由组织的知识信息集合，以供个人使用的数字图书馆\n\n书目参考方法。书目参考工作的立足点是建立在对文献信息的加工上。按照文献信息的加工程度，大体上可分为粗加工的书目参考服务、精加工的专题情报服务及综合性的情报研究等三个服务层次。书目参考工作要注意选题的实用性、针对性、预见性和可行性，选材要注意系统性、权威性、客观性、准确性和新颖性，编排要注意科学性、合理性和方面性。\n文献检索方法。文件检索的过程，实际上是一个调查研究、分析问题和解决问题的过程。必须运用唯物辩证法的观点，认真开展调查研究，正确认识事物，仔细分析事物的矛盾性，运用不同的方法解决不同的矛盾，促进事物的转化。通常采用顺查法、逆查法、抽查法、排除法、限定法、扩展法、跟踪法、参引法、综合法等。\n信息检索策略。网络信息检索是一种新型的检索手段和模式，必须遵循一定的检索步骤和运用有关的检索策略与技巧。\n检索步骤一般包括：\n\n确定检索客体；\n明确检索要求；\n选定检索系统；\n确定检索字段；\n编制检索式：\n执行检索；\n处理检  索结果。\n\n检索策略与技巧，可考虑：\n\n采用广度优先检索方法；\n优先考虑权威机构提供的专题息；\n选定检索系统；\n善于选择和运用相关的关键词；\n设法提高查准率；\n设法提高查全率；\n设法提高上网速度等。\n\n\n\n38.简要回答图书馆参考咨询的特点\n\n服务性，参考咨询工作从本质上说是一种知识信息服务。首先，参考咨询工作是属于直接为读者服务的一种工作。其次，参考咨询工作是图书馆传统读者服务工作的一种深化与发展。再次，参考咨询工作的根本和灵魂是服务。参考咨询工作必须做到面向用户，主动奉献\n针对性，主要指参考咨询服务选题的针对性。首先，参考咨询服务的选题，比较注意从图书馆的方针任务出发。其次，参考咨询服务注意针对读者学习、工作与生活的具体需要。再次，参考咨询服务重视个性化服务。尤其在网络环境下，更为注意开展个性化定制服务，这已经成为数字图书馆建设与发展的一个重要趋势\n智力型，参考咨询服务从业务上说属于一种知识密集型劳动。首先，参考咨询工作突出体现了图书馆的情报职能与教育智能。其次，参考咨询工作业务要求较高，尤其是高层次的咨询服务学术性较强。需要有较丰富的知识、经验和技术\n\n39.数字图书馆的技术构成\n\n知识信息的采集和移植，包括传统的知识信息采集和电子的知识信息采集。其技术手段有扫描、微缩、光学识别（OCR）、语音识别、人工智能，以及文本、图像、音频、视频知识信息的处理\n知识信息组织，包括信息组织技术、压缩技术、海量信息存储技术。目前的数字图书馆的信息组织主要应用有MARC数据、元数据、XML等技术，信息压缩则涉及文本、视频、音频等多种形态和标准，海量信息存储是分布式结构来完成的\n知识信息的检索，包括知识信息以视频、音频、多媒体方式的检索。数字图书馆的文献以全文检索为主，同时还可进行因特网搜索引擎的检索\n知识信息的安全，包括网络安全、信息加密和数字水印等\n\n信息资源组织\n1.什么是信息组织？信息检索工具有哪些类型 P177\n信息资源组织也称为信息组织，是根据使用的需要，以文本及各种类型的信息资源为对象，通过对其内容特征等进行分析、选择、处理、序化、并以适当的方式加以提供的活动。\n对于检索工具的类型，可以有多种不同的区分方法。\n\n按照检索工具的对象和特点，检索工具包括文献目录、索引、机读数据库和网络搜索引擎。\n检索工具按照依据的标识特征，可以分为以信息资源的外部特征为检索依据，提供从资源形式出发进行检索的途径，和以表达信息资源主题内容的标识为依据，提供从内容角度进行检索的途径。\n按照系统中所用的标识是否进行控制，检索工具可以分为受控检索系统与自然语言检索系统两种。\n按照系统中标识组配的特点，检索系统可以分为先组式检索工具和后组式检索工具。\n按照采用的形式，检索工具可分为卡片式、书本式、缩微式、电子方式等类型；\n按照信息资源对象，检索工具可分为图书、期刊、报纸、专利、标准、档案、图像、会议文献等类型。\n\n2.说说信息组织中常见的处理方法 P178\n\n对信息资源的特征进行描述，包括著录或编录，通过记录信息资源的外部特征，供用户识别或确认该资源\n对信息资源的内容进行浓缩，包括编写内容提要或文摘，使用户能迅速了解其内容和特点，供检索查找时选择、判断\n提供检索点，主要是确定检索特征及可检索信息，以便据此对信息资源进行有序化组织\n\n3.什么是信息描述？其作用是什么 P179\n信息资源描述，又称为文献著录，是指根据信息组织和检索的需求，对信息资源的主题内容、形式特征、物质形态进行分析、选择、记录和活动。\n信息描述的作用是：\n\n用来确认和识别该资源对象\n提供信息资源位置的信息\n通过描述数据中提供检索点，方便用户对资源的检索和利用\n供用户对资源的价值进行判断，决定是否选用\n\n4.文献标引有哪些基本的标引方式？分类标引工作一般遵循哪些步骤？ P183\n标引方式：\n\n整体标引，指概况揭示信息资源基本主题内容的标引，亦称浅标引\n全面标引，指充分揭示信息资源论及其所有符合检索系统要求的主题概念的标引，亦称深标引\n对口标引，指只揭示信息资源中适合本专业需要的主题内容的标引，亦称重点标引\n综合标引，指以集合型信息资源（如丛书、多卷书、论文集等）的整体为单位进行的概况标引\n分析标引，指根据资源中部分片段或集合型信息i资源的构成单元进行的标引\n\n分类标引步骤：\n​\t信息描述的操作程序通常为：查重——描述——标引——复合并输入系统\n5.信息资源分类及其作用是什么？分类法有哪些基本类型 P185\n信息资源分类是指根据信息资源的内容属性和其他特征，将各种类型的资源分门别类地、系统地组织和揭示的方法。分类过程中，用来作为区分根据的属性或特征，称为分类标准。信息资源分类的作用包括：进行资源组织，建立分类检索工具，分类统计，兼容工具。\n按照信息资源分类法的编制方式，通常可分为等级列举式、分面组配式、列举一组配式三种类型。\n6.一部完整的分类法一般是由哪些部分组成的？说说类目之间的关系及表现形式 P186 P189\n对分类法的结构组成，国内大致有两种划分方法：一种是按照各部分的功能，将分类法分为类目体系、标记符号、说明与注释、类目索引四个方面；另一种则是按照构成的形式，将其分为编制说明、主表、副表、类目索引等四个部分。\n分类法类目之间的关系：\n\n从属关系，指类目体系中一个类与其直接区分出来的子类之间的关系\n并列关系，指类目体系中同位类之间构成的关系\n交替关系，指交替类目与相应使用类目之间的形成关系\n相关关系，指类目之间除从属、并列、交替等方式以外的其他联系\n\n上述类目关系类型中、丛书关系、并列关系反映了类目的纵向联系；交替类、类目参照则是对纵向联系的补充，揭示被类目体系所分散了的横向关系\n7.对分类标记有哪些基本要求？分类标记的种类、编号制度有哪些？什么是预留空号法、八分法、双位制、借号法 P187 P188\n分类标记符合的要求：\n\n简明性\n表达性\n容纳性\n助记性\n\n号码种类：\n\n纯号码，单充号码指一种具有固定次序的符号系统构成的号码，常用的有数字和字母两种\n混合号码，指由两种或两种以上具有固定次序的符号系统构成的号码，通常由数字、字母组合使用\n\n编号制度：\n按照号码的组成方式，分类标记一般可以分为：\n\n顺序标记制，按照类目在分类体系中的次序、顺序配予号码，号码只表示类目的次序，不揭示类目的等级或其他关系，这种配号方式称为顺序标记制。顺序标记制便于均衡分配号码，使标记简短，容纳性强，适合文献排架，但不能揭示类目体系的结构\n层累标记制，按照类目划分的等级配置相应位数号码，号码不仅可以反映类目次序，并可以根据标记的位数判断出类目的等级，这种标记方式称为层累标记制\n顺序一层类标记制，结合采用顺序制和层类制的标记方式\n分面标记制，通常以特定的符号或组配方式表示各个主题因素所属的分面，使号码不仅能够揭示类目的分面结构\n\n标记技术：\n\n八分法：亦称广九法，即在采用层累数字标记的情况下，当同位类超过10个，不足18个时，前9为以0~8表示，8后面的标记用两位数字表示一次划分，用于解决同为类的号码配置问题。\n双位制：亦称集团表记法，即在采用层累数字标记的情况下，当同位类超过18个时，直接以两位数字表示一次划分，以解决号码的扩充问题。\n借号法：在采用层累数字标记的情况下使用的一种灵活借用上、下位类号码的配号方法。通常在上一级号码较宽裕时，用上级号码标示下级类目，在下级类目超过10个但超过不多时，根据情况，借用9以外的下级号码进行扩充，使号码配置更加灵活。\n预留空号法：指根据科学发展情况和类目设置的可能，在配号时，预先留下一些空号，供类目增补时使用。\n\n8.网络分类法与传统分类法有何区别 P190\n揭示角度不同，类目的设置特点不同，展开的形式不同，同位类排列方式不同，适用特点不同。\n9.主题法的特征是什么？主题法有哪些基本类型 P192 P194\n主题法是指直接以表达主题内容的词语作为检索标识、以字顺为主要检索途径的标引和检索信息资源的方法\n特征：直接选用自然语言中的词语进行标引和检索；以字顺作为主要检索途径，字顺方式始终是它的主要排检依据；以特定的事物、问题、现象，即主题为中心集中信息资源；通过在主题词下设置用、代、属、分、参等多种参考系统等方式解释主题词之间的关系。\n类型：标题法、元词法、叙词法、关系词法、受控主题法与非控主题法\n10.一部完整的叙词表至少应该包括哪两大部分？每个大部分有包含哪些内容？叙词表中叙词之间主要有哪三种关系？叙词表的编制应包括哪几个步骤 P195 P197 P199\n一部完整的叙词表一般应当至少包括字顺显示、系统显示两部分。\n字顺显示包括：\n\n字顺表\n专有词表\n双语种索引\n字顺索引和入口词表\n\n系统显示包括：\n\n范畴索引\n词组索引\n轮排索引\n分类主题一体化词表\n图形显示\n\n叙词表中叙词之间主要有三种关系：同等关系、等级关系和相关关系\n叙词表的编制一般包括总体设计、选词、对词汇进行处理、编表、审核和试标引出版等步骤\n11.概要说明范畴索引和词族索引的作用和编制方法 P196\n范畴索引是一种按照词汇所属学科或专业范畴编制的概略分类系统，是从分类角度查找叙词的辅助工具，范畴索引的作用：\n\n便于从分类角度查找与某一范畴有关的叙词\n可以作为概略分类的依据\n是编表选词、处理词间关系的工具\n\n词族，是指一组具有属分关系的叙词集合。词族索引是一种以词族为款目单元，按照款目词的字顺排列，可以从等级关系的角度查找叙词的索引，词族索引的作用：\n\n可以从词族出发查词\n可在计算机检索系统中，自动进行上位词登录\n可通过等级关系限定词义\n\n12.什么是轮排索引？轮排索引有哪些作用 P197\n轮排索引，亦称轮排表，是将词表中的叙词按词素的字顺排列，是含有同一词素的叙词集中显示与一处的词汇表，是一种从词素的角度查找叙词的辅助工具。作用是：\n\n可以从词素角度出发对叙词进行查找\n利用按词素集中的叙词的特点，帮助使用者选择最合适使用的叙词\n依据其字面成族的特点，进行词间关系处理\n\n13.主题标引时标题确定一般应主意哪些问题。简单说说主题标引中的查词规则 P200\n注意问题：\n\n做好主标题的选择\n对叙词的引用次序作出规定\n规定轮排模式\n\n查词规则：\n\n采用正式叙词标引\n采用专指叙词\n组配标引\n上位叙词标引\n靠词标引\n增词标引\n自由词标引\n注意标引深度的一致性\n\n14.什么是后控词表？后控词表有何特点 P202\n后控词表是一种在检索阶段进行控制的词表，这种词表不像传统的词控制表那样在标引阶段对标识实施控制，而是在检索阶段通过同义控制和相关词推荐方式提供检索帮助。\n特点：\n\n不承担标引功能，只用于检索控制，词汇控制不如先控词表严格\n入口词丰富\n动态性强\n有较强的灵活性和自由度\n具有面向文献和用户的特点\n词间关系不同于传统词表\n\n15.信息资源描述\n信息资源描述，又称为文献著录，是指根据信息组织和检索的需要，对信息资源的内容、特征进行分析、选择、记录的活动。信息描述的结果使描述记录，以及元数据，可以用来代表信息资源组织检索系统\n16.检索点\n检索点是指信息资源所使用的提名、责任者、分类号、主题词等各种提供检索使用的数据\n17.网络分类法\n网络分类法放弃传统分类法以学科为中心的特点，采用以主题为中心、或以主题结合学科的方式建立分类体系\n18.叙词\n国内亦称主题词，是经过规范化处理的，以基本概念为基础的表达文献主题的词和词组\n19.信息资源分类法的类型\n按照信息资源分类法的编制方式，通常可以分为等级列举式、分面组配式、列举—组配式三种类型\n等级列举式：一种将所有类目组织成一个等级系统，并且采用尽量列举的方式编制的分类法。它是目前使用最普遍的分类法形式。比较著名的有：美国的《杜威十进制分类法》（DDC），我国的《中图法》\n分面组配式：是一种依据分析兼综合的原则编制的分类法类型。这种分类法在类表中按照范畴列出各种基本概念，并分别配予相应号码；使用时，先分析标引的主题，根据主题分析的结果，通过类目的组配表达文献主题。典型的分面组配式分类法有印度阮冈纳赞创制的《冒号分类法》（CC）\n列举—组配式：式上述两种编制方式的结合，是一种在详尽类表的基础上，广泛采用各种组配方式的分类法。例如：《国际十进制分类法》（UDC）\n文献分类法还可以根据其涉及的学科领域范围的不同，分为综合性分类法、专业分类法；根据类分文献的规模，分为大型分类法、中小型分类法；根据其适用的文献类型，分为图书分类法、文献分类法、标准分类法、专利分类法、网络资源分类法等\n目录学 P204\n目录学的基本概念分为三类\n\n与书目有关的概念\n与书目情报有关的概念\n与目录学有关的概念\n\n1.什么是书目？书目有哪些基本类型 P205\n书目又称目录，他是著录一批相关文献，按照一定次序编排组织而成的一种揭示和报道文献信息的工具。\n类型：\n\n国家书目：是全面登记与报道一个国家近期或往昔出版物的文献总目\n联合目录：是揭示和报导若干文献收藏单位的全部或部分藏书的目录\n地方文献书目：是全面揭示和报道某一地区的自然、历史、社会文献的书目\n个人著述书目：是记载作者的著述或兼收他人评价作者及其著述的文献书目\n推荐书目：针对特定的读者，采用特定的标准选择图书推荐给读者的书目\n\n2.国内外学者对目录学的研究对象有哪些不同的论点 P211 P212\n\n图书说：认为目录学是研究图书的一门学科\n目录说：主张目录学的研究对象是书目\n图书和目录说：认为当代目录学的任务是辩章学术，考镜源流，推荐好书，指导阅读；研究对象首先是图书，其次是书刊目录\n关系说：有两种说法：①认为目录学的研究对象是研究社会生活中记录图书与利用图书的关系②认为目录学是研究认识与揭示图书的规律的科学，认识图书和揭示图书的活动规律就是目录学研究的对象\n矛盾说：认为目录学是研究书目工作形成和发展的一般规律的科学。\n\n3.谈谈我国古代目录学发展的成就与特点 P215\n先秦——两汉时期的目录学：\n\n开创了官修书目的先例\n为我国目录学的形成奠定了基础\n创立了“七分法”的书目分类体系\n创立了叙录体提要等揭示文献的方法\n\n魏晋南北朝时期的目录学：\n\n书目数量增加，类型增加，如佛经目录、私人书目出现\n对书目的作用有了更充分的认识，订正闻见，炳然区分\n对简明登记书目和提要书目两种体例进行了讨论\n在书目方法方面，创立了“立传”、“明宗”、“总经序、记存亡、设附录、撰缘记”等多种方法\n在书目分类方面，出现了六分发和四步法的竞争\n“流略”一词汇出现和“簙录”类的确立，说明目录学作为一门独立的知识系统已为人们所承认、重视\n\n隋唐时期的目录学：\n\n推荐书目的出现\n\n两宋时期目录学：\n\n专科目录的繁荣，索引的产生，目录著作的考证\n\n元明时期的目录学：\n\n创立了新的编排分类体系\n\n清代目录学：\n\n使用目录学成为“显学”\n\n4.请说说文献揭示的基本原则与方法 P219\n首先注意揭示文献的外部特征，但要以揭示文献的内容特征为主。注意揭示的广度和深度以及文献的社会影响。\n文献揭示的基本方法：\n\n著录法：著录是描述与记录文献题名、作者、版本等外部特征的方法。它是确认某一文献的基本依据，也是为获得文献，提供检索的初步途径。著录的要求是准确、完备、一致\n提要法：提要是简要揭示文献题意，介绍作者生平、学术思想、揭示或评介文献内容的方法。提要有助于了解作者生平事迹与学术渊源；也方便读者熟悉与利用文献，获得读书的门径。它是被广泛应用于图书的编辑、出版、发行、宣传，书目编制与文献编目、古籍整理、读书治学与学科研究中\n文摘法：摘要是以简要准确、不加任何评论的文字摘述文献主要内容的方法\n索引法：是揭示提名及所包含的专名，按照一定方法编排，以供查考它们在文献中的位置的方法\n综述法：是搜集多种同一主题内容文献，进行分析、归纳、总结与系统叙述的方法\n文献评价法：是对一篇文章或者一部著作的介绍与评论的方法\n\n5.简述书目的编纂方法\n书目编纂的整个过程就是综合的方法和分析的方法交替使用的过程，表现为综合——分析——综合的关系。编纂一部大型书目大体上要经过准备、分析、综合、结束等四个基本阶段。\n6.简述文摘的特点、类型和编纂方法 P222\n文摘的特点：\n\n替代性：文摘是对原文的一种替代\n模拟性：文摘是原文的一种情报模型\n转换性：包括语种的转换，表达方式的转换，等等。\n\n类型：\n\n从编撰目的和职能来看，大体分为普及性文摘和情报性文摘\n根据文摘编者又可以分为作者文摘、学科权限文摘和专职文摘员文摘\n根据文摘编写形式可分为文章式文摘和电报式文摘\n根据文摘刊登的地方可分为同址文摘和非同址文摘\n根据文摘的出版形式可分为单券式文摘和期刊式文摘\n\n编纂方法：\n\n确定选题和工作计划\n文献的搜集与选择\n摘要编写与款目著录\n\n7.索引有哪些类型？索引款目一般应由哪些项目组成？一部完整的索引通常应满足哪些条件 P228\n索引的类型：\n\n篇目索引\n语词索引\n主题索引\n著者索引\n专用索引\n\n结构组成：目次、序言、正文、辅助索引、附录\n满足条件：\n\n必须有众多索引款目组合而成\n由标目、修饰语和参照项三者组成款目。目标和参照项不可省略\n款目必须按照一定次序组成\n必须通过一定的方式显示标目之间的关系，而不是简单的排列\n\n8.什么是书目情报服务？书目情报服务一般包括哪些内容 P231\n广义的书目情报服务包括书目文献的编制、传递、利用等过程。侠义的书目情报服务，则仅指书目情报检索与利用服务。\n\n书目情报源建设\n参考咨询服务\n大众书目情报服务\n定题服务\n情报分析与研究服务\n读者书目情报意识的培养\n书目情报理论总结\n\n9.书目情报服务的形式和策略各有哪些 P232\n\n研究读者的提问\n研究提问的实质目的\n检索工具的选择\n确定检索方法\n检索词的选择\n检索式的编写\n检索结果的提供\n\n10.目录学研究的内容 P213\n\n关于目录学基础理论的研究\n关于文献的研究\n关于书目、索引类型及其编纂法的研究\n关于读者书目情报需求特点与书目情报服务的研究\n关于书目工作组织与管理的研究\n关于国内外目录学的研究\n关于中国目录学遗产的研究\n关于目录学方法的研究\n\n11.书目 P204\n数目又称目录，它是著录一批相关文献，按照一定次序编排组织而成的一种揭示和报道文献信息的工具。\n12.国家书目 P205\n国家书目是全面登记与报道一个国家近期或往昔出版物的文献总目。它是一个国家全部出版物现状与历史的记录。\n13.联合目录 P207\n联合目录是揭示和报道若干文献收藏单位的全部或部分藏书的目录。联合目录的特点是集中报道多馆藏书特点及分布情况，指明文献的收藏处所。\n14.推荐书目 P208\n针对特定的读者、采用特定的标准选择图书推荐给读者的书目。包括选读书目、导读书目等。推荐书目的特点是选题有明确的目的，有特定的读者对象，精选适合对象水平的图书，指出阅读方法，书目编排与图书揭示体现导读的精神。\n15.索引 P220\n索引是记录和指引文献中有关事项及单元知识的位置出处，按一定方法组织起来的检索工具。索引有多种称谓，如通检、引得、题录等。不同的称谓反映了不同历史时期人们对索引的认识。根据索引标目的性质划分，可分为：篇目索引、语词索 引、主题索引、 著者索引、专用索引、如分子式索引、化学物质索引等。\n16.文献揭示的基本方法\n[[信息资源管理笔记/图书馆学#目录学#4.请说说文献揭示的基本原则与方法]]\n17.目录学的主要分支学科 P213\n\n专科目录学，专科文献目录学是从特定读者群的需要出发，揭示与报导某一专门学科文献的状况的学科。 如：马克思列宁主义经典著作目录学、文学文献目录学、历史文献目录学、社会科学文献目录学、自然科学文献目录学、技术科学文献目录学等。\n书目控制理论， “书目控制”一词是由美国芝加哥大学的伊根和谢拉于1949年在其《书目控制论》一文中首先提出的。书目控制论研究利用控制论原理，对书目系统进行调节与控制等问题。主要目标是通过国家书目控制和全球文献控制实现全球文献资源共享。\n计量目录学，计量目录学是数学和统计学方法引进目录学以后产生的学科分支。它利用统计学方法来计量文献，目的在于用定量描述各门学科的书目信息特征过程与形式，以及它们的发展趋势，得出规律，以便对文献的累积与利用之间的关系进行量化分析。包括引证规律、文献增长规律、文献分布规律的研究等方面。\n比较目录学，比较方法在目录学中的应用产生了比较目录学。该分支学科主要研究不同文化传统下的书目工作体制、目录学理论的差异，并解释这些差异，寻找目录学共同的发展规律的一门学科。\n\n文献学 P234\n文献是特定知识内容的记录，是具有一定物质形态的载体\n1.试用自己的语言，表述图书在人类历史长河中的作用，以及图书文化的要以 P259\n图书是人类使用文字、图画或符号，记录知识或表达思想的著述载体。\n2.概述雕版印刷和活字印刷 P261\n雕版印刷术是我国传统的“四大发明”之一。中国是世界上最早使用雕版印刷术的国家，时间可以推到唐朝初年，此后雕版印刷业逐渐发展，在宋元时期已经达到顶峰。\n活字印刷术技术也是我国古人的发明。据记载，已知四种以上的印书活字，是中国人首先发明的。它们是：泥活字、木活字、铜活字、铅活字。\n3.图书装帧的本质属性何在？需要遵循的基本原则是什么 P262\n装帧之“帧”，一般指画幅，后用作专指画幅的量词单位。\n图书装帧要遵循四项原则：\n\n要体现思想性、艺术性和科学性的和谐统一\n要具有可欣赏性的艺术特点\n要坚持可行性与实用性的结合\n要强调特色的连贯性和风格多样性\n\n4.区别“阅”和“读”，并给予合理的解释和适当的发挥\n阅的本义是逐一点看，意思是用眼睛一字不漏的仔细观看。\n读的本义是照文字念、诵，通过用嘴出声念诵以达到对与字句意义的理解，如宣读、朗读、诵读。\n广义的阅读，含义及其广泛，它包括了从不知到渐知、从泛知到追求深知的全过程。阅读是主体与客体相互作用的过程，是阅读主体实践活动与精神活动的一种体现。\n侠义的阅读，涉及阅读客体，阅读主体和阅读本体，以及阅读学研究专题等内容。\n5.文献资源\n文献资源是相对于天然资源的一种社会之力资源，是物化了的知识财富，是人们迄今为止收集积累贮存下来的文献资料的总和。具有再生性、积累性、可建性、冗余性、共享性和价值潜在性特点\n6.天一阁\n我国现存最古的藏书楼，在今浙江宁波。明代范钦建于嘉庆年间。原藏书七万余券，后多散佚。\n7.《四库全书》\n古语”盛世修典“，《四库全书》大型丛书就是在清代乾隆盛世修成的一大文化经籍宝藏。从乾隆三十八年乾隆帝采安徽学政朱筠上奏，决定开设四库馆开始，到乾隆四十七年，第一部《四库全书》编成，再到乾隆五十二年，先后陆续完成七份副本的誊录，共历时15年，全书共收书3460余种，79337卷，每部装订成36300余册。可谓煌煌巨制，卷帙浩繁，汇集了从先秦到清前期的历朝历代的主要典籍，被誉为”千古巨制，文化渊薮“\n8.校勘\n用同一部书的不同版本和有关资料加以比较，考订文字的异同，目的在于确定原文的真相\n9.版本\n是基本上根据同样的输入信息制作并且由同一个机构或组织机构或个人发行的一种资源的所有副本。对于古旧单行资源，从基本上是同样的版心在任何时候印刷出来的一种资源的所有副本\n10.简牍\n简牍是对我国古代遗存下来的写有文字的竹简与木牍的概称。用竹片写的书称“简策”，用木版（也作“板”）写的叫“版牍”。在纸发明以前，简牍是我国书籍的最主要形式， 对后世书籍制度产生了深远的影响。\n11.《书林清话》叶德辉\n《书林清话》叶德辉著，为我国第一部系统的版本目录学专著。《书林清话》为读者提供关于古代雕版书籍的各项专门知识，诸如刻书源流、版本名称、校勘掌故、历代副书家历史等。\n12.藏书\n是对历代以图书为主的文献进行搜集、典藏、整理、研究利用乃至刊刻传播的文化活动。\n13.善本\n最早是指校勘严密，刻印精美的古籍，后含义渐广，包括刻印较早、流传较少的各类古籍。实际上，真正的善本仍应主要着眼于书的内容，着眼于古籍的科学研究价值和历史文物价值。具有历史文物性、学术资料性、艺术代表性。\n14.杜定友\n杜定友（1898-1967）是广东南海人，出生在上海。1921于菲律宾大学图书馆学专业毕业后回国，在上海、广州两地从事图书馆事业长 达 50 年，先后担任过广东省立图书馆馆长，南阳大学图书馆主任、中山大学图书馆主任等职。他在图书馆基础理论、图书分类学、 图书目录学、汉字排检法、图书馆理论、图书馆建筑、地方文献等领域皆有突出理论建树。\n15.文献揭示的基本方法\n\n著录法。著录是描述与记录文献题名、作者、版本等外部特征的方法。它是确认某一文献的基本依据，也是为获得某一文献的基本依据，也是为获得文献，提供检索的初步途径。著录的要求是准确、完备、一致。\n提要法。提要是简要解释文献题意，介绍作者生平、学术思想，揭示或评介文献内容的方法。提高有助于了解作者生平事迹与学术渊源；也方便读者熟悉和利用文献，获得读书的门径。它被广泛应用于图书的编辑、出版、发行、宣传， 书目编制与文献编目、古籍整理、读书治学与科学研究中。\n文摘法。摘要是以简要准确、不加任何平衡的文字摘述文献主要内容的方法。\n索引法。索引是揭示题名即所包含的专名，按一定方法编排，以供查考它们在文献中位置的方法。\n综述法。综述是搜集多种同一主题内容文献，进行分析、归纳、总结与系统叙述的方法。\n文献评价法。文献评价是对一篇文章或一部著作的介绍与评论的方法。\n\n16.史料的内容\n\n文字史料。指以往形成的书籍和文书等一切历史性文字材料，它们常常被概称为“历史文献”。\n实物史料。指历代古董、日用品、生产资料、服饰器物、建筑和古迹依存等，它们在历史研究工作中有着十分重要的作用。\n口传史料。指民间长期口语相传的史诗、传说、民谚、民谣、民歌，以及现代的口述史料等。\n记录着特定历史过程中的事实和现象，是一切史料的本质特点。\n\n17.文渊阁《四库全书》的流传于收藏\n《四库全书》：古语日“盛世修典”，《四库全书》大型丛书就是在清代乾隆盛世修成的一大文化经籍宝藏。从乾隆三十八年（1773）乾隆帝采纳安徽学\n政朱筠上奏，决定开设四库馆开始，到乾隆四十七年（1782）第一部《四库全书》编成，再到乾隆五十二年（1787）先后陆续完成七份副本的誉录，共\n历时15年，全书共收书3460余种（或日3503种），79337卷，每部装订成36300余册。可谓煌煌巨制，卷帙浩繁，汇集了从先秦到清前期的历朝历代的\n主要典籍，被誉为“千古巨制，文化渊鼓”。《四库全书》成书后，乾隆帝对其收存贮藏十分重视。他决定仿效“天一阁”规制，专门修建馆阁以收藏之。\n建成承德避暑山庄的文津阁、圆明园的文源阁、故宫的文渊阁、盛京沈阳的文溯阁，文津阁、文渊阁、文源阁、文溯阁史称“内廷四阁全书”或“北四阁”，\n主要是供皇帝阅读备览服务的。后来考虑到天下文人学子读书的需要，又命将陆续抄缮的三套《四库全书》分别送藏于扬州文汇阁、镇江文宗阁和杭州\n文澜阁，史称“江浙三阁全书”或“南三阁”。《四库全书总目》：由清代纪的等编寨，为我国古代最巨大的官修图书目录。四库全书的馆臣们，对眷录入\n库的3400余种图书（称“著录书”）和抄存卷目的6700余种图书（称“存目书”）全部写出提要，这就是《四库全书总目提要》，或简称《四库总目》。\n《四库提要辨证》余嘉锡以毕生精力研究《四库全书总目》总结而成的成果。其目录学理论著作《目录学发微》，系统地阐述了目录学的意义、功用、源\n流及书目的体例、书目分类等问题，在中国目录学史上有着重要地位。\n18.历史文献整理目的与方法\n文献整理的终极目的，是为了确定历史文献原文的真实可靠程度，以便于人们理解和使用，因此构成了历史文献整理的三个不可分割的环节：\n（1）历史文献的实证。即通过辨伪、版本、校勘、辑佚等手段，尽量恢复历史文献的原始面貌，考据原文内容的真实性，使之成为可靠可信的材料。\n（2）历史文献的解释。在上述基础上，通过标点注释、注释、翻译等手段，解释或解读被认为真实的历史文献，以便于人们更加准确地理解原文、把握作者的本意。\n（3）历史文献整序。书目和索引，是整理者对历史文献中蕴涵的知识元素进行开发和组织的一种手段，经过这种整序，可以让历史文献既方便而又快捷地为人们提供服务。\n19.20世纪80年代中国历史文献建立的主要原因\n\n中国历史文献学，作为一个学科的理论探讨是从20世纪80年代开始的。历史文献研究的悠久传统与丰硕成果，很显然，它为历史文献学的形成提供了宝贵的遗产。\n历史文献学的教学需要，也推动了历史文献学的建立。中国历史文献学作为一门学科，它首先是为大学中一门课程来建设的，这门课是在“中国历史文选”或“中国历史要籍介绍及选读”的基础上建立起来的。\n文献学的逐步形成，对历史文献学的建立有重要的影响作用，可以说，历史文献学是历史学的基础科学，同时也是文献学的一个重要分支学科。\n\n12.历史文献学的研究内容\n\n历史文献研究。显而易见，历史文献学研究的主要对象是历史文献。也可以这样说， 历史文献研究的缺失，历史文献学就失去了它存在的基础。如历史 文献的编纂、价值、类别、发展历史、数量、体裁及民族历史文献等一系列问题。\n历史文献整理方法的研究。文献目录（分类）、版本、校勘、考证、辨伪、辑佚、类纂、标点、注释、翻译，这些传统整理文献的方法与手段可以运用 在历史文献的整理上，但我们不能认为它们就是历史文献学的分支学科。历史文献学需要对 这些方法与手段进行一全新的认识，并随着时代的发展，对历史文献整理手段的现代化应有必要的关注。\n历史文献整理成就的总结，这方面的研究也可以看成是历史文献学史的回顾。\n历史文献学的分支学科。正史学、通鉴学、历史档案学等，应成为中国历史文献学的分支学科。随着研究的深入，分支学科也会逐渐增多。 中国历史文献学，作为高等院校中一门专业基础课，为相关本科学生提供必要的基础知识； 同时它又作为一个博士学位授权点，其建设的好坏，关切到本学科高深人才的培养。\n\n论述题\n1.试述数据、信息、知识、文献之间的关系。(P138)\n1.试述数据、信息、知识、文献之间的关系 (P138)\n2.图书馆管理的内容 (P151)\n33.简述图书馆管理的内容 P151\n3.文献信息资源建设的原则 (P155)\n34.文献信息资源建设的原则\n4.咨询的类型及其作用 (P159)\n35.咨询的类型及其作用\n5.图书馆参考咨询的内容 (P159)\n36.图书馆参考咨询的内容\n6.图书馆参考咨询的方法 (P166)\n14.书目参考方法主要有哪些 P166\n7.分类法类目关系 (P189)\n6.一部完整的分类法一般是由哪些部分组成的？说说类目之间的关系及表现形式 P186 P189\n8.目录学的主要分支学科 (P213)\n17.目录学的主要分支学科 P213\n9.文献的属性及基本要素 (P234)\n作为中国国家标准颁布的《文献著录总则》将文献定义为“记录有知识的一切载体”，从而强调了文献的两个基本属性：①是特定知识内容的“记录”，②是具有一定物质形态的“载体”。文献的构成要素：\n\n记录形式，传统的观点主要认为是不同语种的不同文字，如汉字、英文等；或者是同一语种的不同形态，如汉字的文言和白话之别。现代的观点则认为，把知识记录在载体上的形式还有图画、符号、声频、视频、电子刻录等现代手段。\n载体形式，我国历代使用的主要知识载体，依次有龟甲、兽骨、竹简、木牍、缣帛、纸张。古代文明国家如古巴比伦使用过泥版、古印度使用过标榈叶片、古埃及使用过纸莎草纸和羊皮等。中国的造纸术发明并传播到世界各地以后，世界各国都先后采用了价廉物美的纸张。如今，胶卷、胶片、磁带和光盘等新型文献载体开始广泛运用。\n写印形式，在印刷术发明之前，文献主要是以手写手抄的方式传录复制。中国印刷术发明以后，主要是以雕版印刷文献为主流，活字印刷文献和写抄文献只是一脉单传。近代西方印刷技术传入我国以后，中国传统的手工印刷业逐渐被石印、影印、铅印、扫描和摄录等机器印刷方式取代。\n装帧形式，我国文献的装帧形式主要有卷轴装、旋风装、经折装、蝴蝶装、包背装、线装、平装和精装等。雕版印刷、活字印刷文献和写抄文献，用的是质地绵软的传统中国纸，单面写印，折叶装订成为线装本。近代西方机器印刷技术传入后，纸张和装帧相应发生了变革，能够适应机器双面印刷的西式纸张“洋纸”应运而生，随之产生了新式的平装和精装。用这种方式印刷装订的书籍，当年被称为“洋装书”。\n\n10.阐述文献学研究的主要路径(P252)\n20世纪，特别是后20年，文献学的发展是多途径的。概括起来，约有如下几大系列：\n\n古典文献学系列，其主要作者有张舜徽、王欣夫、吴枫、罗孟桢、王燕玉、杜泽逊、程千帆、徐有富等。他们基本上以传统古籍整理的方法，即目录、版本、校勘等，加以立说。张舜徽的《中国文献学》，构建了古典文献学的规模，并建立了这一系列研究的基本范式。\n新型文献学系列，洪湛侯在《中国文献学新编》中，用形体、方法、历史、理论四方面来文献学的框架，但其基本内容还是古典文献学的，只是组配的方式有所不同而己。\n分科文献学系列，目前，有文学文献学，历史文献学、档案文献学、社会科学文献学，科技文献学、教育文献学、中医文献学、法律文献学、经济文献学、文学批评文献学等。这一系列的研究并没有一种统一的范式，各自从本学科文献特点出发，以文献揭示为目标，便于人们了解学科文献的面貌。\n分支文献学系列，主要有文献传播学、文献社会学、文献计量学、文献保护学、文献目录学、文献信息学，以及新近所见的文献经济学。这一领域的作者都比较年轻，他们勇于接受新知，运用相关学科的思想、方法，丰富与发展了文献学的研究，他们将成为文献学研究的生力军。\n文献史研究系列，从广义看，文献的编篆、印刷、出版、发行、交流、收藏的历史以及图书史等，均可看成是文献某一侧面的发展史。这方面的著作很多，主要专题史都已涉及，但仍缺少一部综合反映文献发展的史著。\n文献学史系列，目前可以见到的有中国文献学史、中国古典文献学史、中国历史文献学史，其实三者所涉内容基本相同。\n文献学专题系列，如周文骏《文献交流引论》、张欣毅《现代文献论纲要》、朱渊清《中国出土文献与传统学术》等。\n资料结集与工具书的编篆。\n\n11.图书文化的主要内涵 (P260)\n\n出版文化讨论书籍形成过程中的相关问题，书一旦形成，即可传播与传承，进入了人们收藏的范畴。\n藏书是对书籍进行收藏、整理、研究以及传播的文化活动。在书籍收藏各业务环节基础上形成的藏书环境、藏书风尚、藏书习俗和藏书情趣、藏书精神，构成了藏书文化的基本框架。\n阅读是阅读主体（读者）与文本（可以是一本书，也可以是整个宇宙）相互影响的过程，是阅读主体实践活动与精神活动的一种体现。它可以作为人类的一种文化现象被人们追溯与了解，因为阅读文化要研究书籍生产的最终目的—人类的阅读行为。\n\n12.论述图书馆藏书结构的构成 (P264)\n藏书结构是指藏书体系中各组成要素的构成状况及其相互联系。它决定了藏书体系的功能。藏书结构通常有五种构成方式：\n\n学科结构。它是各学科门类的藏书的比例结构，也是图书馆组织文献信息的基本方式。\n等级结构。图书馆根据文献内容的水平、程度及读者的需求层次，相应地划分出若干层次的收藏级别，并规定各级别应达到的收藏目标。它是图书馆藏书结构的基本框架。\n时间结构。是按文献的出版时间划分的层次结构，反映人类科学文化知识源远流长、继承发展的纵向关系，也是图书馆藏书系统性的反映。如何合理安排藏书的时间结构，是规划藏书体系的重要问题。\n文种结构。是指藏书中各种语言文种出版物的构成状况，反映了图书馆对社会文献信息覆盖的程度，是反映图书馆收藏水平和满足读者需求能力的重要标志。\n文献类型结构。是指藏书中各种不同出版形式、不同知识载体的文献的构成状况。图书馆应加强各种载体的文献资源建设工作，建立多样化的文献类型结构，以满足读者不断增长的多元化需求。\n"},"信息资源管理笔记/复习信息资源管理统考笔记/信息资源管理导论":{"slug":"信息资源管理笔记/复习信息资源管理统考笔记/信息资源管理导论","filePath":"信息资源管理笔记/复习信息资源管理统考笔记/信息资源管理导论.md","title":"信息资源管理导论","links":[],"tags":["信息资源管理学"],"content":"信息资源管理概论\n1.社会信息化包括哪几个层次\n三个层次\n\n生产工具信息化\n社会生产力信息化\n社会生活信息化。\n\n2.什么是信息资源\n人类社会信息活动中围绕信息的搜集，整理和利用而开展的一系列社会经济活动。\n3.信息资源有哪些特征\n\n人类需求性\n稀缺性\n可选择性\n共享性\n时效性\n不可分割性\n不同一性\n驾驭性\n\n4.信息资源有什么功能\n\n经济功能\n管理与协调功能\n选择与决策功能\n研究与开发功能\n\n5.试述信息资源管理的发展过程\n\n传统管理阶段\n信息管理阶段\n信息资源管理阶段\n\n6.信息资源管理的目标是什么\n保证信息资源的开发利用在有领导、有组织的统一规划和管理下，协调一致、有条不紊地进行，使各类信息资源以更高的效率、效能和更低的成本在国家社会进步、经济发展、人民物质文化生活水平的提高中充分发挥应有的作用\n7.信息资源管理的任务包括哪些方面\n\n制定信息资源的开发战略、规划、方针和政策。\n制定信息资源管理的法律、规章和条例。\n利用法律和行政手段协调各部门、各地区和各企业之间的关系。\n加强国家信息基础设施和信息资源管理网络的建设。\n\n8.信息资源管理有何意义\n\n信息资源管理开辟了管理新天地\n合理开发和有效利用的必要条件，保证其在成本最低时经济功能最强\n有利于保证信息资源开发利用机构的合法权益\n\n9.宏观层次的信息资源管理应遵循哪些原则\n\n信息资源是一种重要的经济资源\n实行分级分类管理\n国家的信息资源管理，主要是确定目标、进行投资决策，并为各级政府业务部门中观层次的信息资源管理提供条件\n大力推广使用现代信息技术，提高信息资源的开发水平和利用效果\n确定信息资源管理的保密和保存制度\n协调与其它国家之间的信息资源交流关系\n\n10.信息资源管理有哪些手段\n\n技术手段\n经济手段\n法律手段\n行政手段\n\n11.社会信息化\n人类对信息的依赖程度越来越高，而对物质和依赖程度相对降低。\n12.经济信息化\n主要指信息要素渗透到经济活动之中，社会经济的发展主要不是依赖物质材料的增加和新能源的开发，而是依赖信息力量的推动。\n13.信息资源\n信息资源是指人类社会信息活动中积累起来的以信息为核心的，围绕信息的搜集、整理、提供和利用而开展的一系列社会经济活动。\n14.记录型信息资源\n包括传统介质（纸张竹帛等）和各种现代介质（磁盘、光盘等）记录和存储的知识信息。它是信息资源存在的基本形式，也是信息资源的主体。\n15.零次信息资源\n指各种渠道中由人的口头传播的信息。比如书信、论文手稿、笔记、实验记录、会议记录等\n16.智力型信息资源\n这类信息资源主要表现为人脑存贮的知识信息，包括人们掌握的诀窍、技能和经验，又称为隐性知识。\n17.简述信息资源的选择与决策功能\n\n没有信息就无任何选择和决策可言\n没有信息的反馈，选择和决策就无优化可言\n\n18.信息资源管理的基本要素\n\n管理对象：信息资源\n实施领域：国际、国家和社会组织\n实施方法：综合使用各种管理手段，包括技术手段、经济手段、人文手段等\n信息资源管理的实质，即一种管理思想、管理模式和管理活动\n\n19.简述信息资源管理概念提出的两个背景\n\n信息管理阶段纯粹的技术手段不能实现对信息的有效控制和利用\n当代社会经济发展使得信息成为一种重要的资源，迫切需要从经济的角度思考问题，并对这种资源进行优化配置和管理\n\n信息资源的优化配置\n1.什么是信息资源配置\n指为最大限度减少宏观经济浪费和现实社会福利最大话而对现代技术成果与各种投入要素进行的有机组合。\n2.信息资源配置应遵循哪些原则\n\n社会经济福利最大化原则\n需求导向原则\n公平原则\n市场手段和政府手段互补原则\n\n3.试述信息资源配置的主要内容\n\n横向配置，部门、行业或地区地宏观布局，包括区域配置和行业配置。\n纵向配置，只有符合资源增值地投资项目，才能吸引资源地流入。\n\n4.网络环境下的信息资源应如何配置\n\n时间矢量配置\n空间矢量配置\n品种类型配置\n\n5.信息资源配置的机制是什么\n\n市场配置\n政府配置\n产权配置\n\n6.试述信息资源配置市场失灵的原因\n\n信息的外部效应\n信息的公共物品的属性\n信息商品的垄断性\n信息市场的信息不对称\n信息活动的非营利性\n\n7.政府在信息资源配置方面有何作用\n政府配置机制是指政府利用政策、法律、税收工具，或者通过直接投资和财政补贴来调整信息产出。\n\n信息基础设施建设\n公共信息服务\n教育\n科学研究\n制度建设\n\n8.信息资源产权配置的功能主要体现在哪些方面\n\n可以减少浪费，提高经济效率\n稳定的产权格局或结构，会形成一种资源配置的客观状态\n产权的变动同时也改变资源配置格局\n\n9.应从哪些方面考察信息资源配置的有效性\n\n信息生产的有效性\n信息商品的生产比例的有效性\n信息市场与交换的有效性\n\n10.试述我国文献信息资源的共享模式\n\n垂直型（纵向）共享\n水平型（横向）共享\n网络型共享\n\n11.信息资源配置的经济意义\n\n有效配置信息资源有利于更好地满足人类对资源地需求\n有效配置信息资源有利于最大范围内实现资源共享\n有效配置信息资源有利于防止信息资源污染，实现社会可持续发展\n\n12.信息资源的纵向配置\n指的是只有符合资源增值的投资项目，才能吸引资源的流入。\n13.信息市场失灵\n信息市场的自组织机制，对信息资源的市场配置存在着一些自身无法克服的缺陷，即市场失灵\n14.信息资源的政府配置\n指政府利用政策、法律、税收工具，或通过直接投资和财政补贴来调整信息产出\n15.信息资源产权配置的功能\n通过调整和明晰产权，优化信息资源配置\n16.如何衡量信息资源配置的效率\n\n总体社会资源优化配置和社会福利最大化的宏观目标\n必须立足于信息生产、信息服务有效的微观基础\n衡量信息资源配置效率需要从宏观，中观和微观三种不同的层次来加以考察\n\n17.网络信息资源的空间矢量配置\n网络信息资源不在同地区、不同部门之间分布，实质上是不同使用方向上的分配\n企业信息资源管理\n1.企业从哪些方面来评价信息\n及时性、准确性、综合性、获取简易性、经济性等\n\n能够及时以及适当的方式提供解决问题所需要的依据\n信息符合需求的内容\n信息的可信赖度高\n信息具有综合性\n信息容易获取\n信息费用与目标吻合\n信息发生源\n信息载体\n意图\n可靠性\n从不同的信息源获得同一性质的信息\n内容\n原因\n时间\n地点\n人\n方法\n途径\n状况\n存在率的评价\n适合率的评价\n可靠性的评价\n\n2.企业信息资源管理的任务是什么\n企业为达到预期目标，运用现代的管理方法和手段对与企业相关的信息资源和信息活动进行组织，规划，协调和控制，以实现对企业信息资源的合理开发和有效利用。\n企业信息资源管理的任务就是要有效地搜集、获取和处理企业内外信息，最大限度地提高企业信息资源的质量、可用性和价值，并使企业各部门能够共享这些信息资源\n3.企业信息资源管理的主要内容是什么\n\n提高企业全体人员对信息价值的认识\n促进信息在企业的共享\n为企业建立合理的信息结构\n保证信息的相关性和标准化\n提高信息质量，提高信息安全\n保证信息资源管理系统的可用性\n促进企业员工利用信息\n\n4.试述企业信息资源管理的技术框架\n\nMIS 管理信息系统\nIntranet 及其应用\nExtranet\n\n5.试述企业信息资源管理的组织结构\n\n企业信息总监 CIO\n信息技术人员\n信息管理人员\n信息资源管理辅助人员\n\n6.什么是知识管理\n知识管理是对知识及其创造，收集，组织，传布，利用与宣传等相关过程的系统管理\n7.企业知识管理的目标是什么\n\n提高企业创新能力\n提高企业反映能力\n提高企业效率\n提高企业员工技能\n实现企业知识资产价值\n\n8.试述企业知识管理的职能\n\n为联机和非联机知识建立知识地图\n向用户提供知识利用方面的指导和培训，并提供相应的知识检索工具\n监视企业外部知识源\n\n具体来说：\n\n外化\n内化\n中介\n认知\n评测\n\n9.企业知识管理的基本流程包括哪些步骤\n\n制定知识管理战略\n确定知识管理重点领域\n评估企业知识资源\n制定知识管理方案\n实施知识管理方案\n监督知识利用\n\n10.企业知识管理的实现技术主要\n\n网络技术\n群件技术\n知识库技术\n其它重要技术，Push技术、决策支持系统与专家系统技术、信息检索技术、超文本与电子出版技术\n\n11.知识管理\n是对知识及其创造、收集、组织、传播、利用与宣传等相关过程的系统管理。\n12.记录管理\n记录管理又称文书管理，记录管理政府内部业务工作的信息\n13.简述企业信息的特点\n\n信息的渠道来看，企业不但要搜集、存储和利用外部信息，还要对企业内部信息进行搜集、加工和存储。\n从信息载体来看，既有传统的印刷型，又有磁盘、光盘、微缩胶片、胶卷、网络等新型存储介质，既有文字型，又有实物型；既有书本型，又有报表式；既有正式文献，又有非正式书信。\n从信息内容来看，既有科技信息，又有经济信息、财务信息、市场信息等\n从时效性看，企业信息更要求新颖及时\n\n14.企业信息资源管理的主要内容是什么\n\n提高企业全体人员对信息价值的认识并促进企业活动对信息的需求\n促进信息在企业不同部门和不同群体之间的共享\n为企业建立合理的信息结构\n保证信息的相关性和标准化，以及最大限度地减少信息重复现象\n提高现象地质量，保证信息地安全\n保证信息资源管理系统地可用性和充分运行，以便在任何时候和地点均可及时检索到所需要地信息\n促进企业员工利用信息\n\n政府信息资源管理\n1.试述政府信息资源管理产生的背景\n\n信息资源观的转变\n信息技术的发展\n经济信息化的推进\n政府体制改革的需求\n\n2.试述政府信息资源管理的发展历程\n\n记录管理：文书管理\n信息技术管理\n政府信息资源管理\n电子政务\n\n3.政府信息资源管理有哪几种类型\n\n可以完全对社会公开的信息\n只在部门之间共享的信息\n只在内部共享的信息\n只对特定的个体开放的信息\n\n4.政府信息资源管理有哪些特点\n政府信息具有数量大、价值高、储存分散、搜寻难等特点\n5.试分析政府信息资源共享和保密的关系\n公开是为了让公众共享政府信息和接受公众的舆论监督\n保密则是为确保特定对象对政府信息的有效共享和对政府行为的及时监督。\n6.政府信息资源管理有何意义\n\n政府信息资源管理为实现政府信息化保驾护航\n政府信息资源管理与国家安全息息相关\n政府信息资源管理为培养优秀的政务人员创造条件\n\n7.政府信息资源管理应遵循哪些原则\n\n领导必须重视信息工作\n制定政府信息资源开发和利用规划\n建立有效的政府信息资源网络\n制定政府信息资源管理政策\n\n8.政府信息资源管理的主要任务是什么\n\n增强信息意识\n提高政府信息资源开发和利用的能力\n最大限度地降低政府信息活动的费用\n保障国家信息安全\n\n9.试述政府信息资源管理的主要内容\n\n政府信息化政策的制定\n政府信息化发展计划\n政府信息化的项目管理\n政府信息的管理\n政府信息基础设施的管理\n政府信息技术的管理\n政府应用信息系统管理\n政府信息化的人力资源开发\n政府信息资源管理相关的法律和法规\n政府信息的安全管理\n\n10.试述政府信息资源建设的内容\n\n电子政务：G2G，G2B，G2C\n电子政务的系统构成：网络应用平台子系统，网络安全管理子系统，信息发布子系统，公文运转子系统，经济计划管理子系统，项目管理子系统\n我国电子政务发展对策分析：首先要转变观念，充分认识发展电子政务的重要性和紧迫性，从战略高度重视电子政务的发展\n\n11.试述政府信息化与电子政务的关系\n政府信息化是指用信息技术全面推进政府部门的办公自动化，利用信息技术、信息资源来提高政府的决策水平，和公务人员的信息能力的过程\n电子政务，利用计算机和通信技术，建立的网络化的政府信息系统，并为企业，公众提供政府信息服务\n12.政府信息化建设的优势与劣势分别是什么\n优势：\n\n从动机来看，政府部门是进行信息化建设的倡导者\n从资金来看，政府部门解决了信息化建设所需的财政资金问题\n从信息技术的应用来看，政府部门是现代信息技术的推进者\n\n劣势：\n\n政府部门习惯的管理模式和思维上的惰性是信息化建设的首要障碍\n对信息技术的应用认识不当\n政府部门信息建设各自为政的作风阻碍信息的共享\n\n13. 我国政府信息化建设\n\n起步阶段（20 世纪 80 年代初 —— 20 世纪 90 年代初）\n推进阶段（20 世纪 90 年代初 —— 20 世纪 90 年代末）\n发展阶段（1999 —— 2001年）\n高速发展阶段（2002 至今）\n\n14.电子政务\n电子政务，利用计算机和通信技术，建立的网络化的政府信息系统，并为企业，公众提供政府信息服务\n15.G2B电子政务\nG2B是指政府通过电子网络进行电子采购与招标，为企业提供各种服务\n信息政策与法律\n1.信息政策包括哪些要素\n\n法规问题\n宏观经济问题\n组织问题\n社会问题\n\n2.试述信息政策的体系结构\n三个层次：\n\n产业政策层面\n组织政策层面\n社会政策层面\n\n五个因素：\n\n信息技术因素\n信息市场因素\n信息管理因素\n人力资源因素\n信息法规因素\n\n3.信息法律的主体和客体分别指什么\n信息法律的主体，信息法律关系中依法享有权力承担义务的人或组织。\n信息法律的客体，物化的和非物化的财产\n4.信息法律与信息政策的区别是什么\n\n信息法律是一种法律手段；而信息政策是一种行政手段\n信息法律的问题、对象比信息政策更基本，更普遍、时效更长、相对更稳定；信息政策对象范围和有效时间范围比信息法律要小，更带有阶段性和灵活性\n信息政策是信息立法的基础\n\n5.构建信息法律体系遵循什么原则\n\n创建适于各行的政策法律环境\n必须考虑到信息技术的未来发展\n考虑到信息活动的跨行业、跨地域的特性\n\n6.信息立法要解决哪些问题\n\n信息资源管理问题\n信息技术和信息产业发展问题\n信息市场管理问题\n知识产权保护问题\n信息安全与计算机犯罪问题\n信息利益分配和“信息公平”问题\n信息国际化问题\n\n7.试述信息法律体系的结构\n\n信息基本法\n信息法实施细则及补充规定\n信息资源管理法律制度\n信息市场与信息服务法律制度\n\n8.试述信息法律体系的主要内容\n\n信息资源管理法律制度，包括政府、公益、商用三方面信息资源管理条例\n信息自由与信息安全法律制度\n知识产权保护法律制度\n信息市场与信息服务法律制度\n信息技术与信息产业法律制度\n新闻出版与信息传播法律制度\n信息传输与数据交换法律制度\n国际信息合作与交流法律制度\n\n10.帕累托最优\n无论作何改变都不可能同时使一部分人受益而其他人不受损，也就是说经济运行达到有效时，一部分人进一步改善处境必须以另一些人处境恶化为代价。经济学家经常将资源地最大效率配置称为“帕累托最优”\n11.信息适合率\n获取的信息与所需的内容吻合度。\n论述题\n1.信息资源的功能\n\n经济功能\n管理与协调功能\n选择与决策功能\n研究与开发功能\n\n2.宏观层次的信息资源管理应该遵循哪些原则\n\n信息资源是一种重要的经济资源\n信息资源管理是一项复杂的社会系统工程\n大力推广使用现代信息技术，以提高信息资源的开发水平和利用效果\n确定信息资源管理的保密和保存制度，协调与国际间的交流关系\n\n3.信息资源管理手段及作用\n\n技术手段\n经济手段\n法律手段\n行政手段\n\n4.信息资源管理的经济手段\n信息资源管理的经济手段是指运用各种经济杠杆和诱导作用，促使信息资源开发利用机构从经济利益上关心自己的活动\n\n调节功能\n控制功能\n核算功能\n监督功能\n\n5.试述信息资源配置的经济意义\n\n有效的信息资源有利于更好地满足人类对资源的需求\n有效配置信息资源有利于最大范围内实现资源共享\n有效配置信息资源有利与防止信息资源污染\n\n7.网络环境下的信息资源配置\n\n网络信息资源的时间矢量配置\n网络信息资源的空间矢量配置\n网络信息资源的品种类型配置\n\n8.市场地自组织作用（市场机制对信息生产地自组织作用）\n信息资源市场配置机制是指市场通过价格杠杆自动组织信息地生产和消费。\n\n市场可以减少信息产生地不确定影响\n市场能自动的使企业、个人为信息生产提供动力\n市场通过价格信号引导信息产生\n市场竞争的压力迫使企业不断创新\n市场制度有助于培育创新的企业家\n\n9.信息资源配置机制的生长功能\n利用信息资源对物质资源的替代，在产品价值构成中融入技术的成分。\n信息资源配置机制的生长功能，实际上是技术进步在生产过程中的广泛应用"},"信息资源管理笔记/复习信息资源管理统考笔记/图书馆学":{"slug":"信息资源管理笔记/复习信息资源管理统考笔记/图书馆学","filePath":"信息资源管理笔记/复习信息资源管理统考笔记/图书馆学.md","title":"图书馆学","links":[],"tags":["信息资源管理学"],"content":"图书馆学基础 (P138)\n1.试述数据、信息、知识、文献之间的关系 (P138)\n\n数据是对某种情况的记录\n有价值的数据转化为信息\n信息是知识的原料，知识来源于信息\n文献是知识的载体\n\n2.简述图书馆学研究对象与研究客体的关系 P139\n图书馆学研究对象：图书馆学所要阐释一种社会的本质现象\n图书馆学的研究客体：客观知识、知识集合、知识受众以及相互之间的关系\n3.简述文献单元与知识单元有哪些区别 P141\n文献单元：是专门记录和传递有知识的人工载体单元，如一本书、一篇文章等\n知识单元：是知识系统中有实际意义的基本单位，公式，概念等\n4.简述图书馆读者权力有哪些 P142\n\n平等获取知识权\n自由选择知识权\n知识信息知情权\n知识服务保障权\n批评、建议和监督权\n\n5.举出两位图书馆学家，试述其主要贡献 P146\n文献整理编纂家：刘向刘歆父子，郑樵，王云五\n经营服务扩展家：柳诒徽，袁同礼\n学科理论创建家：梁启超，杜定友，刘国钧\n专业人才教育家：韦隶华女士，王重民\n6.图书馆事业的主要特征有哪些 P148\n\n典型的公益性社会事业\n突出的文化教育性职能\n重要的社会公共知识空间\n\n7.图书馆事业建设应遵循哪些原则 P148\n\n政府承担投资主体的责任\n国家力量和社会力量相结合\n全面规划与分工合作相结合\n发展图书馆学教育\n\n8.试述图书馆工作标准化的作用与意义 P152\n作用：\n\n使图书馆工作由统一和规范\n减少重复劳动，节省各种资源\n确保图书馆工作的成果科学、统一、通用\n加速图书馆工作现代化\n提高图书馆工作的计划性和整体性\n\n意义：\n\n有利于图书馆工作的现代化\n有利于图书馆事业的协调与协作\n有利于图书馆集中统一的管理\n\n9.什么是信息资源建设？请说明信息资源建设的具体内涵 P153\n指人类对信息进行选择、采集、组织和开发等活动，使之形成可资利用的信息资源的全过程\n信息资源建设的内涵：\n\n针对的是无序状态的各种媒介的信息\n对信息进行选择、采集、组织和开发\n形成可资利用的信息资源体系。\n\n信息资源建设的内容：\n\n信息资源体系规划\n信息资源的选择与采集\n馆藏数字化与数据库建设\n网络信息资源的开发利用\n信息资源的组织管理\n信息资源共建和共享\n\n10.什么是数字化信息资源？它包括了哪些类型的信息资源 P152\n指数字化的形式，将文字、图像、声音、动画等多种形式的信息存储在光、磁盘等非纸质载体中，通过计算机再现出来的信息资源\n11.信息资源建设政策有哪些基本内容 P153 P156\n\n图书馆基本情况和任务的分析\n图书馆信息资源现状分析\n信息资源建设目标的确定\n图书馆信息资源结构的规划\n印刷型文献的采访政策\n数字馆藏建设政策\n书刊交换与接受赠书政策\n经费分配政策\n馆藏信息资源管理政策\n\n12.何为参考咨询？参考咨询的基本内容有哪些 P159\n图书馆员对读者在利用文献、情报提供帮助的活动\n内容：\n\n答复咨询\n书目参考\n信息检索\n情报研究\n用户教育\n参考服务评价\n\n13.参考信息源有哪些类型 P164\n传统参考信息源和电子参考信息源\n14.书目参考方法主要有哪些 P166\n\n书目参考工作是建立在对文献信息地加工上\n粗加工：题录、新刊目次报道、新书报道、信息摘编等\n精加工：专题书目、专题索引、专题文摘、引文索引、地方文献书目\n\n15.何为虚拟参考咨询？虚拟参考咨询有什么特点 P167\n虚拟参考服务是建立在网络基础上的，将用户与专家联系起来的问答式服务\n虚拟参考咨询的特点：\n\n参考源多元化\n手段自动化\n方式多样化\n内容信息化\n对象广域化\n队伍专业化\n观念人本化\n\n16.虚拟参考咨询有哪些主要方式 P169\n\nE-mail及Web表单服务\nFAQ服务\n案例库服务\n专题库与特色库服务\nReal-time服务\nBBS服务\n科学导航服务\n电子剪报服务\n电子文献传递服务\n用户教育服务\n\n17.数字图书馆的体系构成有哪些方面 P173 P174\n技术构成：\n\n知识信息的采集和移植\n知识信息的组织\n知识信息的检索\n知识信息的安全\n\n体系构成：\n\n资源加工系统\n储存管理系统\n资源调度系统\n网络运营系统\n\n18.数字图书馆有哪些功能 P174\n\n知识信息的获取与创建\n知识信息的存储与管理\n知识信息的访问与查询\n知识信息的动态与发布\n知识信息的权限管理\n\n19.我国数字图书馆建设面临的问题有哪些 P175\n\n技术发展问题\n知识产权问题\n人才培养问题\n运用模式问题\n\n20.信息资源建设的内容 P154\n\n信息资源体系规划\n信息资源的选择与采集\n馆藏资源数字化与数据库建设\n网络信息资源的开发利用\n信息资源的组织与管理\n信息资源共建与共享\n信息资源建设基本理论与方法研究\n\n21.信息资源建设的原则 P155\n\n实用性原则\n系统性原则\n特色化原则\n共建共享性原则\n\n22.郑樵\n字渔仲，南宋人，编辑的《通志》200券是一部综合历史史料而成的通史\n23.柳诒徽\n字翼谋，江苏镇江人开“住馆读书”之先例\n24.袁同礼\n字守和，河北省徐水人。北京大学图书馆馆长、国立北平图书馆馆长等职。使北平图书馆的业务工作居全国之首\n25.刘国钧\n字衡如，出生于江苏南京，先后在金陵大学图书馆、北平图书馆馆长。任教于北京大学图书馆系系主任。他在图书馆学基础理论、图书分类、图书编目、中国图书史、图书馆工作自动化邻域有着突出的建树\n26.王重民\n字有三，河北高阳县人。向北大校长胡适建议创办了图书馆学专修科，培养了一批图书馆学专业人才\n27.文献采访\n图书馆的采访工作人员通过对馆藏、读者需求和文献源的调查，收集馆藏所需地文献信息资源，以满足读者需求地过程\n28.书目参考\n不直接提供具体答案，只提供资料线索，作为解决有关问题时参考，所以称为书目参考或称专题咨询\n29.参考咨询\n参考咨询馆员以文献为依据，利用检索工具，为用户揭示、检索和传递知识信息服务的工作\n30.数字图书馆\n能够存取海量数字化信息知识的资料库\n31.个人数字图书馆\n指个人为了读书治学、在自己的计算机上采用的全文数据库软件\n32.数字典藏\n是搜集和存储数字化信息以提供长期保存和使用的、具有存储和检索功能的信息资源系统\n33.简述图书馆管理的内容 P151\n\n图书馆系的决策\n图书馆计划\n图书馆组织\n图书馆领导\n图书馆控制\n图书馆协调\n\n35.咨询的类型及其作用\n\n战略咨询\n工程咨询\n管理咨询\n技术咨询\n\n37.图书馆参考咨询的方法\n\n书目参考方法\n文献检索方法\n信息检索策略：\n\n采用广度优检索方法；\n优先考虑权威机构提供的专题息；\n选定检索系统；\n善于选择和运用相关的关键词；\n设法提高查准率；\n设法提高查全率；\n设法提高上网速度等。\n\n\n\n38.简要回答图书馆参考咨询的特点\n\n服务性\n针对性\n智力型\n\n39.数字图书馆的技术构成\n\n知识信息的采集\n知识信息组织\n知识信息的检索\n知识信息的安全\n\n信息资源组织\n1.什么是信息组织？信息检索工具有哪些类型 P177\n信息资源组织也称为信息组织，是根据使用的需要，以信息资源为对象，对其内容特征等进行分析、选择、处理、序化、并提供的活动。\n\n文献目录、索引、机读数据库和网络搜索引擎\n受控检索系统，自然语言检索系统\n先组式检索工具，后组式检索工具。\n卡片式、书本式、缩微式、电子方式等类型\n图书、期刊、报纸、专利、标准、档案、图像、会议文献等类型\n\n2.说说信息组织中常见的处理方法 P178\n\n对信息资源的特征进行描述，包括著录或编录\n对信息资源的内容进行浓缩\n提供检索点\n\n3.什么是信息描述？其作用是什么 P179\n亦称文献著录，根据信息组织和检索的需求，对信息资源的主题内容、形式特征、物质形态进行分析、选择、记录的活动。\n信息描述的作用是：\n\n用来确认该资源对象\n提供信息资源位置的信息\n方便用户对资源的检索和利用\n供用户对资源的价值进行判断\n\n4.文献标引有哪些基本的标引方式？分类标引工作一般遵循哪些步骤？ P183\n标引方式：\n\n整体标引\n全面标引\n对口标引\n综合标引\n分析标引\n\n分类标引步骤：\n​\t查重——描述——标引——复合并输入系统\n5.信息资源分类及其作用是什么？分类法有哪些基本类型 P185\n根据信息资源的内容特征，将各种类型的资源分门别类地、系统地组织和揭示的方法\n作用：\n\n进行资源组织\n建立分类检索工具，分类统计，兼容工具。\n\n编制方式：\n\n等级列举式\n分面组配式\n列举一组配式\n\n6.一部完整的分类法一般是由哪些部分组成的？说说类目之间的关系及表现形式 P186 P189\n\n按照各部分的功能：类目体系、标记符号、说明与注释、类目索引\n按照构成的形式：编制说明、主表、副表、类目索引\n\n分类法类目之间的关系：\n\n从属关系\n并列关系\n交替关系\n相关关系\n\n7.对分类标记有哪些基本要求？分类标记的种类、编号制度有哪些？什么是预留空号法、八分法、双位制、借号法 P187 P188\n分类标记符合的要求：\n\n简明性\n表达性\n容纳性\n助记性\n\n号码种类：\n\n纯号码\n混合号码\n\n编号制度：\n\n顺序标记制\n层累标记制\n顺序一层类标记制\n分面标记制\n\n标记技术：\n\n八分法\n双位制\n借号法\n预留空号法\n\n8.网络分类法与传统分类法有何区别 P190\n\n揭示角度不同\n类目的设置特点不同\n展开的形式不同\n同位类排列方式不同\n适用特点不同\n\n9.主题法的特征是什么？主题法有哪些基本类型 P192 P194\n特征：\n\n直接选用自然语言中的词语进行标引和检索\n以字顺作为主要检索途径\n以特定的事物、问题、现象，即主题为中心集中信息资源\n通过在主题词下设置用、代、属、分、参等多种参考系统等方式解释主题词之间的关系。\n\n类型：\n\n标题法\n元词法\n叙词法\n关系词法\n受控主题法与非控主题法\n\n10.一部完整的叙词表至少应该包括哪两大部分？每个大部分有包含哪些内容？叙词表中叙词之间主要有哪三种关系？叙词表的编制应包括哪几个步骤 P195 P197 P199\n字顺显示：\n\n字顺表\n专有词表\n双语种索引\n字顺索引和入口词表\n\n系统显示：\n\n范畴索引\n词组索引\n轮排索引\n分类主题一体化词表\n图形显示\n\n叙词之间关系：同等关系、等级关系和相关关系\n叙词表的编制一般包括总体设计、选词、对词汇进行处理、编表、审核和试标引出版等步骤\n11.概要说明范畴索引和词族索引的作用和编制方法 P196\n范畴索引是一种按照词汇所属学科或专业范畴编制的概略分类系统，是从分类角度查找叙词的辅助工具\n作用：\n\n便于从分类角度查找与某一范畴有关的叙词\n可以作为概略分类的依据\n是编表选词、处理词间关系的工具\n\n词族，是指一组具有属分关系的叙词集合\n词族索引的作用：\n\n可以从词族出发查词\n可在计算机检索系统中，自动进行上位词登录\n可通过等级关系限定词义\n\n12.什么是轮排索引？轮排索引有哪些作用 P197\n亦称轮排表，是将词表中的叙词按词素的字顺排列，含有同一词素的叙词集中显示的词汇表，是从词素的角度查找叙词的辅助工具\n作用是：\n\n可以从词素角度出发对叙词进行查找\n帮助使用者选择最合适使用的叙词\n依据其字面成族的特点，进行词间关系处理\n\n13.主题标引时标题确定一般应主意哪些问题。简单说说主题标引中的查词规则 P200\n注意问题：\n\n做好主标题的选择\n对叙词的引用次序作出规定\n规定轮排模式\n\n查词规则：\n\n采用正式叙词标引\n采用专指叙词\n组配标引\n上位叙词标引\n靠词标引\n增词标引\n自由词标引\n注意标引深度的一致性\n\n14.什么是后控词表？后控词表有何特点 P202\n是在检索阶段进行控制的词表，在检索阶段通过同义控制和相关词推荐方式提供检索帮助。\n特点：\n\n不承担标引功能，只用于检索控制\n入口词丰富\n动态性强\n有较强的灵活性和自由度\n具有面向文献和用户的特点\n词间关系不同于传统词表\n\n15.信息资源描述\n亦称为文献著录，是指根据信息组织和检索的需要，对信息资源的内容、特征进行分析、选择、记录的活动\n16.检索点\n检索点是指信息资源所使用的提名、责任者、分类号、主题词等各种提供检索使用的数据\n17.网络分类法\n网络分类法放弃传统分类法以学科为中心的特点，采用以主题为中心建立的分类体系\n18.叙词\n国内亦称主题词，是经过规范化处理的，以基本概念为基础的表达文献主题的词和词组\n19.信息资源分类法的类型\n等级列举式：《杜威十进制分类法》《中图法》\n分面组配式：《冒号分类法》\n列举—组配式：《国际十进制分类法》\n目录学 P204\n1.什么是书目？书目有哪些基本类型 P205\n书目又称目录，他是著录一批相关文献，按照次序编排组织的，揭示和报道文献信息的工具。\n类型：\n\n国家书目\n联合目录\n地方文献书目\n个人著述书目\n推荐书目\n\n2.国内外学者对目录学的研究对象有哪些不同的论点 P211 P212\n\n图书说\n目录说\n图书和目录说\n关系说\n矛盾说\n\n3.谈谈我国古代目录学发展的成就与特点 P215\n先秦——两汉时期的目录学：\n\n开创了官修书目的先例\n为我国目录学的形成奠定了基础\n创立了“七分法”的书目分类体系\n创立了叙录体提要等揭示文献的方法\n\n魏晋南北朝时期的目录学：\n\n书目数量增加，类型增加，如佛经目录、私人书目出现\n对书目的作用有了更充分的认识，订正闻见，炳然区分\n对简明登记书目和提要书目两种体例进行了讨论\n在书目方法方面，创立了“立传”、“明宗”、“总经序、记存亡、设附录、撰缘记”等多种方法\n在书目分类方面，出现了六分发和四步法的竞争\n“流略”一词汇出现和“簙录”类的确立，说明目录学作为一门独立的知识系统已为人们所承认、重视\n\n隋唐时期的目录学：\n\n推荐书目的出现\n\n两宋时期目录学：\n\n专科目录的繁荣，索引的产生，目录著作的考证\n\n元明时期的目录学：\n\n创立了新的编排分类体系\n\n清代目录学：\n\n使用目录学成为“显学”\n\n4.请说说文献揭示的基本原则与方法 P219\n揭示文献的外部特征，但要以揭示文献的内容特征为主\n文献揭示的基本方法：\n\n著录法\n提要法\n文摘法\n索引法\n综述法\n文献评价法\n\n5.简述书目的编纂方法\n编纂一部大型书目大体上要经过准备、分析、综合、结束等四个基本阶段\n6.简述文摘的特点、类型和编纂方法 P222\n文摘的特点：\n\n替代性\n模拟性\n转换性\n\n类型：\n\n从编撰目的和职能来看，大体分为普及性文摘和情报性文摘\n根据文摘编者又可以分为作者文摘、学科权限文摘和专职文摘员文摘\n根据文摘编写形式可分为文章式文摘和电报式文摘\n根据文摘刊登的地方可分为同址文摘和非同址文摘\n根据文摘的出版形式可分为单券式文摘和期刊式文摘\n\n编纂方法：\n\n确定选题和工作计划\n文献的搜集与选择\n摘要编写与款目著录\n\n7.索引有哪些类型？索引款目一般应由哪些项目组成？一部完整的索引通常应满足哪些条件 P228\n索引的类型：\n\n篇目索引\n语词索引\n主题索引\n著者索引\n专用索引\n\n结构组成：\n\n目次\n序言\n正文\n辅助索引\n附录\n\n满足条件：\n\n必须有众多索引款目组合而成\n由标目、修饰语和参照项三者组成款目。目标和参照项不可省略\n款目必须按照一定次序组成\n必须通过一定的方式显示标目之间的关系，而不是简单的排列\n\n8.什么是书目情报服务？书目情报服务一般包括哪些内容 P231\n指书目情报检索与利用服务\n\n书目情报源建设\n参考咨询服务\n大众书目情报服务\n定题服务\n情报分析与研究服务\n读者书目情报意识的培养\n书目情报理论总结\n\n9.书目情报服务的形式和策略各有哪些 P232\n\n研究读者的提问\n研究提问的实质目的\n检索工具的选择\n确定检索方法\n检索词的选择\n检索式的编写\n检索结果的提供\n\n10.目录学研究的内容 P213\n\n关于目录学基础理论的研究\n关于文献的研究\n关于书目、索引类型及其编纂法的研究\n关于读者书目情报需求特点与书目情报服务的研究\n关于书目工作组织与管理的研究\n关于国内外目录学的研究\n关于中国目录学遗产的研究\n关于目录学方法的研究\n\n11.书目 P204\n又称目录，它是著录一批相关文献，按照次序编排组织而成的一种揭示文献信息的工具\n12.国家书目 P205\n国家书目是全面登记与报道一个国家近期或往昔出版物的文献总目\n它是一个国家全部出版物现状与历史的记录\n13.联合目录 P207\n联合目录是揭示文献收藏单位的全部或部分藏书的目录\n14.推荐书目 P208\n针对特定的读者、采用特定的标准选择图书推荐给读者的书目\n15.索引 P220\n记录和指引文献中有关事项及单元知识的位置出处，按一定方法组织起来的检索工具\n17.目录学的主要分支学科 P213\n\n专科目录学\n书目控制理论\n计量目录学\n比较目录学\n\n文献学 P234\n文献是特定知识内容的记录，是具有一定物质形态的载体\n1.试用自己的语言，表述图书在人类历史长河中的作用，以及图书文化的要以 P259\n图书是人类使用文字、图画或符号，记录知识或表达思想的著述载体。\n2.概述雕版印刷和活字印刷 P261\n雕版印刷术是我国传统的“四大发明”之一。\n活字印刷术技术也是我国古人的发明。泥活字、木活字、铜活字、铅活字。\n3.图书装帧的本质属性何在？需要遵循的基本原则是什么 P262\n装帧之“帧”，一般指画幅，后用作专指画幅的量词单位。\n原则：\n\n要体现思想性、艺术性和科学性的和谐统一\n要具有可欣赏性的艺术特点\n要坚持可行性与实用性的结合\n要强调特色的连贯性和风格多样性\n\n4.区别“阅”和“读”，并给予合理的解释和适当的发挥\n阅的本义是逐一点看，意思是用眼睛一字不漏的仔细观看。\n读的本义是照文字念、诵，通过用嘴出声念诵以达到对与字句意义的理解，如宣读、朗读、诵读。\n5.文献资源\n文献资源是相对于天然资源的一种社会资源，是物化了的知识财富，是人们收集积累贮存下来的文献资料的总和。\n具有再生性、积累性、可建性、冗余性、共享性和价值潜在性特点\n6.天一阁\n我国现存最古的藏书楼，在今浙江宁波。\n明代范钦建于嘉庆年间。\n原藏书七万余券，后多散佚。\n7.《四库全书》\n古语”盛世修典“，是在清代乾隆盛世修成的一大文化经籍宝藏。”千古巨制，文化渊薮“\n8.校勘\n用同一部书的不同版本和有关资料加以比较，考订文字的异同，目的在于确定原文的真相\n9.版本\n是基本上根据同样的输入信息制作，并且由同一个机构发行的一种资源的所有副本\n10.简牍\n简牍是对我国古代遗存下来的写有文字的竹简与木牍的概称\n11.《书林清话》叶德辉\n《书林清话》叶德辉著，为我国第一部系统的版本目录学专著\n12.藏书\n是对历代以图书为主的文献进行搜集、典藏、整理、研究利用乃至刊刻传播的文化活动。\n13.善本\n最早是指校勘严密，刻印精美的古籍，后含义渐广，包括刻印较早、流传较少的各类古籍\n14.杜定友\n杜定友，广东南海人，出生在上海。广东省立图书馆馆长，南阳大学图书馆主任、中山大学图书馆主任等职。他在图书馆基础理论、图书分类学、 图书目录学、汉字排检法、图书馆理论、图书馆建筑、地方文献等领域皆有突出理论建树。\n15.文献揭示的基本方法\n\n著录法\n提要法\n文摘法\n索引法\n综述法\n文献评价法\n\n16.史料的内容\n\n文字史料\n实物史料\n口传史料\n\n17.文渊阁《四库全书》的流传于收藏\n古语日“盛世修典”，《四库全书》大型丛书就是在清代乾隆盛世修成的一大文化经籍宝藏。从乾隆三十八年（1773）乾隆帝采纳安徽学\n政朱筠上奏，决定开设四库馆开始，到乾隆四十七年（1782）第一部《四库全书》编成，再到乾隆五十二年（1787）先后陆续完成七份副本的誉录，共\n历时15年，全书共收书3460余种（或日3503种），79337卷，每部装订成36300余册。可谓煌煌巨制，卷帙浩繁，汇集了从先秦到清前期的历朝历代的\n主要典籍，被誉为“千古巨制，文化渊鼓”。《四库全书》成书后，乾隆帝对其收存贮藏十分重视。他决定仿效“天一阁”规制，专门修建馆阁以收藏之。\n建成承德避暑山庄的文津阁、圆明园的文源阁、故宫的文渊阁、盛京沈阳的文溯阁，文津阁、文渊阁、文源阁、文溯阁史称“内廷四阁全书”或“北四阁”，\n主要是供皇帝阅读备览服务的。后来考虑到天下文人学子读书的需要，又命将陆续抄缮的三套《四库全书》分别送藏于扬州文汇阁、镇江文宗阁和杭州\n文澜阁，史称“江浙三阁全书”或“南三阁”。《四库全书总目》：由清代纪的等编寨，为我国古代最巨大的官修图书目录。四库全书的馆臣们，对眷录入\n库的3400余种图书（称“著录书”）和抄存卷目的6700余种图书（称“存目书”）全部写出提要，这就是《四库全书总目提要》，或简称《四库总目》。\n《四库提要辨证》余嘉锡以毕生精力研究《四库全书总目》总结而成的成果。其目录学理论著作《目录学发微》，系统地阐述了目录学的意义、功用、源\n流及书目的体例、书目分类等问题，在中国目录学史上有着重要地位。\n18.历史文献整理目的与方法\n文献整理的终极目的，是为了确定历史文献原文的真实可靠程度\n\n历史文献的实证\n历史文献的解释\n历史文献整序\n\n19.20世纪80年代中国历史文献建立的主要原因\n\n历史文献研究的悠久传统与丰硕成果，为历史文献学的形成提供了宝贵的遗产\n历史文献学的教学需要\n文献学的逐步形成，对历史文献学的建立有重要的影响作用\n\n12.历史文献学的研究内容\n\n历史文献研究\n历史文献整理方法的研究\n历史文献整理成就的总结\n历史文献学的分支学科\n\n论述题\n9.文献的属性及基本要素 (P234)\n文献定义：记录有知识的一切载体\n\n是特定知识内容的“记录”\n是具有一定物质形态的“载体”。\n\n文献的构成要素：\n\n记录形式，文字，图画、符号、声频、视频、电子刻录等\n载体形式，龟甲、兽骨、竹简、木牍、缣帛、纸张，胶卷、胶片、磁带和光盘等\n写印形式，手抄、影印、铅印、扫描和摄录等机器印刷\n装帧形式，卷轴装、旋风装、经折装、蝴蝶装、包背装、线装、平装和精装等\n\n10.阐述文献学研究的主要路径(P252)\n\n古典文献学系列\n新型文献学系列\n分科文献学系列\n分支文献学系列\n文献史研究系列\n文献学史系列\n文献学专题系列\n资料结集与工具书的编篆\n\n11.图书文化的主要内涵 (P260)\n\n出版文化\n藏书文化\n阅读文化\n\n12.论述图书馆藏书结构的构成 (P264)\n藏书结构是指藏书体系中各组成要素的构成状况及其相互联系，它决定了藏书体系的功能\n\n学科结构\n等级结构\n时间结构\n文种结构\n文献类型结构\n"},"信息资源管理笔记/复习信息资源管理统考笔记/情报学":{"slug":"信息资源管理笔记/复习信息资源管理统考笔记/情报学","filePath":"信息资源管理笔记/复习信息资源管理统考笔记/情报学.md","title":"情报学","links":[],"tags":["信息资源管理学"],"content":"情报学基础 (P267)\n1.什么是情报学？情报学的主要研究内容是什么 (P270)\n对信息的功能、结构、传递的研究和信息系统管理的研究。\n情报学主要研究的内容包括：\n\n情报学理论\n信息储存与检索\n信息分析与研究\n信息用户与信息服务\n信息技术在信息工作中的应用。\n\n2.信息系统的基本功能是什么 (P272)\n\n输入功能\n储存功能\n处理功能\n输出功能\n控制功能\n\n3.网络环境下信息系统的体系结构主要包括哪些内容  (P274)\n一种开放的系统，让不同系统下的用户相互操作、互相利用对方的资源。\n\n集中式的结构模式\n客户机/服务器（C/S）结构模式\n浏览器/服务器（B/S）结构模式三种。\n\n4.如何进行信息系统的维护、评价与安全防范 (P277 P278)\n\n维护的主要内容：纠正在使用过程中暴露出来的错误，软件维护，硬件维护，数据维护和集成维护技术工具等。\n对系统的技术性能及达到的经济效益等方面做出评价。\n信息系统安全策略建议：\n\n不要追求完美无缺的安全性\n不要误认为问题解决了，根本的问题依据存在\n不要使用复杂的方法\n不要使用太昂贵的设备\n不要使用单一防线\n不要忽略了可能发生的神秘攻击\n不要过分地依赖系统的操作\n不要过分地依赖人的忠诚\n\n\n\n5.试述信息经济学主要研究内容 (P280)\n\n研究市场信息的经济效用\n研究信息系统经济\n研究信息经济与信息产业的理论和测度方法\n研究信息社会的经济理论\n研究国际信息经济理论\n\n6.信息市场的基本特征与功能是什么 (P285)\n特征：\n\n多样性\n复杂性\n隐蔽性\n双重性\n主导性\n扩张性\n\n功能：\n\n资源配置和调节功能\n技术创新推进功能\n利益分配的信号功能\n媒介功能\n储存与正序功能\n检验\n评价与监督功能\n扩散与传递功能\n\n7.信息经济效益的评价原则是什么 (P287)\n\n信息商品价值和使用价值相统一的原则\n经济效益与社会效益相统一的原则\n短期效益与长期效益相统一的原则\n宏观经济效益与微观经济效益相统一的原则\n生产和消费时间节约与质量相统一的原则\n直接效益与间接效益相统一的原则\n\n8.文献计量学的定义是什么？文献计量学研究的主要内容是什么 (P288)\n定义：是指将数学和统计学的方法运用于文献研究的一门学科\n研究内容包括：\n\n文献分布\n文献的老化\n文献的增长\n文献的分散于集中\n文献作者的分布\n文献利用的分散与集中\n文献用词的词频分布\n文献的语言分布\n文献的地区分布\n\n9.信息工作  (P269)\n指对信息的搜集、整理、加工、存储、检索、传递，并为用户提供信息服务的工作\n10.信息机构  (P269)\n是搜集、整理、存储、检索、传递信息并提供信息服务的组织\n11.情报学 (P270)\n是指对信息的功能、结构、传递的研究和信息系统管理的研究\n12.信息系统 (P271)\n是指由计算机硬件、网络和通信设备、计算机软件、信息资源、信息用户和规章制度组成的以处理信息流为目的的人机一体化系统\n13.信息经济学 (P279)\n是把信息和信息活动当作普遍存在的社会经济现象来加以研究的学科\n14.信息商品 (P281)\n用来交换，并能满足人们需要的，具有信息功能的商品\n15.信息市场 (P284)\n信息市场是进行信息商品交换的场所\n16.文献计量学 (P288)\n采用数学、统计学方法定量研究文献分布和变化规律的科学\n17.信息计量学 (P289)\n是采用定量方法来研究信息的现象、过程和规律的学科\n18.文献老化 (P291)\n随着文献“年龄”的增长，作为情报源的价值不断降低，越来越少被人们利用的过程\n19.论文作者分布定律 (P293)\n也称洛特卡定律。揭示论文作者的频率和论文篇数之间的关系：f(x)=C/x^a\n20.引文 (P295)\n文献后列出的参考文献称为该文献的引文\n21.引文分析 (P295)\n利用数学、统计学等方法对文献之间的引用关系进行分析，以揭示文献群内存在的数量特征和内在规律的一种文献计量研究方法\n22.期刊影响因子 (P297)\n某期刊前两年发表的论文在当年被引用的总次数和该期刊前两年发表论文的总数之比，其数学表达式为：\nIF_k = (n_{k-1}+n_{k-2}) / (N_{k-1}+N_{k-2})。\n影响因子是评价一种期刊质量的一个重要指标，一般来说，影响因子越大，表明该期刊的利用率越高，论文的质量也越高\n23.网络信息计量学(P299)\n运用数学、统计学等定量方法对Web上的信息资源、结构和技术的构成与应用进行定量研究的学科\n25.简述情报的类型 (P269)\n\n按内容不同，政治情报、经济情报、军事情报、科技情报和社会生活情报等\n按载体的不同，文字情报、声像情报和实物情报\n按照加工程度的不同，情报可以分为一次情报、二次情报和三次情报\n\n27.信息系统的特点 (P272)\n\n信息系统具有开放性\n信息系统为满足用户的情报需求\n信息系统是一种复杂的动态系统\n信息系统的种类繁多\n信息系统的发展与信息技术的进步相关\n\n29.信息商品的特征 (P282)\n\n非物质性\n消费无损耗性\n非占有性\n累积性与再生性\n\n信息检索 P301\n1.什么是信息检索？它有哪些主要类型 P301 P302\n指将信息，组织和存储起来，并能根据用户的需要找出其中相关信息的过程\n\n文献检索\n事实检索\n数据检索\n文本检索\n数值检索\n音频与视频检索\n\n2.试论述信息检索的基本原理 P303\n对信息集合与需求集合的匹配与选择\n3.信息检索主要经历了哪些不同发展阶段？各阶段有何特点   P305\n\n手工检索阶段\n计算机化检索阶段\n网络检索阶段\n\n4.常用的文本检索方法有哪些？各有何作用 P307\n\n布尔检索\n截词检索\n短语检索与位置检索\n限制检索\n\n5.什么情况下需要扩大检索结果？有哪些具体方法或措施\n被检索的数据库数量较少。\n减少逻辑与（AND）的使用，增加逻辑或（OR），截词检索\n6.什么情况下需要缩小或约束检索结果？有哪些具体方法或措施\n被检索的数据库数量较多。\n使用逻辑与（AND），限制检索\n7.请举例说明一次比较完整的信息检索操作的基本步骤（或流程） P313 P320\n\n分析用户信息检索请求\n选择检索工具\n制定检索策略\n执行检索步骤\n获取并整理检索结果\n分析评价检索结果\n\n8.什么是检索策略？有哪些常用的联机检索策略 P317\n指为实现检索目标而制定的全盘计划和方案\n联机检索策略有：积木型、引文珠形增长、逐层分馏\n9.什么是检索的查全率和查准率 P320\n查全率（Recall Ratio，简称R） = 检出的相关文献数量/系统中全部的相关文献数量\n查准率（Precision Ratio，简称P） = 检出的相关文献数量/检出文献总量\n10.主要的信息检索服务系统有哪些类型？各自的特点是什么？请分别举例说明 P321\n书目数据库检索系统：能指引用户到另一信息源以获取原文的数据库\n书目数据库检索系统特点：\n\n历史悠久\n数据量大\n使用上没有限制，开放性较好\n记录结构简单固定，标准化程度高\n\n全文数据库检索系统：能够直接提供原始资料或具体数据的自足性数据库\n全文数据库检索系统特点：\n\n自足性，可以直接检索并获取全文信息\n详尽性，从理论上讲可以检索到原文中的任何字、词、短语信息\n资源消耗性，需要占用大量的储存空间以及大量的运行时间\n后处理能力强\n\n事实与数值型数据库检索系统：\n事实数据库：提供直接可用的事实为主要内容的数据库；\n数值数据库，是指专门提供以数值方式表示的数据的数据库。\n事实与数值型数据库检索系统特点：\n\n高度专门化的，主题单一性强，具有明确的学科特性\n一般不对外公开或免费使用，使用范围受到较为严格的限制。\n难以形成统一的模式或标准\n与特定的检索软件包合并在一起发行\n建设特别需要国家政策的支持、全国范围内的协调以及国际合作\n\n图像数据库检索系统：基于CBR技术的图像检索服务系统\n11.参考数据库和源数据库的定义分别是什么？两者的区别何在 P321 P327\n参考数据库，能指引用户到另一信息源以获取原文或其他细节的数据库.\n源数据库，是能够直接提供原始资料或具体数据的自足性数据库\n12 什么是书目数据库？常用的书目数据库检索系统有哪些 P321 P326\n书目数据库为：储存某个领域的二次文献的一类数据库，简称“二次文献数据库”\n\n美国工程信息公司的工程索引数据库\n美国科学信息服务社的引文索引系列数据库\n英国电气工程师学会的科学文摘数据库\n美国生物科学信息服务社的生物学文摘数据库\n美国化学文摘服务社的化学文摘数据库\n应该德温特出版公司的专利数据库\n美国剑桥科学文摘公司的书目数据库\n\n13.什么是事实数据库？什么是数值数据库？它们的主要特点是什么 P330\n事实数据库，是指专门提供直接可用的事实为主要内容的数据库；\n数值数据库，是指专门提供以数值方式表示的数据的数据库。\n\n高度专门化的，主题单一性强，具有明确的学科特性\n一般不对外公开或免费使用，使用范围受到较为严格的限制。\n难以形成统一的模式或标准\n与特定的检索软件包合并在一起发行\n建设特别需要国家政策的支持、全国范围内的协调以及国际合作\n\n14.什么是独立搜索引擎？它的基本构成如何\n采用关键词检索的方式提供信息查询\n\n数据采集\n数据分析与标引\n数据组织\n数据检索\n数据挖掘\n\n15.什么是元搜索引擎？它的基本构成如何\n元搜索引擎是独立搜索引擎基础之上建立起来的、在一个统一查询界面就可以同时或分时查询多个搜索引擎的WWW站点\n\n用户接口\n查询代理\n汇总输出\n\n16.布尔检索 (P307)\n布尔检索是各类检索工具提供的一种最基本的检索方式。使用不同的布尔逻辑运算符号把检索词与检索词连接起来，以较为准确地表达检索要求\n17.截词检索 (P308)\n截词检索，指在检索时使用词的一个局部进行检索匹配\n18.检索策略 (P317)\n检索策略，指为实现检索目标而制定的全盘计划和方案\n19.查全率 (P320)\n查全率（R）= 检出的相关文献数量 / 系统中全部的相关文献数量\n反映系统在实施某一检索作业时检出相关文献的能力\n20.查准率 (P320)\n查准率 = 检出的相关文献数量 / 检出文献总量\n21.搜索引擎 (P335)\n是指WWW环境中能够进行网络信息搜集、组织并能提供查询服务的一种信息服务系统\n22.元搜索引擎 (P336)\n是指在独立搜索引擎基础之上建立起来的、在一个统一查询界面就可以同时或分时查询多个搜索引擎的WWW站点\n24.简述多媒体信息基于内容检索的基本思想。(P313)\n利用模式识别、语音识别、图像理解等技术，直接对音频、图形、图像、视频等数据进行内容分析，从中提取其听觉、视觉等特征，并对这些特征加以组织形成索引，用户据此作为检索的依据，实现对这些形象化信息的查询与定位\n25.搜索引擎系统的基本结构 (P337)\n\n数据采集\n数据分析与标引\n数据组织\n数据检索\n信息挖掘\n\n信息分析与预测 P343\n\n对已知信息内容的分析\n是分析基础上的对未知信息的科学预测\n\n1.信息分析与预测活动主要包括哪几个主要环节 P355 P366\n\n课题选择\n课题计划\n信息搜集\n信息整理、评价和分析\n\n2.试述问卷调查的优点和缺点\n优点：\n\n问卷法节省时间、经费和人力\n问卷法调查结果容易量化\n由于问卷调查结果便于统计处理与分析\n有大量的相关软件可以帮助我们进行数据收集与分析\n\n缺点：\n\n面向设计的问题问卷调查比较难\n调查结果的质量得不到保证\n问卷调查的回收率难以保证\n\n3.什么是综述性研究报告？它有什么特点 P362\n综述性研究报，对某一课题相关信息进行，综合分析和浓缩加工，后形成的一种产品。\n综述性研究报告具有，叙述性、综合性、浓缩性和具体性的特点。\n4.试述评价信息分析与预测产品的实践意义 P364\n\n产品本身的质量、所提供内容的内在价值和可使用价值以及用户对产品的反映\n产品使用后给科技、经济、社会和环境带来的最终影响和后果。\n\n5.什么是模糊综合评价法 P365\n借助于模糊数学中模糊变换和综合评判方法对信息分析与预测产品进行模糊综合评价的方法\n6.试述德尔菲法的主要步骤 P371\n\n成立预测领导小组\n明确预测目标\n选择参加预测的专家\n编制调查表\n进行反馈调查和专家意见的汇总整理、统计分析与预测\n编写和提交预测报告\n\n7.试述头脑风暴法优缺点 P371\n优点：\n\n有利于捕捉瞬间的思路\n激发创造性思维\n获取的信息量大\n考虑的因素多\n所提供的计划，方案等也比较全面和广泛\n\n缺点：\n\n它是专家会议调查的一种类型\n专家缺乏代表性\n易受表达能力的限制\n普遍存在着逻辑不严密、意见不全面、论证不充分等问题\n\n8.试述布拉德福定律的主要内容 P379\n\n文献在登载该文献的期刊中数量分布规律的总结\n主要体现在文献信息的搜集环节上\n揭示论文在相应期刊中集中与离散的分布规律\n研究某一科学发展的特点以及学科之间的交叉影响\n\n9.试述层次分析法的基本原理 P380\n\n层次分析法是一种定性与定量完美结合的半定量方法\n根据人类的辩证思维过程，将一个复杂的研究对象划分为递层次结构\n同一层的各种元素具有大致相等的地位，不同层次元素间具有某种联系\n采用逐层叠加的方法，从最高层开始，由高向低逐层进行计算，推算出所有层次对最高层次总排序值。对每一层的递推，都必须作相应的层次总排序的一致性检验\n层次分析法的独特性在于递阶层次结构、判断矩阵的构成和一致性检验三方面\n\n10.试比较移动平均法与指数平滑法的异同点 P383\n移动平均法：处理对象是一组无规则波动的数据，其基本方法是每次在时间序列上移动一步求平均值。\n指数平滑法：是对移动平均法的改进。移动平均法中的每个数据权重相等，意味着不同时间上的数据具有相同的价值，这在一般的预测中是不合理的。为了区分出参加计算的每一数据对预测结果的影响程度不同，对这些数据分别给予不同的权值\n指数平滑法的实质是指数加权移动平均法。权值的选择取决于分析与预测人员的预测经验。对影响较大的数据，可赋予较大权值\n11.什么是竞争情报？它有哪些特点 P388\n竞争情报是关于企业内部和外部的，提高企业的经济竞争力有关的信息。\n竞争情报的特点：客观性、系统性、综合性、创造性、隐蔽性、效益性、时效性、对抗性、谋略性、动态性\n12.试述竞争情报分析与预测的主要程序 P390\n\n规划与定向\n信息搜集\n信息整理\n信息分析与生产\n信息传递\n\n13.什么是定标比超？试举例说明定标比超在竞争情报分析与预测中的作用 P393\n指选择基准目标进行对比，找出差距，并努力敢上和超过。\n在企业竞争情报领域，定标比超是指将本企业的状况与竞争对手的企业进行对照分析的过程\n作用：\n\n对评价竞争对手有重要参考价值\n获取有价值的信息\n深刻认识和掌握用户的信息需求\n鼓励企业员工，形成“比、学、赶、超”的创新热潮\n\n14.德尔菲法 (P371)\n\n德尔菲法是在专家个人判断和专业会议调查的基础上发展起来的\n它是向专家进行调查的方法，能够反映出专家的主观判断能力\n具有匿名性、反馈性、统计性的特点\n\n15.头脑风暴法 (P376)\n借助专家的创造性思维，来索取未知信息的一种直观预测方法\n16.层次分析法 (P380)\n层次分析法，它是一种定性与定量结合的半定量方法，其特性：\n\n递阶层次结构\n判断矩阵的构成\n一致性检验三方面\n\n17.竞争情报 (P388)\n是为企业战略管理服务，提高企业竞争优势的信息行为\n18.信息分析与预测的概念 (P344)\n\n对已知信息内容的深入分析\n在这种分析基础上，对未知信息的科学预测。\n\n具体：\n\n针对用户特定的信息需求，制定研究课题\n通过文献调查和社会调查，搜集与该课题有关的已知信息\n经过加工整理、价值评价和分析研究，使信息得以系统化、有序化\n运用科学的理论、方法和技术，对未来信息作出预测\n最后将预测的成果传递给用户\n\n19.信息分析与预测的功能 (P344)\n\n整理功能\n评价功能\n预测功能\n反馈功能\n\n20.科学决策的主要环节有哪些？(P345)\n\n决策要实施\n决策是在多个方案中选优的\n决策是为了达到目标\n决策要有应变方案\n\n信息服务\n1.什么是信息服务 P396\n用不同的方式向用户提供所需信息的一项活动\n\n对分散在不同载体上的信息进行收集、评价、选择、组织、存储，使之有序化并成为便于利用的形式\n对用户及信息需求进行研究，以便向他们提供有价值的信息\n\n2.在信息服务活动中应遵循哪些原则 P397\n\n针对性原则\n及时性原则\n易用性原则\n成本原则\n\n3.试列举主要的信息服务方式 P398\n\n信息检索服务\n信息报道与发布服务\n信息咨询服务\n网络信息服务\n\n4.信息用户及其信息需求研究对信息工作有何实践意义 P402\n\n是信息机构开展服务工作的依据\n是建立信息系统的基本依据\n是提供优化信息服务的条件\n可以争取更多的用户\n更全面地利用信息服务\n扩大信息机构的服务对象和范围\n有助于信息的传递\n\n5.试述信息用户及其信息需求研究的基本任务 P402\n\n是研究本信息服务用户的职业特点，数量范围、知识素养等有关用户结构的情况\n研究各类用户信息需求的特点\n\n6.试述信息用户及其信息需求研究的主要内容 P402\n\n用户构成及分类研究\n用户信息需求调查分析\n用户信息需求行为的心里规律研究\n用户吸收信息的机理研究\n影响用户信息需求的因素研究\n用户信息安全研究\n\n7.用户的信息行为有何特征 P403\n\n用户解决问题的重要性是决定用户需求是否转化为信息行为的根本原因\n用户是否利用信息服务，取决于，信息的可获得性，信息资源的易用性\n用户需求信息的过程体现出“省力法则”\n用户寻找信息，即通过正规渠道，也通过非正规渠道\n用户对提供信息服务的时间要求快、新\n用户向信息中心的信息服务工作者提出咨询\n\n8.信息需要和信息需求行为的互逆性在信息活动中是如何体现的 P460\n\n从信息需要到信息需求的发展过程\n从信息需求行为到信息需要的回归过程\n\n9.评价用户信息需求的标准有哪些 P460\n\n用户本身的特性指标\n用户所需信息的主题\n用户所需的信息类型\n用户所需的信息资料数量\n用户需求提供信息的完整性和准确性\n用户所需信息的阶段性特点\n提供信息的时间性指标\n用户获取信息的方法和习惯的评价\n\n10.询问法用于调查用户及其需求有何特点及不足 P409\n具有较强的交互性，无需作大量统计分析就能获得所需的结果\n不足之处是调查范围十分有限，需要花费大量时间和人力\n11.间接调查法主要有哪几种方式 P410\n\n文献分析法\n用户资料分析法\n信息业务资料分析法\n\n13.信息需要 (P404)\n根据马斯洛需要等级理论，信息需要就是人们在解决实际问题时，对信息的不满足感和必要感，是人们对信息的欲望。\n论述题\n2.信息系统主要有哪些类型 (P273)\n\n数据处理系统\n管理信息系统\n决策支持系统\n专家系统\n办公自动化与虚拟办公室\n\n4.音频信息基于内容检索的主要类型有哪些 (P313)\n\n语音检索\n音乐检索\n其他音频检索：例如脚步声、雨声、电话铃声、发动机声等\n\n5.基于内容的图像检索的类型 (P314)\n\n基于颜色的图像检索\n基于纹理的图像检索\n基于形状的图像检索\n基于空间关系的图像检索\n基于综合特征的图像检索\n\n6.网络信息检索的流程 (P315)\n\n确定检索客体；\n明确检索要求；\n选定检索系统；\n确定检索字段；\n编制检索式：\n执行检索；\n处理检索结果。\n\n7.情报检索的基本流程(P315)\n一、分析用户请求\n\n检索的目的\n内容分析\n形式分析\n\n二、了解检索工具\n\n检索系统情况\n数据库的收录范围\n数据库的标引规则，使用的词表\n检索途径及相应功能\n其他必要说明\n\n三、制定检索策略\n\n积木型\n引文珠形增长\n逐次分馏\n\n四、拟定并执行具体检索步骤\n\n选择并输入检索词\n选择检索词的组配连接符号\n选择检索路径、实施检索\n初步浏览检索结果\n对检索结果进行优化\n\n五、获取并整理检索结果\n\n输出格式的选择\n排序方式的选择\n处理方式的选择：显示、打印、下载等\n原始文献获取方式的选择\n\n六、分析评价检索操作与检索结果\n\n检索结果分析\n检索系统功能的评价\n\n8.事实和数值型数据库的特点(P330)\n\n高度专门化的，主题单一性强，具有明确的学科特性\n一般不对外公开或免费使用，使用范围受到较为严格的限制。\n难以形成统一的模式或标准\n与特定的检索软件包合并在一起发行\n建设特别需要国家政策的支持、全国范围内的协调以及国际合作\n\n9.信息分析与预测的特点 (P346)\n\n针对性\n系统性\n科学性\n近似性\n局限性\n\n10.试述竞争情报分析与预测的主要程序 (P390)\n\n规划与定向\n信息收集\n信息整理\n信息分析与生产\n信息传递\n\n11.试述互联网信息服务的特点 (P400)\n\n信息资源数字化\n信息存取网络化\n软件服务系列化\n信息咨询社会化\n信息服务管理现范化\n"},"信息资源管理笔记/复习信息资源管理统考笔记/档案学":{"slug":"信息资源管理笔记/复习信息资源管理统考笔记/档案学","filePath":"信息资源管理笔记/复习信息资源管理统考笔记/档案学.md","title":"档案学","links":[],"tags":["信息资源管理学"],"content":"档案学基础\n1.试述档案学的研究对象与范围 P413\n研究对象：\n\n档案\n档案工作\n档案事业\n档案学科本身。\n\n研究范围：\n\n档案学基础理论研究\n档案学应用理论研究\n档案学应用技术研究\n档案学相关的内容\n\n2.试述欧美档案学主要研究内容与特点 P417\n\n西欧档案学理论具有较扎实的基础\n北美是借鉴吸收欧洲档案学理论的基础上，结合北美国家档案管理的传统来研究文件管理理论和档案管理理论。\n\n3.欧美档案学对我国档案学研究具有哪些借鉴意义\n\n中国古代档案工作的整体发展水平领先于西方，但中国档案学产生晚于西方\n中国档案学的研究对象局限于档案现象，而西方将档案学的研究对象扩大至广义的文件现象\n中国档案学以马克思主义哲学为指导思想，侧重于历史方法和逻辑方法\n西方档案学以不同的哲学思想为指导，运用多门学科的研究方法\n中国许多档案学者或缺良好的高等教育，或缺丰富的实践经验\n西方档案学者大多两者兼具\n\n4.简述我国档案学发展阶段及其主要特点 P422\n\n形成时期（1949年10月——1965年）：档案学作为一门独立学科为国家所确立\n初步发展时期（1957——1966年）：开创性、针对性等特点\n停滞时期（1966——1967年）：在这一时期没有任何发展\n恢复时期（1976——1982年）：档案学的学科地位被重新恢复和确立\n全面发展时期（1983——）：档案学专业从历史文献学中独立，档案学从历史学科规划到管理学，我国档案事业进入法制轨道，形成完整的教育体系\n\n5.为什么说档案具有凭证价值 P427\n\n档案凭证价值，是档案不同于和优于其他各种资料的最基本的特点\n档案凭证价值，是由档案形成过程及其结果的内容特点决定的\n它客观地记录了以往的历史情况，是历史证据\n\n6.实现档案价值具有哪些规律性 P428\n\n档案价值的扩展律\n档案价值的时效律\n档案价值的条件律\n\n7.我国档案工作的主要内容有哪些 P430\n\n档案管理工作\n档案行政管理工作\n档案教育工作\n档案宣传工作\n档案科学技术研究工作\n档案国际合作交流工作\n\n8.我国档案工作的基本原则是什么 P432\n\n全国档案工作实行统一领导、分级管理\n维护档案的完整与安全\n便于社会各方面的利用，满足社会对档案的需要\n\n9.档案事业行政管理机构的主要职责是什么 P436\n在统一管理党、政档案工作的原则下，分层负责地掌管国家档案事务\n10.档案室的主要职责是哪些 P436\n\n管理本机关的档案\n为党和国家积累档案史料\n对本机关文件材料的归档作指导和监督\n管理本单位的档案和相关资料\n组织，利用档案和相关资料\n向档案馆移交有长远保存价值的档案\n\n11.我国档案馆有哪些类型 P436\n\n国家档案管\n专业档案馆\n企业档案馆\n\n12.试述档案事业管理体制 P438\n指档案事业管理的体系和组织制度，包括档案行政管理部门的设置及其隶属关系、权限划分等\n档案事业管理体制类型：分散式，集中式\n13.我国档案法规体系由哪些部分构成 P438\n\n档案工作法律\n中央档案行政法规\n中央档案行政规章\n地方性档案工作法规\n地方性档案工作行政规章\n\n14.《档案法》的主要内容是什么 P439\n共六章27条。其中包括：\n\n总则\n档案机构及其职责\n档案的管理\n档案的利用和公布\n法律责任和附则\n\n15.档案工作职业道德包括哪些内容？如何理解 P441\n忠于职守，爱岗敬业，遵纪守法，严守机密，博学求进，公正服务\n16.应用档案学 (P416)\n研究档案管理中的原则、问题、方法与技术的应用。\n其分支学科是：档案事业管理学、档案管理学科技档案管理学、专门档案管理学、档案文献编纂学、档案法规学等\n17.档案的本质属性 (P423)\n原始记录性是其本质属性\n档案是人们在社会实践中直接形成的原始性信息记录，对以往社会实践具有直接的原始记录作用\n18.公务档案 (P426)\n公务档案指人们在公务活动中形成的档案\n公务文书，法律、法规、行政公文等\n19.现行档案 (P426)\n指形成时间较晚，离现在的时间较近，且主要起现时性查考作用的档案，在我国一般指1949年新中国成立之后的档案\n20.科技档案 (P426)\n指人们在科技、生产活动中形成的，由科技文件转化而成的档案，如图纸、设计任务书、科研报告等\n21.档案馆 (P426)\n指永久保存档案的基地，利用档案信息的文化事业机构，属于国家文献信息系统的重要组成部分\n22.专门档案 (P426)\n是通过选择并保存下来，以备查考的，各种专门文件组成的有机体系的总称。\n如会计档案、人事档案、诉讼档案、病例档案\n23.人事档案 (P427)\n人事档案是一种专门档案。\n是记录一个人的个人情况的文件材料，起着凭证、依据和参考的作用\n24.革命历史档案 (P427)\n革命历史档案，又称革命政权档案，指1949年10月1日中华人民共和国成立之前，由中国共产党及其所领导的军队、政权、企事业单位、社团等社会组织及个人所形成的归国家所有的档案\n25.档案价值 (P427)\n档案的价值，是指档案对国家、社会组织或个人的有益性、有用性\n26.档案价值形态 (P427)\n是指档案价值的具体表现形式\n27.档案价值时效 (P428)\n档案的有用性是有时限的，某些档案在一定时期内对利用者是有价值的，超过这个时间限制后则降低或丧失了价值\n28.档案的行政作用 (P429)\n档案对于各级各类机构、社会组织，国家工作人员的，政策、体制、秩序、决策的科学性具有【凭证】和【参考作用】，这种作用可以称为资质作用或行政作用\n29.档案的法律作用 (P429)\n指档案在解决争端、处理案件等活动中所发挥的证据作用\n法律作用是档案凭证价值的集中体现\n30.档案工作标准 (P434)\n\n是档案工作中为事物和概念制定的各种工作标准的总称\n是档案工作中单位和个人应当遵守的共同准则和依据\n\n31.档案室 (P436)\n是机关组织，统一保存和管理档案的内部机构，是整个机关的组成部分\n32.中国第一历史档案馆 (P437)\n中国第一历史档案馆，其前身是北京故宫博物馆文献部，1951年改称档案馆，1955年故宫档案馆划归国家档案局领导，改称现名。该馆是以管理明清时期中央机关档案馆为主的文化事业机构，馆藏明清两代重要机关和少数地方机关的档案共74个全宗，1000多万件\n33.文件中心 (P438)\n一种社会化、集约化和专业化的档案文件管理机构，介于文件形成单位和档案馆之间的一种过渡性档案文件管理机构\n34.档案事业管理体制 (P438)\n指档案事业管理的体系和组织制度，包括档案行政管理部门的设置及其隶属关系、权限划分等\n35.档案学的学科特点 (P414)\n\n综合性\n应用性\n实践性\n\n36.档案的文化性 (P424)\n档案是人类社会发展到一定历史阶段的产物，是人类文明进化的结果。\n档案是人类社会连续性发展的桥梁，是人类社会历史的浓缩，透过档案我们能够触摸到人类社会绵延不断的“脉搏”\n37.简述档案信息化建设的内容 (P435)\n\n档案信息数字化\n档案网站建设\n数字档案馆建设\n\n38.档案室的任务是什么？(P436)\n档案室机关统一保存和管理档案的内部机构，是整个机关的组成部分\n\n管理本机关的档案\n为党和国家积累档案史料\n对本机关文件材料的归档作指导和监督\n管理本单位的档案和相关资料\n组织，利用档案和相关资料\n向档案馆移交有长远保存价值的档案\n\n文件管理\n1.简述文书与文件的基本概念 P443\n文书：它所包含的对象是公务文书和私人文书，即公务文书和私务文书的总称\n文件：含义为领导机关所制发的，具有法定效力，并设有特定版头的公文\n2.公文概念是什么？他有哪些特点 P444\n公文是法定机关与组织在公务活动中，形成的书面文字材料，作为传达意图、办理公文与记载活动工作的一种工具。\n公文的特点：\n\n是由法定作者制成和发布的\n具有法定的权威和效力\n具有规范的体式\n具有特定的处理程序\n具有现行效用\n\n3.试述公文的作用 P445\n\n法规作用\n书面指导作用\n公务联系作用\n宣传教育作用\n凭证和依据作用\n\n4.简述专用文件和通用文件的概念及种类 P447\n通用文件：单位在工作活动中普遍使用的文件。如命令、决定、指示、请示、批复、报告、通知等\n专用文件：是指在一定范围，根据特殊需要专门使用的文件，如外交文件、军事文件、司法文件等\n5.公文的体式及撰写要求是什么 P449\n文理通顺、便于阅读、准确无误、讲求实效、严谨周密、简明精炼、庄重得体、符合规范、整齐划一等\n6.简述文书工作的组织形式 P451\n文书工作是指在社会组织内部围绕着文书工作的活动程序。\n组织形式：集中式、分散式、复合式\n7.什么叫行文规则？机关之间的行文方式有哪几种 P451\n行文规则，各社会之间文件往来运行所必须遵守的统一规则。\n机关之间的行文方式主要有下行文、上行文和平行文三种。\n8.简述文书处理的程序 P452\n\n收文处理程序：文件的收进、启封、登记、分类、分送、拟办、承办、催办、注办、清退、归档等\n发文处理程序：拟稿、核稿、签发、缮印、校对、盖印、登记、封装、发出、注办、归档等\n\n9.归档文件的整理原则与质量要求是什么 P455\n\n归档文件的整理原则：即遵循文件的形成规律，保持文件之间的有机联系，区分不同价值，便于保管和利用\n归档文件的质量要求：归档文件应齐全完整，整理归档文件所使用的书写材料、纸张、装订材料等应符合档案保护要求\n\n10.归档文件的整理方法包括哪几个方面 P456\n\n装订\n分类\n排列\n编号\n编目\n装盒\n\n11.试述电子文件的含义和特点 P457 P459\n电子文件是以代码形式记录于磁盘、磁带、光盘等载体，依赖计算机系统存取并可在通信网络上传输的文件\n电子文件的特点：\n\n信息的非人工识读性\n系统依赖性，必须借助于计算机\n信息与特定载体之间的可分离性\n信息的易变形\n信息存储的高密度性\n多种信息媒体的集体性\n信息的可操作性\n\n12.电子文件的管理原则有哪些 P460\n\n全程管理原则\n前端控制原则\n真实性保障原则\n完整性保障原则\n可读性保障原则\n\n13.电子文件的管理有哪些模式 P462\n\n机构内部电子文件的管理模式\n档案馆电子文件的管理模式\n数字档案馆\n\n14.电子文件鉴定包括哪些内容 P465\n\n电子文件的鉴别\n电子文件的内容鉴别\n电子文件的技术鉴定\n电子文件的处置\n\n15.简述元数据的含义及与著录信息的区别 P467\n元数据是数据的数据。电子文件管理中的元数据是指电子文件系统自动记录的关于文件形成时间、地点、人员、活动、文件系统、结构及内容等方面的具体数据。\n元数据与著录信息区别：\n1. 就内容而言：两者呈交叉关系，著录信息无法从元数据中提取，需要人工编制而成\n2. 就作用而言：元数据的作用要广泛的多，著录的作用包括登记、介绍、报道、交流和检索\n3. 就生成方式而言，元数据是文件管理系统在文件形成与管理过程中自动形成的，而著录信息的生成则是借助于自动著录和人工著录相结合的方式\n16.电子文件管理系统的实现方式有哪些 P471\n\n嵌入式\n联合式\n独立式\n\n17.行政文件 (P447)\n是指国家机关在日常公务活动中形成和使用的文件，具有行政指挥、领导指导和公务联系的作用\n18.归档文件整理 (P455)\n指的是将归档文件以件为单位进行装订、分类、排列、编号、编目、装盒、使之有序化的过程\n19.电子文件 (P457)\n是以代码形式记录于磁带、磁盘、光盘等载体，依赖计算机系统存取并可在通信网络上传输的文件\n20.文件生命周期理论 (P480)\n是研究文件从形成到销毁或永久保存的整个运动过程，是对文件运动过程和规律的客观描述的科学抽象\n21.《档案的整理与编目手册》(P495)\n1898年，荷兰三位档案学者萨穆•缪勒、约翰•斐斯和罗伯特•福罗英出版了专著《档案的整理与编目手册》。\n三位学者通过科学阐释全宗的定义、性质以及全宗内档案整理系统的特点，完成了对来源原则的理论论证\n22.来源原则 (P473)\n就是指档案馆按照档案的来源进行整理和分类，要求保持同一来源的档案不可分散、不同来源的档案不得混淆的整理原则\n23.公文的特点 (P444)\n\n是由法定作者制成和发布的\n具有法定的权威和效力\n具有规范的体式\n具有特定的处理程序\n具有现行效用\n\n24.公文的现行效用 (P445)\n所谓现行效用，是指公文在其内容所针对的现行公务活动中直接发挥实际效力，具有依据和凭证功能\n25.简述文书处理程序 (P452)\n文书处理程序指的是在一个机关内部，按照公务文书的制发、办理与管理的规律，对文书工作的一系列操作环节、工作步骤的有序组合和合理安排。\n\n收文处理程序：文件的收进、启封、登记、分送、拟办、承办、催办、注办、清退\n发文处理程序：拟稿、核稿、签发、缮印、校对、盖印、登记、装封、发出、注办、归档等\n\n26.归档电子文件的质量要求\n\n归档的光盘应清洁，无病毒、无划痕，归档信息填写清楚\n电子文件归档，要提供电子文件的技术环境、相关软件、版本、数据类型、格式、被操作数据、检测数据等相关信息\n电子文件存储载体，要采用只读光盘\n归档的电子文件必须是可读文件\n特殊格式的电子文件，应在存储载体中同时存有相应的支持软件。\n\n档案管理\n1.简述来源原则的基本内容 P477\n尊重来源，尊重全宗的完整性，尊重全宗的原始整理体系\n2.试述文件生命周期理论的基本内容 P481\n文件生命周期理论的基本内容，主要有三点：\n\n文件从其形成到销毁或永久保存，是一个完整的运动过程\n由于文件价值形态的变化，这一完整过程可划分为若干阶段\n文件在每一阶段因其特定的价值形态而与服务对象、保存场所、管理形式之间存在一种内在的对应关系\n\n3.试述档案鉴定标准 P490\n\n档案的来源标准\n档案的内容标准\n档案的形式特征标准\n档案的相对价值标准\n档案的效益标准\n\n4.档案收集的概念与内容是什么 P492\n档案收集的概念：就是按照档案形成的规律，把分散在各机关、个人手中及其其他地方的档案材料接受、征集、集中起来\n档案收集的内容：\n\n对本机关需要归档案卷的接受工作\n对机关具有长久保存价值的档案的集中和接受工作\n对历史档案的接收和征集工作\n\n5.档案整理工作的内容与原则是什么 P495\n档案整理工作的内容：区分全宗、全宗内档案的分类、确立保管单位、案卷的排列和案卷目录的编制\n档案整理工作的原则：\n\n充分利用原有的整理基础\n必须保持文件之间的历史联系\n必须便于保管和利用\n\n6.科技档案的基本分类方法有哪些 P500\n\n工程项目分类法\n型号分类法\n课题分类法\n专业分类法\n地域分类法\n时间分类法\n\n7.复式档案分类法有哪几种 P500\n\n年度——组织机构分类法\n组织机构——年度分类法\n年度——问题分类法\n问题——年度分类法\n\n8.档案鉴定的内容与方法是什么 P501\n档案鉴定的内容：\n\n制定鉴定档案价值的有关标准\n判定档案材料的价值\n确定其保管期限\n拣出无保存价值和保管期满的档案，进行销毁或作相应的处理\n\n鉴定档案价值的基本工作方法是直接审查档案，通常把这种方法称为直接鉴定法\n9.简述档案鉴定工作的概念及保管期限表的类型 P501\n档案鉴定工作的概念：指对档案真伪和档案价值的鉴定\n档案保管期限表的类型：\n\n通用档案保管期限表\n专门档案保管期限\n同系统机关档案保管期限\n同类型机关档案保管期限表\n机关档案保管期限表\n\n10.简述档案保管的概念与内容 P502\n档案保管的概念：是指根据档案的成分和状况所采取的存放和安全防护措施\n档案保管的内容：\n\n档案的库房管理\n档案流动过程中的保护\n保护档案的专门措施\n\n11.档案统计的概念与内容是什么 P503\n档案统计，以表册、数字的形式，揭示档案和档案工作有关情况\n内容，档案的收进、移出、整理、鉴定、保管数量和状况的登记，基本统计和其他专门统计\n12.档案行政管理机关和档案馆的基本统计包括哪些内容 P503\n\n档案构成统计\n档案利用统计\n档案工作人员情况统计\n档案馆建设情况统计\n档案室建立情况统计\n\n13.全宗 (P497)\n全宗是机关、组织或人物在社会活动中形成的档案有机整体。\n14.组织机构分类法 (P499)\n根据文书承办单位进行分类\n15.档案保管期限表 (P501)\n用表册形式列举档案的来源、内容和形式，并指明其保管期限的一种指导性文件\n16.档案直接鉴定法 (P502)\n鉴定档案价值的基本的工作方法之一，通过直接审查档案进行价值鉴定\n17.档案寄存中心 (P502)\n是指国家综合档案馆设立的，为各类企业、社会团体以及个人提供档案寄存有偿服务的机构\n18.文件生命周期理论的重要意义 (P483)\n\n揭示了文件运动地整体性和内在联系\n揭示了文件运动的阶段性变换，为文件的阶段式管理奠定了理论基础\n揭示了文件运动过程地前后衔接和各阶段的相互影响，为实现从现行文件到档案的一体化管理，为档案部门前端控制提供了理论依据\n\n档案的提供利用\n1.简述档案提供利用工作的含义 P505\n档案提供利用工作，亦称档案利用服务，是档案保管部门以所收藏的档案为依据，通过一定的方式与方法，直接提供档案信息，为社会各项事业服务的一项业务活动\n2.试述档案提供利用工作的基本要求P506\n档案馆应当为档案的利用创造基本的条件，简化手续、提供方便，主动开展档案的利用活动，及时掌握和分析档案的利用效果\n3.档案提供利用的基本途径和主要方式有哪些 P506\n\n提供档案原件\n提供档案副本或复制品\n提供档案信息的加工品\n\n4.试述开放档案及其意义 P510\n开放档案就是将可以公开的和保密期满的档案，向社会开放。\n开放档案的意义：\n\n有利于社会的新方针\n加快我国政治民主化进程的一个新步骤\n是现代档案馆自身发展的一项重大措施\n可以促进档案馆的各项业务建设\n\n5.公布档案的方式有哪些 P513\n\n通过报纸、刊物、图书出版物发表档案\n通过电台、电视台播放档案\n陈列、展览档案\n出版发行档案史料、汇编\n公开出售档案复制件\n散发或张贴档案复制件\n在公开场合宣读、播放档案原文等\n\n6.什么是档案编研？档案编研工作的主要内容包括哪些 P514\n档案编辑研究，指档案部门，以档案馆藏档案为基础和对象所进行的编辑和研究工作。\n\n档案史料和现行机关文件汇编\n编辑档案文摘汇编\n编写档案参考资料\n编史修志\n\n7.试述档案编研工作的特点 P515\n\n研究性\n思想性\n政策性\n\n8.简述大事记及其基本编写方法 P517\n大事记是一种按照时间顺序记载一定范围内发生的重大事件和重要活动的参考资料。\n编排方式：\n\n完全按照时间顺序记述大事\n按照时间的性质分类后按照时间顺序记述大事\n\n9.什么是组织沿革？它的内容主要包括哪些方面 P518\n组织沿革是系统记载一个机关、地区或专业系统的体制、组织机构和人员编制等反面变革情况的参考资料。\n内容：\n\n机关、地区或专业系统的历史概况\n行政区划、建制变更情况\n机关的性质、任务、职权范围和隶属关系\n机关内部组织机构的设置和人员编制的变化情况\n机关领导人的任免情况，机关名称的变更、印信的启用与作废、机关办公地点的迁移等\n\n10.简述企业年鉴及其主要内容 P519\n企业年鉴，是以年度为时限，分栏目叙述该年度本企业基本情况的综合性著述型的编研成果。\n内容：\n\n企业生产经营、科技工作及各项管理工作概况\n基本建设施工、设备购置和引进、技术改造活动\n各项基础数据统计，重要会议的报告、总结、提案和报道\n重大的外事活动，劳动模范、先进集体的材料，相关照片，当年的大事记，等等\n\n11.什么是科技成果简介 P519\n科技成果简介，也称科技成果摘要、文摘或提要。\n他是介绍本单位科研、设计成果等主要内容的摘要型科技档案编研成品。\n12.档案证明 (P508)\n指档案馆根据有关档案用户的询问和申请，为核查某种事实在档案馆档案中记载情况而编写的书面证明材料\n13.档案展览服务 (P508)\n指档案收藏部门按照一定的主题，以展出档案原件或其复制品的方式，揭示档案馆藏中有关档案的内容与成分\n14.开放档案 (P510)\n开放档案就是将可以公开的和保密期满的档案，向社会开放。\n15.档案提供利用的主要方式 (P507)\n\n档案阅览服务\n档案的外借服务\n档案的展览服务\n制发档案复本服务\n制发档案证明服务\n档案目录信息服务\n档案咨询服务。\n\n16.《中华人民共和国档案法》的作用\n《中华人民共和国档案法》规定：属于国家所有的档案，由国家授权的档案馆或者有关机关公布；未经档案馆或有关机关同意，任何组织或个人无权公布。集体所有的和个人所有的档案，档案的所有者有权公布，但必须遵守国家有关规定，不得损害国家安全和利益，不得侵犯他人的合法权益，必要时应当申请当地档案行政管理部门的批准。\n《中华人民共和国档案法》制定并颁布，说明我国档案事业开始进入法制轨道。\n17.简述档案提供利用工作的基本内容 (P505)\n亦称档案利用服务，是档案保管部门以所收藏的档案为依据，通过提供档案信息，为社会服务的业务活动\n内容：\n\n了解馆藏档案信息的内容和成分\n熟悉各种档案检索工具的使用方法\n分析预测社会对档案信息的需求特点\n开展档案咨询服务\n向档案用户提供他们所需要的档案文献\n\n论述题\n2.档案工作的性质 (P433)\n\n档案工作的管理性\n档案工作的政治性\n档案工作的服务型\n档案工作的科学性\n档案工作的文化性\n\n5.试述前苏联全宗理论对来源原则的丰富和发展 (P476)\n\n根据时代发展及时丰富和发展了全宗定义\n创造性地提出了文件全宗概念\n构建了一个由国家档案全宗统辖的全宗概念体系，并以此为基础提出了一套完整的全宗理论\n\n6.试述我国全宗理论的主要内容 (P477)\n\n全宗的定义和基本含义\n全宗的构成条件\n划分全宗类型的主要标准\n提出了“全宗群”概念\n全宗内档案分类的科学方法\n\n7.档案《来源原则》基本内容、理论意义和实践价值 (P477)\n\n尊重来源：首先应按照来源标准整理档案，保持档案与其形成者之间的来源联系\n尊重全宗的完整性：全宗是一个整体，整理档案必须维护全宗的完整性，同一个全宗的档案不可分散，不同全宗的档案不得混淆。\n尊重全宗内的原始整理体系，尊重全宗的原始整理顺序和方法，不轻易打乱重整\n\n来源原则的理论意义和实践价值：\n\n为档案馆馆藏的实体整理和分类提供了合理的客观依据\n保持了档案的本质属性\n来源原则是兼具理论性和实践性的管理思想和原则。\n确立了档案实体整理和分类的基本单元\n它划清了档案馆与图书馆的界限，保证了档案专业的独立性\n是档案理论研究的原则之一，有助于揭示档案的来龙去脉，保持历史的本来面貌\n\n8.我国档案价值鉴定的原则 (P490)\n档案价值鉴定原则是根据档案价值的发展规律总结和提炼出来的一种工作原则，对鉴定实际工作具有普遍指导意义\n内容：\n\n“从社会的总体需求出发”的鉴定基本指导思想和出发点\n“全面、历史、发展”这三个鉴定基本观点\n\n全面的观点要求全面分析档案；\n历史的观点要求必须结合档案形成的历史环境和条件来分析其价值\n发展的观点要求尊重档案价值的发展特点和规律\n\n\n\n9.档案整理工作的原则 (P495)\n\n充分利用原有的整理基础\n保持文件之间的历史联系\n必须便于保管和利用\n\n10.档案提供利用工作的指导思想 (P505)\n\n全面地为档案用户服务\n及时地为档案用户服务\n准确地为档案用户服务\n主动地为档案用户服务\n"},"信息资源管理笔记/情报学":{"slug":"信息资源管理笔记/情报学","filePath":"信息资源管理笔记/情报学.md","title":"情报学","links":["信息资源管理笔记/情报学"],"tags":["信息资源管理学"],"content":"情报学基础 (P267)\n1.什么是情报学？情报学的主要研究内容是什么 (P270)\n对信息的功能、结构、传递的研究和信息系统管理的研究。\n情报学主要研究的内容包括情报学理论、信息储存与检索、信息分析与研究、信息用户与信息服务、信息技术在信息工作中的应用。\n\n情报学理论包括情报学理论体系、情报学的哲学基础、信息系统的一般原理、文献和信息计量学、信息的经济规律等。\n信息存储与检索一直是情报学的研究重点，将信息以各种方法组织起来，以手工或自动的方式存储在一个系统内，根据用户的需求，可以方便地从系统中查找和检索出所需的信息。\n信息分析与研究是情报学另一个重要的传统研究领域。由于信息的急剧增长，出现了信息污染和信息泛滥，这就要求从大量的信息中识别出准确的信息，同时对信息进一步分析和处理，为用户提供简捷而有用的信息。\n情报学研究的一个重要目的是为用户提供满意的信息服务，这就要求对用户进行研究，包括用户的信息需求、用户的信息素养、用户的信息查询特点。\n信息技术为情报学的理论研究和实践提供了有力的方法和手段，在信息的搜集、加工、整理、存储、检索、传递各个过程中，没有一环离得开计算机\n\n2.信息系统的基本功能是什么 (P272)\n我们一般可以将信息系统的功能划分为输入、存储、处理、输出和控制五个基本功能。\n\n输入功能：信息系统的输入功能决定于系统所要达到的目的及系统的能力和信息环境的许可。一般情况下，信息系统的输入内容包括信息资源的采集、控制指令的输入、信息检索条件的输入等\n储存功能：信息系统的存储功能是与输入功能紧密联系在一起的。存储功能指的是系统存储各种信息资料和数据的能力\n处理功能：大量的信息资料和数据得以存储之后，必须及时进行加工处理。处理时信息系统内部的生产过程\n输出功能：信息经过处理后，输出内容包括经过信息系统加工处理后的资料信息，信息系统运行过程中状态的反馈信息和需要人工干预时的提示信息\n控制功能：系统的控制功能体现在两个方面：其一是对构成系统的各种信息处理设备，如计算机、通信网、人员等进行控制和管理；其二是对整个信息加工、处理、输入、输出等环节通过各种程序进行控制\n\n3.网络环境下信息系统的体系结构主要包括哪些内容  (P274)\n网络环境下的信息系统是一种开放的系统，这种系统能让不同系统下的用户相互操作、互相利用对方的资源。\n信息系统的结构模式有集中式的结构模式、客户机/服务器（C/S）结构模式和浏览器/服务器（B/S）结构模式三种。\n4.如何进行信息系统的维护、评价与安全防范 (P277 P278)\n维护是信息系统经常性的工作，也是信息系统生存周期持续时间最长、代价最大的一个阶段。维护的主要内容包括：纠正在使用过程中暴露出来的错误，使系统适应外部环境的变化，改进和完善原有的软件，为了升级或改进将来的可维护性和可靠性而进行的预防性维护。具体维护工作包括软件维护，硬件维护，数据维护和集成维护技术工具等。\n要对系统的技术性能及达到的经济效益等方面做出评价，以检查系统是否达到预期的目标，技术性能是否达到设计要求，满足用户需求的程度如何，系统的各种资源是否得到充分的利用，尤其是硬件软件资源的使用情况，经济效益是否理想，社会效益如何等。同时，还要指出系统的长处与不足，提出意见与方法，并以书面报告的形式给出，为以后系统的改进或扩展提供依据。\n信息系统安全策略建议：\n\n不要追求完美无缺的安全性\n不要误认为问题解决了，根本的问题依据存在\n不要使用复杂的方法\n不要使用太昂贵的设备\n不要使用单一防线\n不要忽略了可能发生的神秘攻击\n不要过分地依赖系统的操作\n不要过分地依赖人的忠诚\n\n5.试述信息经济学主要研究内容 (P280)\n\n研究市场信息的经济效用。包括最优信息系统选择理论、信息资源配置理论、信息（商品）需求与供给、信息（商品）成本价值与价格分析、微观信息商品市场及其均衡理论等内容。\n研究信息系统经济。即信息系统的经济分析和信息系统管理与营销。\n研究信息经济与信息产业的理论和测度方法。包括信息经济基础、信息技术的产业化和社会化、信息经济结构与规律、信息经济测度模型、社会信息化与经济增长、信息产业基础与发展、信息投入产出模型、信息部门和信息劳动者的构成与信息服务的经济分析、信息商品市场的培育与运行机制以及信息产业政策及其福利分析。\n研究信息社会的经济理论。包括信息社会经济的基础和基本特征、信息社会经济的结构与发展、信息财富理论、商品信息物质比的测度与分析、信息生产力理论、信息力与国家竞争理论、信息（知识）价值理论以及信息社会经济行为的一般理论。\n研究国际信息经济理论。包括世界信息经济的发展，如世界信息经济发展的动力与环境，信息技术对国际贸易、国际金融的影响，世界信息经济发展的规模，信息竞争力与国际信息贸易等\n\n6.信息市场的基本特征与功能是什么 (P285)\n信息市场的特征：无论信息商品市场中流通的商品以何种形式出现，其实质都是知识信息，信息商品的特殊性决定了信息商品市场具有特殊性质：信息市场形态的多样性和复杂性，信息市场形态的隐蔽性，信息市场交易不受时空限制，信息市场具有双重性，信息市场垄断因素的主导性，信息市场供求关系具有扩张性，信息市场的发展要求有合理的价格制度和较强的法制系统。\n信息市场的功能：信息市场既是商品流通渠道，又是信息流通渠道，这种双重特性，使我们可以从两个方面分析信息市场的功能。对整个经济运行和大市场体系而言，信息市场具有如下功能，资源配置和调节功能，技术创新推进功能，利益分配的信号功能，媒介功能，储存与正序功能，检验、评价与监督功能，扩散与传递功能\n7.信息经济效益的评价原则是什么 (P287)\n\n信息商品价值和使用价值相统一的原则\n经济效益与社会效益相统一的原则\n短期效益与长期效益相统一的原则\n宏观经济效益与微观经济效益相统一的原则\n生产和消费时间节约与质量相统一的原则\n直接效益与间接效益相统一的原则\n\n8.文献计量学的定义是什么？文献计量学研究的主要内容是什么 (P288)\n定义：是指将数学和统计学的方法运用于文献及其他交流介质研究的一门学科\n研究内容包括：文献集合中各种文献特征的数量及其分布规律，包括文献的老化，文献的增长，文献的分散于集中，文献作者的分布，文献利用的分散于集中，文献用词的词频分布，文献的语言分布，文献的地区分布等。\n9.信息工作  (P269)\n指对信息进行科学的有组织的搜集、整理、加工、存储、检索、传递，并为用户提供各类信息服务的工作\n10.信息机构  (P269)\n是搜集、整理、存储、检索、传递信息并提供信息服务的各类专门组织\n11.情报学 (P270)\n是指对信息的功能、结构、传递的研究和信息系统管理的研究\n12.信息系统 (P271)\n是指由计算机硬件、网络和通信设备、计算机软件、信息资源、信息用户和规章制度组成的以处理信息流为目的的人机一体化系统\n13.信息经济学 (P279)\n是把信息和信息活动当作普遍存在的社会经济现象来加以研究的学科\n14.信息商品 (P281)\n指的是用来交换，并能满足人们某种需要的信息产品。也可以把信息商品定义为具有信息功能的商品。信息商品的物质载体是其形， 而物质载体所负载的信息内容是信息商品的核，是信息商品具有信息功能的原因。\n15.信息市场 (P284)\n从狭义上说，信息市场是进行信息商品交换的场所。\n从广义上说，信息市场不仅指信息商品交换的场所，而且还包括购买信息商品的用户及其与信息生产者、经营者之间的经济关系，是信息商品供求关系的总和\n16.文献计量学 (P288)\n文献计量学是采用数学、统计学方法定量研究文献信息分布和变化规律的一门科学\n17.信息计量学 (P289)\n信息计量学（Informetrics）是采用定量方法来描述和研究信息的现象、过程和规律的一门学科，是数学、统计学和情报学广泛结合而形成的一个情报学分支学科\n18.文献老化 (P291)\n文献老化是指随着文献“年龄”的增长，其内容日益陈旧，作为情报源的价值不断降低，越来越少被人们利用的过程\n19.论文作者分布定律 (P293)\n也称洛特卡定律。该定律揭示论文作者的频率和论文篇数之间的关系，其数学表达式为：f(x)=C/x^a\n式中x为研究人员发表论文的篇数，f(x)为发表了x篇论文的作者占被统计的作者总数的百分比，C常数\n20.引文 (P295)\n为了识别和统计方便，一般情况下，只将文献后列出的参考文献称为该文献的引文\n21.引文分析 (P295)\n是指利用数学、统计学等方法对文献之间的引用关系进行分析，以揭示文献群内存在的数量特征和内在规律的一种文献计量研究方法\n22.期刊影响因子 (P297)\n某期刊前两年发表的论文在当年被引用的总次数和该期刊前两年发表论文的总数之比，其数学表达式为：IF_k = (n_{k-1}+n_{k-2}) / (N_{k-1}+N_{k-2})式中IF_k为k年的影响因子，n_{k-1}和n_{k-2}分别为前一年和前两年该期刊发表论文的被引用次数，(N_{k-1}和N_{k-2})分别为该期刊前一年和前两年的载文量。影响因子是评价一种期刊质量的一个重要指标，一般来说，影响因子越大，表明该期刊的利用率越高，论文的质量也越高\n23.网络信息计量学(P299)\n网络信息计量学是运用数学、统计学等定量方法对Web上的信息资源、结构和技术的构成与应用进行定量研究的一门学科\n25.简述情报的类型 (P269)\n\n按内容所属领域的不同，情报可以分为政治情报、经济情报、军事情报、科技情报和社会生活情报等。\n按照载体的不同，情报可以分为文字情报、声像情报和实物情报。文字情报是指用文字记录下来的情报资料。声像情报是指音频和视频介质传播的情报。实物情报是指以样品等实物为传播情报的实物载体。\n按照加工程度的不同，情报可以分为一次情报、二次情报和三次情报。一次情报是指没有加工过的原始情报，如实验记录、科研论文等。二次情报是指在转换一次情报过程中产生的情报，如文摘、索引等。三次情报是指在一次和二次情报基础上进一步提炼和分析而产生的情报，如述评、综述等。\n\n26.情报学的研究内容 (P270)\n1.什么是情报学？情报学的主要研究内容是什么 (P270)\n27.信息系统的特点 (P272)\n\n信息系统具有开放性，它与外界环境始终保持密切的联系，这是信息系统最重要的特点；\n信息系统是情报用户所在系统中的一个子系统，它的最优化目标是能充分满足用户所在系统的情报需求；\n信息系统是一种复杂的动态系统，是多层次、多变量、多目标、多功能的复杂系统，要受到多种因素的制约；\n信息系统的种类繁多，其所占时间与空间的范围较大；\n信息系统的发展与信息处理技术的进步密切相关，现代化的技术设备是建立有效信息系统的物质基础。\n\n28.信息系统结构与功能 (P272)\n2.信息系统的基本功能是什么 (P272)\n29.信息商品的特征 (P282)\n\n非物质性。信息商品是信息的有序结合，无论它存在于何种物质载体，都表现为非物质的信息状态。\n消费无损耗性。信息商品在使用和消费中表现为信息内容从一种物质载体转移到另一种物质载体，无论怎样转移，都不会对信息商品起消灭作用，也不会使信息商品失去原来的使用价值或效用。\n非占有性。信息商品的共享性来自信息本身的共享性，信息商品由于对物质载体的独立性，其消费和使用表现为载体转换，这种转换不会引起信息商品的损耗和丧失，交换的结果不是转手而是共享。\n累积性与再生性。信息商品一旦生产出来，不仅可以满足同时期人类的需要，而且可以通过信息的保存、积累、传递达到时间点上的延续，满足后代的需要。信息商品在满足社会需求和利用的同时，不仅不会被消耗掉，还会生产出新的信息商品。\n\n30.信息市场的特征及功能 (P285)\n6.信息市场的基本特征与功能是什么 (P285)\n31.文献计量学的定义是什么？文献计量学研究的主要内容是什么？？(P289)\n8.文献计量学的定义是什么？文献计量学研究的主要内容是什么 (P288)\n信息检索 P301\n1.什么是信息检索？它有哪些主要类型 P301 P302\n所谓信息检索，广义地说，是信息储存与检索，它是指将信息按照一定的方式组织和存储起来，并能根据用户的需要找出其中相关信息的过程\n在通常情况下，大多数人讲到“信息检索”时，一般只涉及“取”，即主要关注如何从存储的信息集合中快速获取各种需要的信息。这时，信息检索也可以称为“信息查询”或“信息查找”。是对信息检索概念的一种狭义理解\n按照信息检索对象的不同，早期信息检索一般分为文献检索、事实检索、数据检索三种不同类型。\n当前，信息检索类型出现了一种新的三分方法，即：文本检索、数值检索、音频与视频检索。\n2.试论述信息检索的基本原理 P303\n对信息集合与需求集合的匹配与选择\n3.信息检索主要经历了哪些不同发展阶段？各阶段有何特点   P305\n\n手工检索阶段（1830年——20世纪70年代初期）：以印刷文献为主要检索对象，以各类文摘、题录和目录性工具书为可利用的主要检索工具，以图书馆的参考咨询部门为开展信息检索服务的中心机构。\n计算机化检索阶段（20世纪50年代初期——20实际90年代初期）：以各类机读数据库为检索对象；各类情报所、联机服务中心作为新兴的信息服务部门而存在；信息检索用户逐渐由专业检索人员相个人终端用户转移，信息检索的社会普及化程度日益提高\n网络检索阶段（20世纪90年代初期—— ）：超文本/超媒体技术的应用、基于内容检索与多媒体信息检索系统的开发、自然语言理解/处理、海量规模的网络信息组织与检索、语义检索与逻辑推理、知识检索服务\n\n4.常用的文本检索方法有哪些？各有何作用 P307\n\n布尔检索：通常，用户在检索时需要使用不同的布尔逻辑运算符号把检索词与检索词连接起来，以较为准确地表达检索要求\n截词检索：是一种隐含的“逻辑或”（OR）运算，能提高查全率，扩大检索结果，防止漏检的有力手段，对西方语言信息尤为适用\n短语检索与位置检索：使用专门的运算符号把多个检索词汇组织成特定的短语，或者对各个检索词在检索结果中出现的相对位置进行限定，较好的完成检索任务\n限制检索：主要目的是为了提高检索的准确率\n\n5.什么情况下需要扩大检索结果？有哪些具体方法或措施\n被检索的数据库数量较少因此需要扩大检索范围，增加文献信息检出量，具体措施，减少逻辑与（AND）的使用，增加逻辑或（OR），截词检索\n6.什么情况下需要缩小或约束检索结果？有哪些具体方法或措施\n被检索的数据库数量较多因此需要缩小检索范围，具体方法，使用逻辑与（AND），限制检索\n7.请举例说明一次比较完整的信息检索操作的基本步骤（或流程） P313 P320\n\n分析用户信息检索请求：明确检索目的、检索请求的内容特征分析、检索请求的形式特征分析\n了解检索工具/系统的基本情况：检索工具或检索系统的研制者情况、检索工具或数据库的收录范围，通常会涉及科学主体、信息/文献类型、使用语言种类、年代跨度等许多方面、检索工具或系统提供的主要检索途径及相应功能、其他必要说明\n制定检索策略：积木型、引文珠形增长、逐层分馏\n拟定并执行具体检索步骤：选择并输入检索词、选择检索词的组配连接符号、选择路径，事实检索、初步浏览检索结果、使用合适的反馈调整方法，对检索结果进行优化\n获取并整理检索结果：输出（或显示）格式的选择、排序方式的选择、处理方式的选择、原始文献获取方式的选择\n分析评价检索操作与检索结果：查准率、查全率\n\n8.什么是检索策略？有哪些常用的联机检索策略 P317\n所谓检索策略，是指为实现检索目标而制定的全盘计划和方案，是对整个检索过程的谋划和指导。\n常用的联机检索策略有：积木型、引文珠形增长、逐层分馏\n9.什么是检索的查全率和查准率 P320\n查全率（Recall Ratio，简称R） = 检出的相关文献数量/系统中全部的相关文献数量\n查准率（Precision Ratio，简称P） = 检出的相关文献数量/检出文献总量\n10.主要的信息检索服务系统有哪些类型？各自的特点是什么？请分别举例说明 P321\n书目数据库检索系统：\n是参考数据库中的一种，是一类能指引用户到另一信息源以获取原文或其他细节的数据库，书目数据库的定义为：储存某个领域的二次文献的一类数据库，简称“二次文献数据库”\n书目数据库检索系统特点：\n\n历史悠久\n数据量大，连续性与累积性强\n使用上没有限制，开放性较好\n记录结构简单固定，标准化程度高\n\n全文数据库检索系统：是指储存文献全文或其中主要部分的一种源数据库，能够直接提供原始资料或具体数据的自足性数据库，用户查阅了源数据库之后，通常不再需要查阅其他信息源\n全文数据库检索系统特点：\n\n自足性，可以直接检索并获取全文信息\n详尽性，从理论上讲可以检索到原文中的任何字、词、短语信息\n资源消耗性，需要占用大量的储存空间以及大量的运行时间\n后处理能力强\n\n事实与数值型数据库检索系统：所谓事实数据库，是指专门提供直接可用的事实为主要内容的数据库；所谓数值数据库，是指专门提供以数值方式表示的数据的数据库。\n事实与数值型数据库检索系统特点：\n\n事实与数值数据库一般都是高度专门化的，主题单一性强，具有明确的学科特性，因此，编制工作必须由相应的权威机构、专业性学术机构和专家参与，以保证质量\n事实与数值数据库具有较大的赢利空间，因此一般不会对外公开或免费使用，使用范围受到较为严格的限制。\n使用方法和检索方式上的不兼容，难以形成统一的模式或标准\n许多数值型数据库总是与特定的检索软件包合并在一起发行\n事实与数值型数据库的建设特别需要国家政策的支持、全国范围内的协调以及国际合作\n\n图像数据库检索系统：基于CBR技术的图像检索服务系统\n11.参考数据库和源数据库的定义分别是什么？两者的区别何在 P321 P327\n参考数据库，是一类能指引用户到另一信息源以获取原文或其他细节的数据库.\n源数据库，是能够直接提供原始资料或具体数据的自足性数据库\n12 什么是书目数据库？常用的书目数据库检索系统有哪些 P321 P326\n书目数据库为：储存某个领域的二次文献的一类数据库，简称“二次文献数据库”\n\n美国工程信息公司的工程索引数据库\n美国科学信息服务社的引文索引系列数据库\n英国电气工程师学会的科学文摘数据库\n美国生物科学信息服务社的生物学文摘数据库\n美国化学文摘服务社的化学文摘数据库\n应该德温特出版公司的专利数据库\n美国剑桥科学文摘公司的书目数据库\n\n13.什么是事实数据库？什么是数值数据库？它们的主要特点是什么 P330\n所谓事实数据库，是指专门提供直接可用的事实为主要内容的数据库；所谓数值数据库，是指专门提供以数值方式表示的数据的数据库。\n事实与数值型数据库检索系统特点：\n\n事实与数值数据库一般都是高度专门化的，主题单一性强，具有明确的学科特性，因此，编制工作必须由相应的权威机构、专业性学术机构和专家参与，以保证质量\n事实与数值数据库具有较大的赢利空间，因此一般不会对外公开或免费使用，使用范围受到较为严格的限制。\n使用方法和检索方式上的不兼容，难以形成统一的模式或标准\n许多数值型数据库总是与特定的检索软件包合并在一起发行\n事实与数值型数据库的建设特别需要国家政策的支持、全国范围内的协调以及国际合作\n\n14.什么是独立搜索引擎？它的基本构成如何\n采用关键词检索的方式提供信息查询\n\n数据采集\n数据分析与标引\n数据组织\n数据检索\n数据挖掘\n\n15.什么是元搜索引擎？它的基本构成如何\n元搜索引擎是独立搜索引擎基础之上建立起来的、在一个统一查询界面就可以同时或分时查询多个搜索引擎的WWW站点，是一类新型的网络信息查询系统。\n\n用户接口\n查询代理\n汇总输出\n\n16.布尔检索 (P307)\n布尔检索是各类检索工具提供的一种最基本的检索方式。通常，用户在检索时需要使用不同的布尔逻辑运算符号把检索词与检索词连接起来，以较为准确地表达检索要求\n17.截词检索 (P308)\n所谓截词检索，是指在检索时使用词的一个局部（某些位置上的字符被截去）进行检索匹配，并认为凡满足这个词局部中的所有字符要求的记录，都为命中结果。截词检索需要使用专门符号（截词符），以指定截词的具体位置和截断字符的数量\n18.检索策略 (P317)\n所谓“检索策略”，是指为实现检索目标而制定的全盘计划和方案，是对整个检索过程的谋划和指导\n19.查全率 (P320)\n查全率（R）= 检出的相关文献数量 / 系统中全部的相关文献数量\n反映系统在实施某一检索作业时检出相关文献的能力\n20.查准率 (P320)\n查准率 = 检出的相关文献数量 / 检出文献总量\n21.搜索引擎 (P335)\n是指WWW环境中能够进行网络信息搜集、组织并能提供查询服务的一种信息服务系统。它们主要通过网络搜索软件或多种人工方式，将WWW上大量网站的页面信息收集、传输到本地，经过加工处理建成索引数据库或目录指南，从而能够对用户提出的各种查询请求作出响应，并提供用户所需要的信息\n22.元搜索引擎 (P336)\n是指在独立搜索引擎基础之上建立起来的、在一个统一查询界面就可以同时或分时查询多个搜索引擎的WWW站点，是一类新型的网络信息查询系统\n23.信息检索的基本概念 (P301)\n1.什么是信息检索？它有哪些主要类型 P301 P302\n24.简述多媒体信息基于内容检索的基本思想。(P313)\n多媒体信息基于内容检索的基本思想，是指利用模式识别、语音识别、图像理解等技术领域可能提供的有效方法和工具，直接对音频、图形、图像、视频等数据进行内容分析，从中提取其听觉、视觉等特征（例如颜色、形状、纹理、节奏、旋律、镜头等），并对这些特征加以组织形成索引，用户据此作为检索的依据，实现对这些形象化信息的查询与定位\n25.搜索引擎系统的基本结构 (P337)\n独立搜索引擎系统的基本结构一般应该包括以下五个不同的功能模块：\n\n数据采集。负责按照一定的方式和要求对网络上的WWW站点等资源进行搜集，并将搜集、发现到的WWW页面信息经网络传输，存入到搜索引擎的临时数据库中\n数据分析与标引。负责对收集到的网页信息进行分析，从中提取有检索（或查询）价值的内容——网页关键词、网页的分类类别等，并对关键词进行权值计算\n数据组织。负责形成规范的索引数据库或便于浏览的层次型分类目录结构\n数据检索。负责帮助用户用一定的方式检索索引数据库或浏览目录结构，获取符合用户需要的WWW信息\n信息挖掘。主要负责提取用户相关信息，以利用这些信息来提高检索服务的质量。通过对用户注册的个人兴趣信息及以前检索活动/行为的跟踪、分析与学习，信息挖掘模块在个性化服务中起到关键作用\n\n信息分析与预测 P343\n信息分析与预测是一项内容广泛的信息深加工活动，其活动的侧重点主要体现在两个方面：一是对已知信息内容的深刻分析，二是在这种分析基础上的对未知或未来信息的科学预测\n1.信息分析与预测活动主要包括哪几个主要环节 P355 P366\n\n课题选择\n课题计划\n信息搜集\n信息整理、评价和分析\n\n2.试述问卷调查的优点和缺点\nP358 （PS：没在书上找到答案，以下答案来自百度知道）\n优点：\n\n问卷法节省时间、经费和人力\n问卷法调查结果容易量化\n由于问卷调查结果便于统计处理与分析\n现在有大量的相关统计分析软件可以帮助我们进行数据分析，有些甚至能直接帮助我们设计问卷。方便实施和分析。也方便进行数据挖掘。问卷调查法可以进行大规模的调查\n\n缺点：\n\n面向设计的问题问卷调查比较难。面向未来的调查很多时候需要了解用户的意图、动机和思维过程。问卷调查这类问题往往效果不佳，或者说问题设计比较难。而开放式的问题，回收质量、分析和统计等工作也会受影响\n问卷调查采用由用户自己填答问卷的方式，所以其调查结果的质量常常得不到保证。问卷调查的回收率难以保证\n\n3.什么是综述性研究报告？它有什么特点 P362\n综述性研究报告是在一定的时空范围内对某一课题的大量相关信息进行综合分析和浓缩加工后形成的一种产品。\n综述性研究报告具有叙述性、综合性、浓缩性和具体性的特点。\n4.试述评价信息分析与预测产品的实践意义 P364\n即时评价是指产品交付用户使用或以某种方式面世后，随即或稍后进行的一种评价。这种评价的依据主要是产品本身的质量、所提供内容的内在价值和可使用价值以及用户对产品的初步反映，而不要求考察产品可能产生的最终效果。最终评价是对产品消费后产生的最终效果进行一种长远评价。这种评价的重点不是产品本身的质量、所提供内容的内在价值和可使用价值以及用户的初步反映，而是产品使用后给科技、经济、社会和环境带来的最终影响和后果。\n5.什么是模糊综合评价法 P365\n模糊综合评价法是借助于模糊数学中模糊变换和综合评判方法对信息分析与预测产品进行模糊综合评价的方法，模糊综合评价法的关键是模糊数学模型的建立。评价时，首先根据评价指标集和评语集得到评价矩阵，归一化处理各评价指标的权重集并使之与评价矩阵进行模糊运算，从而得到单层次评价指标的模糊综合评价结果\n6.试述德尔菲法的主要步骤 P371\n\n成立预测领导小组\n明确预测目标\n选择参加预测的专家\n编制调查表\n进行反馈调查和专家意见的汇总整理、统计分析与预测\n编写和提交预测报告\n\n7.试述头脑风暴法优缺点 P371\n优点：\n\n通过信息交流，有利于捕捉瞬间的思路，激发创造性思维，产生富有创见性的思想“火花”\n通过头脑风暴会议，获取的信息量大，考虑的因素多，所提供的计划，方案等也比较全面和广泛\n\n缺点：\n\n它是专家会议调查的一种类型，因而具备专家会议调查法的一些缺陷，如专家缺乏代表性，易受“权威”、会议“气氛”和“潮流”等因素的影响、易受表达能力的限制等\n由于是即兴发言，因而普遍存在着逻辑不严密、意见不全面、论证不充分等问题\n\n8.试述布拉德福定律的主要内容 P379\n布拉德福定律是文献计量学最基本的定律之一，是关于专业文献在登载该文献的期刊中数量分布规律的总结。\n布拉德福定律在信息分析与预测中的应用主要体现在文献信息的搜集环节上。利用布拉德福定律所揭示的专业论文在相应期刊中集中与离散的分布规律，可以使信息搜集人员在搜集文献信息时做到心中有数。\n布拉德福定律也可用研究某一科学发展的特点以及学科之间的交叉影响和相互渗透关系，并以此确定某些新科学的生长点\n9.试述层次分析法的基本原理 P380\n层次分析法是一种定性与定量完美结合的半定量方法。\n它根据人类的辩证思维过程，先将一个复杂的研究对象划分为递层次结构，同一层的各种元素具有大致相等的地位，不同层次元素间具有某种联系；在对但层次的元素构造判断矩阵以得出层次单排序，并进行一致性检验；最后为了计算层次总排序，采用逐层叠加的方法，从最高层开始，由高向低逐层进行计算，推算出所有层次对最高层次总排序值。对每一层的递推，都必须作相应的层次总排序的一致性检验。层次分析法的独特性在于递阶层次结构、判断矩阵的构成和一致性检验三方面\n10.试比较移动平均法与指数平滑法的异同点 P383\n移动平均法：处理对象是一组无规则波动的数据，其基本方法是每次在时间序列上移动一步求平均值。这样的处理可对原始的无规则数据进行“修匀”，消除掉样本序列中的随机干扰成分，突出序列本身的固有规律\n指数平滑法：是对移动平均法的改进。移动平均法中的每个数据权重相等，意味着不同时间上的数据具有相同的价值，这在一般的预测中是不合理的。为了区分出参加计算的每一数据对预测结果的影响程度不同，对这些数据分别给予不同的权值。指数平滑法的实质是指数加权移动平均法。权值的选择取决于分析与预测人员的预测经验。对影响较大的数据，可赋予较大权值\n11.什么是竞争情报？它有哪些特点 P388\n竞争情报是一个正在发展中的概念。从总体上看，竞争情报是关于企业内部和外部的一起与提高企业的经济竞争力有关的信息。\n竞争情报的特点：客观性、系统性、综合性、创造性、隐蔽性、效益性、时效性、对抗性、谋略性、动态性\n12.试述竞争情报分析与预测的主要程序 P390\n\n规划与定向\n信息搜集\n信息整理\n信息分析与生产\n信息传递\n\n13.什么是定标比超？试举例说明定标比超在竞争情报分析与预测中的作用 P393\n定标比超是国内学者在接触英文Benchmarking及其活动时采用的一个概念。所谓定标比超，本义上时指选择基准目标进行对比，找出差距，并力争敢上和超过。在企业竞争情报领域，定标比超是指将本企业各方面的状况与竞争对手或行业内外一流的企业进行对照分析的过程，是将外部企业的成就业绩作为自身企业的内部发展目标并将外界的最佳做法移植到本企业的一种方法\n作用：\n\n可以挖掘出许多对评价竞争对手竞争态势有重要参考价值的信息\n可以从任何产业中的一流企业那里获取有价值的信息\n可以深刻认识和掌握用户的信息需求，使企业的竞争战略能够贴近目标是市场和用户\n可以鼓励和引导本企业的员工“从干中学”和“从用中学”，形成“比、学、赶、超”的创新热潮\n\n14.德尔菲法 (P371)\n德尔菲法是在专家个人判断和专业会议调查的基础上发展起来的。它是一种按规定程序向专家进行调查的方法，能够比较精确地反映出专家的主观判断能力。\n德尔菲法本质上是建立在诸多专家的专业知识、经验和主观判断能力的基础上的，因而特别适用于缺少信息资料和历史数据，而又较多地受到社会的、政治的、人为的因素影响的信息分析与预测课题。\n具有匿名性、反馈性、统计性的特点\n15.头脑风暴法 (P376)\n头脑风暴法是借助于专家的创造性思维来索取未知或未来信息的一种直观预测方法\n16.层次分析法 (P380)\n层次分析法，它是一种定性与定量完美结合的半定量方法，其独特性在于递阶层次结构、判断矩阵的构成和一致性检验三方面\n17.竞争情报 (P388)\n竞争情报是企业在法律和商业道德范围内从事的、主要为企业战略管理（战略制定和战略实施）服务，以提高企业竞争优势为根本目的的信息行为\n18.信息分析与预测的概念 (P344)\n信息分析与预测是一项内容广泛的信息深加工活动，其活动的侧重点主要体现在两个方面：一是对已知信息内容的深入分析，二是建立在这种分析基础上的对未知或未来信息的科学预测。\n具体来说，就是：针对用户特定的信息需求，制定研究课题；通过文献调查和社会调查，广泛系统地搜集与该课题有关的已知信息，经过加工整理、价值评价和分析研究，使已知信息的内容得以系统化、有序化，以揭示客观事物的运动规律，并在此基础上，运用科学的理论、方法和技术，对客观事物的未知或未来信息作出合理的预测；最后以某种信息产品的形式将预测的成果通过适当的传递渠道传递给用户，满足用户的需要。\n19.信息分析与预测的功能 (P344)\n\n整理功能：对信息进行搜集、组织，使之由无序变为有序。\n评价功能：对信息价值进行评价，以去粗存精、去伪存真、辨新、权重、评价、荐优。\n预测功能：通过对已知信息内容的分析获取未知或未来信息。\n反馈功能：根据用户的实际消费效果对预测结论进行审议、评价、修改和补充。\n\n20.科学决策的主要环节有哪些？(P345)\n科学决策要有四个环节，这四个环节缺一不可、否则就不属于科学决策。这四个环节是：\n\n决策总是要付诸实施，围绕即定目标拟定各种实施方案是决策的基本要求\n决策总是在若干个有价值的实施方案中进行比较和选优，没有比较和选优，也就不成为决策，更谈不上科学决策\n决策总是为了达到一个即定的目标，没有目标就无从决策，目标不准或错误，会导致决策错误\n决策既要考虑实施过程中的情况的不断变化及应变的方案，还要考虑实现目标之后的经济效果和社会效果。没有应变方案和不计社会效果的决策，是不完全的决策，更不是科学的决策。\n\n21.德尔菲法的特点 (P371)\n14.德尔菲法 (P371)\n6.试述德尔菲法的主要步骤 P371\n信息服务\n1.什么是信息服务 P396\n信息服务就是用不同的方式向用户提供所需信息的一项活动。\n它包括两个方面的内容：一是对分散在不同载体上的信息进行收集、评价、选择、组织、存储，使之有序化并成为便于利用的形式；二是对用户及信息需求进行研究，以便向他们提供有价值的信息。\n2.在信息服务活动中应遵循哪些原则 P397\n\n针对性原则，满足特定用户在特定时间的特定需求是信息服务的出发点\n及时性原则，在特定时间范围内的信息才能发挥其效用\n易用性原则，信息服务机构应为用户获取、利用信息提供最大的便利条件\n成本/效益原则，信息服务既要讲社会效益，也要讲经济效益\n\n3.试列举主要的信息服务方式 P398\n\n信息检索服务\n信息报道与发布服务\n信息咨询服务\n网络信息服务\n\n4.信息用户及其信息需求研究对信息工作有何实践意义 P402\n\n调查和研究用户及其信息需求是信息机构开展服务工作的依据，是提供优化信息服务的条件\n调查和研究用户及其信息需求是设计和建立信息机构或信息系统的基本依据\n调查和研究用户及其信息需求，可以争取更多的用户更全面地利用信息服务，扩大信息机构的服务对象和范围\n调查和研究用户及其信息需求，有助于发挥信息交流的非正规渠道传递信息的功能\n\n5.试述信息用户及其信息需求研究的基本任务 P402\n用户及其信息需求研究有两项基本任务：\n\n是研究本信息机构或系统所服务用户的职业特点，数量范围、知识素养等有关用户结构的情况及其发展变化的趋势\n研究各类用户信息需求的特点\n\n6.试述信息用户及其信息需求研究的主要内容 P402\n\n用户构成及分类研究：研究用户划分的依据或标准以及用户划分的类型，再次基础上研究各类用户的数量构成和知识构成\n用户信息需求调查分析：调查分析信息用户需要什么内容、形式、范围和期限之内的信息，什么时候要，习惯以什么方式，从什么途径去获取所需求的信息\n用户信息需求行为的心里规律研究：对于信息需求可依据用户表达的形式从总体上进行分类，然后再加以分析研究，得出具有普遍性的结论\n用户吸收信息的机理研究：调查用户如何选择和评价信息，研究用户吸收信息作出决策或创造价值的整个过程\n影响用户信息需求的因素研究：信息用户的信息需求是一种特定的社会需要，它受到许多因素影响，不仅要研究受社会政治、发展国民经济和社会技术的方针政策影响广大信心用户信息需求呈现出的一版新特点及其总的发展趋势，还要分别研究用户本身的知识素养、个人兴趣、信息能力和职业特点对信息需求的影响\n用户信息保证研究：用户的信息保证是指通过一切可能的途径和方式给用户提供适当的各种形式的信息你，供其选择、使用，它是“信息服务”的系统化，是满足用户信息需求的关键\n\n7.用户的信息行为有何特征 P403\n\n用户解决问题的重要性或所需要的价值大小是决定用户需求是否转化为信息行为的根本原因\n信息和信息服务的可获得性以及信息资源和信息系统的易用性是决定情报用户是否利用某种信息服务的最重要因素\n用户需求信息的过程体现出“省力法则”，即用户首先从个人资料库中查询相关信息，然后转向非正规渠道，取得同行的帮助，若仍未达到目的，才考虑到信息系统或信息中心\n任何信息用户，即通过正规渠道，也通过非正规渠道寻找所需要的信息。而且对于许多用户来说，非正规渠道被认为比正规渠道更重要\n用户对提供信息服务及信息资料的时间要求呈现出的总趋势是更快、更新\n用户向信息中心的信息服务工作者提出咨询\n\n8.信息需要和信息需求行为的互逆性在信息活动中是如何体现的 P460\n\n从信息需要到信息需求的发展过程\n从信息需求行为到信息需要的回归过程\n\n9.评价用户信息需求的标准有哪些 P460\n\n用户本身的特性指标\n用户所需信息的主题内容特点\n用户所需的信息类型\n用户所需的信息资料的数量指标\n用户需求提供信息的完整性和准确性\n提供信息的时间性指标\n用户获取信息的方法和习惯的评价\n用户所需信息的阶段性特点\n\n10.询问法用于调查用户及其需求有何特点及不足 P409\n具有较强的交互性，无需作大量统计分析就能获得所需的结果；不足之处是调查范围十分有限，需要花费大量时间和人力\n11.间接调查法主要有哪几种方式 P410\n\n文献分析法\n用户资料分析法\n信息业务资料分析法\n\n12.信息服务 (P396)\n1.什么是信息服务 P396\n13.信息需要 (P404)\n根据马斯洛需要等级理论，信息需要就是人们在解决实际问题时对信息的不满足感和必要感，是人们内在的对信息的欲望。\n14.信息服务的原则 (P397)\n2.在信息服务活动中应遵循哪些原则 P397\n论述题\n1.信息系统的结构和功能 (P272)\n28.信息系统结构与功能 (P272)\n2.信息系统主要有哪些类型 (P273)\n\n数据处理系统，主要用于操作层的天天重复、变化不大的各种过程处理和事务处理，位于管理工作的底层。DPS执行基本的数据处理任务：数据搜集、数据操作、数据存储和文档准备\n管理信息系统，是一种能使信息被有相似信息需求的用户（实体）获得的一种计算机系统。这里的信息包括历史信息、现有信息和对未来的预测信息\n决策支持系统，它利用人的智慧有效地解决了MIS中不能解决的半结构决策问题，即现代管理决策中目标含糊不清，多个目标相互冲突，方案的比较和选取没有固定规则和程序可循的问题。\n专家系统，致力于研究如何使机器具有人的职能，它的研究领域包括神经网络、认知系统、知识工程、机器人学习、智能硬件和自然语言处理。专家系统是试图以启发式教育法的形式将人类专家的知识呈现出来的一种计算机程序，它使得管理者比依靠专家咨询做得更好\n办公自动化与虚拟办公室，是以先进的技术设备为基础，由办公人员和技术设备共同构成的人机信息处理系统。OA将人们以电子方式连接在一起，使办公室工作必须在办公室里进行成为历史，员工可以在任何有电子通信系统连接的地方完成工作。\n\n3.试述信息经济学的主要研究内容(P280)\n5.试述信息经济学主要研究内容 (P280)\n4.音频信息基于内容检索的主要类型有哪些 (P313)\n对于音频信息来说，其基于内容检索的主要类型有：\n\n语音检索：语音信息一般具有字、词、语法等语言要素。语音信息检索的主要研究思想是：利用对语音信号处理的研究成果（如自动语音识别ASR技术）把语音信息转换为书面的文本信息，进而再使用文本检索技术\n音乐检索：音乐信息一般具有节奏、旋律或和声等要素，是除语音信息之外的另一类常见且大量的音频信息。由于音乐可用乐谱来表示，因此，音乐检索主要通过乐谱的匹配来实现，或者通过提取其旋律或曲调并进行匹配来实现\n其他音频检索：除语音、音乐信息外，还有其他音频信息，例如脚步声、雨声、电话铃声、发动机声等。对于这类音频信息的检索处理，目前研究人员还没有提出更多有实用价值的技术方法。\n\n5.基于内容的图像检索的类型 (P314)\n\n基于颜色的图像检索：颜色是图像的一种最重要的视觉特征，基于颜色的检索一般不依赖于领域知识，在技术上较容易实现。在图像检索系统中，用户在检索界面上通过使用调色板，可以指定所需查询图像的颜色百分比或颜色的位置分布，从而实现对图像的颜色查询。\n基于纹理的图像检索：纹理较依赖于人的主观认识与鉴别。在目前情况下，进一步的纹理检索研究还需要进行大量的心理实验\n基于形状的图像检索：形状是图像的而重要可视化内容之一。在二维平面图像中，形状通常被认为是一条封闭的轮廓曲线所包围的区域。因此，目前基于形状的检索方法大多围绕着从形状的轮廓特征和形状的区域特征来建立图像索引，用户在检索界面输入草图或轮廓图，或者使用系统提供的范围来进行检索。\n基于空间关系的图像检索：目前这方面的研究还远不像基于颜色、纹理、形状等特征的图像检索那样进展迅速\n基于综合特征的图像检索：基于综合特征的图像检索主要有以下比较典型的研究方案：颜色和纹理特征的结合、颜色和形状特征的结合、纹理和形状特征的结合、空间关系和其他特征的结合等\n\n6.网络信息检索的流程 (P315)\n网络信息检索是一种新型的检索手段和模式，必须遵循一定的检索步骤和运用有关的检索策略与技巧。检索步骤一般包括：\n\n确定检索客体；\n明确检索要求；\n选定检索系统；\n确定检索字段；\n编制检索式：\n执行检索；\n处理检索结果。\n\n7.情报检索的基本流程(P315)\n一、分析用户的信息检索请求\n\n明确检索的目的：为了什么而需要检索系统；\n检索请求的内容特征分析：对用户信息查询请求所属的学科范围与主题范围的确定，需要使用的主要概念及其相互关系的分析，以及相关名词术语的选择等；\n检索请求的形式特征分析：主要包括信息检索所要获取的检索结果数量的估计、所要求的文献的语种、年代、类型。\n\n二、了解检索工具/系统的基本情况\n\n检索工具或检索系统的研制者情况。\n检索工具或数据库的收录范围，通常会涉及学科主题、信息/文献类型、使用语言种类、年代跨度等许多方面。\n索引或数据库的标引处理规则及所使用的词表。\n检索工具或系统提供的主要检索途径及相应功能。\n其他必要说明。\n\n三、制定检索策略\n常用的联机检索策略有：\n\n积木型\n引文珠形增长\n逐次分馏\n\n四、拟定并执行具体检索步骤\n\n选择并输入检索词\n选择检索词的组配连接符号\n选择检索路径、实施检索\n初步浏览检索结果\n使用合适的反馈调整方法，对检索结果进行优化。\n\n五、获取并整理检索结果\n\n输出（或显示）格式的选择\n排序方式的选择\n处理方式的选择：包括联机显示、联机/脱机打印、下载等\n原始文献获取方式的选择\n\n六、分析评价检索操作与检索结果\n检索结果分析，检索系统功能的评价与认识，检索操作中存在的主要问题等\n8.事实和数值型数据库的特点(P330)\n\n事实、数值型数据库一般都是高度专门化的，主题单一性强，具有明确的学科特性，因此，编制工作必须由相应的权威性机构、专业性学术机构和专家参与，以保证质量；\n事实、数值型数据库具有较大的赢利空间，因此一般不会对外公开或免费使用，使用范围受到较为严格的限制；\n由于学科、专业、行业等的差异性，不同数值型数据库的一般在数据库的参数结构、描述方式、编排体例等方面都是各不相同的，由此也带来使用方法和检索方式上的彼此不兼容，难以形成统一的模式或标准；\n除一般检索功能外，很多数值型数据库通常还提供数据运算、数据分析、图形处理等特殊功能，以满足特定用户对专业数据进一步处理的需求。因此，许多数值型数据库总是与特定的检索软件包合并在一起发行；\n事实与数值型数据库的建设特别需要国家政策的支持、全国范围内的协调以及国际合作。\n\n9.信息分析与预测的特点 (P346)\n\n针对性。信息分析与预测的目的是为各级各类科学决策、R&amp;D和市场开拓活动提供依据。不论是何种来源的信息分析与预测课题，都必须针对上述某一既定的具体目标来进行。针对性是信息分析与预测的重要特点，是其能否发挥作用，是否具有生命力的体现\n系统性。信息分析与预测最基本的一项工作是使大量有关研究课题的信息系统化，具体来说，就是使大量分散无序的信息密集化、有序化，使不同层面的信息联串化，使不同时空的信息整体化。\n科学性。信息分析与预测是以事物过去的信息、现在的情况和经验等为根据，运用一定的程序和方法，分析研究对象及其与相关因素的相互联系，从而揭示出研究对象的特性和变化规律\n近似性。信息分析与预测是在事件发生之前对其未来状态的预计和推测，或者对已发生事件的未知状态的估计和推断。这些预（估）计和推测（断），尽管有科学的依据、科学的态度和科学的方法作基础，但毕竟是简约化后对事物发展变化实际情况的一种近似反映\n局限性。信息分析与预测人员对研究对象的认识，往往受到其学识、经验、观察分析能力的限制，受到所搜集到的原始信息的质和量的限制，受到信息处理方式的限制。因此，信息分析与预测有时并不够深刻、全面，其结果往往具有一定的局限性。\n\n10.试述竞争情报分析与预测的主要程序 (P390)\n\n规划与定向：规划与定向是一个了解信息需求，并针对信息需求选定竞争情报分析与预测目标的环节。这一环节的实质性工作是选择课题。竞争情报分析与预测课题通常可分为三种类型，即进攻性、信息型和防御型。进攻型课题的主要任务是对某一战略或战术性行动可能会给本产业或竞争者带来的影响以及竞争者可能会产生的反应作出恰如其分的评价；信息型课题的主要任务是深入了解所在产业和竞争对手的有关信息，其结果可能会导致进攻性或防御性的行动；防御型课题的目的主要是为了了解竞争对手可能会采取什么行动来威胁本企业的竞争地位，并制定出相应措施，以缓解对本企业的威胁。\n信息收集：信息收集环节的关键任务是确定信息源和选择信息搜集方法。竞争情报分析与预测的信息源很多，各种文献信息源和非文献信息源都可以是竞争情报分析与预测的信息源。\n信息整理：信息整理的主要任务是对所搜集的信息进行初步加工，目的是使之由无序变为有序，成为便于利用的形式。这一环节包含这对某些明显重复或不符合课题要求的信息的淘汰。\n信息分析与生产：这是一个需要高度智力和技巧的环节。其主要任务是通过信息分析、预测以及竞争情报产品的制作和评价，形成对竞争战略的制定有决策支持作用的竞争情报产品。\n信息传递：信息传递的主要任务是将竞争情报产品及时送达竞争情报用户手中。\n\n上述五个环节及其功能组合就构成了企业竞争情报系统（CIS）。CIS是面向企业竞争发展需要的，其目的旨在为企业提供有助于增强信息优势的决策性竞争情报，因此又称谓竞争支持系统（CSS），是决策支持系统（DSS）的重要组成部分。CIS一般同时致力于企业内外两方面的信息搜集、加工整理、分析预测和提供活动，并注意对企业外部竞争对手予以连续的监视，必要时还会自动发出预警信号，因而是增强企业信息优势极好的工具\n11.试述互联网信息服务的特点 (P400)\n当代网络技术的迅速发展和全方位应用引发了人类信息需求和利用的多样化、综合化和社会化，并进一步推动了信息服务方式的革命性变革。从总体上看，互联网信息服务呈现以下五个明显的特点和趋势。\n\n信息资源数字化。信息资源数字化指信息以计算机可读的形式存储，其核心主要是数据库的建造。\n信息存取网络化。新兴的全球信息基础设施彻底改变了传统的信息提供和获取方式，它将分散于不同载体、不同地理位置的信息资源以数字方式存储，通过通信网络相互连接，提供即时利用。互联网不再是一个单独的实体或信息存储处，而是一个有序的信息空间，它以用户为中心，实现真正的资源共享\n软件服务系列化。随着计算机的普及和网络化的快速推进，软件已成为现代社会运作的基础和工具。软件产品的生产在许多国家早已形成规模，并经过更新换代，实现了系列化。随着互联网在各行各业的普及和应用，这些行业和部门设计的软件正源源不断地推出，形成规模系列，伴随着应运而生的配套服务，最终必将发展成为巨大的产业\n信息咨询社会化。互联网的发展和完善，使现代咨询进入了普通百姓家，成为整个信息服务业中举足轻重的行业。随着社会经济的发展和人民生活水平的逐步提高，人们对信息咨询的要求越来越高，需求范围越来越广。在计算机和互联网的支持下，信息咨询将实现现代化、市场化和社会化\n信息服务管理现范化。信息服务越现代化、社会化，其管理就越需要规范化和法制化，亦即需要采用相应的标准、规范、政策、法规对信息服务业和信息服务过程实施管理。推行标准化是信息服务规范化管理的根本保证。在网络环境下，必须加强整个信息服务业规范和标准的制定，全方位推行标准化，在制定我国信息服务标准的同时，尽可能采用国际通行标准，以促使我国信息服务业走向世界，在全球范围内实现资源共享。为此，需要根据我国信息服务业的现状、管理体制和信息化的进程，建立横向的标准化协作或协调机构，通过协商，在全国推行统一的规范和标准。\n\n12.信息用户及其信息需求研究的内容 (P402)\n6.试述信息用户及其信息需求研究的主要内容 P402"},"信息资源管理笔记/档案学":{"slug":"信息资源管理笔记/档案学","filePath":"信息资源管理笔记/档案学.md","title":"档案学","links":["信息资源管理笔记/档案学"],"tags":["信息资源管理学"],"content":"档案学基础\n1.试述档案学的研究对象与范围 P413\n档案学的研究对象：档案、档案工作、档案事业和档案学科本身。\n档案学的研究范围：主要包括档案学基础理论研究、档案学应用理论研究、档案学应用技术研究以及与档案学相关的内容\n2.试述欧美档案学主要研究内容与特点 P417\n西欧档案学理论具有较扎实的基础，对许多档案学理论课题的研究有着突破性进展。如德国档案学者布伦内克提出的自由来源原则对于档整理、档案馆组织档案等方面都具有重要意义。\n北美是借鉴吸收欧洲档案学理论的基础上，结合北美国家档案管理的传统来研究文件管理理论和档案管理理论。\n3.欧美档案学对我国档案学研究具有哪些借鉴意义\nPS：没在书中找到完整的答案，以下答案来自 西方档案学对中国档案学的借鉴意义 作者：徐拥军 刊名：档案学通讯\n中国古代档案工作的整体发展水平领先于西方 ,但中国档案学产生晚于西方。中国档案学的研究对象局限于档案现象 ;而西方将档案学的研究对象扩大至广义的文件现象。中国档案学以马克思主义哲学为指导思想 ,侧重于历史方法和逻辑方法 ;西方档案学以不同的哲学思想为指导 ,运用多门学科的研究方法。中国许多档案学者或缺良好的高等教育 ,或缺丰富的实践经验 ;而西方档案学者大多两者兼具。中国档案学应该借鉴西方档案学的长处。\n4.简述我国档案学发展阶段及其主要特点 P422\n\n形成时期（1949年10月——1965年）：档案学作为一门独立学科为国家所确立\n初步发展时期（1957——1966年）：开创性、针对性等特点\n停滞时期（1966——1967年）：在这一时期没有任何发展\n恢复时期（1976——1982年）：档案学的学科地位被重新恢复和确立\n全面发展时期（1983——）：档案学专业从历史文献学中独立，档案学从历史学科规划到管理学，我国档案事业进入法制轨道，形成完整的教育体系\n\n5.为什么说档案具有凭证价值 P427\n档案的凭证价值，是档案不同于和优于其他各种资料的最基本的特点。档案之所以由凭证价值，是由档案形成过程及其结果的内容和形式特点决定的。首先，从档案的内容看，他是从当时当事直接使用的文件转化而来的，并非事后为使用而另行编制的，因此它客观地记录了以往的历史情况，是令人信服的历史证据。其次从档案的形式特征来看，文件上保留着真切的历史标记\n6.实现档案价值具有哪些规律性 P428\n\n档案价值的扩展律\n档案价值的时效律\n档案价值的条件律\n\n7.我国档案工作的主要内容有哪些 P430\n\n档案管理工作\n档案行政管理工作\n档案教育工作\n档案宣传工作\n档案科学技术研究工作\n档案国际合作交流工作\n\n8.我国档案工作的基本原则是什么 P432\n\n全国档案工作实行统一领导、分级管理，全国档案工作实行统一领导、分级管理，是我国档案工作的组织原则和管理体制。其含义是指国家档案按规定分别由各级、各类档案保管机构集中管理，全国档案工作在各级党和政府领导下，由各级档案行政管理机构统一、分级、分专业管理，对党政档案和党政档案工作实行统一管理。\n维护档案的完整与安全，是档案工作的基本要求。维护档案的完整，包括档案材料收集齐全和整理系统两个方面。收集齐全是指凡有保存价值的档案，都要尽量收集齐全，实现一个单位、一个系统、一个地区，直到整个国家真正有保存价值的档案的完整。只有这样，才能完整反映一个单位、一个系统、一个地区和整个国家社会基本活动的历史面貌。整理系统是指凡有保存价值的档案，必须按照它的形成规律，组成有机联系的整体。只有按档案的形成规律，将其组织成一个有机联系的整体，才能全面反映出一定社会活动主体从事社会活动的过程和本来面貌。维护档案的安全，包括档案实体的安全和档案内容安全两个方面。维护档案的实体安全，是指在管理上和技术上采取相关措施，使档案不受损坏，尽量延长档案的寿命。维护档案的内容安全，是指对机密档案和需要控制利用范围的档案实行严格的管理，确保机密档案不丢失、不泄密、不超范围扩散。\n便于社会各方面的利用，满足社会对档案的需要，这是档案工作的根本目的。因此，是否便于社会各方面对档案的利用，是检验档案工作效果的重要标准\n\n9.档案事业行政管理机构的主要职责是什么 P436\n档案行政管理机构，是党和国家指导和管理档案工作的部门，如国家、省、市、县档案局等，其基本职责是：在统一管理党、政档案工作的原则下，分层负责地掌管国家档案事务\n10.档案室的主要职责是哪些 P436\n档案室的基本任务是：集中统一地管理本机关各部门形成的各种门类和载体的全部档案，为本机关各项工作服务，并为党和国家积累档案史料。档案室的具体内容是：对本机关文书部门或业务部门文件材料的归档作专门指导和监督，负责管理本单位的全部档案和相关资料，并积极组织提供利用，定期把具有长远保存价值的档案向档案馆移交\n11.我国档案馆有哪些类型 P436\n\n国家档案管\n专业档案馆\n企业档案馆\n\n12.试述档案事业管理体制 P438\n档案事业管理体制是指档案事业管理的体系和组织制度，包括档案行政管理部门的设置及其隶属关系、权限划分等，从世界范围看，按照中央、地方档案机构之间的关系，档案事业管理体制一般可分为分散式和集中式两种类型。\n13.我国档案法规体系由哪些部分构成 P438\n\n档案工作法律\n中央档案行政法规\n中央档案行政规章\n地方性档案工作法规\n地方性档案工作行政规章\n\n14.《档案法》的主要内容是什么 P439\n共六章27条。其中包括：总则、档案机构及其职责、档案的管理、档案的利用和公布、法律责任和附则\n15.档案工作职业道德包括哪些内容？如何理解 P441\n忠于职守，爱岗敬业，遵纪守法，严守机密，博学求进，公正服务\n16.应用档案学 (P416)\n应用档案学是研究档案管理中具有普遍性和规律性的具体原则、具体问题、具体方法以及某些相关学科理论与技术在某一方面的具体应用。其分支学科是：档案事业管理学、档案管理学科技档案管理学、专门档案管理学、档案文献编纂学、档案法规学等\n17.档案的本质属性 (P423)\n档案是人们在社会实践中直接形成的原始性信息记录，对以往社会实践具有直接的原始记录作用。所以一般认为原始记录性是其本质属性\n18.公务档案 (P426)\n公务档案指人们在公务活动中形成的档案，其形成主体主要是公务机关或其他社会组织，其存在形态主要是过去的公务文书，如法律、法规、行政公文等\n19.现行档案 (P426)\n现行档案指形成时间较晚，离现在的时间距离较近且主要起现时性查考作用，即对人们的现时工作、生活依然有具体的实际作用的档案，在我国一般指1949年新中国成立之后的档案\n20.科技档案 (P426)\n指人们在科技、生产活动中形成的由纯业务性的科技文件材料转化而成的档案，如图纸、设计任务书、科研报告等\n21.档案馆 (P426)\n是指由馆藏档案、工作人员、建筑设备、经费、信息等要素，按一定方式组合而成的，永久保存档案的基地和各项工作利用档案信息的科学文化事业机构，属于国家文献信息系统的重要组成部分\n22.专门档案 (P426)\n是人们通过创造性劳动选择并保存下来以备查考的各种专门文件组成的有机体系的总称。如会计档案、人事档案、诉讼档案、病例档案\n23.人事档案 (P427)\n人事档案是一种专门档案。人事档案是记录一个人的主要经历、政治面貌、品德作风等个人情况的文件材料，起着凭证、依据和参考的作用，在个人转正定级、职称申报、办理养老保险等相关证明时，都需要使用档案\n24.革命历史档案 (P427)\n革命历史档案，又称革命政权档案，指1949年10月1日中华人民共和国成立之前，由中国共产党及其所领导的军队、政权、企事业单位、社团等社会组织及个人所形成的归国家所有的档案\n25.档案价值 (P427)\n档案的价值，是指档案对国家、社会组织或个人的有用性。档案价值具有较高的抽象性，通常是从总体上、抽象的、一般意义上表示档案的有益性、有用性\n26.档案价值形态 (P427)\n是指档案价值的具体表现形式。研究档案的价值形态，对于正确理解和把握档案价值，完善档案价值理论体系乃至科学地鉴定档案的价值，具有重要的理论和实践意义\n27.档案价值时效 (P428)\n档案对社会的有用性是有时限的，某些档案在一定时期内对利用者是有价值的，超过这个时间限制后则降低或丧失了价值\n28.档案的行政作用 (P429)\n档案对于各级各类机构、社会组织乃至国家工作人员察往知来保持政策、体制、秩序、工作方法的连续性、有效性，以及决策的科学性具有不可或缺、无可替代的凭证和参考作用，这种作用可以称为资质作用或行政作用\n29.档案的法律作用 (P429)\n档案的法律作用是指档案在解决争端、处理案件等活动中所发挥的证据作用，法律作用是档案凭证价值的集中体现\n30.档案工作标准 (P434)\n是以档案工作领域中的重要性的事物和概念为对象而制定或修订的各种工作标准的总称，是档案工作中有关单位和个人应当遵守的共同准则和依据\n31.档案室 (P436)\n是各机关（包括团体、学校、工厂、企业、事业单位等）统一保存和管理机关档案的内部机构，是整个机关的组成部分，是属于机关管理和研究咨询的专业机构\n32.中国第一历史档案馆 (P437)\n中国第一历史档案馆，其前身是北京故宫博物馆文献部，1951年改称档案馆，1955年故宫档案馆划归国家档案局领导，改称现名。该馆是以管理明清时期中央机关档案馆为主的文化事业机构，馆藏明清两代重要机关和少数地方机关的档案共74个全宗，1000多万件\n33.文件中心 (P438)\n一种社会化、集约化和专业化的档案文件管理机构，它的设置一般不像档案室一样隶属于一个文件形成单位，而是按地区按系统建立的介于文件形成单位和地方综合性档案馆之间的一种过渡性档案文件管理机构\n34.档案事业管理体制 (P438)\n档案事业管理体制是指档案事业管理的体系和组织制度，包括档案行政管理部门的设置及其隶属关系、权限划分等\n35.档案学的学科特点 (P414)\n\n综合性。由于档案学的研究对象和研究内容都具有一定的综合性特征，因而档案学依然具有综合性的特点。档案学的研究对象与内容，从总体上看其社会科学属性强，档案学的综合性是偏重于社会科学的综合性。\n应用性。档案学研究的许多内容具有很强的技术性和应用性特征，使得档案学这个学科具有很强的实用性。\n实践性。档案学的对象与研究内容都是与档案工作实践密切相关的，档案学理论不是抽象的理论，它是来自于档案工作实践的理论，是对档案工作实践中具有普遍意义的经验、成果进行概括、总结、提炼和升华后形成的理论。\n\n36.档案的文化性 (P424)\n档案是人类社会发展到一定历史阶段的产物，是人类文明进化的结果。档案是人类社会连续性发展的桥梁，是人类社会历史的浓缩，透过档案我们能够触摸到人类社会绵延不断的“脉搏”\n37.简述档案信息化建设的内容 (P435)\n\n档案信息数字化，一般来说，档案信息数字化的内容有两个不同层次：一是档案目录信息的数字化，其目标是建立档案目录数据库：二是档案全文信息的数字化。\n档案网站建设，档案网站是档案机构在公共信息服务网络上建立的站点，它一般是以主页方式提供相关档案服务和开展档案宣传。它是档案信息化建设的重要步骤，是档案部门联系社会的重要窗口。\n数字档案馆建设，数字档案馆，是利用电子网络远程获取档案文件信息的一种方式。现行实体档案馆的馆藏档案是数字档案馆形成的基础；数字档案馆的出现对现行实体档案馆的馆藏建设提出了新的要求，并为现行实体档案馆提供了新的管理和服务机制\n\n38.档案室的任务是什么？(P436)\n档案室是各机关（包括团体、学校、工厂、企业、事业单位等）统一保存和管理机关档案的内部机构，是整个机关的组成部分，是属于机关管理和研究咨询的专业机构。\n档案室的基本任务是：集中统一地管理本机关各部门形成的各种门类和载体的全部档案，为本机关各项工作服务，并为党和国家积累档案史料。\n档案室的具体任务是：对本机关文书部门或业务部门文件材料的归档工作专门指导和监督，负责管理本单位的全部档案和相关资料，并积极组织提供利用，定期把具有长远保存价值的档案向档案馆移交\n文件管理\n1.简述文书与文件的基本概念 P443\n文书：它所包含的对象是公务文书和私人文书，即公务文书和私务文书的总称\n文件：含义为领导机关根据自己的职责范围所制发的具有法定效力并设有特定版头的公文\n2.公文概念是什么？他有哪些特点 P444\n公文是法定机关与组织在公务活动中，按照特定的形式形成、经过一定处理程序制成的书面文字材料，作为传达意图、办理公文与记载活动工作的一种工具。\n公文的特点：\n\n是由法定作者制成和发布的\n具有法定的权威和效力\n具有规范的体式\n具有特定的处理程序\n具有现行效用\n\n3.试述公文的作用 P445\n\n法规作用\n书面指导作用\n公务联系作用\n宣传教育作用\n凭证和依据作用\n\n4.简述专用文件和通用文件的概念及种类 P447\n通用文件：是党、政、军各级机关和团体、企业、事业单位等，在工作活动中普遍使用的文件。如命令、决定、指示、请示、批复、报告、通知等\n专用文件：是指在一定工作部门和业务范围，根据特殊需要专门使用的文件，如外交文件、军事文件、司法文件等\n5.公文的体式及撰写要求是什么 P449\n文理通顺、便于阅读、准确无误、讲求实效、严谨周密、简明精炼、庄重得体、符合规范、整齐划一等\n6.简述文书工作的组织形式 P451\n文书工作的组织形式，是指在社会组织内部围绕着文书工作的活动程序，各部门之间的分工方式。文书工作基本上采用三种组织形式，即集中式、分散式、复合式\n7.什么叫行文规则？机关之间的行文方式有哪几种 P451\n为确保公文运行的畅通无阻，国家有关部门专门制定了旨在控制行文方向、行文方式和行文数量的忧患规定，这就是行文规则，即各社会之间文件往来运行所必须遵守的统一规则。\n机关之间的行文方式主要有下行文、上行文和平行文三种。\n8.简述文书处理的程序 P452\n\n收文处理程序：文件的收进、启封、登记、分类、分送、拟办、承办、催办、注办、清退、归档等\n发文处理程序：拟稿、核稿、签发、缮印、校对、盖印、登记、封装、发出、注办、归档等\n\n9.归档文件的整理原则与质量要求是什么 P455\n\n归档文件的整理原则：即遵循文件的形成规律，保持文件之间的有机联系，区分不同价值，便于保管和利用\n归档文件的质量要求：归档文件应齐全完整，整理归档文件所使用的书写材料、纸张、装订材料等应符合档案保护要求\n\n10.归档文件的整理方法包括哪几个方面 P456\n\n装订\n分类\n排列\n编号\n编目\n装盒\n\n11.试述电子文件的含义和特点 P457 P459\n电子文件是以代码形式记录于磁盘、磁带、光盘等载体，依赖计算机系统存取并可在通信网络上传输的文件\n电子文件的特点：\n\n信息的非人工识读性，电子文件第一次使用了人工可识读的记录符号—数字代码，人无法直接识读和理解，只有通过计算机特定的程序代码，使之还原为输入前的状态，人工才能识读它\n系统依赖性，电子文件的制作、处理，以至归档后的全部管理活动都必须借助于计算机系统才能实现，不兼容的计算机和应用软件生成的文件在交换使用时会遇到很大困难\n信息与特定载体之间的可分离性，由于电子文件中的信息可以在不同的载体上同时存在或相互转换，可以根据需要随时改变和扩展、缩小其存储空间。因而电子文件中的信息是可流动的，具有相对独立性\n信息的易变形，电子文件信息具有易变性，造成电子文件信息发生变化的主要原因有：\n第一，计算机系统中信息的相对独立性（不受载体的限制）。\n第二，电子文件载体性能的不稳定性。\n第三，电子信息技术的发展，新的信息编码方案、存储格式、系统软件的不断出现更是对电子信息稳定性的巨大冲击\n信息存储的高密度性，电子文件的存储密度大大高于以往各种人工可识读的信息介质，随着技术的进步，电子文件介质的存储密度还将继续加大\n多种信息媒体的集体性，纸质文件主要承载文字和图形信息，而电子文件可以将文字、图形、图像、影像、声音等各种信息形式加以有机组合，这种文件称为“多媒体文件”\n信息的可操作性，数字信息的可操作性主要表现在两个方面：\n第一，由于信息的易变性，可以方便地改变其存在状态。\n第二，可以利用已存在的文件信息做其他事情。\n\n12.电子文件的管理原则有哪些 P460\n\n全程管理原则：根据电子文件的特点和管理要求，必须建立一个完整的管理体系对电子文件从产生到永久保存或销毁的整个生命周期进行全程管理电子文件的全程管理是一种全面的管理，因为它涵盖电子文件全部管理活动；是一种系统的管理，因为它强调各项管理内容和要求的无缝链接、系统整合和总体效应；是一种过程管理，因为它是通过过程控制实现结果控制\n前端控制原则：前端控制以文件生命周期理论为基础，把文件从形成到永久保存或销毁的不同阶段看作一个完整的过程。在这个过程中，文件的形成是前端，处理、鉴定、整理、编目等具体管理活动是中端，永久保管或销毁是末端。前端控制是对整个管理过程的目标、要求和规则进行系统分析、科学整合，把需要和可能在文件形成阶段实现或部分实现的管理功能尽量在这一阶段实现。前端控制原则必要性主要体现在两个方面：\n第一，前端控制是确保电子文件真实可靠、完整安全、长期可读的有效策略。\n第二，前端控制是优化管理功能，提高管理效率的科学理念。\n真实性保障原则：电子文件的真实性是指文件内容、结构背景信息经过传输、迁移等处理后依然保持不变，与形成时的原始状态一致。真实性是保证电子文件行政有效性和法律证据性的基础，而电子文件真实性的维护一直都是档案界面临的难题，主要原因来自两个方面：\n一是电子文件信息的易更改性\n二是电子文件信息的软硬件依赖性\n保证电子文件的真实性应注意四点：\n第一，对电子文件的操作者进行可靠的身份识别，权限控制；\n第二，设置符合安全要求的操作日志，随时自动记录操作实施的人员、时间、设备、项目、内容等；\n第三，对具有凭证作用的电子文件逐页加注可靠的防错漏和防调换的标记；\n第四，对电子化的印记、签章等采取防止非法使用的措施\n完整性保障原则：电子文件的完整性包括两个方面的含义：\n一是作为记录社会活动的真实面貌的具有有机联系的电子文件及其他形式的相关文件数量齐全\n二是每一份电子文件的内容、结构和背景信息没有缺损。完整性是电子文件价值的重要保障。\n为了确保相关电子文件的齐全完整，必须掌握电子文件的形成规律和分布状况；为了确保每一份电子文件的完整性，应分析电子文件的构成要素。\n电子文件的基本构成要素包括：载体、内容、物理和智能格式、活动、人员、档案链、背景等，电子文件的完整性是这些要素的有机结合\n可读性保障原则：电子文件的可读性是指文件经过存储、传输、压缩、加密、媒体转换、迁移等处理后能够以人工可以识读、可以理解的方式输出，并保持其内容的真实性。电子文件的可读性是其存在和保存价值的基础\n\n13.电子文件的管理有哪些模式 P462\n\n机构内部电子文件的管理模式\n档案馆电子文件的管理模式\n数字档案馆\n\n14.电子文件鉴定包括哪些内容 P465\n\n电子文件的鉴别\n电子文件的内容鉴别\n电子文件的技术鉴定\n电子文件的处置\n\n15.简述元数据的含义及与著录信息的区别 P467\n元数据是数据的数据。电子文件管理中的元数据是指电子文件系统自动记录的关于文件形成时间、地点、人员、活动、文件系统、结构及内容等方面的具体数据。\n元数据与著录信息既有联系又有区别。就内容而言，两者呈交叉关系，系统自动记录的元数据中某写可以直接转化为著录信息，但不是所有元数据都是著录信息源；此外某些著录信息无法从元数据中提取，需要人工编制而成。\n就作用而言，元数据的作用要广泛的多，著录的作用包括登记、介绍、报道、交流和检索，而元数据的作用除了便于检索外，还包括电子文件的昌吉真实性，保证其凭证性，维护电子文件的长期可读性，便于电子化业务活动开展，有助于将文件管理等原则、方法、纳入电子文件管理系统中，有利于电子文件管理的自动化等\n就生成方式而言，元数据是文件管理系统在文件形成与管理过程中自动形成的，而著录信息的生成则是借助于自动著录和人工著录相结合的方式。\n16.电子文件管理系统的实现方式有哪些 P471\n\n嵌入式：以机构内部各个事物处理系统为母体，将电子文件管理功能需求和所使用的技术措施嵌入其中\n联合式：在将必要的文档管理功能嵌入机构各种事务处理系统后，设计一个独立的电子文件管理系统，并且通过某些必要的技术措施，保证这个系统和其他事务处理系统的无缝整合，共同支持机构的业务活动和机构的文件管理\n独立式：独立式的电子文件管理系统出现在业务活动主体，及生成和处理文件的机构或部门中。\n\n17.行政文件 (P447)\n是指国家机关在日常公务活动中形成和使用的文件，具有行政指挥、领导指导和公务联系的作用\n18.归档文件整理【管理】 (P455)\n指的是将归档文件以件为单位进行装订、分类、排列、编号、编目、装盒、使之有序化的过程\n19.电子文件 (P457)\n是以代码形式记录于磁带、磁盘、光盘等载体，依赖计算机系统存取并可在通信网络上传输的文件\n20.文件生命周期理论 (P480)\n是研究文件从最初形成到最终销毁或永久保存的整个运动过程、研究文件属性与管理者主体行为之间关系的一种理论，是对文件—档案运动过程和规律的客观描述和科学抽象\n21.《档案的整理与编目手册》(P495)\n1898年，荷兰三位档案学者萨穆•缪勒、约翰•斐斯和罗伯特•福罗英出版了专著《档案的整理与编目手册》。三位学者通过科学阐释全宗的定义、性质以及全宗内档案整理系统的特点，完成了对来源原则的理论论证\n22.来源原则 (P473)\n就是指档案馆按照档案的来源进行整理和分类，要求保持同一来源的档案不可分散、不同来源的档案不得混淆的整理原则\n23.公文的特点 (P444)\n\n是由法定作者制成和发布的；\n具有法定的权威和效力；\n具有规范的体式；\n具有特定的处理程序；\n具有现行效用\n\n24.公文的现行效用 (P445)\n所谓现行效用，是指公文在其内容所针对的现行公务活动中直接发挥实际效力，具有依据和凭证功能。这种现象效用具有一定的时间性，只在特定的时间范围内发生效力，这种效力也因具体文种不同而有所差异\n25.简述文书处理程序 (P452)\n文书处理程序指的是在一个机关内部，按照公务文书的制发、办理与管理的规律，对文书工作的一系列操作环节、工作步骤的有序组合和合理安排。\n文书处理程序是基本相同的，通常可分为收文处理和发文处理等具体过程。\n\n收文处理程序的一般内容包括：文件的收进、启封、登记、分送、拟办、承办、催办、注办、清退\n发文处理程序的一般内容包括：拟稿、核稿、签发、缮印、校对、盖印、登记、装封、发出、注办、归档等\n\n26.归档电子文件的质量要求\n\n归档的光盘应清洁，无病毒、无划痕，归档信息填写清楚（版本、设备环境、记录格式等）。\n电子文件归档，要提供电子文件的技术环境、相关软件、版本、数据类型、格式、被操作数据、检测数据等相关信息\n电子文件存储载体，要采用只读光盘，一次性写入光盘，不得用可改写的载体作为归档电子文件长期存储的载体保存。\n归档的电子文件必须是可读文件，长期保存的应把电子文件与相应的机读目录储存在同一载体上\n特殊格式的电子文件，应在存储载体中同时存有相应的支持软件。\n\n档案管理\n1.简述来源原则的基本内容 P477\n来源原则的基本内容可以归纳为三个基本点，即尊重来源，尊重全宗的完整性，尊重全宗的原始整理体系\n2.试述文件生命周期理论的基本内容 P481\n文件生命周期理论的基本内容，主要有三点：\n\n文件从其形成到销毁或永久保存，是一个完整的运动过程\n由于文件价值形态的变化，这一完整过程可划分为若干阶段\n文件在每一阶段因其特定的价值形态而与服务对象、保存场所、管理形式之间存在一种内在的对应关系\n\n3.试述档案鉴定标准 P490\n\n档案的来源标准\n档案的内容标准\n档案的形式特征标准\n档案的相对价值标准\n档案的效益标准\n\n4.档案收集的概念与内容是什么 P492\n档案收集的概念：就是按照档案形成的规律，把分散在各机关、个人手中及其其他地方的档案材料接受、征集、集中起来\n档案收集的内容：\n\n对本机关需要归档案卷的接受工作\n对各现行机关和撤销机关具有长久保存价值的档案的集中和接受工作\n对历史档案的接收和征集工作\n\n5.档案整理工作的内容与原则是什么 P495\n档案整理工作的内容：区分全宗、全宗内档案的分类、确立保管单位、案卷的排列和案卷目录的编制\n档案整理工作的原则：\n\n充分利用原有的整理基础\n必须保持文件之间的历史联系\n必须便于保管和利用\n\n6.科技档案的基本分类方法有哪些 P500\n\n工程项目分类法\n型号分类法\n课题分类法\n专业分类法\n地域分类法\n时间分类法\n\n7.复式档案分类法有哪几种 P500\n\n年度——组织机构分类法\n组织机构——年度分类法\n年度——问题分类法\n问题——年度分类法\n\n8.档案鉴定的内容与方法是什么 P501\n档案鉴定的内容：\n\n制定鉴定档案价值的有关标准，包括单行规定和档案保管期限表等\n具体判定档案材料的价值，确定其保管期限\n拣出本无保存价值和保管期满的档案，按规定进行销毁或作相应的处理\n\n鉴定档案价值的基本工作方法是直接审查档案，通常把这种方法称为直接鉴定法\n9.简述档案鉴定工作的概念及保管期限表的类型 P501\n档案鉴定工作的概念：档案鉴定一般指对档案真伪和档案价值的鉴定，而实际的档案业务工作中主要是后者。\n档案保管期限表的类型：\n\n通用档案保管期限表\n专门档案保管期限\n同系统机关档案保管期限\n同类型机关档案保管期限表\n机关档案保管期限表\n\n10.简述档案保管的概念与内容 P502\n档案保管的概念：是指根据档案的成分和状况所采取的存放和安全防护措施\n档案保管的内容：\n\n档案的库房管理\n档案流动过程中的保护\n保护档案的专门措施\n\n11.档案统计的概念与内容是什么 P503\n档案统计，就是以表册、数字的形式，揭示档案和档案工作有关情况。他的内容，包裹档案的收进、移出、整理、鉴定、保管数量和状况的登记，档案利用情况的登记以及档案构成、档案机构和人员等情况的基本统计和其他专门统计\n12.档案行政管理机关和档案馆的基本统计包括哪些内容 P503\n\n档案构成统计\n档案利用统计\n档案工作人员情况统计\n档案馆建设情况统计\n档案室建立情况统计\n\n13.全宗 (P497)\n全宗是一个独立的机关、组织或人物在社会活动中形成的档案有机整体。基本含义包括三个方面：全宗是一个有机整体，全宗是在一定的历史活动中形成的，全宗是以一定的社会单位为基础而构成的\n14.组织机构分类法 (P499)\n组织机构分类法，就是根据文书处理阶段形成和处理文书承办单位进行分类，即按照立档单位的内容组织机构将全宗内档案分成各个类别\n15.档案保管期限表 (P501)\n档案保管期限表，就是用表册形式列举档案的来源、内容和形式，并指明其保管期限的一种指导性文件\n16.档案直接鉴定法 (P502)\n鉴定档案价值的基本的工作方法之一，特点是通过直接审查档案进行价值鉴定。它要求鉴定人员逐件逐张地审查文件，而不是仅仅根据案卷目录和案卷标题就判定其价值\n17.档案寄存中心 (P502)\n是指国家综合档案馆设立的，为各类企业、社会团体以及个人提供档案寄存有偿服务的机构\n18.文件生命周期理论的重要意义 (P483)\n20.文件生命周期理论 (P480)\n2.试述文件生命周期理论的基本内容 P481\n文件生命周期理论是现代档案学成熟的重要标志，它与来源原则并称为现代档案学的基本理论。文件生命周期理论的重要意义可概况为三个方面：\n\n文件生命周期理论准确地揭示了文件运动地整体性和内在联系，为文件地全过程管理奠定了理论基础\n文件生命周期理论准确地揭示了文件运动地阶段性变换，为文件地阶段式管理奠定了理论基础\n文件生命周期理论准确地揭示了文件运动过程地前后衔接和各阶段地相互影响，为实现从现行文件到档案地一体化管理，为档案部门或人员对文件进行前端控制提供了理论依据\n\n档案的提供利用\n1.简述档案提供利用工作的含义 P505\n档案提供利用工作，亦称档案利用服务，是档案保管部门以所收藏的档案为依据，通过一定的方式与方法，直接提供档案信息，为社会各项事业服务的一项业务活动\n2.试述档案提供利用工作的基本要求P506\n档案馆应当为档案的利用创造基本的条件，简化手续、提供方便，主动开展档案的利用活动，及时掌握和分析档案的利用效果\n3.档案提供利用的基本途径和主要方式有哪些 P506\n\n通过提供档案原件，满足有关档案用户的利用需要\n通过提供档案副本或复制品，满足有关档案用户的利用需要\n通过提供档案信息的加工品，满足有关档案用户的利用需要\n\n4.试述开放档案及其意义 P510\n开放档案就是将一般可以公开的和保密期满的档案，接触封闭，想社会开发，允许档案用户在履行简便的手续后，即可通过一定的方式，进行开发利用。\n开放档案的意义：有利于社会的新方针，是加快我国政治民主化进程的一个新步骤，是现代档案馆自身发展的一项重大措施，可以促进档案馆的各项业务建设。\n5.公布档案的方式有哪些 P513\n公布档案的方式主要有：通过报纸、刊物、图书出版物发表档案的全部或部分原文，通过电台、电视台播放档案的全部或部分原文，陈列、展览档案或其复制件，出版发行档案史料、汇编以及公开出售档案复制件，散发或张贴档案复制件，在公开场合宣读、播放档案原文等\n6.什么是档案编研？档案编研工作的主要内容包括哪些 P514\n档案编研，即档案编辑研究，是指档案部门根据一定的需求，以档案馆藏档案为基础和对象所进行的编辑和研究工作。主要内容包括：\n\n档案史料和现行机关文件汇编，这项工作通常称为档案文献编纂，即按照一定的作者、专题、时间或文种等特征，把档案文件选编成册，在一定的范围使用或公开出版\n编辑档案文摘汇编，档案文摘是对档案原文的缩写，它以简练的文字概要地揭示档案文件的主要内容，是一种档案二次文献。根据一定的专题或采用定期的方式将档案文摘汇集起来加以公布，是档案编研的一项重要工作\n编写档案参考资料，档案参考资料是根据档案内容加工编写的一种书面材料，如大事记、组织沿革、专题概要、会议简介、统计数字汇集等\n编史修志，以馆藏档案为基础，参加历史研究和编史修志\n\n7.试述档案编研工作的特点 P515\n\n研究性，档案编研工作中的“编”和“研”是互有联系的、相互统一的概念，即编中有研，编研结合。档案编研工作的每一项内容都带有很强的研究性\n思想性，档案编研不仅是对档案原件的简单照录，它往往反映了编研人员的观点和认识，具有明显的思想性\n政策性，档案编研成果通常要一定范围内公开使用，因此需要主要可能涉及的政策或法律方面的问题\n\n8.简述大事记及其基本编写方法 P517\n大事记是一种按照时间顺序记载一定范围内发生的重大事件和重要活动的参考资料。\n大事记一般采用编年体，以月为经，以事实为纬，将大事条目按时间顺序排列，以便反映同时期中大事之间的联系，在具体编排方式上有以下两种：一是完全按照时间顺序记述大事，二是按照时间的性质分类后按照时间顺序记述大事。\n9.什么是组织沿革？它的内容主要包括哪些方面 P518\n组织沿革是系统记载一个机关、地区或专业系统的体制、组织机构和人员编制等反面变革情况的参考资料。\n组织沿革的内容主要包括：机关、地区或专业系统的历史概况、行政区划、建制变更情况，机关的性质、任务、职权范围和隶属关系，机关内部组织机构的设置和人员编制的变化情况，机关领导人的任免情况，机关名称的变更、印信的启用与作废、机关办公地点的迁移等\n10.简述企业年鉴及其主要内容 P519\n企业年鉴，是以年度为时限，分栏目叙述该年度本企业基本情况的综合性著述型的编研成果。\n企业年鉴的主要内容有：企业生产经营、科技工作及各项管理工作概况，基本建设施工、设备购置和引进、技术改造活动，各项基础数据统计，重要会议的报告、总结、提案和报道，重大的外事活动，劳动模范、先进集体的材料，相关照片，当年的大事记，等等\n11.什么是科技成果简介 P519\n科技成果简介，也称科技成果摘要、文摘或提要。他是扼要介绍本单位科研、设计成果等主要内容的摘要型科技档案编研成品。\n12.档案证明 (P508)\n档案证明是指档案馆（室）根据有关档案用户的询问和申请，为核查某种事实在档案馆（室）档案中记载情况（有无记载和如何记载）而摘抄编写的书面证明材料\n13.档案展览服务 (P508)\n是指档案收藏部门按照一定的主题（1分），以展出档案原件或其复制品的方式（1分），系统地揭示和介绍档案馆（室）藏中有关档案的内容与成分的一种具体服务方式\n14.开放档案 (P510)\n就是将一般可以公开的和保密期满的档案，解除封闭，向社会开放，允许档案用户在履行简便的手续后，即可通过一定的方式，进行开发利用。其具体含义是：确立了开放档案的范围；明确了开放档案的用户对象；简化了利用手续\n15.档案提供利用的主要方式 (P507)\n\n档案阅览服务；\n档案的外借服务；\n档案的展览服务；\n制发档案复本服务；\n制发档案证明服务；\n档案目录信息服务；\n档案咨询服务。\n\n16.《中华人民共和国档案法》的作用\n《中华人民共和国档案法》规定：属于国家所有的档案，由国家授权的档案馆或者有关机关公布；未经档案馆或有关机关同意，任何组织或个人无权公布。集体所有的和个人所有的档案，档案的所有者有权公布，但必须遵守国家有关规定，不得损害国家安全和利益，不得侵犯他人的合法权益，必要时应当申请当地档案行政管理部门的批准。《中华人民共和国档案法》制定并颁布，说明我国档案事业开始进入法制轨道。\n17.简述档案提供利用工作的基本内容 (P505)\n档案提供利用工作，亦称档案利用服务，是档案保管部门以所收藏地档案为依据，通过一定地方式与方法，直接提供档案信息，为社会各项事业服的一项业务活动。\n基本内容主要包括，了解和熟悉馆（室）藏档案信息的内容和成分，各种档案检索工具的使用方法；分析预测社会对档案信息的需求特点，把握档案利用需求地发展规律；向档案用户介绍和报道馆藏相关地档案信息线索，积极开展档案咨询服务；向档案用户提供他们所需要的档案文献\n论述题\n1.档案工作的基本原则 (P432)\n8.我国档案工作的基本原则是什么 P432\n2.档案工作的性质 (P433)\n\n档案工作的管理性。档案工作是一项管理档案的专门业务，档案工作是信息资源管理的重要内容，档案工作是社会管理活动的组成部分；\n档案工作的政治性。档案工作是一种社会现象。在阶级社会中，档案工作体现着一定的阶级关系和阶级利益；\n档案工作的服务型。档案工作是通过提供档案为社会实践活动服务来推动生产力发展与社会进步，档案工作实现自身价值的这种特殊途径，决定了它是一项服务型工作；\n档案工作的科学性。档案工作是一项为科学研究和科学管理工作提供必要条件的工作，它在实践中包含着特殊的规律和丰富的科学内容，它需要大量运用现代管理科学的内容和信息技术的成果；\n档案工作的文化性。档案馆馆藏档案是社会文化的组成部分，档案馆具有保存历史文化遗产的作用、具有传播社会文化知识与信息的作用、具有社会文化教育的作用以及具有发展科学文化的作用。\n\n3.电子文件的特点 (P459)\n11.试述电子文件的含义和特点 P457 P459\n4.电子文件管理的原则 (P460)\n12.电子文件的管理原则有哪些 P460\n5.试述前苏联全宗理论对来源原则的丰富和发展 (P476)\n\n根据时代发展及时丰富和发展了全宗定义。前苏联早期规定全宗是指机关或个人活动过程中有机形成的档案总和；到80年代将全宗视为泛指概念，定义为彼此具有历史联系和/或逻辑联系的交由国家保管的文件综合体。\n创造性地提出了文件全宗概念。传统的全宗概念都是针对档案馆馆藏提出的，前苏联首创文件全宗作为档案全宗的源头，充分体现了对文件运动过程和规律的尊重。\n构建了一个由国家档案全宗统辖的全宗概念体系，并以此为基础提出了一套完整的全宗理论。前苏联构建的全宗体系分为国家档案全宗、全宗属概念、全宗种概念三个互相统辖的层次，其中全宗种概念又包括机关全宗、个人全宗、联合全宗、档案汇集、科技文件综合体等若干类型。前苏联通过建立层次分明的全宗概念体系并明确规定每一种概念的定义，清晰地展示了全宗理论的层次性和整体联系，加强了来源原则的实践操作色彩\n\n6.试述我国全宗理论的主要内容 (P477)\n\n明确提出了全宗的定义和基本含义，认为全宗是指一个独立的机关、组织或人物在社会活动中形成的档案有机整体，它的基本含义有三点，即全宗是一个有机整体；全宗是在一定的历史活动中形成的，全宗是以一定的社会单位为基础而构成的\n明确提出了全宗（实质为形成全宗的“立档单位”）的构成条件：独立行使职权，能主要以自己的名义单独对外行文；设有会计单位或经济核算单位，自己可以编造预算或财务计划：设有管理人事的机构或人员，并有一定的人事任免权\n明确提出了划分全宗类型的主要标准，一是按照全宗形成者区分为机关组织全宗和人物全宗；二是按照全宗的范围和构成方式区分为独立全宗、联合全宗、全宗汇集合档案汇集\n明确提出了“全宗群”概念，通过提出全宗群原则来维护全宗之间的有机联系。我国的全宗理论使来源原则的理论内容更加丰富、理论体系更加成熟\n明确了全宗内档案分类的科学方法。我国对全宗内档案分类的规定吸取了苏联的合理经验，要求根据立档单位及其档案的实际情况，单独选用或是综合运用组织机构、年度、问题分类方法进行，以求最大限度地保持文件之间的历史联系，便于保管和利用\n\n7.档案《来源原则》基本内容、理论意义和实践价值 (P477)\n来源原则的基本内容可以归纳为三个基本点，即尊重来源，尊重全宗的完整性，尊重全宗内的原始整理体系。\n\n尊重来源：是指档案馆首先应按照来源标准整理档案，保持档案与其形成者之间的来源联系。\n尊重全宗的完整性：是指一个全宗是一个有机整体，整理档案必须维护全宗的完整性，做到同一个全宗的档案不可分散，不同全宗的档案不得混淆。\n尊重全宗内的原始整理体系，是指全宗内的档案整理必须充分利用原有的整理基础，尊重全宗在形成机关获得的原始整理顺序和方法，不宜轻易打乱重整\n\n来源原则的三个基本点是紧密联系、层层递进的。尊重来源是基础，尊重全宗和尊重全宗内的原始整理体系都是尊重来源的重要体现和延伸\n来源原则的理论意义和实践价值主要可以概括为三个方面：\n\n来源原则从历史主义思路出发，充分体现了档案形成的历史联系，为档案馆馆藏的实体整理和分类提供了合理的客观依据。\n来源原则有力地维护和保持了档案的本质属性，成为档案整理与分类中的至善原则。\n来源原则既不是一种纯观念性的抽象信条，也不是一种纯实践性的操作经验，而是兼具理论性和实践性的管理思想和原则。\n\n\n来源原则既是档案整理原则，因为它确立了档案实体整理和分类的基本单元，来源原则又是档案馆组织原则，因为它划清了档案馆与图书馆的界限，从根本上保证了档案专业的独立性，同时来源原则还是档案理论研究的原则之一，因为它促使研究者深入挖掘档案的历史形成过程和有机联系，有助于揭示档案的来龙去脉，保持历史的本来面貌。\n8.我国档案价值鉴定的原则 (P490)\n档案价值鉴定原则是根据档案价值的发展规律总结和提炼出来的一种工作原则，对鉴定实际工作具有普遍指导意义。\n我国档案价值鉴定原则的内容是：必须从社会的总体需求出发用全面的、历史的、发展的观点来判定档案的价值。这一鉴定原则包含两层含义：\n\n确立了“从社会的总体需求出发”的鉴定基本指导思想和出发点\n确定了“全面、历史、发展”这三个鉴定基本观点。\n\n全面的观点要求全面分析档案的内容、来源和形式等各种因素以及档案文件之间的有机联系，全面预测社会多层次、多角度和多方面对档案的利用需求，并促使上述两方面有机结合；\n历史的观点要求必须结合档案形成的历史环境和历史条件来分析其价值，不能超越时代界限，从现在的角度去看待过去的档案，这样才能维护历史的本来面目；\n发展的观点要求尊重档案价值的发展特点和规律，用发展的眼光来衡量档案的价值，既要看到档案为当前的作用，也要看到它对未来可能具有的作用。\n\n\n\n9.档案整理工作的原则 (P495)\n\n充分利用原有的整理基础\n我国档案界惯称的所谓“利用原基础”整理档案，是已为多年来实践所证明的一种比较切实的办法，也是科学地组织档案工作的一条原则。它的含义和要求主要有以下两个方面：\n\n充分地重视和利用先前的整理基础，以确定档案整理的任务和要求，不要轻易打乱重整。一般地说，只要不是零散文件，已经整理而有规可循、有目可查，即应力求保持其原有的整理体系。或者不再重新整理，而通过完善检索工具等其他环节进行弥补；或者只作必要的部分加工整理。\n在档案整理过程中，应该充分研究和利用原来整理的成果，不要轻易破坏以往整理和保存的历史状况。\n\n\n必须保持文件之间的历史联系\n所谓文件之间的历史联系，就是文件在产生和处理过程中所形成的内部相互关系。文件之间的历史联系，主要表现在文件的来源、时间、内容和形式几个方面。\n\n文件在来源方面的联系。文件是以一定的机关及其内部组织机构或一定的个人为单位，有机地形成的。形成文件的这些单位，使文件构成了来源方面不可分割的历史联系。整理档案必须保持其来源方面的固有联系，不允许随意拆散和任意“搬家”。\n文件在时间方面的联系。形成档案的机关或个人所进行的具体活动，都有一定的过程和阶段性，因而使文件之间具有自然的时间联系，整理档案时应注意保持文件之间的这种时间联系。\n文件在内容方面的联系。文件是机关或个人在履行一定职责的各种活动中，为了解决一定问题而产生的。文件的形成者的特定活动，使文件之间在内容上具有密切联系。整理时必须适当地保持文件之间在内容上的联系。\n文件在形式方面的联系。文件的内容必然通过一定的形式表现出来。所谓文件的形式，主要是指文件的种类、名称和载体、记录方式等。文件的形式，标志着文件产生的特定作用，在一定的程度上也反映了文件的来源、时间和文件内容的性质，整理档案时，要考虑这一因素。\n\n\n必须便于保管和利用\n保持文件之间的历史联系，不是整理档案的主要目的，所以不能“为联系而联系”。\n\n10.档案提供利用工作的指导思想 (P505)\n\n全面地为档案用户服务：档案馆（室）对合法用户，应当通过各种服务方式与方法，满足其利用需求。\n及时地为档案用户服务：档案用户在利用档案时，普遍存在着“求快”心理。档案馆（室）必须建立科学的检索体系，同时档案人员也应熟悉库藏档案的内容与成分，这样才能及时地满足档案用户的需要。\n准确地为档案用户服务：实际工作中，有些档案用户提出的利用需求是清晰的、准确的，但是也有一些用户往往不能准确地表达其利用需求。因此，档案人员应善于分析用户利用需求，准确地向用户提供所需的档案，不能超出用户需求的范围，提供一些无关的档案。\n主动地为档案用户服务：一方面，档案部门要积极主动地为档案用户查找档案与资料，努力提高档案利用服务的质量；另一方面，应组织人力和物力，大力开发库藏的档案信息资源，建立完善的检索系统，编辑出版各种档案参考资料，不断提高档案部门为社会提供档案信息服务的能力，主动为利用者服务\n\n11.什么是档案编研？档案编研工作的主要内容包括哪些？(P514)\n6.什么是档案编研？档案编研工作的主要内容包括哪些 P514\n12.试述档案编研工作的特点 (P515)\n7.试述档案编研工作的特点 P515"},"大模型/使用-LLaMA-Factory-微调-LLaMA3":{"slug":"大模型/使用-LLaMA-Factory-微调-LLaMA3","filePath":"大模型/使用 LLaMA-Factory 微调 LLaMA3.md","title":"使用 LLaMA-Factory 微调 LLaMA3","links":[],"tags":[],"content":"官网\nllamafactory.readthedocs.io/zh-cn/latest/\n快速开始：\nzhuanlan.zhihu.com/p/695287607\n1. 实验环境\n1.1 机器\n\nUbuntu\nPyTorch 2.1.0\nPython 3.12 (Ubuntu 22.04)\nCuda 12.2\nA800 * 8\n\n1.2 基座模型\n基于原版的 LLaMA3 8B 模型：\nwww.modelscope.cn/models/LLM-Research/Meta-Llama-3.1-8B/files\n（可选）配置 hf 国内镜像站：\npip install -U huggingface_hub\npip install huggingface-cli\n \nexport HF_ENDPOINT=hf-mirror.com\n \nhuggingface-cli download --resume-download /Llama3-8B --local-dir /root/autodl-tmp/models/Llama3-8B\n2. LLaMA-Factory 框架\n2.1 安装\ngit clone github.com/hiyouga/LLaMA-Factory.git\ncd LLaMA-Factory\npip install -e .\n2.2 准备训练数据\n训练数据：\n\nfintech.json\nidentity.json\n\n将训练数据放在 LLaMA-Factory/data/fintech.json\n并且修改数据注册文件：LLaMA-Factory/data/dataset_info.json\n&quot;fintech&quot;: {\n  &quot;file_name&quot;: &quot;fintech.json&quot;,\n  &quot;columns&quot;: {\n    &quot;prompt&quot;: &quot;instruction&quot;,\n    &quot;query&quot;: &quot;input&quot;,\n    &quot;response&quot;: &quot;output&quot;,\n    &quot;history&quot;: &quot;history&quot;\n  }\n}\n2.3 启动 Web UI\ncd LLaMA-Factory\nllamafactory-cli webui\n2.4 微调模型\nsft微调参数\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n名称描述model_name_or_path模型名称或路径stage训练阶段，可选: rm(reward modeling), pt(pretrain), sft(Supervised Fine-Tuning), PPO, DPO, KTO, ORPOdo_traintrue用于训练, false用于评估finetuning_type微调方式。可选: freeze, lora, fulllora_target采取LoRA方法的目标模块，默认值为 all。dataset使用的数据集，使用”,”分隔多个数据集template数据集模板，请保证数据集模板与模型相对应。output_dir输出路径logging_steps日志输出步数间隔save_steps模型断点保存间隔overwrite_output_dir是否允许覆盖输出目录per_device_train_batch_size每个设备上训练的批次大小gradient_accumulation_steps梯度积累步数max_grad_norm梯度裁剪阈值learning_rate学习率lr_scheduler_type学习率曲线，可选 linear, cosine, polynomial, constant 等。num_train_epochs训练周期数bf16是否使用 bf16 格式warmup_ratio学习率预热比例warmup_steps学习率预热步数push_to_hub是否推送模型到 Huggingface\n参数详解\n\n模型配置\n\nmodel_name_or_path:\n指定基础模型的名称或存储路径。可以是预训练模型的名称（如 llama-7b）或者一个本地路径（如 ./models/llama）。\n作用：加载指定的基础模型用于微调。\n\n\n训练阶段\n\nstage:\n指定训练的阶段，主要有以下几种：\n\nrm (Reward Modeling): 训练奖励模型，用于评估生成的文本质量。\npt (Pretrain): 对模型进行额外的预训练。\nsft (Supervised Fine-Tuning): 使用标注数据对模型进行监督微调。\nPPO (Proximal Policy Optimization): 强化学习阶段，优化生成质量。\nDPO: 一种强化学习技术。\nKTO/ORPO: 其他高级训练技术。\n\n作用：决定训练任务的具体目标。\n\n\n是否训练\n\n\ndo_train:\n\n设置为 true：执行训练。\n设置为 false：仅进行模型评估。\n\n作用：区分训练和评估模式。\n\n\n\n微调类型\n\nfinetuning_type:\n决定微调方式，有以下选项：\n\nfreeze: 冻结模型大部分参数，仅训练特定层或模块。\nlora: 使用 LoRA（低秩适配器），仅调整少量参数，计算资源效率更高。\nfull: 对整个模型进行全量微调。\n作用：影响训练的效率与性能。\n\n\nlora_target:\n当选择 lora 微调方式时，指定目标模块（如 transformer.h.10）或者选择 all 来应用于所有模块。\n作用：细化 LoRA 微调的范围。\n\n\n数据集相关\n\ndataset:\n指定训练使用的数据集名称，可以通过逗号分隔多个数据集。例如，dataset1,dataset2。\n作用：加载训练或评估所需的数据。\ntemplate:\n数据集模板，用于将数据集格式化为模型的输入/输出格式。\n例如，如果模型需要问答数据格式，模板会确保数据集按该格式提供数据。\n作用：保证数据与模型结构匹配。\n\n\n输出路径\n\n\noutput_dir:\n设置训练结果的保存路径，包括模型权重、日志、配置文件等。\n作用：保存训练后的模型及相关信息。\n\n\noverwrite_output_dir:\n是否允许覆盖 output_dir 的内容：\n\n设置为 true：允许覆盖。\n设置为 false：避免覆盖已有数据。\n\n\n\n\n日志与保存\n\nlogging_steps:\n每隔多少步输出一次训练日志。例如，设置为 50 时，每隔 50 个训练步骤记录一次日志。\n作用：控制日志记录的频率，方便监控训练。\nsave_steps:\n每隔多少步保存一次模型。例如，设置为 1000，模型将在每 1000 步时保存一个断点。\n作用：在长时间训练时保存中间成果，防止意外中断丢失进度。\n\n\n训练参数\n\nper_device_train_batch_size:\n每个设备上使用的批次大小。\n例如，设置为 8 时，单张 GPU 每次会处理 8 个样本。\n作用：影响显存占用和训练速度。\ngradient_accumulation_steps:\n梯度累积步数。例如，设置为 4 时，每计算 4 次梯度后才更新模型权重。\n作用：在显存有限的情况下通过累积模拟更大的批次大小。\nmax_grad_norm:\n梯度裁剪阈值，用于防止梯度爆炸。例如，设置为 1.0 时，所有梯度值会被裁剪到不超过 1.0。\n作用：保证训练稳定性。\nlearning_rate:\n学习率，控制模型参数的更新幅度。较高的学习率加快收敛，但可能导致震荡，较低的学习率稳定但收敛慢。\n作用：影响训练效率和性能。\nlr_scheduler_type:\n学习率调整策略：\n\nlinear: 线性递减。\ncosine: 余弦衰减。\npolynomial: 多项式衰减。\nconstant: 固定学习率。\n作用：动态调整学习率以提升训练效果。\n\n\nnum_train_epochs:\n指定训练的总轮数。例如，设置为 3 时，数据集会被遍历 3 次。\n作用：决定训练时长和模型最终精度。\n\n\n硬件加速\n\nbf16:\n是否使用 bf16（半精度浮点数）格式。\n设置为 true 时，节省显存并加速训练，但要求 GPU 支持（如 A100）。\n作用：提升硬件资源利用效率。\nwarmup_ratio 和 warmup_steps:\n\nwarmup_ratio：学习率预热的比例，例如 0.1 表示前 10% 的训练步骤进行预热。\nwarmup_steps：直接指定预热步数。\n作用：缓慢增加学习率，避免训练初期不稳定。\n\n\n\n\n推送到模型仓库\n\npush_to_hub:\n是否将模型推送到 Huggingface Hub，用于共享和管理模型。\n\n设置为 true：训练完成后自动上传。\n设置为 false：仅本地保存。\n\n\n\n\n\n如果显存有限，可以使用 lora 微调，并减小 per_device_train_batch_size。\n对于学习率，通常从 5e-5 或 1e-4 开始调试。\n使用 bf16 和 gradient_accumulation_steps 可以充分利用硬件资源。\n\n命令\n训练\nCUDA_VISIBLE_DEVICES=0,1 指定显卡1和2\nUDA_VISIBLE_DEVICES=0,1 llamafactory-cli train cust/train_llama3_lora_sft.yaml\n对话\nCUDA_VISIBLE_DEVICES=0 llamafactory-cli chat cust/chat_train_llama3_lora_sft.yaml  \n模型合并\n将 base model 与训练好的 LoRA Adapter 合并成一个新的模型。\n⚠️ 不要使用量化后的模型或 quantization_bit 参数来合并 LoRA 适配器。\nllamafactory-cli export cust/merge_llama3_lora_sft.yaml\n\n使用合并后的模型进行预测，就不需要再加载 LoRA 适配器。\n"},"大模型/模型微调工具":{"slug":"大模型/模型微调工具","filePath":"大模型/模型微调工具.md","title":"模型微调工具","links":[],"tags":[],"content":"\n2.1 模型微调工具\n2.1.1 推荐工具\ngithub.com/hiyouga/LLaMA-Factoryhttps://github.com/modelscope/ms-swift\ngithub.com/unslothai/unsloth\ngithub.com/huggingface/pefi\n2.1.2 其它工具\ngithub.com/huggingface/autotrain-advancedhttps://github.com/huggingface/trl\ngithub.com/Lightning-Al/litgpt\ngithub.com/bigscience-workshop/petals\ngithub.com/h2oai/h2o-lmstudio\ngithub.com/internLM/xtuner\ngithub.com/eosphoros-ai/DB-GPT-Hub(3月前更新)github.com/georgian-io/LLM-Finetuning-Toolkit (2月前更新)github.com/codefuse-ai/MFTCoder (影响力不大)"},"机器学习实战/1.-机器学习实战":{"slug":"机器学习实战/1.-机器学习实战","filePath":"机器学习实战/1. 机器学习实战.md","title":"1. 机器学习实战","links":[],"tags":["机器学习","机器学习实战"],"content":"此blog是机器学习实战这本书的读书笔记\n机器学习基础\n用计算机来彰显数据背后真正的意义，这才是机器学习的真正含义。\n在分类算法中目标变量的类型通常是标称型的，而在回归算法中通常是连续型的。\n训练样本必须知道目标变量的值，以便机器学习算法可以发现特征和目标变量之间的关系。\n特征或者属性通常是训练样本集的列，它们是独立测量的结果，多个特征联系在一起共同组成一个训练样本。\n监督学习：\nk-邻近算法  线性回归    朴素贝叶斯算法  局部加权线性回归\n支持向量机  Ridge回归   决策树  Lasso最小回归系数\n\n无监督学习：\nK-均值  最大期望算法    DBSCAN  Parzen窗设计\n\n如何选择合适的算法\n如果要预测目标变量的值，选择监督学习算法，否则无监督学习算法。\n确定目标变量类型：\n离散型：True/False，1/2/3，A/B/C等，选择分类算法\n连续型：0.0 ~ 100， -99 ~ 99， +∞ ~ -∞等，选择回归算法\n一般来说发现最好的算法的关键是反复试错迭代。\n开发机器学习应用程序的步骤\n\n收集数据\n准备输入数据\n分析输入数据\n训练算法\n测试算法\n使用算法\n"},"机器学习实战/10.-机器学习实战":{"slug":"机器学习实战/10.-机器学习实战","filePath":"机器学习实战/10. 机器学习实战.md","title":"10. 机器学习实战","links":[],"tags":["聚类","K-均值"],"content":"利用K-均值聚类算法对未标注数据分组\n聚类是一种无监督的学习，它将相似的对象归到同一个簇中，它有点像全自动分类。聚类方法几乎可以应用于所有对象，簇内的对象越相似，聚类的效果越好。\nK-means聚类算法，它可以发现k个不同的簇，且每个簇中心采用簇中所含值的均值计算而成。\n簇识别（cluster identification）。簇识别给出聚类结果的含义。假定有一些数据，现在将相似数据归到一起，簇识别会告诉我们这些簇到底都是些什么。聚类与分类的最大不同在于，分类的目标事先已知，而聚类则不一样。因为其产生的结果与分类相同，而只是类别没有预先定义，聚类有时也被称为无监督分类（unsupervised classification）。\n聚类分析试图将相似对象归入同一簇，将不相似对象归到不同簇。\nK-均值聚类算法\n优点：容易实现\n缺点：可能收敛到局部最小值，在大规模数据集上收敛较慢\n适用数据类型：数值型数据\n\nK-means是发现给定数据集的k个簇的算法。簇的个数k是用户给定的，每一个簇通过其质心（centroid），即簇中所有点的中心来描述。\nK-means算法的工作流程，首先随机确定k个初始点作为质心，然后将数据集的每个点分配到一个簇中，具体来讲，为每个点找距其最近的质心，并将其分配给该质心所对应的簇。这一步完成之后，每个簇的质心更新为该簇所有点的平均值。\n伪代码如下：\n创建k个点作为其实质心\n当任意一个点的簇分配结果发生改变时\n    对数据集中的每个数据点\n        对每个质心\n            计算质心与数据点之间的距离\n        将数据点分配到距离其最近的簇\n    对每一个簇，计算簇中所有点的均值并将均值作为质心\n\nK-means聚类的一般流程\n\n收集数据\n准备数据：需要数值型数据来计算距离，也可以将标称型数据映射为二值型数据再用于距离计算\n分析数据\n训练算法：不适用无监督学习，即无监督学习没有训练过程\n测试算法：应用聚类算法，观察结果。可以使用量化的误差指标如误差平方和来评价算法的结果\n使用算法：可以用于所希望的任何应用。通常情况下，簇质心可以代表整个簇的数据来做出决策\n\nfrom numpy import *\n \ndef loadDataSet(fileName):\n    dataMat = []\n    fr = open(fileName)\n    for line in fr.readlines():\n        curLine = line.strip().split(&#039;\\t&#039;)\n        fltLine = map(float, curLine)\n        fltLine = list(fltLine)\n        dataMat.append(fltLine)\n    return dataMat\n \ndef distEclud(vecA, vecB):\n    return sqrt(sum(power(vecA - vecB, 2)))\n \ndef randCent(dataSet, k):\n    n = shape(dataSet)[1]\n    centroids = mat(zeros((k, n)))\n    for j in range(n):\n        minJ = min(dataSet[:, j])\n        rangeJ = float(max(dataSet[:, j]) - minJ)\n        centroids[:, j] = minJ + rangeJ * random.rand(k, 1)\n    return centroids\ndistEclud()是计算两个向量的欧式距离\nrandCent()是为给定数据集构建一个包含k个随机质心的集合。随机质心必须要在整个数据集的边界之内，这可以找到数据集每一维的最小值和最大值来完成。然后随机生成0到1.0之间的随机数并通过取值范围和最小值，以便确保随机点在数据的边界之内。\ndatMat = mat(loadDataSet(&#039;MLiA_SourceCode/Ch10/testSet.txt&#039;))\nmin(datMat[:, 0])\nmatrix([[-5.379713]])\n\nmin(datMat[:, 1])\nmatrix([[-4.232586]])\n\nmax(datMat[:, 1])\nmatrix([[5.1904]])\n\nmax(datMat[:, 0])\nmatrix([[4.838138]])\n\n看下randCent()生成的值是否在max和min之间。\nrandCent(datMat, 2)\nmatrix([[-0.55362342, -1.69185255],\n        [ 0.43137443, -4.17749883]])\n\n测试计算距离的函数\ndistEclud(datMat[0], datMat[1])\n5.184632816681332\n\ndef kMeans(dataSet, k, distMeas=distEclud, createCent=randCent):\n    m = shape(dataSet)[0]\n    clusterAssment = mat(zeros((m, 2)))\n    \n    centroids = createCent(dataSet, k)\n    clusterChanged = True\n    while clusterChanged:\n        clusterChanged = False\n        for i in range(m):\n            minDist = inf\n            minIndex = -1\n            for j in range(k): # 寻找最近的质心\n                distJI = distMeas(centroids[j, :], dataSet[i, :])\n                if distJI &lt; minDist:\n                    minDist = distJI\n                    minIndex = j\n            if clusterAssment[i, 0] != minIndex:\n                clusterChanged = True\n            clusterAssment[i, :] = minIndex, minDist**2\n        print(centroids)\n        for cent in range(k):\n            ptsInClust = dataSet[nonzero(clusterAssment[:, 0].A == cent)[0]] # 更新质心的位置\n            centroids[cent, :] = mean(ptsInClust, axis=0)\n    return centroids, clusterAssment\ncentroids, clusterAssment = kMeans(datMat, 4)\ncentroids\n[[ 4.01323567  3.90379869]\n [-3.02008248 -3.35713241]\n [ 0.85731381  0.6868651 ]\n [ 0.45281866 -3.89960214]]\n[[ 2.72275519  3.38230919]\n [-3.53973889 -2.89384326]\n [-0.92392975  2.12807596]\n [ 2.42776071 -3.19858565]]\n[[ 2.6265299   3.10868015]\n [-3.53973889 -2.89384326]\n [-2.31079352  2.63181095]\n [ 2.7481024  -2.90572575]]\n[[ 2.6265299   3.10868015]\n [-3.53973889 -2.89384326]\n [-2.46154315  2.78737555]\n [ 2.65077367 -2.79019029]]\n\n\nmatrix([[ 2.6265299 ,  3.10868015],\n        [-3.53973889, -2.89384326],\n        [-2.46154315,  2.78737555],\n        [ 2.65077367, -2.79019029]])\n\nimport matplotlib.pyplot as plt\ndef plotScatter(data):\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ax.scatter(data[:, 0].T.tolist()[0], data[:, 1].T.tolist()[0], 10, c=&#039;red&#039;)\n    ax.scatter(centroids[:, 0].T.tolist()[0], centroids[:, 1].T.tolist()[0], 50, marker=&#039;*&#039;)\n    plt.show()\nplotScatter(datMat)\n\n上面的结果得到四个质心。\n使用后处理来提高聚类性能\n利用点到质心的距离的平方值，来评价聚类质量。\n另一种用于度量聚类效果的指标是SSE（sum of squared error，误差平方和），对应clusterAssment矩阵第一列之和。SSE值越小表示数据点越接近于他们的质心，聚类效果也越好。因为对误差取了平方，因此更加重视那些远离中心的点。\n二分K-均值算法\n为了克服K-均值算法收敛于局部最小值的问题，有人提出了一个称为二分K-均值的算法。该算法首先将所有点作为一个簇，然后将该簇一分为二。之后选择另一个簇进行划分，选择哪一个簇进行划分取决于对其划分是否可以最大程度降低SSE值。\n伪代码如下：\n将所有点看成一个簇\n当簇数目小于k时\n    对每一个簇\n        计算总误差\n        在给定的簇上面进行K-均值聚类（k=2）\n        计算将该簇一分为二后的总误差\n    选择使得误差最小的那个簇进行划分操作\n\ndef biKmeans(dataSet, k, distMeas=distEclud):\n    m = shape(dataSet)[0]\n    clusterAssment = mat(zeros((m,2)))\n    centroid0 = mean(dataSet, axis=0).tolist()[0]\n    centList = [centroid0] # 创建一个初始簇\n    \n    # 计算每个点到平均值的距离的平方\n    for j in range(m):\n        clusterAssment[j, 1] = distMeas(mat(centroid0), dataSet[j, :])**2\n    \n    while (len(centList) &lt; k):\n        lowestSSE = inf\n        for i in range(len(centList)):\n            ptsInCurrCluster = dataSet[nonzero(clusterAssment[:, 0].A==i)[0], :] # 获取当前数据集i中的数据点\n            centroidMat, splitClustAss = kMeans(ptsInCurrCluster, 2, distMeas)\n            sseSplit = sum(splitClustAss[:, 1]) # 将SEE与当前的最小值进行比较\n            sseNotSplit = sum(clusterAssment[nonzero(clusterAssment[:, 0].A!=i)[0], 1])\n            print(&quot;sseSplit, and notSplit: &quot;, sseSplit,sseNotSplit)\n            if (sseSplit + sseNotSplit) &lt; lowestSSE:\n                bestCentToSplit = i\n                bestNewCents = centroidMat\n                bestClustAss = splitClustAss.copy()\n                lowestSSE = sseSplit + sseNotSplit\n        bestClustAss[nonzero(bestClustAss[:,0].A == 1)[0],0] = len(centList)\n        bestClustAss[nonzero(bestClustAss[:,0].A == 0)[0],0] = bestCentToSplit\n        print(&#039;the bestCentToSplit is: &#039;,bestCentToSplit)\n        print(&#039;the len of bestClustAss is: &#039;, len(bestClustAss))\n        centList[bestCentToSplit] = bestNewCents[0,:].tolist()[0] # 用两个最佳质心替换一个质心\n        centList.append(bestNewCents[1,:].tolist()[0])\n        clusterAssment[nonzero(clusterAssment[:,0].A == bestCentToSplit)[0],:]= bestClustAss # 更新簇的分配结果\n    return mat(centList), clusterAssment\ndatMat3 = mat(loadDataSet(&#039;MLiA_SourceCode/Ch10/testSet2.txt&#039;))\ncentroids, clusterAssment = biKmeans(datMat3, 3)\ncentroids\n[[ 0.94619158 -0.94117951]\n [ 3.40877878 -0.10489155]]\n[[-1.70351595  0.27408125]\n [ 2.93386365  3.12782785]]\nsseSplit, and notSplit:  541.2976292649145 0.0\nthe bestCentToSplit is:  0\nthe len of bestClustAss is:  60\n[[-2.2226205   2.99958388]\n [-2.07781041  4.61823253]]\n[[-1.32962218 -0.58601139]\n [-3.466158    4.32880371]]\n[[-0.45965615 -2.7782156 ]\n [-2.94737575  3.3263781 ]]\nsseSplit, and notSplit:  67.2202000797829 39.52929868209309\n[[3.31450775 4.54204866]\n [2.24406919 1.79975326]]\n[[3.26127644 3.86529411]\n [2.66598045 2.52444636]]\n[[3.43738162 3.905037  ]\n [2.598185   2.60968842]]\nsseSplit, and notSplit:  28.094839828868793 501.7683305828214\nthe bestCentToSplit is:  0\nthe len of bestClustAss is:  40\n\n\nmatrix([[-0.45965615, -2.7782156 ],\n        [ 2.93386365,  3.12782785],\n        [-2.94737575,  3.3263781 ]])\n\nplotScatter(datMat3)\n\n实例：对地图上的点进行聚类\ndef distSLC(vecA, vecB):#Spherical Law of Cosines\n    a = sin(vecA[0,1]*pi/180) * sin(vecB[0,1]*pi/180)\n    b = cos(vecA[0,1]*pi/180) * cos(vecB[0,1]*pi/180) * \\\n                      cos(pi * (vecB[0,0]-vecA[0,0]) /180)\n    return arccos(a + b)*6371.0 #pi is imported with numpy\n \nimport matplotlib\nimport matplotlib.pyplot as plt\ndef clusterClubs(numClust=5):\n    datList = []\n    for line in open(&#039;MLiA_SourceCode/Ch10/places.txt&#039;, &#039;r&#039;).readlines():\n        lineArr = line.split(&#039;\\t&#039;)\n        datList.append([float(lineArr[4]), float(lineArr[3])])\n    datMat = mat(datList)\n    myCentroids, clustAssing = biKmeans(datMat, numClust, distMeas=distSLC)\n    fig = plt.figure()\n    rect=[0.1,0.1,0.8,0.8]\n    scatterMarkers=[&#039;s&#039;, &#039;o&#039;, &#039;^&#039;, &#039;8&#039;, &#039;p&#039;, \\\n                    &#039;d&#039;, &#039;v&#039;, &#039;h&#039;, &#039;&gt;&#039;, &#039;&lt;&#039;]\n    axprops = dict(xticks=[], yticks=[])\n    ax0=fig.add_axes(rect, label=&#039;ax0&#039;, **axprops)\n    imgP = plt.imread(&#039;MLiA_SourceCode/Ch10/Portland.png&#039;)\n    ax0.imshow(imgP)\n    ax1=fig.add_axes(rect, label=&#039;ax1&#039;, frameon=False)\n    for i in range(numClust):\n        ptsInCurrCluster = datMat[nonzero(clustAssing[:,0].A==i)[0],:]\n        markerStyle = scatterMarkers[i % len(scatterMarkers)]\n        ax1.scatter(ptsInCurrCluster[:,0].flatten().A[0], ptsInCurrCluster[:,1].flatten().A[0], marker=markerStyle, s=90)\n    ax1.scatter(myCentroids[:,0].flatten().A[0], myCentroids[:,1].flatten().A[0], marker=&#039;+&#039;, s=300)\n    plt.show()\nclusterClubs()\n[[-122.56739405   45.62557076]\n [-122.63486396   45.51036263]]\n[[-122.842918     45.646831  ]\n [-122.62856971   45.5103284 ]]\n[[-122.76690133   45.612314  ]\n [-122.62552961   45.50776091]]\n[[-122.729442     45.58514429]\n [-122.62063813   45.5040831 ]]\n[[-122.74941346   45.545862  ]\n [-122.60434434   45.50451707]]\n[[-122.74823556   45.52585431]\n [-122.59648847   45.50821685]]\n[[-122.72797062   45.51642875]\n [-122.58031918   45.51010827]]\n[[-122.7142141    45.51492203]\n [-122.56818551   45.5102949 ]]\n[[-122.70981637   45.51478609]\n [-122.56409551   45.51016235]]\nsseSplit, and notSplit:  3073.8303715312386 0.0\nthe bestCentToSplit is:  0\nthe len of bestClustAss is:  69\n[[-122.74578835   45.53605534]\n [-122.83598851   45.6117388 ]]\n[[-122.70552277   45.51052658]\n [-122.842918     45.646831  ]]\nsseSplit, and notSplit:  1351.7802960650447 1388.799845546737\n[[-122.51444985   45.56152247]\n [-122.6350006    45.49520857]]\n[[-122.54062592   45.52653233]\n [-122.607424     45.47994085]]\n[[-122.54052872   45.52505652]\n [-122.613193     45.47913283]]\nsseSplit, and notSplit:  917.0774766267409 1685.0305259845018\nthe bestCentToSplit is:  1\nthe len of bestClustAss is:  37\n[[-122.79462233   45.64436218]\n [-122.77702826   45.44723276]]\n[[-122.72070683   45.59796783]\n [-122.70730319   45.49559031]]\nsseSplit, and notSplit:  1047.9405733077342 917.0774766267409\n[[-122.52922184   45.56204495]\n [-122.41440445   45.48137939]]\n[[-122.55266787   45.52993361]\n [-122.4009285    45.46897   ]]\nsseSplit, and notSplit:  361.2106086859341 1898.9745985610039\n[[-122.63507677   45.48340811]\n [-122.60541227   45.407053  ]]\n[[-122.6105264   45.4923452]\n [-122.626526    45.413071 ]]\nsseSplit, and notSplit:  81.83580692942014 2388.16393003474\nthe bestCentToSplit is:  0\nthe len of bestClustAss is:  32\n[[-122.82888364   45.5832033 ]\n [-122.71555836   45.59441721]]\n[[-122.842918    45.646831 ]\n [-122.6962646   45.5881952]]\nsseSplit, and notSplit:  24.09829508946755 1797.0816445068451\n[[-122.52145964   45.55057242]\n [-122.48348974   45.49208357]]\n[[-122.57237273   45.5439008 ]\n [-122.4927627    45.4967901 ]]\nsseSplit, and notSplit:  307.68720928070644 1261.8846458842365\n[[-122.60429789   45.49458255]\n [-122.55803361   45.45874909]]\n[[-122.61647322   45.49408122]\n [-122.60335233   45.43428767]]\n[[-122.6105264   45.4923452]\n [-122.626526    45.413071 ]]\nsseSplit, and notSplit:  81.83580692942014 1751.0739773579726\n[[-122.78221469   45.49159997]\n [-122.66948399   45.51952507]]\n[[-122.7680632    45.4665528 ]\n [-122.66932819   45.51373875]]\n[[-122.761804     45.46639582]\n [-122.66733593   45.5169996 ]]\nsseSplit, and notSplit:  335.01842722575645 1085.0138820543707\nthe bestCentToSplit is:  3\nthe len of bestClustAss is:  26\n\n\n总结\n聚类是一种无监督学习方法。所谓无监督学习是指事先不知道要寻找的内容，即没有目标变量。聚类将数据点归到多个簇中，其中相似数据点处于同一簇，而不相似数据点处于不同簇中。聚类可以使用多种不同的方法来计算相似度。\n一种广泛使用的聚类算法是K-均值算法，其中k是用户指定的要创建的簇的数目。K-均值聚类算法以k个随机质心开始。算法会计算每一个点到直线的距离。每个点会被分配到距其最近的簇质心，然后紧接着基于新分配到簇的点更新簇质心。重复以上过程数次，直到质心不再改变。\n为获得更好的聚类效果，可以使用二分K-均值的聚类算法。二分K-均值算法首先将所有点作为一个簇，然后使用K-均值算法(k=2)对其划分。下一次迭代时，选择有最大误差的簇进行划分。该过程重复直到k个簇创建成功为止。\n二分K-均值的聚类效果要好于K-均值算法。"},"机器学习实战/11.-机器学习实战":{"slug":"机器学习实战/11.-机器学习实战","filePath":"机器学习实战/11. 机器学习实战.md","title":"11. 机器学习实战","links":[],"tags":["关联分析","Apriori算法"],"content":"使用Apriori算法进行关联分析\n在大规模数据集中寻找物品的隐含关系被称作关联分析(association analysis)或者关联规则学习(association rule learning)，例如商品的定向推荐。\n关联分析\nAprior算法\n优点：易编码事先\n缺点：在大数据集上可能较慢\n适用数据类型：数值型或标称型数据\n\n关联分析是一种大规模数据集中寻找有趣关系的任务。这些关系可以有两种形式：频繁项集或者关联规则。频繁项集（frequent item sets）是经常出现在一块的物品的集合，关联规则（association rules）暗示两种物品之间可能存在很强的关系。\n频繁项集是指那些经常出现在一起的物品集合（啤酒和尿布）\n一个项集的支持度（support）被定义为数据集中包含该项集的记录所占比例。支持度是针对项集来说的，因此可以定义一个最小支持度，而只保留满足最小支持度的项集。\n可信度或置信度（confidence）是针对一条关联规则来定义的。\n支持度和可信度是用来量化关联分析是否成功的方法。\nApriori原理\nApriori算法的一般过程\n\n收集数据\n准备数据\n分析数据\n训练数据：使用apriori算法来找到频繁项集\n测试算法：不需要测试过程\n使用算法：用来发现频繁项集以及物品之间的关联规则\n\nApriori原理，如果某个项集是频繁的，那么他的所有子集也是频繁的。对于下图的例子意味着，{0, 1}是频繁的，那么{0}，{1}也是频繁的，这个原理反过来看就是，如果说一个项集是频繁的，那么它的所有超集也是频繁的。\n\n使用Apriori算法来发现频繁集\nApriori算法的两个输入参数分别是最小支持度和数据集。该算法首先会生成所有单个物品的项集列表。接着扫描交易记录来查看哪些项集满足最小支持度要求，那些不满足最小支持度的集合会被去掉。然后对剩下的集合进行组合生成包含两个元素的项集。接下重新扫描交易记录，去掉不满足最小支持度的项集。重复该过程直到所有项集都被去掉。\n生成候选集\n创建一个用于构建初始集合的函数，和一个通过扫描数据集以寻找交易记录子集的函数。\n数据集扫描的伪代码如下：\n对数据集中的每条交易记录tran\n对每个候选项集can:\n    检查一下can是否是tran的子集:\n    如果是，则增加can的计数值\n对每个候选集：\n如果其支持度不低于最小值，则保留该项集\n返回所有频繁项集列表\n\ndef loadDataSet():\n    return [[1, 3, 4], [2, 3, 5], [1, 2, 3, 5], [2, 5]]\n \ndef createC1(dataSet):\n    C1 = []\n    for transaction in dataSet:\n        for item in transaction:\n            if not [item] in C1:\n                C1.append([item])\n    C1.sort()\n    ret = map(frozenset, C1) # 返回一个冻结的集合\n    return list(ret)\n \ndef scanD(D, Ck, minSupport):\n    ssCnt = {}\n    for tid in D:\n        for can in Ck:\n            if can.issubset(tid):\n                if not can in ssCnt:\n                    ssCnt[can] = 1\n                else:\n                    ssCnt[can] += 1\n    numItems = float(len(D))\n    retList = []\n    supportData = {}\n    for key in ssCnt:\n        support = ssCnt[key]/numItems\n        if support &gt;= minSupport:\n            retList.insert(0, key)\n        supportData[key] = support\n    return retList, supportData\nC1是大小为1的所有候选项集的集合。\nL1是满足最低要求的项集构成的集合L1。\ncreateC1()函数将构建第一个候选项集的列表C1，scanD()有三个参数，分别是数据集，候选项集列表Ck，以及感兴趣项集的最小支持度minSupport。\ndataSet = loadDataSet()\ndataSet\n[[1, 3, 4], [2, 3, 5], [1, 2, 3, 5], [2, 5]]\n\nC1 = createC1(dataSet)\nC1\n[frozenset({1}),\n frozenset({2}),\n frozenset({3}),\n frozenset({4}),\n frozenset({5})]\n\nD = list(map(set, dataSet))\nD\n[{1, 3, 4}, {2, 3, 5}, {1, 2, 3, 5}, {2, 5}]\n\nL1, suppData0 = scanD(D, C1, 0.5)\nsuppData0, L1\n({frozenset({1}): 0.5,\n  frozenset({3}): 0.75,\n  frozenset({4}): 0.25,\n  frozenset({2}): 0.75,\n  frozenset({5}): 0.75},\n [frozenset({5}), frozenset({2}), frozenset({3}), frozenset({1})])\n\n上述4个项集构成了L1列表，该列表中的每个单物品项集至少出现在50%以上的记录中。由于物品4并没有达到最小支持度，所以不在L1中。\n组织完整的Apriori算法\n整个Apriori算法的伪代码如下：\n当集合中项的个数大于0时\n    构建一个k个项目组成的候选项集的列表\n    检查数据以确认每个项集都是频繁的\n    保留频繁项集并构建k+1项组成的候选项的列表\n\ndef aprioriGen(Lk, k):\n    retList = []\n    lenLk = len(Lk)\n    for i in range(lenLk):\n        for j in range(i+1, lenLk):\n            L1 = list(Lk[i])[: k-2]\n            L2 = list(Lk[j])[: k-2]\n            L1.sort()\n            L2.sort()\n            if L2 == L2:\n                retList.append(Lk[i] | Lk[j])\n    return retList\n \ndef apriori(dataSet, minSupport=0.5):\n    C1 = createC1(dataSet)\n    D = map(set, dataSet)\n    D = list(D)\n    L1, supportData = scanD(D, C1, minSupport)\n    L = [L1]\n    k = 2\n    \n    while (len(L[k-2]) &gt; 0):\n        Ck = aprioriGen(L[k-2], k)\n        Lk, supK = scanD(D, Ck, minSupport)\n        supportData.update(supK)\n        L.append(Lk)\n        k += 1\n    return L, supportData\nL, suppData = apriori(dataSet)\nL\n[[frozenset({5}), frozenset({2}), frozenset({3}), frozenset({1})],\n [frozenset({2, 3}), frozenset({3, 5}), frozenset({2, 5}), frozenset({1, 3})],\n [frozenset({2, 3, 5})],\n []]\n\nL[0]\n[frozenset({5}), frozenset({2}), frozenset({3}), frozenset({1})]\n\nL[1]\n[frozenset({2, 3}), frozenset({3, 5}), frozenset({2, 5}), frozenset({1, 3})]\n\nL[2]\n[frozenset({2, 3, 5})]\n\nL[3]\n[]\n\n每个项集都是在apriori()中调用函数aprioriGen()来生成的。\naprioriGen(L[0], 2)\n[frozenset({2, 5}),\n frozenset({3, 5}),\n frozenset({1, 5}),\n frozenset({2, 3}),\n frozenset({1, 2}),\n frozenset({1, 3})]\n\nL, suppData = apriori(dataSet, minSupport=0.7)\nL\n[[frozenset({5}), frozenset({2}), frozenset({3})], [frozenset({2, 5})], []]\n\n变量suppData是一个字典，它包含我们项集的支持度值。\n从频繁项集中挖掘关联规则\n要找到关联规则，我们首先从一个频繁项集开始。我们知道集合中的元素是不重复的，但我们想知道基于这些元素能否获得其他内容。某个元素或者某个元素集合可能会推导出另一个元素。\n关联规则的量化指标是，可信度。一条规则P→H的可信度定义为support(P|H)/support(P)。\n\n对于频繁项集{0，1，2，3}的关联规则网络示意图。阴影区域给出的是低可信度的规则。如果发现0，1，2—&gt;3是一条低可信度规则，那么所有其以3作为后件的规则可信度也会较低。\ndef generateRules(L, supportData, minConf=0.7):\n    bigRuleList = []\n    for i in range(1, len(L)):\n        for freqSet in L[i]:\n            H1 = [frozenset([item]) for item in freqSet]\n            if (i &gt; 1):\n                rulesFromConseq(freqSet, H1, supportData, bigRuleList, minConf)\n            else:\n                calcConf(freqSet, H1, supportData, bigRuleList, minConf)\n    return bigRuleList    \n \ndef calcConf(freqSet, H, supportData, brl, minConf=0.7):\n    prunedH = [] \n    for conseq in H:\n        conf = supportData[freqSet]/supportData[freqSet-conseq]\n        if conf &gt;= minConf: \n            print(freqSet-conseq,&#039;--&gt;&#039;,conseq,&#039;conf:&#039;,conf)\n            brl.append((freqSet-conseq, conseq, conf))\n            prunedH.append(conseq)\n    return prunedH\n \ndef rulesFromConseq(freqSet, H, supportData, brl, minConf=0.7):\n    m = len(H[0])\n    if (len(freqSet) &gt; (m + 1)): \n        Hmp1 = aprioriGen(H, m+1)\n        Hmp1 = calcConf(freqSet, Hmp1, supportData, brl, minConf)\n        if (len(Hmp1) &gt; 1):\n            rulesFromConseq(freqSet, Hmp1, supportData, brl, minConf)\nL, suppData=apriori(dataSet, minSupport=0.5)\nrules = generateRules(L, suppData, minConf=0.7)\nfrozenset({5}) --&gt; frozenset({2}) conf: 1.0\nfrozenset({2}) --&gt; frozenset({5}) conf: 1.0\nfrozenset({1}) --&gt; frozenset({3}) conf: 1.0\nfrozenset({5}) --&gt; frozenset({2, 3}) conf: 2.0\nfrozenset({3}) --&gt; frozenset({2, 5}) conf: 2.0\nfrozenset({2}) --&gt; frozenset({3, 5}) conf: 2.0\n\nrules = generateRules(L, suppData, minConf=0.5)\nfrozenset({3}) --&gt; frozenset({2}) conf: 0.6666666666666666\nfrozenset({2}) --&gt; frozenset({3}) conf: 0.6666666666666666\nfrozenset({5}) --&gt; frozenset({3}) conf: 0.6666666666666666\nfrozenset({3}) --&gt; frozenset({5}) conf: 0.6666666666666666\nfrozenset({5}) --&gt; frozenset({2}) conf: 1.0\nfrozenset({2}) --&gt; frozenset({5}) conf: 1.0\nfrozenset({3}) --&gt; frozenset({1}) conf: 0.6666666666666666\nfrozenset({1}) --&gt; frozenset({3}) conf: 1.0\nfrozenset({5}) --&gt; frozenset({2, 3}) conf: 2.0\nfrozenset({3}) --&gt; frozenset({2, 5}) conf: 2.0\nfrozenset({2}) --&gt; frozenset({3, 5}) conf: 2.0\n\n一旦降低可信度阈值，就可以获得更多的规则。\n实例：发现国会投票中的模式\n\n收集数据：使用votesmart模块访问投票纪录\n准备数据：构造一个函数来将投票转化为一串交易记录\n分析数据：查看准备的数据以确保其正确性\n训练算法：使用aprioi()和generateRules()函数来发现投票纪录中的有趣信息\n测试算法：不适用，即没有测试过程\n使用算法\n\n实例：发现毒蘑菇的相似特征\nmushDatSet = [line.split() for line in open(&#039;MLiA_SourceCode/Ch11/mushroom.dat&#039;).readlines()]\n第一个特征表示有毒或者可使用，有毒为2，可食用为1，下个特征是蘑菇伞的形状，有六种可能的值。为了找到毒蘑菇中存在的公共特征，可以运用Apriori算法来寻找特征值为2的频繁项集。\n使用 Apriori 工具包\nfrom efficient_apriori import apriori\nL, suppData = apriori(mushDatSet, min_support=0.3, min_confidence=1)\nfor item in L[2]:\n    if &#039;2&#039; in item:\n        print(item)\n(&#039;2&#039;, &#039;23&#039;)\n(&#039;2&#039;, &#039;34&#039;)\n(&#039;2&#039;, &#039;36&#039;)\n(&#039;2&#039;, &#039;39&#039;)\n(&#039;2&#039;, &#039;59&#039;)\n(&#039;2&#039;, &#039;63&#039;)\n(&#039;2&#039;, &#039;67&#039;)\n(&#039;2&#039;, &#039;76&#039;)\n(&#039;2&#039;, &#039;85&#039;)\n(&#039;2&#039;, &#039;86&#039;)\n(&#039;2&#039;, &#039;90&#039;)\n(&#039;2&#039;, &#039;93&#039;)\n(&#039;2&#039;, &#039;28&#039;)\n(&#039;2&#039;, &#039;53&#039;)\n\n总结\n关联分析是用于发现大数据集中元素间有趣关系的一个工具集，可以采用两种方式来量化这些关系。第一种方式是频繁项集，它会给出经常在一起出现的元素项。第二种方式是关联规则。\nApriori原理是说如果一个元素项是不频繁的，那么这些包含该元素的超集也是不频繁的。Apriori算法从单元素项集开始，通过组合满足最小支持度要求的项集来形成更大的集合。支持度用来独立一个集合在原始数据中出现的频率。\n每次增加频繁项集的大小，Apriori算法都会重新扫描整个数据集。当数据集很大时，这会显著降低频繁项集发现的速度。"},"机器学习实战/12.-机器学习实战":{"slug":"机器学习实战/12.-机器学习实战","filePath":"机器学习实战/12. 机器学习实战.md","title":"12. 机器学习实战","links":[],"tags":["FP-growth"],"content":"使用FP-growth算法来高效发现频繁项集\nFP-growth算法只会对数据库进行两次扫描，而Apriori算法对于每个潜在的频繁项集都会扫描数据集判断给定模式是否频繁，因此FP-growth算法的速度要比Apriori算法快。\nFP树：用于编码数据集的有效方式\nFP-growth算法\n优点：一般要快于Apriori\n缺点：实现比较困难，在某些数据集上性能会下降\n适用数据类型：标称型数据类型\n\nFP-growth算法将数据存储在一种称为FP树的紧凑数据结构中。FP代表频繁模式（Frequent Pattern）。一颗FP树看上去与计算机科学中的其他树结构类似，但是它通过链接（link）来连接相似元素，被连起来的元素项可以看成一个链表。\n\n同搜索树不同的是，一个元素可以在一棵FP树中出现多次。FP树会存储项集的出现频率，而每个项集会以路径的方式存储在树中。存在相似元素的集合会共享树的一部分。只有当集合之间完全不同时，树才会分叉。树节点上给出集合中的单个元素以及在序列中的出现次数，路径会给出该序列的出现次数。\n相似项之间的链接即节点链接（node link），用于快速发现相似的位置。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n事务ID事务中的元素项目001r, z, h, j, p002z, y, x, w, v, u, t, s003z004r, x, n, o, s005y, r, x, z, q, t, p006y, z, x, e, q, s, t, m\n在上表中，元素z出现了5次，集合{r,z}出现了1次（001和005都出现了，但书中写1次）。于是可以得出结论：z一定是自己本身或者其他符号一起出现了3次。我们再看下z的其他可能性。集合{t,s,y,x,z}出现了2次，集合{t,r,y,x,z}出现了1次。元素项z的右边标的是5，表示z出现了5次，其中刚才已经给出了4次出现，所以它一定单独出现过1次。\n吐槽：这书这两章写的云山雾绕\nFP-growth的一般流程\n\n收集数据\n准备数据：由于存储的是集合，所以需要离散数据。如果要处理连续数据，需要将它们量化为离散值。\n分析数据\n训练算法：构建一个FP树，并对树进行挖掘\n测试算法：没有测试过程\n使用算法：可用于识别经常出现的元素项，从而用于制定决策，推荐元素或者进行预测等应用中。\n\n构建FP树\nFP树的类定义\nclass treeNode:\n    def __init__(self, nameValue, numOccur, parentNode):\n        self.name = nameValue\n        self.count = numOccur\n        self.nodeLink = None\n        self.parent = parentNode\n        self.children = {}\n    \n    def inc(self, numOccur):\n        self.count += numOccur\n    \n    def disp(self, ind=1):\n        print(&#039; &#039;*ind, self.name, &#039; &#039;, self.count)\n        for child in self.children.values():\n            child.disp(ind+1)\n上面的程序给出了FP树中结点的类定义。类中包含用于存放节点名字的变量和1个计数值，nodeLinke变量用于链接相似的元素项。\nrootNode = treeNode(&#039;pyramid&#039;, 9, None)\nrootNode.children[&#039;eye&#039;] = treeNode(&#039;eye&#039;, 13, None)\nrootNode.disp()\n  pyramid   9\n   eye   13\n\nrootNode.children[&#039;phoenix&#039;] = treeNode(&#039;phoenix&#039;, 3, None)\nrootNode.disp()\n  pyramid   9\n   eye   13\n   phoenix   3\n\n构建FP树\n需要一个头指针表来指向给定类型的第一个实列。利用头指针表，可以快速访问FP树中一个给定类型的所有元素。\n\n使用一个字典来保存头指针表。头指针表还可以用来保存FP树中每类元素的总数。\n对不满足最小支持度的数据进行去除，然后后重排序得到下表\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n事务ID事务中的元素项目过滤及重排序后的事务001r, z, h, j, pz,r002z, y, x, w, v, u, t, sz, x, y, s, t003zz004r, x, n, o, sx, s, r005y, r, x, z, q, t, pz, x, y, r, t006y, z, x, e, q, s, t, mz, x, y, s, t\nFP树构建函数\ndef createTree(dataSet, minSup=1):\n    headerTable = {}\n    # 查看数据集两次\n    for trans in dataSet: # 第一遍计数出现的频率\n        for item in trans:\n            headerTable[item] = headerTable.get(item, 0) + dataSet[trans]\n \n    for k in list(headerTable.keys()):  # 删除不满足最小支持度的元素项\n        if headerTable[k] &lt; minSup: \n            del(headerTable[k])\n            \n    freqItemSet = set(headerTable.keys())\n \n    if len(freqItemSet) == 0:\n        return None, None  # 没有满足最小支持度的项目则返回\n    \n    for k in headerTable:\n        headerTable[k] = [headerTable[k], None]\n \n    retTree = treeNode(&#039;Null Set&#039;, 1, None) # create tree\n \n    for tranSet, count in dataSet.items():  # 第二次遍历数据集\n        localD = {}\n        for item in tranSet:   # 更具全局频率对每个事务中的元素进行排序\n            if item in freqItemSet:\n                localD[item] = headerTable[item][0]\n        if len(localD) &gt; 0:\n            orderedItems = [v[0] for v in sorted(localD.items(), key=lambda p: p[1], reverse=True)]\n            updateTree(orderedItems, retTree, headerTable, count) # 对剩下的元素迭代调用upadteTree函数\n    return retTree, headerTable\n \ndef updateTree(items, inTree, headerTable, count):\n    if items[0] in inTree.children:\n        inTree.children[items[0]].inc(count)\n    else:\n        inTree.children[items[0]] = treeNode(items[0], count, inTree)\n        if headerTable[items[0]][1] == None:\n            headerTable[items[0]][1] = inTree.children[items[0]]\n        else:\n            updateHeader(headerTable[items[0]][1], inTree.children[items[0]])\n    if len(items) &gt; 1:\n        updateTree(items[1::], inTree.children[items[0]], headerTable, count)\n \ndef updateHeader(nodeToTest, targetNode):\n    while (nodeToTest.nodeLink != None):\n        nodeToTest = nodeToTest.nodeLink\n    nodeToTest.nodeLink = targetNode\ndef loadSimpDat():\n    simpDat = [[&#039;r&#039;, &#039;z&#039;, &#039;h&#039;, &#039;j&#039;, &#039;p&#039;],\n               [&#039;z&#039;, &#039;y&#039;, &#039;x&#039;, &#039;w&#039;, &#039;v&#039;, &#039;u&#039;, &#039;t&#039;, &#039;s&#039;],\n               [&#039;z&#039;],\n               [&#039;r&#039;, &#039;x&#039;, &#039;n&#039;, &#039;o&#039;, &#039;s&#039;],\n               [&#039;y&#039;, &#039;r&#039;, &#039;x&#039;, &#039;z&#039;, &#039;q&#039;, &#039;t&#039;, &#039;p&#039;],\n               [&#039;y&#039;, &#039;z&#039;, &#039;x&#039;, &#039;e&#039;, &#039;q&#039;, &#039;s&#039;, &#039;t&#039;, &#039;m&#039;]]\n    return simpDat\n \ndef createInitSet(dataSet):\n    retDict = {}\n    for trans in dataSet:\n        retDict[frozenset(trans)] = 1\n    return retDict\nsimpDat = loadSimpDat()\nsimpDat\n[[&#039;r&#039;, &#039;z&#039;, &#039;h&#039;, &#039;j&#039;, &#039;p&#039;],\n [&#039;z&#039;, &#039;y&#039;, &#039;x&#039;, &#039;w&#039;, &#039;v&#039;, &#039;u&#039;, &#039;t&#039;, &#039;s&#039;],\n [&#039;z&#039;],\n [&#039;r&#039;, &#039;x&#039;, &#039;n&#039;, &#039;o&#039;, &#039;s&#039;],\n [&#039;y&#039;, &#039;r&#039;, &#039;x&#039;, &#039;z&#039;, &#039;q&#039;, &#039;t&#039;, &#039;p&#039;],\n [&#039;y&#039;, &#039;z&#039;, &#039;x&#039;, &#039;e&#039;, &#039;q&#039;, &#039;s&#039;, &#039;t&#039;, &#039;m&#039;]]\n\ninitSet = createInitSet(simpDat)\ninitSet\n{frozenset({&#039;h&#039;, &#039;j&#039;, &#039;p&#039;, &#039;r&#039;, &#039;z&#039;}): 1,\n frozenset({&#039;s&#039;, &#039;t&#039;, &#039;u&#039;, &#039;v&#039;, &#039;w&#039;, &#039;x&#039;, &#039;y&#039;, &#039;z&#039;}): 1,\n frozenset({&#039;z&#039;}): 1,\n frozenset({&#039;n&#039;, &#039;o&#039;, &#039;r&#039;, &#039;s&#039;, &#039;x&#039;}): 1,\n frozenset({&#039;p&#039;, &#039;q&#039;, &#039;r&#039;, &#039;t&#039;, &#039;x&#039;, &#039;y&#039;, &#039;z&#039;}): 1,\n frozenset({&#039;e&#039;, &#039;m&#039;, &#039;q&#039;, &#039;s&#039;, &#039;t&#039;, &#039;x&#039;, &#039;y&#039;, &#039;z&#039;}): 1}\n\nmyFPtree, myHeaderTab = createTree(initSet, 3)\nmyFPtree.disp()\n  Null Set   1\n   z   5\n    r   1\n    x   3\n     s   2\n      y   2\n       t   2\n     r   1\n      y   1\n       t   1\n   x   1\n    r   1\n     s   1\n\n上面给出的元素项及其对应的频率计数值，其中每个缩进表示所处的树的深度。\n从一棵FP树中挖掘频繁项集\n从FP树中抽取频繁项集的三个基本步骤如下：\n\n从FP树中获得条件模式基\n利用条件模式基，构建一个条件FP树\n迭代重复1，2步骤，直到树包含一个元素为止\n\n抽取调剂模式基\n发现以给定元素项结尾的所有路径的函数\ndef ascendTree(leafNode, prefixPath):\n    if leafNode.parent != None:\n        prefixPath.append(leafNode.name)\n        ascendTree(leafNode.parent, prefixPath)\n \ndef findPrefixPath(basePat, treeNode):\n    condPats = {}\n    while treeNode != None:\n        prefixPath = []\n        ascendTree(treeNode, prefixPath)\n        if len(prefixPath) &gt; 1:\n            condPats[frozenset(prefixPath[1:])] = treeNode.count\n        treeNode = treeNode.nodeLink\n    return condPats\n上述代码用于给定元素项生成一个条件模式基，这通过访问树中所有包含给定元素项的节点来完成。\nfindPrefixPath(&#039;x&#039;, myHeaderTab[&#039;x&#039;][1])\n{frozenset({&#039;z&#039;}): 3}\n\nfindPrefixPath(&#039;z&#039;, myHeaderTab[&#039;z&#039;][1])\n{}\n\nfindPrefixPath(&#039;r&#039;, myHeaderTab[&#039;r&#039;][1])\n{frozenset({&#039;z&#039;}): 1, frozenset({&#039;x&#039;}): 1, frozenset({&#039;x&#039;, &#039;z&#039;}): 1}\n\n创建条件FP树\n递归查找频繁项集的mineTree函数\ndef mineTree(inTree, headerTable, minSup, preFix, freqItemList):\n    bigL = [v[0] for v in sorted(headerTable.items(), key=lambda p: p[1][0])]\n    for basePat in bigL: \n        newFreqSet = preFix.copy()\n        newFreqSet.add(basePat)\n        freqItemList.append(newFreqSet)\n        condPattBases = findPrefixPath(basePat, headerTable[basePat][1])\n        myCondTree, myHead = createTree(condPattBases, minSup)\n        if myHead != None:\n            print(&#039;conditional tree for: &#039;,newFreqSet)\n            myCondTree.disp(1)\n            mineTree(myCondTree, myHead, minSup, newFreqSet, freqItemList)\nfreqItems = []\nmineTree(myFPtree, myHeaderTab, 3, set([]), freqItems)\nconditional tree for:  {&#039;s&#039;}\n  Null Set   1\n   x   3\nconditional tree for:  {&#039;y&#039;}\n  Null Set   1\n   x   3\n    z   3\nconditional tree for:  {&#039;y&#039;, &#039;z&#039;}\n  Null Set   1\n   x   3\nconditional tree for:  {&#039;t&#039;}\n  Null Set   1\n   y   3\n    x   3\n     z   3\nconditional tree for:  {&#039;t&#039;, &#039;x&#039;}\n  Null Set   1\n   y   3\nconditional tree for:  {&#039;t&#039;, &#039;z&#039;}\n  Null Set   1\n   y   3\n    x   3\nconditional tree for:  {&#039;t&#039;, &#039;x&#039;, &#039;z&#039;}\n  Null Set   1\n   y   3\nconditional tree for:  {&#039;x&#039;}\n  Null Set   1\n   z   3\n\nfreqItems\n[{&#039;r&#039;},\n {&#039;s&#039;},\n {&#039;s&#039;, &#039;x&#039;},\n {&#039;y&#039;},\n {&#039;x&#039;, &#039;y&#039;},\n {&#039;y&#039;, &#039;z&#039;},\n {&#039;x&#039;, &#039;y&#039;, &#039;z&#039;},\n {&#039;t&#039;},\n {&#039;t&#039;, &#039;y&#039;},\n {&#039;t&#039;, &#039;x&#039;},\n {&#039;t&#039;, &#039;x&#039;, &#039;y&#039;},\n {&#039;t&#039;, &#039;z&#039;},\n {&#039;t&#039;, &#039;y&#039;, &#039;z&#039;},\n {&#039;t&#039;, &#039;x&#039;, &#039;z&#039;},\n {&#039;t&#039;, &#039;x&#039;, &#039;y&#039;, &#039;z&#039;},\n {&#039;x&#039;},\n {&#039;x&#039;, &#039;z&#039;},\n {&#039;z&#039;}]\n\n实例：从新闻网站点击流中挖掘\n有近100万条用户浏览数据。\nparsedDat = [line.split() for line in open(&#039;MLiA_SourceCode/Ch12/kosarak.dat&#039;).readlines()]\ninitSet = createInitSet(parsedDat)\nmyFPtree, myHeaderTab = createTree(initSet, 100000)\nmyFreqList = []\nmineTree(myFPtree, myHeaderTab, 100000, set([]), myFreqList)\nconditional tree for:  {&#039;1&#039;}\n  Null Set   1\n   6   107404\nconditional tree for:  {&#039;3&#039;}\n  Null Set   1\n   6   186289\n    11   117401\n   11   9718\nconditional tree for:  {&#039;3&#039;, &#039;11&#039;}\n  Null Set   1\n   6   117401\nconditional tree for:  {&#039;11&#039;}\n  Null Set   1\n   6   261773\n\nmyFreqList\n[{&#039;1&#039;},\n {&#039;1&#039;, &#039;6&#039;},\n {&#039;3&#039;},\n {&#039;11&#039;, &#039;3&#039;},\n {&#039;11&#039;, &#039;3&#039;, &#039;6&#039;},\n {&#039;3&#039;, &#039;6&#039;},\n {&#039;11&#039;},\n {&#039;11&#039;, &#039;6&#039;},\n {&#039;6&#039;}]\n\n测试库函数\n%pip install pyfpgrowth\nCollecting pyfpgrowth\n  Downloading files.pythonhosted.org/packages/d2/4c/8b7cd90b4118ff0286d6584909b99e1ca5642bdc9072fa5a8dd361c864a0/pyfpgrowth-1.0.tar.gz (1.6MB)\nInstalling collected packages: pyfpgrowth\n  Running setup.py install for pyfpgrowth: started\n    Running setup.py install for pyfpgrowth: finished with status &#039;done&#039;\nSuccessfully installed pyfpgrowth-1.0\n\nimport pyfpgrowth\npatterns = pyfpgrowth.find_frequent_patterns(initSet, 100000)\nrules = pyfpgrowth.generate_association_rules(patterns, 0.7)\npatterns\n{(&#039;1&#039;,): 140597,\n (&#039;1&#039;, &#039;6&#039;): 107404,\n (&#039;11&#039;, &#039;3&#039;): 127119,\n (&#039;11&#039;, &#039;3&#039;, &#039;6&#039;): 117401,\n (&#039;3&#039;, &#039;6&#039;): 186289,\n (&#039;11&#039;,): 282963,\n (&#039;11&#039;, &#039;6&#039;): 261773,\n (&#039;6&#039;,): 412762}\n\nrules\n{(&#039;1&#039;,): ((&#039;6&#039;,), 0.7639138815195203),\n (&#039;11&#039;, &#039;3&#039;): ((&#039;6&#039;,), 0.9235519473878806),\n (&#039;11&#039;,): ((&#039;6&#039;,), 0.9251138841473974)}\n\n总结\nFP-growth算法是一种用于发现数据集中频繁模式的有效方法。FP-growth算法利用Apriori原则，执行更快。Apriori算法产生候选项集，然后扫描数据集来检查它们是否频繁发。由于只对数据集扫描两次，因此FP-growth算法执行更快。在FP-growth算法中，数据集存储在一个称为FP树的结构中。FP树构建完成后，可以通过查找元素项的条件基及构建条件FP树来发现频繁项集。该过程不断以更多元素作为条件重复进行，知道FP树只包含一个元素为止。"},"机器学习实战/13.-机器学习实战":{"slug":"机器学习实战/13.-机器学习实战","filePath":"机器学习实战/13. 机器学习实战.md","title":"13. 机器学习实战","links":[],"tags":["PCA","降维","主成分分析"],"content":"利用PCA来简化数据\n降维（dimensionality reduction），数据在低纬度时更容易处理。\n降维技术\n数据进行简化的原因：\n使得数据集更容易使用\n降低很多算法的计算开销\n去除噪声\n使得结果易懂\n\n降维方法，主成分分析（Principal Component Analysis，PCA），数据从原来的坐标系转换到新的坐标系，新的坐标系的选择由数据本身决定的。第一个新坐标轴选择的是原始数据中方差最大的方向，第二个新坐标轴的选择和第一个坐标轴正交且具有最大方差的方向。该过程一直重复，重复次数为原始数据中特征的数目。\n另一种降维技术是，因子分析（Factor Analysis），在因子分析中，我们假设在观察数据的生成中有一些观察不到的隐变量（latent variable）。假设观察数据是这些隐变量和某些噪声的线性组合。那么隐变量的数据可能比观察数据的数目少，也就是说通过找到隐变量就可以实现数据的降维。\n还有一种降维技术就是独立成分分析（Independent Component Analysis，ICA），ICA假设数据是从N个数据源生成的，这一点和因子分析有些类似。假设数据为多个数据源的混合观察结果，这些数据源之间在统计上是相互独立的，而在PCA中假设数据是不相关的。同因子分析一样，如果数据源的数目少于观察数据的数目，则可以实现降维过程。\nPCA\n主成分分析\n优点：降低数据的复杂性，识别最重要的多个特征\n缺点：不一定需要，且可能损失有用的信息\n适用数据类型：数值型数据\n\n在Numpy中实现PCA\n伪代码大致如下：\n去除平均值\n计算协方差矩阵\n计算协方差矩阵的特征值和特征向量\n将特征值从大到小排序\n保留最上面的N个特征向量\n将数据转换到上述N个特征向量构建的空间中\n\nfrom numpy import *\n \ndef loadDataSet(fileName, delim=&#039;\\t&#039;):\n    fr = open(fileName)\n    stringArr = [line.strip().split(delim) for line in fr.readlines()]\n    datArr = [list(map(float, line)) for line in stringArr]\n    return mat(datArr)\n \ndef pca(dataMat, topNfeat=9999999):\n    meanVals = mean(dataMat, axis=0)\n    meanRemoved = dataMat - meanVals # 去平均值\n    covMat = cov(meanRemoved, rowvar=0)\n    eigVals, eigVects = linalg.eig(mat(covMat))\n    eigValInd = argsort(eigVals)            # 从小到大排序\n    eigValInd = eigValInd[: -(topNfeat+1): -1]  # 去掉多余的\n    redEigVects = eigVects[:, eigValInd]\n    lowDDataMat = meanRemoved * redEigVects # 将数据转换为新的维度\n    reconMat = (lowDDataMat * redEigVects.T) + meanVals\n    return lowDDataMat, reconMat\ndataMat = loadDataSet(&#039;MLiA_SourceCode/Ch13/testSet.txt&#039;)\nlowDMat, reconMat = pca(dataMat, 1)\nshape(lowDMat)\n(1000, 1)\n\n将原始数据和降维后的数据绘制出来\nimport matplotlib\nimport matplotlib.pyplot as plt\n \nfig = plt.figure()\nax = fig.add_subplot(111)\nax.scatter(dataMat[:, 0].flatten().A[0], dataMat[:, 1].flatten().A[0], marker=&#039;^&#039;, s=90)\nax.scatter(reconMat[:, 0].flatten().A[0], reconMat[:, 1].flatten().A[0], marker=&#039;o&#039;, s=90)\nplt.show()\n\n实例：利用PCA对半导体制造数据降维\n数据拥有590个特征，包含许多的缺失值，这些缺失值是以NaN标识的。用平均值来代替缺失值。\ndef replaceNanWithMean(): \n    datMat = loadDataSet(&#039;MLiA_SourceCode/Ch13/secom.data&#039;, &#039; &#039;)\n    numFeat = shape(datMat)[1]\n    for i in range(numFeat):\n        meanVal = mean(datMat[nonzero(~isnan(datMat[:,i].A))[0],i]) # 计算所有非Nan的平均值\n        datMat[nonzero(isnan(datMat[:,i].A))[0],i] = meanVal  # 将所有的Nan设置为平均值\n    return datMat\ndataMat = replaceNanWithMean()\n观察pca()的工作过程\n首先是去除均值\nmeanVals = mean(dataMat, axis=0)\nmeanRemoved = dataMat - meanVals\n然后计算协方差矩阵：\ncovMat = cov(meanRemoved, rowvar=0)\n对该矩阵进行特征值分析\neigVals, eigVects = linalg.eig(mat(covMat))\n观察特征值结果\neigVals\narray([ 5.34151979e+07+0.00000000e+00j,  2.17466719e+07+0.00000000e+00j,\n        8.24837662e+06+0.00000000e+00j,  2.07388086e+06+0.00000000e+00j,\n        ...\n        ...\n        0.00000000e+00+0.00000000e+00j,  0.00000000e+00+0.00000000e+00j,\n        0.00000000e+00+0.00000000e+00j,  0.00000000e+00+0.00000000e+00j,\n        0.00000000e+00+0.00000000e+00j,  0.00000000e+00+0.00000000e+00j,\n        0.00000000e+00+0.00000000e+00j,  0.00000000e+00+0.00000000e+00j,\n        0.00000000e+00+0.00000000e+00j,  0.00000000e+00+0.00000000e+00j])\n\n我们可以看到一堆的值，发现其中有超过20%的特征值都是0。这就意味着这些特征都是其他特征的副本，也就是说，他们可以通过其他特征来表示，而本身并没有提供额外的信息。\n总结\n降维技术使得数据变得更易使用，并且它们往往能够去除数据中的噪声，使得其他机器学习任务更加精确。降维往往作为i预处理的步骤，在数据应用到其他算法之前清洗数据。很多技术可以用于数据降维，在这些技术中，独立成分分析，因子分析和主成分分析比较流行，其中又以主成分分析应用最广泛。\nPCA可以从数据中识别其主要特征，他是通过沿着数据最大方差方向旋转坐标轴来实现的。选择方差最大的方向作为第一条坐标轴，后续坐标轴则与前面的坐标轴正交。协方差矩阵上的特征值分析可以用一系列的正交坐标轴来获取。"},"机器学习实战/14.-机器学习实战":{"slug":"机器学习实战/14.-机器学习实战","filePath":"机器学习实战/14. 机器学习实战.md","title":"14. 机器学习实战","links":[],"tags":["SVD"],"content":"利用SVD简化数据\n奇异值分解（singular value decomposition，SVD）\nSVD的应用\n奇异值分解\n优点：简化数据，去除噪声，提高算法的结果\n缺点：数据的转换可能难以理解\n适用数据类型：数值型数据\n\n利用SVD实现，我们能够用小得多的数据集来表示原始数据集。这样做，实际上是去除噪声和冗余信息。\n隐性语义索引\nSVD的应用之一就是信息检索。我们称利用SVD的方法为隐性语义索引（Latent Semantic Indexing，LSI）。\n在LSI中，一个矩阵是由文档和词语组成的。当我们在该矩阵上应用SVD时，就会构建出多个奇异值。这些奇异值代表了文档中的概念或主题，这一特点可以用于更高效的文档搜索。在词语拼写错误时，只基于词语存在与否的简单搜索方法会遇到问题。简单搜索的另一个问题就是同义词的使用。\n推荐系统\nSVD的另一个应用就是推荐系统。简单版本的推荐系统能够计算项或人之间的相似度。更先进的方法则先利用SVD从数据中构建一个主题空间，然后再再该空间下计算相似度。\n矩阵分解\nSVD是矩阵分解的一种类型，矩阵分解是将数据矩阵分解为多个独立部分的过程。\nSVD将原始的数据集矩阵Data分解成三个矩阵U,\\Sigma,V^T。如果原始矩阵Data是m行n列，那么U,\\Sigma,V^T分别是m行m列，m行n列，n行n列。上述过程可以写成下面结果：\nData_{m×n}=U_{m×m}\\Sigma_{m×n}V^T_{n×n}\n上述分解中会构建一个矩阵\\Sigma，该矩阵只有对角元素，其他元素均为0。另一个惯例就是\\Sigma的对角元素是从大到小排列的。这些对角元素称为奇异值（Singular Value），它们对应了原始数据矩阵Data的奇异值。奇异值和特征值是有关系的，这里的奇异值就是矩阵Data*Data^T特征值的平方根。\n利用Python实现SVD\n在numpy中提供了svd的实现\nfrom numpy import *\n \nU, Sigma, VT = linalg.svd([[1, 1], [7, 7]])\nU\narray([[-0.14142136, -0.98994949],\n       [-0.98994949,  0.14142136]])\n\nSigma\narray([10.,  0.])\n\nVT\narray([[-0.70710678, -0.70710678],\n       [-0.70710678,  0.70710678]])\n\n注意到，矩阵Sigma以行向量array([10., 0.])返回，而非如下矩阵\narray([[10,  0],\n       [ 0,  0]])\n\n这是由于矩阵处理对角元素其他均为0，因此这种仅返回对角元素的方式能节省空间，我们需要知道Sigma是一个矩阵。\n构建以下矩阵\ndef loadExData():\n    return[[0, 0, 0, 2, 2],\n           [0, 0, 0, 3, 3],\n           [0, 0, 0, 1, 1],\n           [1, 1, 1, 0, 0],\n           [2, 2, 2, 0, 0],\n           [5, 5, 5, 0, 0],\n           [1, 1, 1, 0, 0]]\nData = loadExData()\nU, Sigma, VT = linalg.svd(Data)\nSigma\narray([9.64365076e+00, 5.29150262e+00, 7.40623935e-16, 4.05103551e-16,\n       2.21838243e-32])\n\n前三个数值比其他值大了很多，于是我们可以将最后两个数值去掉了。\n接下来我们的原始数据集就可以用如下结果来近似\nData_{m×n}≈U_{m×3}\\Sigma_{3×3}V^T_{3×n}\n\nSVD示意图。矩阵Data被分解。浅灰色区域是原始数据，深灰色区域是矩阵近似计算仅需的数据\n我们试图重构原始矩阵，首先构建一个3×3的矩阵Sig3\nSig3 = mat([[Sigma[0], 0, 0], \n            [0, Sigma[1], 0], \n            [0, 0, Sigma[2]]])\n接下来重构原始矩阵的近似矩阵。由于Sig3仅为3×3的矩阵，因而我们只需要使用矩阵U的前3列和V^T的前3行。\ndata = U[:, :3]*Sig3*VT[:3, :]\ndata.astype(&#039;int&#039;)\nmatrix([[0, 0, 0, 2, 2],\n        [0, 0, 0, 3, 3],\n        [0, 0, 0, 1, 1],\n        [1, 1, 1, 0, 0],\n        [2, 2, 2, 0, 0],\n        [5, 5, 5, 0, 0],\n        [1, 1, 1, 0, 0]])\n\nData\n[[0, 0, 0, 2, 2],\n [0, 0, 0, 3, 3],\n [0, 0, 0, 1, 1],\n [1, 1, 1, 0, 0],\n [2, 2, 2, 0, 0],\n [5, 5, 5, 0, 0],\n [1, 1, 1, 0, 0]]\n\n通过SVD我们可以用一个小很多的矩阵来表示一个大矩阵。\n基于协同过滤的推荐引擎\n相似度计算\n欧氏距离\n皮尔逊相关系数（pearson correlation）\n余弦相似度（cosine similarity）\n\n相似度计算\nfrom numpy import *\nfrom numpy import linalg as la\n \ndef eulidSim(inA, inB):\n    return 1.0/(1.0+la.norm(inA - inB))\n \ndef pearsSim(inA, inB):\n    if len(inA) &lt; 3:\n        return 1.0\n    return 0.5+0.5*corrcoef(inA, inB, rowvar=0)[0][1]\n \ndef cosSim(inA, inB):\n    num = float(inA.T * inB)\n    denom = la.norm(inA) * la.norm(inB)\n    return 0.5+0.5*(num/denom)\n测试一下这三个函数\nmyMat = mat(loadExData())\n欧氏距离：\necludSim(myMat[:, 0], myMat[:, 4])\n0.12973190755680383\n\necludSim(myMat[:, 0], myMat[:, 0])\n1.0\n\n余弦相似度\ncosSim(myMat[:, 0], myMat[:, 4])\n0.5\n\ncosSim(myMat[:, 0], myMat[:, 0])\n1.0\n\n皮尔逊相关系数\npearsSim(myMat[:, 0], myMat[:, 4])\n0.20596538173840329\n\npearsSim(myMat[:, 0], myMat[:, 0])\n1.0\n\n上面的相似度计算都是假设数据采用了列向量方式进行表示。如果利用上述函数来计算两个向量的相似度就会遇到问题（我们很容易对上述函数进行修改以计算行向量之间的相似度）。这里采用列向量的表示方法，暗示着我们将利用基于物品的相似度计算方法。\n基于物品的相似度还是基于用户的相似度\n我们计算了两个餐馆菜肴之间的距离，这称为基于物品（item-based）的相似度。另一种计算用户距离的方法称为基于用户（user-based）。基于物品相似度计算的时间会随着物品数量的增加而增加，基于用户的相似度计算时间则会随用户数量的增加而增加。\n推荐引擎的评价\n通常用于推荐引擎的评价指标称为最小均方根误差（Root Mean Squared Error，RMSE）的指标，它首先计算均方误差的平均值再取其平方根。\n实例：餐馆菜肴推荐引擎\n推荐未尝过的菜肴\n\n寻找用户没有评级的菜肴，即在用户物品矩阵中的0值\n在用户没有评级的所有物品中，对每个物品预计一个可能的评级分数。（我们认为用户可能对物品的打分）\n对这些物品的评分从高到低排序，返回前N个物品\n\ndef standEst(dataMat, user, simMeas, item):\n    n = shape(dataMat)[1]\n    simTotal = 0.0\n    ratSimTotal = 0.0\n    for j in range(n):\n        userRating = dataMat[user, j]\n        if userRating == 0:\n            continue\n        overLap = nonzero(logical_and(dataMat[:,item].A&gt;0, dataMat[:,j].A&gt;0))[0]\n        if len(overLap) == 0:\n            similarity = 0\n        else:\n            similarity = simMeas(dataMat[overLap,item], dataMat[overLap,j])\n        print(&#039;the %d and %d similarity is: %f&#039; % (item, j, similarity))\n        simTotal += similarity\n        ratSimTotal += similarity * userRating\n    if simTotal == 0:\n        return 0\n    else:\n        return ratSimTotal/simTotal\n \ndef recommend(dataMat, user, N=3, simMeas=cosSim, estMethod=standEst):\n    unratedItems = nonzero(dataMat[user, :].A==0)[1]\n    if len(unratedItems) == 0:\n        return &#039;you rated everything&#039;\n    itemScores = []\n    for item in unratedItems:\n        estimatedScore = estMethod(dataMat, user, simMeas, item)\n        itemScores.append((item, estimatedScore))\n    return sorted(itemScores, key=lambda jj: jj[1], reverse=True)[:N]\n第一个函数standEst()用来计算给定相似度计算方法的条件下，用户对物品的评估分值。第二个函数recommend()也就是推荐引擎，他会调用standEst()函数。\nmyMat = mat(loadExData())\nmyMat[0, 1] = myMat[0, 0] = myMat[1, 0] = myMat[2, 0] = 4\nmyMat[3, 3] = 2\nmyMat\nmatrix([[4, 4, 0, 2, 2],\n        [4, 0, 0, 3, 3],\n        [4, 0, 0, 1, 1],\n        [1, 1, 1, 2, 0],\n        [2, 2, 2, 0, 0],\n        [5, 5, 5, 0, 0],\n        [1, 1, 1, 0, 0]])\n\nrecommend(myMat, 2)\nthe 1 and 0 similarity is: 1.000000\nthe 1 and 3 similarity is: 0.928746\nthe 1 and 4 similarity is: 1.000000\nthe 2 and 0 similarity is: 1.000000\nthe 2 and 3 similarity is: 1.000000\nthe 2 and 4 similarity is: 0.000000\n\n[(2, 2.5), (1, 2.0243290220056256)]\n\nrecommend(myMat, 2, simMeas=ecludSim)\nthe 1 and 0 similarity is: 1.000000\nthe 1 and 3 similarity is: 0.309017\nthe 1 and 4 similarity is: 0.333333\nthe 2 and 0 similarity is: 1.000000\nthe 2 and 3 similarity is: 0.500000\nthe 2 and 4 similarity is: 0.000000\n\n[(2, 3.0), (1, 2.8266504712098603)]\n\nrecommend(myMat, 2, simMeas=pearsSim)\nthe 1 and 0 similarity is: 1.000000\nthe 1 and 3 similarity is: 1.000000\nthe 1 and 4 similarity is: 1.000000\nthe 2 and 0 similarity is: 1.000000\nthe 2 and 3 similarity is: 1.000000\nthe 2 and 4 similarity is: 0.000000\n\n[(2, 2.5), (1, 2.0)]\n\n这个例子给出了如何利用基于物品相似度和多个相似度计算方法来进行推荐的过程。\n利用SVD提高推荐效果\n下面是一个更大的矩阵\ndef loadExData2():\n    return[[0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 5],\n           [0, 0, 0, 3, 0, 4, 0, 0, 0, 0, 3],\n           [0, 0, 0, 0, 4, 0, 0, 1, 0, 4, 0],\n           [3, 3, 4, 0, 0, 0, 0, 2, 2, 0, 0],\n           [5, 4, 5, 0, 0, 0, 0, 5, 5, 0, 0],\n           [0, 0, 0, 0, 5, 0, 1, 0, 0, 5, 0],\n           [4, 3, 4, 0, 0, 0, 0, 5, 5, 0, 1],\n           [0, 0, 0, 4, 0, 4, 0, 0, 0, 0, 4],\n           [0, 0, 0, 2, 0, 2, 5, 0, 0, 1, 2],\n           [0, 0, 0, 0, 5, 0, 0, 0, 0, 4, 0],\n           [1, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0]]\nU, Sigma, VT = la.svd(mat(loadExData2()))\nSigma\narray([15.77075346, 11.40670395, 11.03044558,  4.84639758,  3.09292055,\n        2.58097379,  1.00413543,  0.72817072,  0.43800353,  0.22082113,\n        0.07367823])\n\n首先对Sigma中的值求平方\nSig2 = Sigma**2\nsum(Sig2)\n541.9999999999995\n\n在计算总能量的90%\nsum(Sig2)*0.9\n487.7999999999996\n\n然后计算前两个元素包含的能量\nsum(Sig2[:2])\n378.8295595113579\n\n该值低于总能量的90%，于是计算前三个元素所包含的能量\nsum(Sig2[:3])\n500.50028912757926\n\n该值高于90%，这就可以了。于是我们将一个11维的矩阵转换为一个3维的矩阵，下面对转换后的三维空间构造一个相似度计算函数。我们利用SVD将所有的菜肴映射到一个低维空间中去。\ndef svdEst(dataMat, user, simMeas, item):\n    n = shape(dataMat)[1]\n    simTotal = 0.0\n    ratSimTotal = 0.0\n    U,Sigma,VT = la.svd(dataMat)\n    Sig4 = mat(eye(4)*Sigma[:4])\n    xformedItems = dataMat.T * U[:,:4] * Sig4.I\n    for j in range(n):\n        userRating = dataMat[user, j]\n        if userRating == 0 or j==item:\n            continue\n        similarity = simMeas(xformedItems[item,:].T, xformedItems[j,:].T)\n        print(&#039;the %d and %d similarity is: %f&#039; % (item, j, similarity))\n        simTotal += similarity\n        ratSimTotal += similarity * userRating\n    if simTotal == 0:\n        return 0\n    else:\n        return ratSimTotal/simTotal\nmyMat = mat(loadExData2())\nrecommend(myMat, 1, estMethod=svdEst)\nthe 0 and 3 similarity is: 0.490950\nthe 0 and 5 similarity is: 0.484274\nthe 0 and 10 similarity is: 0.512755\nthe 1 and 3 similarity is: 0.491294\nthe 1 and 5 similarity is: 0.481516\nthe 1 and 10 similarity is: 0.509709\nthe 2 and 3 similarity is: 0.491573\nthe 2 and 5 similarity is: 0.482346\nthe 2 and 10 similarity is: 0.510584\nthe 4 and 3 similarity is: 0.450495\nthe 4 and 5 similarity is: 0.506795\nthe 4 and 10 similarity is: 0.512896\nthe 6 and 3 similarity is: 0.743699\nthe 6 and 5 similarity is: 0.468366\nthe 6 and 10 similarity is: 0.439465\nthe 7 and 3 similarity is: 0.482175\nthe 7 and 5 similarity is: 0.494716\nthe 7 and 10 similarity is: 0.524970\nthe 8 and 3 similarity is: 0.491307\nthe 8 and 5 similarity is: 0.491228\nthe 8 and 10 similarity is: 0.520290\nthe 9 and 3 similarity is: 0.522379\nthe 9 and 5 similarity is: 0.496130\nthe 9 and 10 similarity is: 0.493617\n\n[(4, 3.344714938469228), (7, 3.329402072452697), (9, 3.328100876390069)]\n\n尝试另一种方法\nrecommend(myMat, 1, simMeas=pearsSim, estMethod=svdEst)\nthe 0 and 3 similarity is: 0.341942\nthe 0 and 5 similarity is: 0.124132\nthe 0 and 10 similarity is: 0.116698\nthe 1 and 3 similarity is: 0.345560\nthe 1 and 5 similarity is: 0.126456\nthe 1 and 10 similarity is: 0.118892\nthe 2 and 3 similarity is: 0.345149\nthe 2 and 5 similarity is: 0.126190\nthe 2 and 10 similarity is: 0.118640\nthe 4 and 3 similarity is: 0.450126\nthe 4 and 5 similarity is: 0.528504\nthe 4 and 10 similarity is: 0.544647\nthe 6 and 3 similarity is: 0.923822\nthe 6 and 5 similarity is: 0.724840\nthe 6 and 10 similarity is: 0.710896\nthe 7 and 3 similarity is: 0.319482\nthe 7 and 5 similarity is: 0.118324\nthe 7 and 10 similarity is: 0.113370\nthe 8 and 3 similarity is: 0.334910\nthe 8 and 5 similarity is: 0.119673\nthe 8 and 10 similarity is: 0.112497\nthe 9 and 3 similarity is: 0.566918\nthe 9 and 5 similarity is: 0.590049\nthe 9 and 10 similarity is: 0.602380\n\n[(4, 3.346952186702173), (9, 3.3353796573274694), (6, 3.3071930278130366)]\n\n实例：基于SVD的图像压缩\n我们可以使用SVD来对数据降维，从而实现图像的压缩。\ndef printMat(inMat, thresh=0.8):\n    for i in range(32):\n        for k in range(32):\n            if float(inMat[i, k] &gt; thresh):\n                print(1, end=&#039;&#039;)\n            else:\n                print(0, end=&#039;&#039;)\n        print(&#039;&#039;)\n \ndef imgCompress(numSV=3, thresh=0.8):\n    myl = []\n    for line in open(&#039;MLiA_SourceCode/Ch14/0_5.txt&#039;).readlines():\n        newRow = []\n        for i in range(32):\n            newRow.append(int(line[i]))\n        myl.append(newRow)\n    myMat = mat(myl)\n    print(&#039;***originam matrix*****&#039;)\n    printMat(myMat, thresh)\n    U, Sigma, VT = la.svd(myMat)\n    SigRecon = mat(zeros((numSV, numSV)))\n    for k in range(numSV):\n        SigRecon[k, k] = Sigma[k]\n    reconMat = U[:, :numSV] * SigRecon*VT[:numSV, :]\n    print(&quot;****reconstructed matrix using %d singular values******&quot; % numSV)\n    printMat(reconMat, thresh)\nimgCompress(2)\n***originam matrix*****\n00000000000000110000000000000000\n00000000000011111100000000000000\n00000000000111111110000000000000\n00000000001111111111000000000000\n00000000111111111111100000000000\n00000001111111111111110000000000\n00000000111111111111111000000000\n00000000111111100001111100000000\n00000001111111000001111100000000\n00000011111100000000111100000000\n00000011111100000000111110000000\n00000011111100000000011110000000\n00000011111100000000011110000000\n00000001111110000000001111000000\n00000011111110000000001111000000\n00000011111100000000001111000000\n00000001111100000000001111000000\n00000011111100000000001111000000\n00000001111100000000001111000000\n00000001111100000000011111000000\n00000000111110000000001111100000\n00000000111110000000001111100000\n00000000111110000000001111100000\n00000000111110000000011111000000\n00000000111110000000111111000000\n00000000111111000001111110000000\n00000000011111111111111110000000\n00000000001111111111111110000000\n00000000001111111111111110000000\n00000000000111111111111000000000\n00000000000011111111110000000000\n00000000000000111111000000000000\n****reconstructed matrix using 2 singular values******\n00000000000000000000000000000000\n00000000000000000000000000000000\n00000000000001111100000000000000\n00000000000011111111000000000000\n00000000000111111111100000000000\n00000000001111111111110000000000\n00000000001111111111110000000000\n00000000011110000000001000000000\n00000000111100000000001100000000\n00000000111100000000001110000000\n00000000111100000000001110000000\n00000000111100000000001110000000\n00000000111100000000001110000000\n00000000111100000000001110000000\n00000000111100000000001110000000\n00000000111100000000001110000000\n00000000111100000000001110000000\n00000000111100000000001110000000\n00000000111100000000001110000000\n00000000111100000000001110000000\n00000000111100000000001110000000\n00000000111100000000001110000000\n00000000111100000000001110000000\n00000000111100000000001110000000\n00000000111100000000001110000000\n00000000111100000000001100000000\n00000000001111111111111000000000\n00000000001111111111110000000000\n00000000001111111111110000000000\n00000000000011111111100000000000\n00000000000011111111000000000000\n00000000000000000000000000000000\n\n可以看到，只需要两个奇异值就能相当精确的对图像实现重构。\n总结\nSVD是一种强大的降维工具，我们可以利用SVD来逼近矩阵并从中提取重要特征。通过保留矩阵80%~90%的能量，就可以得到重要的特征并去掉噪声。SVD已经运用到了多个应用中，其中一个成功的案例就是推荐引擎。\n推荐引擎将物品推荐给用户，协调过滤则是一种基于用户喜好或行为数据的推荐实现方法。协调过滤的核心是相似度计算方法，很多相似度计算方法都可以用计算物品或用户之间的相似度。通过降低空间下计算相似度，SVD提高了推荐引擎的效果。\n在大规模数据集上，SVD的计算和推荐可能是一个很困难的工程问题。通过离线方式来进行分解和相似度计算，是一种减少冗余计算和推荐所需时间的办法。"},"机器学习实战/2.-机器学习实战":{"slug":"机器学习实战/2.-机器学习实战","filePath":"机器学习实战/2. 机器学习实战.md","title":"2. 机器学习实战","links":[],"tags":["k-邻近算法","KNN"],"content":"k-邻近算法概述\nk-邻近算法采用测量不同特征之间的距离方法进行分类。\n优点：精度高，对异常值不敏感，无数据输入假定\n缺点：计算复杂度高，空间复杂度高\n适用数据范围：数值型和标称型\n\n准备使用Python导入数据\n首先写一个简单的程序来理解python是如何解析和加载数据的\nfrom numpy import *\nimport operator\n \ndef createDataSet():\n    group = array([[1.0, 1.1], [1.0, 1.0], [0, 0], [0, 0.1]])\n    labels = [&#039;A&#039;, &#039;A&#039;, &#039;B&#039;, &#039;B&#039;]\n    return group, labels\n \ngroup, labels = createDataSet()\ngroup\narray([[1. , 1.1],\n       [1. , 1. ],\n       [0. , 0. ],\n       [0. , 0.1]])\n\nlabels\n[&#039;A&#039;, &#039;A&#039;, &#039;B&#039;, &#039;B&#039;]\n\n实施KNN分类算法\n对未知类别属性的数据集中的每个点依次执行以下操作：\n\n计算已知类别数据集中的点与当前点之间的距离\n按照距离递增次序排序\n选取与当前距离最小的k个点\n确定前k个点所在类别的出现频率\n返回前k个点出现频率最高的类别当作当前点的预测分类\n\n计算距离的方法使用欧氏距离计算公式:\nd=\\sqrt{(x_1-x_2)^2+(y_1-y_2)^2}\n具体程序如下：\ndef classify0(inX, dataSet, labels, k):\n    dataSetSize = dataSet.shape[0]\n    diffMat = tile(inX, (dataSetSize, 1)) - dataSet\n    sqDiffMat = diffMat**2\n    sqDistances = sqDiffMat.sum(axis=1)\n    distances = sqDistances ** 0.5\n    sortedDistIndicies = distances.argsort()\n    classCount = {}\n    for i in range(k):\n        voteIlabe1 = labels[sortedDistIndicies[i]]\n        classCount[voteIlabe1] = classCount.get(voteIlabe1, 0) + 1\n    sortedClassCount = sorted(classCount.items(), key=operator.itemgetter(1), reverse=True)\n    return sortedClassCount[0][0]\nclassify0 有四个参数\ninX：需要分类的向量\ndataSet：训练集的特征\nlabels：训练集的标签\nk：最近邻居的数目\n\n测试下我们写的算法是否正确，我们传入（0，0）的点，正确的分类点应该是B点\nclassify0([0,0], group, labels, 3)\n&#039;B&#039;\n\n使用k-邻近算法改进约会网站的配对结果\n约会网站通过三个特征把样本分为3类\n\n\n\n\n\n\n\n\n\n每年飞行里程玩游戏消耗的时间百分比每周消耗的冰淇淋公升数样本分类\n准备数据：从文本文件中解析数据\n共有1000行数据存放在datingTestSet2.txt中，行之间用\\n分割，列之间\\t分割，现在需要把这个文件里的数据转换为一个向量\ndef file2matrix(filename):\n    fr = open(filename, &#039;r&#039;)\n    arrayOLines = fr.readlines()\n    numberOfLines = len(arrayOLines)\n    returnMat = zeros((numberOfLines, 3))\n    classLabelVector = []\n    index = 0\n    for line in arrayOLines:\n        line = line.strip()\n        listFromLine = line.split(&#039;\\t&#039;)\n        returnMat[index, :] = listFromLine[0: 3]\n        classLabelVector.append(int(listFromLine[-1]))\n        index += 1\n    fr.close()\n    return returnMat, classLabelVector\ndatingDataMat, datingLabels = file2matrix(&#039;./MLiA_SourceCode/machinelearninginaction/Ch02/datingTestSet2.txt&#039;)\ndatingDataMat\narray([[4.0920000e+04, 8.3269760e+00, 9.5395200e-01],\n       [1.4488000e+04, 7.1534690e+00, 1.6739040e+00],\n       [2.6052000e+04, 1.4418710e+00, 8.0512400e-01],\n       ...,\n       [2.6575000e+04, 1.0650102e+01, 8.6662700e-01],\n       [4.8111000e+04, 9.1345280e+00, 7.2804500e-01],\n       [4.3757000e+04, 7.8826010e+00, 1.3324460e+00]])\n\ndatingLabels[:20]\n[3, 2, 1, 1, 1, 1, 3, 3, 1, 3, 1, 1, 2, 1, 1, 1, 1, 1, 2, 3]\n\n分析数据：使用matplotlib绘制散点图\n数据使用第二列和第三列的值分别是\nx轴：玩游戏消耗的时间百分比\ny轴：每周消费的冰淇淋公升数\n\nimport matplotlib\nimport matplotlib.pyplot as plt\n \nfig = plt.figure()\nax = fig.add_subplot(111)\nax.scatter(datingDataMat[:, 1], datingDataMat[:, 2])\nplt.show()\n\n从上图很难看到有用的信息，我们通过颜色标记不同的样本\nfig = plt.figure()\nax = fig.add_subplot(111)\nax.scatter(datingDataMat[:, 1], datingDataMat[:, 2], 15.0*array(datingLabels), 15.0*array(datingLabels))\nplt.show()\n\n利用颜色和尺寸我们基本可以看出三种样本的区域轮廓。\n准备数据：归一化数值\n我们发现飞行里程的数值远远大于另两个数值，这样我们计算的结果会被飞行里程严重影响，我们认为三个特征是同等重要的，所以作为三个等权重的特征，飞行里程不应该严重影响结果。\n在处理这种不同取值范围的特征时，我们需要先归一化数值，将数值范围处理到0到1或-1到1之间，通常使用的公式是\nnewValue = (oldValue-min) / max-min)\n程序如下：\ndef autoNorm(dataSet):\n    minVals = dataSet.min(0)\n    maxVals = dataSet.max(0)\n    ranges = maxVals - minVals\n    normDataSet = zeros(shape(dataSet))\n    m = dataSet.shape[0]\n    normDataSet = dataSet - tile(minVals, (m, 1))\n    normDataSet = normDataSet/tile(ranges, (m, 1))\n    return normDataSet, ranges, minVals\nnormMat, ranges, minVals = autoNorm(datingDataMat)\nnormMat\narray([[0.44832535, 0.39805139, 0.56233353],\n       [0.15873259, 0.34195467, 0.98724416],\n       [0.28542943, 0.06892523, 0.47449629],\n       ...,\n       [0.29115949, 0.50910294, 0.51079493],\n       [0.52711097, 0.43665451, 0.4290048 ],\n       [0.47940793, 0.3768091 , 0.78571804]])\n\nranges\narray([9.1273000e+04, 2.0919349e+01, 1.6943610e+00])\n\nminVals\narray([0.      , 0.      , 0.001156])\n\n测试算法：构建完整程序验证分类器\n选择90%的数据作为训练集，10%作为测试集，记录每一次错误的分类，最后用预测错误的总数除以测试集数据的总数就计算出了错误率。\ndef datingClassTest():\n    hoRatio = 0.10\n    datingDataMat, datingLabels = file2matrix(&#039;./MLiA_SourceCode/machinelearninginaction/Ch02/datingTestSet2.txt&#039;)\n    normMat, ranges, minVals = autoNorm(datingDataMat)\n    m = normMat.shape[0]\n    numTestVecs = int(m*hoRatio)\n    errorCount = 0.0\n    for i in range(numTestVecs):\n        classifierResult = classify0(normMat[i, :], normMat[numTestVecs:m, :],\\\n                                    datingLabels[numTestVecs:m], 3)\n        #print(&quot;the classifier came back with: %d, the real answer is: %d&quot;% (classifierResult, datingLabels[i]))\n        if classifierResult != datingLabels[i]:\n            errorCount += 1.0\n    print(&quot;the total error rate is: %f&quot;%(errorCount/float(numTestVecs)))\ndatingClassTest()\nthe total error rate is: 0.050000\n\n这里计算出的错误率为5%，书中写的为2.4%，担心算错用source code跑了一遍错误率为6.6%\n使用算法：构建完整的可用系统\n手写识别系统\n现在开始第二个示例，识别手写数字，数据已经被图形软件处理为文本格式，先打开几个观察一下\ndef readTrainingDigits(filename):\n    fr = open(filename, &#039;r&#039;)\n    data = fr.read()\n    print(data)\n    fr.close()\nreadTrainingDigits(&#039;./MLiA_SourceCode/machinelearninginaction/Ch02/digits/trainingDigits/0_0.txt&#039;)\nreadTrainingDigits(&#039;./MLiA_SourceCode/machinelearninginaction/Ch02/digits/trainingDigits/1_0.txt&#039;)\nreadTrainingDigits(&#039;./MLiA_SourceCode/machinelearninginaction/Ch02/digits/trainingDigits/2_0.txt&#039;)\n00000000000001111000000000000000\n00000000000011111110000000000000\n00000000001111111111000000000000\n00000001111111111111100000000000\n00000001111111011111100000000000\n00000011111110000011110000000000\n00000011111110000000111000000000\n00000011111110000000111100000000\n00000011111110000000011100000000\n00000011111110000000011100000000\n00000011111100000000011110000000\n00000011111100000000001110000000\n00000011111100000000001110000000\n00000001111110000000000111000000\n00000001111110000000000111000000\n00000001111110000000000111000000\n00000001111110000000000111000000\n00000011111110000000001111000000\n00000011110110000000001111000000\n00000011110000000000011110000000\n00000001111000000000001111000000\n00000001111000000000011111000000\n00000001111000000000111110000000\n00000001111000000001111100000000\n00000000111000000111111000000000\n00000000111100011111110000000000\n00000000111111111111110000000000\n00000000011111111111110000000000\n00000000011111111111100000000000\n00000000001111111110000000000000\n00000000000111110000000000000000\n00000000000011000000000000000000\n\n00000000000000001111000000000000\n00000000000000011111111000000000\n00000000000000011111111100000000\n00000000000000011111111110000000\n00000000000000011111111110000000\n00000000000000111111111100000000\n00000000000000111111111100000000\n00000000000001111111111100000000\n00000000000000111111111100000000\n00000000000000111111111100000000\n00000000000000111111111000000000\n00000000000001111111111000000000\n00000000000011111111111000000000\n00000000000111111111110000000000\n00000000001111111111111000000000\n00000001111111111111111000000000\n00000011111111111111110000000000\n00000111111111111111110000000000\n00000111111111111111110000000000\n00000001111111111111110000000000\n00000001111111011111110000000000\n00000000111100011111110000000000\n00000000000000011111110000000000\n00000000000000011111100000000000\n00000000000000111111110000000000\n00000000000000011111110000000000\n00000000000000011111110000000000\n00000000000000011111111000000000\n00000000000000011111111000000000\n00000000000000011111111000000000\n00000000000000000111111110000000\n00000000000000000111111100000000\n\n00000000001111111000000000000000\n00000000011111111100000000000000\n00000000011111111110000000000000\n00000000011111111111100000000000\n00000000111111111111100000000000\n00000001111111111111110000000000\n00000011111110001111110000000000\n00000001111110000111111000000000\n00000001111110000111111000000000\n00000001111110000111111000000000\n00000001111100000111111000000000\n00000001111110000011111100000000\n00000001111111000011111100000000\n00000000111111000011111000000000\n00000000111110000111111000000000\n00000000001110000011111100000000\n00000000000000000011111000000000\n00000000000000000111110000000000\n00000000000000000111111000000000\n00000000000000001111110000000000\n00000000000000011111110000000000\n00000000000000111111100000000000\n00000000000000011111110000000000\n00000000000000111111100000000000\n00000000000001111111000000000000\n00000000000011111110000000000000\n00000000001111111111111111111000\n00000000011111111111111111111100\n00000000111111111111111111111100\n00000000011111111111111111111100\n00000000001111111111111111111100\n00000000000111111111111111110000\n\n这是一个32×32的数字矩阵并且可以很明显的看出数字的轮廓。\n准备数据：将图像转换为测试向量\n如上所示，每一个文件都是一个32×32构成的数字矩阵，为了使用前面写的分类器，我们必须将图像格式化处理为一个1×1024的向量\ndef img2vector(filename):\n    returnVect = zeros((1,1024))\n    fr = open(filename)\n    for i in range(32):\n        lineStr = fr.readline()\n        for j in range(32):\n            returnVect[0,32*i+j] = int(lineStr[j])\n    return returnVect\ntestVector = img2vector(&#039;./MLiA_SourceCode/machinelearninginaction/Ch02/digits/testDigits/0_13.txt&#039;)\ntestVector[0, 0:31]\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.,\n       1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n\ntestVector[0, 32:63]\narray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1.,\n       1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n\n测试算法：使用k-近邻算法识别手写数字\nimport os\nfrom tqdm import trange\n \ndef handwritingClassTest():\n    hwLabels = []\n    trainingFileList = os.listdir(&#039;./MLiA_SourceCode/machinelearninginaction/Ch02/digits/trainingDigits&#039;)           #load the training set\n    m = len(trainingFileList)\n    trainingMat = zeros((m,1024))\n    for i in range(m):\n        fileNameStr = trainingFileList[i]\n        fileStr = fileNameStr.split(&#039;.&#039;)[0]     #take off .txt\n        classNumStr = int(fileStr.split(&#039;_&#039;)[0])\n        hwLabels.append(classNumStr)\n        trainingMat[i,:] = img2vector(&#039;./MLiA_SourceCode/machinelearninginaction/Ch02/digits/trainingDigits/%s&#039; % fileNameStr)\n    testFileList = os.listdir(&#039;./MLiA_SourceCode/machinelearninginaction/Ch02/digits/testDigits&#039;)        #iterate through the test set\n    errorCount = 0.0\n    mTest = len(testFileList)\n    for i in trange(mTest):\n        fileNameStr = testFileList[i]\n        fileStr = fileNameStr.split(&#039;.&#039;)[0]     #take off .txt\n        classNumStr = int(fileStr.split(&#039;_&#039;)[0])\n        vectorUnderTest = img2vector(&#039;./MLiA_SourceCode/machinelearninginaction/Ch02/digits/testDigits/%s&#039; % fileNameStr)\n        classifierResult = classify0(vectorUnderTest, trainingMat, hwLabels, 3)\n        #print (&quot;the classifier came back with: %d, the real answer is: %d&quot; % (classifierResult, classNumStr))\n        if (classifierResult != classNumStr): errorCount += 1.0\n    print (&quot;\\nthe total number of errors is: %d&quot; % errorCount)\n    print (&quot;\\nthe total error rate is: %f&quot; % (errorCount/float(mTest)))\nhandwritingClassTest()\n100%|██████████| 946/946 [00:30&lt;00:00, 31.30it/s]\n\n\nthe total number of errors is: 10\n\nthe total error rate is: 0.010571\n\n错误率只有1%，\n总结\nk-近邻算法是分类数据最简单有效的算法，使用算法时我们必须有接近实际数据的训练样本，必须保存全部数据集，如果训练数据集很大，必须使用大量的存储空间，此外，由于必须对数据集中的每个数据计算距离值，实际使用会非常耗时,它的另一个缺陷是无法给出任何数据的基本结构信息，因此我们无法知晓平均实例样本和典型实例样本具体有什么特征。"},"机器学习实战/3.-机器学习实战":{"slug":"机器学习实战/3.-机器学习实战","filePath":"机器学习实战/3. 机器学习实战.md","title":"3. 机器学习实战","links":[],"tags":["决策树","ID3"],"content":"决策树的简介\n你是否玩过二十个问题的游戏，就是你在脑海中想某个事物，向你提问二十个问题推测出你想的东西。这个游戏的原理和决策树类似，下面是一个判断垃圾邮件的决策树。\n\n决策树的构造\n决策树\n优点：计算复杂度不高，输出的结果易于理解，对中间值的缺失不敏感，可以处理不相关特征的数据\n缺点：可能会产生过度匹配的问题\n适用数据类型：数值型和标称型\n\n在构造决策树时，首先要确定哪些特征在划分数据分类时起到决定性的作用，为了划分出最好的结果，我们必须评估每个特征，创建分支的伪代码createBranch()函数如下\nif so returen 类标签\nelse\n    寻找划分数据集的最好特征\n    划分数据集\n    创建分支节点\n        for 每个划分的子集\n            调用函数createBranch并增加返回结果到分支节点中\n    return 分支节点\n\n上面的伪代码createBranch()是一个递归函数，在倒数第二行直接调用自己。\n决策树的一般流程：\n\n收集数据\n准备数据\n分析数据\n训练算法\n测试算法\n使用算法\n\n信息增益\n划分数据集的大原则是：将无序的数据变得更加有序，如何能知道数据是向有序的方向划分呢？方法有很多，这里的方法为香浓熵（其它方法还有基尼系数）。\n熵的定义为信息的期望值，如果待分类的事物可能划分在多个分类之中，则符号x_i的信息定义为\nl(x_i)=-\\log_2p(x_i)\n其中p(x_i)是选择分类的概率\n为了计算熵，我们需要计算所有类别所有可能值包含的信息期望值，通过下面公式得到：\nH = -\\sum_{i=1}^n p(x_i)\\log_2p(x_i)\n其中n是分类的数目，下面用python计算信息熵\nfrom math import log\n \ndef calcShannonEnt(dataSet):\n    numEntries = len(dataSet)\n    labelCounts = {}\n    # 为所有可能分的类创建字典，如果类别已经记录，则记录当前类别出现的次数\n    for featVec in dataSet:\n        currentLabel = featVec[-1]\n        if currentLabel not in labelCounts.keys():\n            labelCounts[currentLabel] = 0\n        labelCounts[currentLabel] += 1\n    shannonEnt = 0.0\n    # 计算每个类别的出现的概率，然后套入公式求出熵\n    for key in labelCounts:\n        prob = float(labelCounts[key]) / numEntries\n        shannonEnt -= prob * log(prob, 2)\n    return shannonEnt\n下面创建了一个数据集测试一下\ndef createDataSet():\n    dataSet = [[1, 1, &#039;yes&#039;],\n               [1, 1, &#039;yes&#039;],\n               [1, 0, &#039;no&#039;],\n               [0, 1, &#039;no&#039;],\n               [0, 1, &#039;no&#039;],\n              ]\n    labels = [&#039;no surfacing&#039;, &#039;flippers&#039;]\n    return dataSet, labels\nmyDat, labels = createDataSet()\nmyDat\n[[1, 1, &#039;yes&#039;], [1, 1, &#039;yes&#039;], [1, 0, &#039;no&#039;], [0, 1, &#039;no&#039;], [0, 1, &#039;no&#039;]]\n\ncalcShannonEnt(myDat)\n0.9709505944546686\n\n熵越高，则混合的数据也越多，我们可以在数据集中添加更多的分类，观察熵是如何变化的，添加一个maybe的类别\nmyDat[0][-1]=&#039;maybe&#039;\nmyDat\n[[1, 1, &#039;maybe&#039;], [1, 1, &#039;yes&#039;], [1, 0, &#039;no&#039;], [0, 1, &#039;no&#039;], [0, 1, &#039;no&#039;]]\n\ncalcShannonEnt(myDat)\n1.3709505944546687\n\n划分数据集\n得到熵后我们就可以按照获取最大信息增益的方法划分数据集\ndef splitDataSet(dataSet, axis, value):\n    retDataSet = []\n    for featVec in dataSet:\n        if featVec[axis] == value:\n            reducedFeatVec = featVec[:axis]\n            reducedFeatVec.extend(featVec[axis+1:])\n            retDataSet.append(reducedFeatVec)\n    return retDataSet\nsplitDatSet()有三个参数：待划分的数据集，划分数据集的特征列，需要返回的特征值\n# 注意append和extend的不同\na = [1, 2, 3]\nb = [4, 5, 6]\na.append(b)\na\n[1, 2, 3, [4, 5, 6]]\n\na = [1, 2, 3]\na.extend(b)\na\n[1, 2, 3, 4, 5, 6]\n\n用前面简单的数据集测试一下\nmyDat, labels = createDataSet()\nmyDat\n[[1, 1, &#039;yes&#039;], [1, 1, &#039;yes&#039;], [1, 0, &#039;no&#039;], [0, 1, &#039;no&#039;], [0, 1, &#039;no&#039;]]\n\nsplitDataSet(myDat, 1, 1)\n[[1, &#039;yes&#039;], [1, &#039;yes&#039;], [0, &#039;no&#039;], [0, &#039;no&#039;]]\n\nsplitDataSet(myDat, 0, 0)\n[[1, &#039;no&#039;], [1, &#039;no&#039;]]\n\n接下来遍历整个数据集，循环计算香农熵splitDataSet()函数，找到最好的特征划分方式。\ndef chooseBestFeatureToSplit(dataSet):\n    numFeatures = len(dataSet[0]) - 1\n    baseEntropy = calcShannonEnt(dataSet)\n    bestInfoGain = 0.0\n    bestFeature = -1\n    for i in range(numFeatures):\n        featList = [example[i] for example in dataSet]\n        uniqueVals = set(featList)\n        newEntropy = 0.0\n        for value in uniqueVals:\n            subDataSet = splitDataSet(dataSet, i, value)\n            prob = len(subDataSet)/float(len(dataSet))\n            newEntropy += prob * calcShannonEnt(subDataSet)\n        infoGain = baseEntropy - newEntropy\n        if (infoGain &gt; bestInfoGain):\n            bestInfoGain = infoGain\n            bestFeature = i\n    return bestFeature\nchooseBestFeatureToSplit(myDat)\n0\n\nmyDat\n[[1, 1, &#039;yes&#039;], [1, 1, &#039;yes&#039;], [1, 0, &#039;no&#039;], [0, 1, &#039;no&#039;], [0, 1, &#039;no&#039;]]\n\n代码运行告诉我们第0个特征划分最好\n递归构建决策树\n目前我们已经构建好所有决策树算法所需的子功能模块，其工作原理如下：\n得到原始数据集，然后基于最好的属性划分数据集，由于特征值可能多于两个，因此可能存在大于两个分支的数据集划分。第一次划分后，数据将被向下传递到树分支的下一节点，在这个节点上，我们可以再次划分数据，因此我们可以采用递归的原则处理数据集。\n递归结束的条件是：程序遍历完所有的划分数据集的属性，或者每个分支下的所有实例都具有相同的分类。如果所有实列具有相同的分类，则得到一个叶子节点或者终止块。\ndef majorityCnt(classList):\n    classCount={}\n    for vote in classList:\n        if vote not in classCount.keys():\n            classCount[vote] = 0\n            classCount[vote] += 1\n        sortedClassCount = sorted(classCount,iteritems(), key=operator.itemgetter(1), reverse=True)\n    return sortedClassCount[0][0]\n上面代码的作用市，当遍历完所有的特征时，我们用投票表决的方法，返回出现次数最多的类别\ndef createTree(dataSet, labels):\n    classList = [example[-1] for example in dataSet]\n    if classList.count(classList[0]) == len(classList):\n        return classList[0]\n    if len(dataSet[0]) == 1:\n        return majorityCnt(classList)\n    bestFeat = chooseBestFeatureToSplit(dataSet)\n    bestFeatLabel = labels[bestFeat]\n    myTree = {bestFeatLabel:{}}\n    del(labels[bestFeat])\n    featValues = [example[bestFeat] for example in dataSet]\n    uniqueVals = set(featValues)\n    for value in uniqueVals:\n        subLabels = labels[:]\n        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value), subLabels)\n    return myTree\nmyDat, labels = createDataSet()\nmyTree = createTree(myDat, labels)\nmyTree\n{&#039;no surfacing&#039;: {0: &#039;no&#039;, 1: {&#039;flippers&#039;: {0: &#039;no&#039;, 1: &#039;yes&#039;}}}}\n\n绘制树形图\n为了更清晰的看出我们创建的树，可以用matplotlib绘图\n使用文本注解绘制树节点\nimport matplotlib.pyplot as plt\n \n# 定义文本框和箭头格式\ndecisionNode = dict(boxstyle=&quot;sawtooth&quot;, fc=&quot;0.8&quot;)\nleafNode = dict(boxstyle=&quot;round4&quot;, fc=&quot;0.8&quot;)\narrow_args = dict(arrowstyle=&quot;&lt;-&quot;)\n \n# 绘制带箭头的注解\ndef plotNode(nodeTxt, centerPt, parentPt, nodeType):\n    createPlot.ax1.annotate(nodeTxt, xy=parentPt, xycoords=&#039;axes fraction&#039;,\n                            xytext=centerPt, textcoords=&#039;axes fraction&#039;,\n                            va=&#039;center&#039;, ha=&#039;center&#039;, bbox=nodeType, arrowprops=arrow_args)\n \ndef createPlot():\n    fig = plt.figure(1, facecolor=&#039;white&#039;)\n    fig.clf()\n    createPlot.ax1 = plt.subplot(111, frameon=False)\n    plotNode(&#039;决策节点&#039;, (0.5, 0.1), (0.1, 0.5), decisionNode)\n    plotNode(&#039;叶节点&#039;, (0.8, 0.1), (0.3, 0.8), leafNode)\n    plt.show()\ncreatePlot()\n\n构造注解树\n绘制一颗完整的树需要一些技巧，我们必须有x,y坐标，知道多少个叶节点，确定x轴的长度，树的深度，确定y轴的高度，这里编写两个函数getNumLeafs()和getTreeDepth()，来获取叶节点数和树的深度。\ndef getNumLeafs(myTree):\n    numLeafs = 0\n    firstStr = list(myTree.keys())[0]\n    secondDict = myTree[firstStr]\n    for key in list(secondDict.keys()):\n        if type(secondDict[key]).__name__==&#039;dict&#039;:\n            numLeafs += getNumLeafs(secondDict[key])\n        else:\n            numLeafs += 1\n    return numLeafs\n \ndef getTreeDepth(myTree):\n    maxDepth = 0\n    firstStr = list(myTree.keys())[0]\n    secondDict = myTree[firstStr]\n    for key in list(secondDict.keys()):\n        if type(secondDict[key]).__name__==&#039;dict&#039;:\n            thisDepth = 1 + getTreeDepth(secondDict[key])\n        else:\n            thisDepth = 1\n        if thisDepth &gt; maxDepth: maxDepth = thisDepth\n    return maxDepth\ndef retrieveTree(i):\n    listOfTrees =[{&#039;no surfacing&#039;: {0: &#039;no&#039;, 1: {&#039;flippers&#039;: {0: &#039;no&#039;, 1: &#039;yes&#039;}}}},\n                  {&#039;no surfacing&#039;: {0: &#039;no&#039;, 1: {&#039;flippers&#039;: {0: {&#039;head&#039;: {0: &#039;no&#039;, 1: &#039;yes&#039;}}, 1: &#039;no&#039;}}}}\n                  ]\n    return listOfTrees[i]\nretrieveTree(1)\n{&#039;no surfacing&#039;: {0: &#039;no&#039;,\n  1: {&#039;flippers&#039;: {0: {&#039;head&#039;: {0: &#039;no&#039;, 1: &#039;yes&#039;}}, 1: &#039;no&#039;}}}}\n\nmyTree = retrieveTree(0)\ngetNumLeafs(myTree)\n3\n\ngetTreeDepth(myTree)\n2\n\n接下来我们画出这颗树\ndef plotMidText(cntrPt, parentPt, txtString):\n    xMid = (parentPt[0]-cntrPt[0])/2.0 + cntrPt[0]\n    yMid = (parentPt[1]-cntrPt[1])/2.0 + cntrPt[1]\n    createPlot.ax1.text(xMid, yMid, txtString)\n \ndef plotTree(myTree, parentPt, nodeTxt):\n    numLeafs = getNumLeafs(myTree)\n    depth = getTreeDepth(myTree)\n    firstStr = list(myTree.keys())[0]\n    cntrPt = (plotTree.xOff + (1.0 + float(numLeafs))/2.0/plotTree.totalW, plotTree.yOff)\n    plotMidText(cntrPt, parentPt, nodeTxt)\n    plotNode(firstStr, cntrPt, parentPt, decisionNode)\n    secondDict = myTree[firstStr]\n    plotTree.yOff = plotTree.yOff - 1.0/plotTree.totalD\n    for key in list(secondDict.keys()):\n        if type(secondDict[key]).__name__ == &#039;dict&#039;:\n            plotTree(secondDict[key], cntrPt, str(key))\n        else:\n            plotTree.xOff = plotTree.xOff +  1.0/plotTree.totalW\n            plotNode(secondDict[key], (plotTree.xOff, plotTree.yOff), cntrPt, leafNode)\n            plotMidText((plotTree.xOff, plotTree.yOff), cntrPt, str(key))\n    plotTree.yOff = plotTree.yOff + 1.0/plotTree.totalD\n \ndef createPlot(inTree):\n    fig = plt.figure(1, facecolor=&#039;white&#039;)\n    fig.clf()\n    axprops = dict(xticks=[], yticks=[])\n    createPlot.ax1 = plt.subplot(111, frameon=False, **axprops)\n    plotTree.totalW = float(getNumLeafs(inTree))\n    plotTree.totalD = float(getTreeDepth(inTree))\n    plotTree.xOff = -0.5/plotTree.totalW\n    plotTree.yOff = 1.0\n    plotTree(inTree, (0.5, 1.0), &#039;&#039;)\n    plt.show()\nmyTree = retrieveTree(0)\ncreatePlot(myTree)\n\n添加一组数据再绘制\nmyTree[&#039;no surfacing&#039;][3]=&#039;maybe&#039;\ncreatePlot(myTree)\n\n测试和存储分类器\n测试算法：使用决策树执行分类\n依靠训练数据构造了决策树之后，我们可以将它用于实际数据的分类，在执行数据分类时，需要使用决策树以及用于构造决策树的标签向量。然后程序比较测试数据与决策树上的数值，递归执行该过程，直到叶子节点；最后将测试数据定义为叶子节点所属的类型。\ndef classify(inputTree, featLabels, testVec):\n    firstStr = list(inputTree.keys())[0]\n    secondDict = inputTree[firstStr]\n    featIndex = featLabels.index(firstStr)\n    for key in list(secondDict.keys()):\n        if testVec[featIndex] == key:\n            if type(secondDict[key]).__name__ == &#039;dict&#039;:\n                classLables = classify(secondDict[key], featLabels, testVec)\n            else:\n                classLables = secondDict[key]\n    return classLables\nmyDat, labels = createDataSet()\nlabels\n[&#039;no surfacing&#039;, &#039;flippers&#039;]\n\nmyTree = retrieveTree(0)\nmyTree\nclassify(myTree, labels, [1, 0])\n&#039;no&#039;\n\nclassify(myTree, labels, [1, 1])\n&#039;yes&#039;\n\n使用算法：决策树的存储\n决策树的构造是很耗时的任务，但如果使用创建好的决策树则可以很快的解决分类问题，这里需要使用pickle模块把决策树保存到本地。\ndef storeTree(inputTree, filename):\n    import pickle\n    fw = open(filename, &#039;wb&#039;)\n    pickle.dump(inputTree, fw)\n    fw.close()\n \ndef grabTree(filename):\n    import pickle\n    fr = open(filename, &#039;rb&#039;)\n    return pickle.load(fr)\nstoreTree(myTree, &#039;classifierStorage.txt&#039;)\ngrabTree(&#039;classifierStorage.txt&#039;)\n{&#039;no surfacing&#039;: {0: &#039;no&#039;, 1: {&#039;flippers&#039;: {0: &#039;no&#039;, 1: &#039;yes&#039;}}, 3: &#039;maybe&#039;}}\n\n通过上述的代码，我们将分类器储存在本地，则不用每一次分类都重新学习一遍\n实例：使用决策树预测隐形眼镜的类型\n根据隐形眼镜的材质等信息预测患者需要的眼镜类型，流程如下\n\n收集数据：提供的文本文件\n准备数据：解析tab键分割的数据行\n分析数据：快速检查数据正确性，使用createPlot()函数绘制树形图\n训练算法：使用creatTree()函数\n测试算法：编写测试函数验证决策树的正确率\n使用算法：储存树的数据结构，以便下次使用\n\nfr = open(&#039;./MLiA_SourceCode/machinelearninginaction/Ch03/lenses.txt&#039;)\nlenses = [inst.strip().split(&#039;\\t&#039;) for inst in fr.readlines()]\nlensesLabels = [&#039;age&#039;, &#039;prescript&#039;, &#039;astigmatic&#039;, &#039;tearRate&#039;]\nlensesTree = createTree(lenses, lensesLabels)\nlensesTree\n{&#039;tearRate&#039;: {&#039;reduced&#039;: &#039;no lenses&#039;,\n  &#039;normal&#039;: {&#039;astigmatic&#039;: {&#039;yes&#039;: {&#039;prescript&#039;: {&#039;myope&#039;: &#039;hard&#039;,\n      &#039;hyper&#039;: {&#039;age&#039;: {&#039;pre&#039;: &#039;no lenses&#039;,\n        &#039;young&#039;: &#039;hard&#039;,\n        &#039;presbyopic&#039;: &#039;no lenses&#039;}}}},\n    &#039;no&#039;: {&#039;age&#039;: {&#039;pre&#039;: &#039;soft&#039;,\n      &#039;young&#039;: &#039;soft&#039;,\n      &#039;presbyopic&#039;: {&#039;prescript&#039;: {&#039;myope&#039;: &#039;no lenses&#039;,\n        &#039;hyper&#039;: &#039;soft&#039;}}}}}}}}\n\ncreatePlot(lensesTree)\n\n通过观察树我们知道，医生最多只需要问四个问题就能确定患者需要佩戴的眼镜。\n虽然决策树非常好的匹配了实验数据，但匹配的选项太多了，我们将这种问题称为过度匹配（overfitting），为了减少过度匹配问题，我们可以裁剪决策树，去掉一些不必要的叶子节点。\n总结\n本章使用的算法称为ID3，它无法处理数值型数据，如果特征太多，也会面临其它问题。\n决策树分类器就像带有终止块的流程图，终止块表示分类结果。开始处理数据集时，我们首先需要测量集合中数据的不一致性，也就是熵，然后寻找最优方案划分数据集，知道数据集中的所有数据属于同一分类。ID3可以划分标称型数据集。"},"机器学习实战/4.-机器学习实战":{"slug":"机器学习实战/4.-机器学习实战","filePath":"机器学习实战/4. 机器学习实战.md","title":"4. 机器学习实战","links":[],"tags":["朴素贝叶斯"],"content":"前两章的分类器只能给出分类结果，而不能给出概率，这一章将学习一个最简单的概率分类器，朴素贝叶斯分类器。之所以称为朴素，是因为整个形式化过程只做最原始，最简单的假设。\n基于贝叶斯决策理论的分类方法\n朴素贝叶斯\n优点：在数据较少的情况下仍然有效，可以处理多分类问题\n缺点：对输入数据的准备方式较为敏感\n适用数据类型：标称型数据\n\n朴素贝叶斯是贝叶斯理论的一部分，假设我们有一个数据集，它由两类组成\n\n我们现在用p1(x,y)，表示数据点(x,y)属于类别1（图中圆点表示的类别）的概率，用p2(x,y)表示数据点(x,y)属于类别2（图中用三角形表示的类别）的概率，那么对于一个新数据点(x,y)数据点，可以用下面的规则来判断它的类别：\n如果p1(x,y) &gt; p2(x,y)，那么类别为1\n如果p2(x,y) &gt; p1(x,y)，那么类别为2\n\n也就是说我们会选择高概率对应的类别，这就是贝叶斯决策理论的核心思想，即选择具有最高概率的决策。\n条件概率\np(c|x)=\\frac{p(x|c)p(c)}{p(x)}\n读作c在x发生的条件下发生的概率\n使用条件概率\n根据上面所说我们可以知道\np(c_i|x)=\\frac{p(x|c_i)p(c_i)}{p(x)}\n如果P(c1|x,y) &gt; P(c2|x,y)，那么类别为C1\n如果P(c1|x,y) &lt; P(c2|x,y)，那么类别为C2\n\n使用贝叶斯准则，我们可以通过已知的三个概率值来计算未知的概率值。\n注释：P(c1|x,y)读作：c1在x发生的条件下发生的概率与y的联合概率。联合概率表示两个事件共同发生的概率。A与B的联合概率表示为 P(AB) 或者P(A,B),或者P(A∩B)\n使用朴素贝叶斯进行文档分类\n朴素贝叶斯是适用于文档分类的常用算法，我们可以观察文档中出现的词，并把每个词出现或者不出现作为一个特征，这样得到的特征数目就会跟词汇表中的词目一样多。\n朴素贝叶斯的一般过程\n\n收集数据：可以使用任何方法，本章使用的是RSS源\n准备数据：需要数值型或者布尔型数据\n分析数据：有大量特征时，绘制特征作用不大，此时使用直方图效果更好\n训练算法：计算不同的独立特征的条件概率\n测试算法：计算错误率\n使用算法：一个常见的朴素贝叶斯应用是文档分类。可以在任意分类场景中使用。\n\n假设词汇表有1000个单词，想要得到好的概率分布，就需要足够的样本，假定样本数为N。由统计学知，如果每个特征需要N个样本，那么对于10个特征将需要N^{10}个样本，对于包含1000个特征的词汇表将需要N^{1000}个样本。所需要的样本数会随着特征数目增大而迅速增长。\n如果特征之间相互独立，那么样本数可以从N^{1000}减少到1000×N个。所谓的独立（independence）指的是统计意义上的独立，即一个特征或单词出现的可能性与它和其他单词相邻没有关系。另一个要求是是说每个特征的重要程度是相同的。当然这在现实中是不可能的。\n使用Python进行文本分类\n如何从文本中获取特征，我们要构建一个文本词条（token），它是一些单词的组合，然后将一个文本段表示为一个向量词条，其中值为1表示单词出现在文本中，0表示单词未出现在文本中。\n准备数据：从文本中构建词向量\nfrom numpy import *\ndef loadDataSet():\n    postingList=[[&#039;my&#039;, &#039;dog&#039;, &#039;has&#039;, &#039;flea&#039;, &#039;problems&#039;, &#039;help&#039;, &#039;please&#039;],\n                 [&#039;maybe&#039;, &#039;not&#039;, &#039;take&#039;, &#039;him&#039;, &#039;to&#039;, &#039;dog&#039;, &#039;park&#039;, &#039;stupid&#039;],\n                 [&#039;my&#039;, &#039;dalmation&#039;, &#039;is&#039;, &#039;so&#039;, &#039;cute&#039;, &#039;I&#039;, &#039;love&#039;, &#039;him&#039;],\n                 [&#039;stop&#039;, &#039;posting&#039;, &#039;stupid&#039;, &#039;worthless&#039;, &#039;garbage&#039;],\n                 [&#039;mr&#039;, &#039;licks&#039;, &#039;ate&#039;, &#039;my&#039;, &#039;steak&#039;, &#039;how&#039;, &#039;to&#039;, &#039;stop&#039;, &#039;him&#039;],\n                 [&#039;quit&#039;, &#039;buying&#039;, &#039;worthless&#039;, &#039;dog&#039;, &#039;food&#039;, &#039;stupid&#039;]]\n    classVec = [0,1,0,1,0,1]    #1 is abusive, 0 not\n    return postingList,classVec\n \ndef createVocabList(dataSet):\n    # 创建一个空集\n    vocabSet = set([])\n    for document in dataSet:\n        # 创建两个集合的并集\n        vocabSet = vocabSet | set(document)\n    return list(vocabSet)\n \ndef setOfWords2Vec(vocabList, inputSet):\n    # 创建一个所有元素都为0的向量\n    returnVec = [0]*len(vocabList)\n    for word in inputSet:\n        if word in vocabList:\n            returnVec[vocabList.index(word)] = 1\n        else:\n            print(&quot;the word: %s is not in my Vocabulary!&quot; % word)\n    return returnVec\n第一个函数createVocabList()创建了一个实验样本。该函数返回几个切分好的文本词条，已经去除标点符号，第二个返回值是一个类别标签的集合，有两类，侮辱性和非侮辱性。\ncreateVocabList()函数创建了一个包含文档所有单词的列表，列表中没有重复值。\nsetOfWords2Vec()输入参数是词汇表，和某个文档，输出是这个文档的向量。\nlistOPosts, listClasses = loadDataSet()\nmyVocabList = createVocabList(listOPosts)\nmyVocabList\n[&#039;posting&#039;,\n &#039;to&#039;,\n &#039;please&#039;,\n &#039;help&#039;,\n &#039;him&#039;,\n &#039;worthless&#039;,\n &#039;mr&#039;,\n &#039;love&#039;,\n &#039;is&#039;,\n &#039;stop&#039;,\n &#039;has&#039;,\n &#039;stupid&#039;,\n &#039;flea&#039;,\n &#039;I&#039;,\n &#039;quit&#039;,\n &#039;problems&#039;,\n &#039;steak&#039;,\n &#039;cute&#039;,\n &#039;garbage&#039;,\n &#039;food&#039;,\n &#039;park&#039;,\n &#039;dog&#039;,\n &#039;dalmation&#039;,\n &#039;licks&#039;,\n &#039;buying&#039;,\n &#039;ate&#039;,\n &#039;not&#039;,\n &#039;maybe&#039;,\n &#039;take&#039;,\n &#039;so&#039;,\n &#039;how&#039;,\n &#039;my&#039;]\n\nsetOfWords2Vec(myVocabList, listOPosts[0])\n[0,\n 0,\n 1,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1]\n\nsetOfWords2Vec(myVocabList, listOPosts[3])\n[1,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 1,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 1,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0,\n 0]\n\n训练算法：从词向量计算概率\n我们使用前面的贝叶斯公式，将x,y替换位w,w表示一个向量，它由多个数值组成：\np(c_i|w)=\\frac{p(w|c_i)p(c_i)}{p(w)}\n计算方法：\np(c_i)=类别i中的单词数\\div总的单词数\np(w|c_i)=p(w_0,w_1,w_2...w_N|c_i)=p(w_0|c_i)p(w_1|c_i)p(w_2|c_i)...p(w_N|c_i)\n伪代码：\n计算每个类别中的单词数\n对每篇训练文档：\n    对每个类别：\n    如果词条出现在文档中-&gt;增加该词条的计数值\n    增加所有词条的计数值\n对每个类别：\n    对每个词条：\n    将该词条的数目除以总词条数目得到条件概率\n返回每个类别的条件概率\n\ndef trainNB0(trainMatrix, trainCategory):\n    # 初始化概率\n    numTrainDocs = len(trainMatrix)\n    numWords = len(trainMatrix[0])\n    pAbusive = sum(trainCategory)/float(numTrainDocs)\n    p0Num = zeros(numWords)\n    p1Num = zeros(numWords)\n    p0Denom = 0.0\n    p1Denom = 0.0\n    for i in range(numTrainDocs):\n        if trainCategory[i] == 1:\n            # 向量相加\n            p1Num += trainMatrix[i]\n            p1Denom += sum(trainMatrix[i])\n        else:\n            p0Num += trainMatrix[i]\n            p0Denom += sum(trainMatrix[i])\n    # 对每个元素做除法\n    p1vect = p1Num/p1Denom\n    p0vect = p0Num/p0Denom\n    return p0vect, p1vect, pAbusive\n代码中的输入为文档矩阵trainMatrix，和每篇文档类别标签所构成的向量trainCategory。首先计算侮辱性文档（class=1）的概率，即P(1).因为这是个二分类问题，所有可以通过计算p(0)=1-p(1)\ntrainMat = []\nfor postinDoc in listOPosts:\n    trainMat.append(setOfWords2Vec(myVocabList, postinDoc))\np0V, p1V, pAb = trainNB0(trainMat, listClasses)\np0V\narray([0.        , 0.04166667, 0.04166667, 0.04166667, 0.08333333,\n       0.        , 0.04166667, 0.04166667, 0.04166667, 0.04166667,\n       0.04166667, 0.        , 0.04166667, 0.04166667, 0.        ,\n       0.04166667, 0.04166667, 0.04166667, 0.        , 0.        ,\n       0.        , 0.04166667, 0.04166667, 0.04166667, 0.        ,\n       0.04166667, 0.        , 0.        , 0.        , 0.04166667,\n       0.04166667, 0.125     ])\n\np1V\narray([0.05263158, 0.05263158, 0.        , 0.        , 0.05263158,\n       0.10526316, 0.        , 0.        , 0.        , 0.05263158,\n       0.        , 0.15789474, 0.        , 0.        , 0.05263158,\n       0.        , 0.        , 0.        , 0.05263158, 0.05263158,\n       0.05263158, 0.10526316, 0.        , 0.        , 0.05263158,\n       0.        , 0.05263158, 0.05263158, 0.05263158, 0.        ,\n       0.        , 0.        ])\n\npAb\n0.5\n\n测试算法：根据实际情况修改分类器\n在计算多个概率的乘积一获得分档属于某个类别的概率，即计算p(w_0|c_i)p(w_1|c_i)p(w_2|c_i)时候，如果其中一个概率的值为0，那么最后的乘积也为0，为了降低这种影响，我们将所有词出现的次数初始化为1，将分母初始化为2。\n另一个遇到的问题是下溢，是由于太多的很小的数相乘造成的，可以求对数避免下溢。\n\n观察上图发现，f(x)和ln(f(x))的曲线趋势是相同的\ndef trainNB0(trainMatrix, trainCategory):\n    # 初始化概率\n    numTrainDocs = len(trainMatrix)\n    numWords = len(trainMatrix[0])\n    pAbusive = sum(trainCategory)/float(numTrainDocs)\n    # 初始化为 1\n    p0Num = ones(numWords)\n    p1Num = ones(numWords)\n    # 分母改为 2\n    p0Denom = 2.0\n    p1Denom = 2.0\n    for i in range(numTrainDocs):\n        if trainCategory[i] == 1:\n            # 向量相加\n            p1Num += trainMatrix[i]\n            p1Denom += sum(trainMatrix[i])\n        else:\n            p0Num += trainMatrix[i]\n            p0Denom += sum(trainMatrix[i])\n    # 对每个元素做除法，并求对数\n    p1vect = log(p1Num/p1Denom)\n    p0vect = log(p0Num/p0Denom)\n    return p0vect, p1vect, pAbusive\n朴素贝叶斯分类函数：\ndef classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):\n    p1 = sum(vec2Classify * p1Vec) + log(pClass1)\n    p0 = sum(vec2Classify * p0Vec) + log(1.0-pClass1)\n    if p1 &gt; p0:\n        return 1\n    else:\n        return 0\n \ndef testingNB():\n    listOposts, listClasses = loadDataSet()\n    myVocabList = createVocabList(listOPosts)\n    trainMat = []\n    for postinDoc in listOposts:\n        trainMat.append(setOfWords2Vec(myVocabList, postinDoc))\n    p0V, p1V, pAb = trainNB0(array(trainMat), array(listClasses))\n    testEntry = [&#039;love&#039;, &#039;my&#039;, &#039;dalmation&#039;]\n    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))\n    print(testEntry, &#039;classif as &#039;, classifyNB(thisDoc, p0V, p1V, pAb))\n    testEntry = [&#039;stupid&#039;, &#039;garbage&#039;]\n    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))\n    print(testEntry, &#039;classif as &#039;, classifyNB(thisDoc, p0V, p1V, pAb))\ntestingNB()\n[&#039;love&#039;, &#039;my&#039;, &#039;dalmation&#039;] classif as  0\n[&#039;stupid&#039;, &#039;garbage&#039;] classif as  1\n\n测试结果，第一句话是非侮辱性的，第二句是侮辱性的，分类正确\n准备数据：文档词袋模型\n每个词的出现次数作为一个特征，这个可以被描述为词集模型（set of word model），如果一个词在文档中的出现不止一次，这种方法被称为词袋模型（bag of words model），修改setOfWords2Vec()函数为bagOfWords2Vec()\ndef bagOfWords2Vec(vocabList, inputSet):\n    returnVec = [0]*len(vocabList)\n    for word in inputSet:\n        if word in vocabList:\n            returnVec[vocabList.index(word)] += 1\n    return returnVec\n现在分类器已经构建好了，下面利用该分类器过滤垃圾邮件。\n实例：使用朴素贝叶斯过滤垃圾邮件\n\n收集数据：提供的文本文件\n准备数：将文本文件解析成词条向量\n分析数据：检查词条确保解析的正确性\n训练算法：使用我们之前建立的trainBN()函数\n测试算法：使用classifyNB()，并且构建一个新的测试函数来计算文档集的错误\n使用算法：构建一个完整的程序过程对一组文档进行分类，将错分的文档输出到屏幕上\n\n准备数据：切分文本\n使用python的string.split()方法切分\n使用re.compile(‘\\W*‘)去除标点和数字。\n去除空字符串\n使用.lower()转换为小写\nimport re\nregEx = re.compile(&#039;\\W&#039;)\nemailText = open(&#039;MLiA_SourceCode/machinelearninginaction/Ch04/email/ham/6.txt&#039;).read()\nlistOfTokens = regEx.split(emailText)\nlistOfTokens = [tok.lower() for tok in listOfTokens if len(tok) &gt; 0 and re.search(&#039;[^0-9]&#039;, tok)]\n测试算法：使用朴素贝叶斯进行交叉验证\ndef textParse(bigString):\n    listOfTokens = re.split(r&#039;\\W&#039;, bigString)\n    return [tok.lower() for tok in listOfTokens if len(tok) &gt; 2 and re.search(&#039;[^0-9]&#039;, tok)]\n \ndef spamTest():\n    docList = []\n    classList = []\n    fullText = []\n    for i in range(1, 26):\n        wordList = textParse(open(&#039;MLiA_SourceCode/machinelearninginaction/Ch04/email/spam/%d.txt&#039; % i).read())\n        docList.append(wordList)\n        fullText.extend(wordList)\n        classList.append(1)\n        wordList = textParse(open(&#039;MLiA_SourceCode/machinelearninginaction/Ch04/email/ham/%d.txt&#039; % i).read())\n        docList.append(wordList)\n        fullText.extend(wordList)\n        classList.append(0)\n    vocabList = createVocabList(docList)\n    trainingSet = list(range(50))\n    testSet = []\n    for i in range(10):\n        randIndex = int(random.uniform(0, len(trainingSet)))\n        testSet.append(trainingSet[randIndex])\n        del(trainingSet[randIndex])\n    trainMat = []\n    trainClasses = []\n    for docIndex in trainingSet:\n        trainMat.append(setOfWords2Vec(vocabList, docList[docIndex]))\n        trainClasses.append(classList[docIndex])\n    p0V, p1V, pSpam = trainNB0(array(trainMat), array(trainClasses))\n    errorCount = 0\n    for docIndex in testSet:\n        wordVector = setOfWords2Vec(vocabList, docList[docIndex])\n        if classifyNB(array(wordVector), p0V, p1V, pSpam) != classList[docIndex]:\n            errorCount += 1\n            print(&quot;classification error&quot;,docList[docIndex])\n    print(&#039;the error rate is &#039;, float(errorCount)/len(testSet))\n    return float(errorCount)/len(testSet)\nspamTest()\nthe error rate is  0.0\n\n0.0\n\nspamTest()\nclassification error [&#039;this&#039;, &#039;mail&#039;, &#039;was&#039;, &#039;sent&#039;, &#039;from&#039;, &#039;notification&#039;, &#039;only&#039;, &#039;address&#039;, &#039;that&#039;, &#039;cannot&#039;, &#039;accept&#039;, &#039;incoming&#039;, &#039;mail&#039;, &#039;please&#039;, &#039;not&#039;, &#039;reply&#039;, &#039;this&#039;, &#039;message&#039;, &#039;thank&#039;, &#039;you&#039;, &#039;for&#039;, &#039;your&#039;, &#039;online&#039;, &#039;reservation&#039;, &#039;the&#039;, &#039;store&#039;, &#039;you&#039;, &#039;selected&#039;, &#039;has&#039;, &#039;located&#039;, &#039;the&#039;, &#039;item&#039;, &#039;you&#039;, &#039;requested&#039;, &#039;and&#039;, &#039;has&#039;, &#039;placed&#039;, &#039;hold&#039;, &#039;your&#039;, &#039;name&#039;, &#039;please&#039;, &#039;note&#039;, &#039;that&#039;, &#039;all&#039;, &#039;items&#039;, &#039;are&#039;, &#039;held&#039;, &#039;for&#039;, &#039;day&#039;, &#039;please&#039;, &#039;note&#039;, &#039;store&#039;, &#039;prices&#039;, &#039;may&#039;, &#039;differ&#039;, &#039;from&#039;, &#039;those&#039;, &#039;online&#039;, &#039;you&#039;, &#039;have&#039;, &#039;questions&#039;, &#039;need&#039;, &#039;assistance&#039;, &#039;with&#039;, &#039;your&#039;, &#039;reservation&#039;, &#039;please&#039;, &#039;contact&#039;, &#039;the&#039;, &#039;store&#039;, &#039;the&#039;, &#039;phone&#039;, &#039;number&#039;, &#039;listed&#039;, &#039;below&#039;, &#039;you&#039;, &#039;can&#039;, &#039;also&#039;, &#039;access&#039;, &#039;store&#039;, &#039;information&#039;, &#039;such&#039;, &#039;store&#039;, &#039;hours&#039;, &#039;and&#039;, &#039;location&#039;, &#039;the&#039;, &#039;web&#039;, &#039;http&#039;, &#039;www&#039;, &#039;borders&#039;, &#039;com&#039;, &#039;online&#039;, &#039;store&#039;, &#039;storedetailview_98&#039;]\nthe error rate is  0.1\n\n0.1\n\n每一次得到的错误率都不同，要想更好的评估错误率，可以重复多次，十次计算求平均错误率为6%\nerrorRate = 0\nfor i in range(10):\n    errorRate += spamTest()\nerrorRate/10\nthe error rate is  0.2\nthe error rate is  0.1\nthe error rate is  0.0\nthe error rate is  0.0\nthe error rate is  0.1\nthe error rate is  0.1\nthe error rate is  0.1\nthe error rate is  0.0\nthe error rate is  0.1\nthe error rate is  0.0\n\n0.06999999999999999\n\n实例：使用朴素贝叶斯分类器从个人广告中获取区域倾向\n下面将使用来自不同城市的广告训练一个分类器，然后观察分类的效果，我们的目的不是使用该分类器进行分类，而是通过观察单词和条件概率值来发现与特定城市相关的内容，\n收集数据：导入RSS源\n利用python下载RSS的文本。\n首先需要安装feedparser,github.com/kurtmckee/feedparser\n%pip install feedparser\nimport feedparser\n接下来作者使用了RSS源newyork.craigslist.org/stp/index.rss 已经不能访问了\n书中作者的意思是以来自源 newyork.craigslist.org/stp/index.rss 中的文章作为分类为1的文章，以来自源 sfbay.craigslist.org/stp/index.rss 中的文章作为分类为0的文章\n为了能够跑通示例代码，可以找两可用的RSS源作为替代。\n我用的是这两个源：\nNASA Image of the Day：www.nasa.gov/rss/dyn/image_of_the_day.rss\nYahoo Sports - NBA - Houston Rockets News：sports.yahoo.com/nba/teams/hou/rss.xml\n也就是说，如果算法运行正确的话，所有来自于 nasa 的文章将会被分类为1，所有来自于yahoo sports的休斯顿火箭队新闻将会分类为0\nny=feedparser.parse(&#039;www.nasa.gov/rss/dyn/image_of_the_day.rss&#039;)\nlen(ny[&#039;entries&#039;])\n60\n\ndef calcMostFreq(vocabList,fullText):\n    import operator\n    freqDict = {}\n    for token in vocabList:\n        freqDict[token]=fullText.count(token)\n    sortedFreq = sorted(freqDict.items(), key=operator.itemgetter(1), reverse=True)\n    return sortedFreq[:30]\n \ndef localWords(feed1,feed0):\n    import feedparser\n    docList=[]\n    classList = []\n    fullText =[]\n    minLen = min(len(feed1[&#039;entries&#039;]),len(feed0[&#039;entries&#039;]))\n    for i in range(minLen):\n        wordList = textParse(feed1[&#039;entries&#039;][i][&#039;summary&#039;])\n        docList.append(wordList)\n        fullText.extend(wordList)\n        classList.append(1) #NY is class 1\n        wordList = textParse(feed0[&#039;entries&#039;][i][&#039;summary&#039;])\n        docList.append(wordList)\n        fullText.extend(wordList)\n        classList.append(0)\n    vocabList = createVocabList(docList)#create vocabulary\n    top30Words = calcMostFreq(vocabList,fullText)   #remove top 30 words\n    for pairW in top30Words:\n        if pairW[0] in vocabList: vocabList.remove(pairW[0])\n    trainingSet = list(range(2*minLen))\n    testSet=[]           #create test set\n    for i in range(20):\n        randIndex = int(random.uniform(0,len(trainingSet)))\n        testSet.append(trainingSet[randIndex])\n        del(trainingSet[randIndex])  \n    trainMat=[]; trainClasses = []\n    for docIndex in trainingSet:#train the classifier (get probs) trainNB0\n        trainMat.append(bagOfWords2Vec(vocabList, docList[docIndex]))\n        trainClasses.append(classList[docIndex])\n    p0V,p1V,pSpam = trainNB0(array(trainMat),array(trainClasses))\n    errorCount = 0\n    for docIndex in testSet:        #classify the remaining items\n        wordVector = bagOfWords2Vec(vocabList, docList[docIndex])\n        if classifyNB(array(wordVector),p0V,p1V,pSpam) != classList[docIndex]:\n            errorCount += 1\n    print(&#039;the error rate is: &#039;,float(errorCount)/len(testSet))\n    return vocabList,p0V,p1V\ncalcMostFreq()函数的功能是遍历词汇表中的每个词并统计它在文本中出现的次数，然后根据出现次数从高到低对词典进行排序，返回排序最高的30个单词。\nny=feedparser.parse(&#039;www.nasa.gov/rss/dyn/image_of_the_day.rss&#039;)\nsf=feedparser.parse(&#039;sports.yahoo.com/nba/teams/hou/rss.xml&#039;)\nvocabList, pSF, pNY=localWords(ny, sf)\nthe error rate is:  0.5\n\nvocabList, pSF, pNY=localWords(ny, sf)\nthe error rate is:  0.35\n\n分析数据：显示地域相关的用词\n先对pSF和pNY进行排序，然后按照顺序将词打印出来。\ndef getTopWords(ny,sf):\n    import operator\n    vocabList,p0V,p1V=localWords(ny,sf)\n    topNY=[]; topSF=[]\n    for i in range(len(p0V)):\n        if p0V[i] &gt; -6.0 : topSF.append((vocabList[i],p0V[i]))\n        if p1V[i] &gt; -6.0 : topNY.append((vocabList[i],p1V[i]))\n    sortedSF = sorted(topSF, key=lambda pair: pair[1], reverse=True)\n    print(&quot;SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**&quot;)\n    for item in sortedSF:\n        print(item[0])\n    sortedNY = sorted(topNY, key=lambda pair: pair[1], reverse=True)\n    print(&quot;NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**&quot;)\n    for item in sortedNY:\n        print(item[0])\ngetTopWords(ny,sf)\nthe error rate is:  0.2\nSF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**\nwestbrook\nbut\nfund\nmichael\nlos\nalso\namid\nNY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**\narms\nstation\nprogram\nagency\nspacecraft\nmilky\n\n总结\n对于分类而言，使用概率有时要比使用硬规则更为有效。贝叶斯概率即贝叶斯准则提供了一种利用一直值来估计未知概率的有效方法。\n可以通过特征之间的条件独立性假设，降低对数据量的需求。独立性假设是指一个词出现的概率不依赖与文档中的其他词。\n编程贝叶斯时需要考虑很多实际因素。下溢出就是其中之一，可以通过对概率取对数来解决。还有其他方法改进，比如移除停用词。"},"机器学习实战/5.-机器学习实战":{"slug":"机器学习实战/5.-机器学习实战","filePath":"机器学习实战/5. 机器学习实战.md","title":"5. 机器学习实战","links":[],"tags":["逻辑回归"],"content":"Logistic回归是一个最优化算法，比如如何在最短时间从A点到达B点？\n回归：假设我们有一些数据点，我们用一条直线对这些点进行拟合（该线称为最佳拟合直线），这个拟合过程就叫做回归。\n根据现有的数据对分类边界线建立回归公式，依次进行分类。这里的“回归”一次源于最佳拟合，表示要找到最佳拟合参数集。\nLogistic回归的一般过程：\n\n收集数据\n准备数据：由于需要进行距离计算，因此数据类型必须为数值型，另外结构化数据格式最佳\n分析数据\n训练算法：大部分时间将用于训练，训练的目的是为了找到最佳的分类回归系数\n测试算法\n使用算法：首先，需要输入一些数据，并将其转换成对应的结构化数值；接着，基于训练好的回归系数就可以对这些数值进行简单的回归计算，判定它们属于哪个类别；在这之后，我们就可以在输出的类别上做一些其他分析工作\n\n基于Logistic回归和Sigmoid函数的分类\nlogistic回归\n优点：计算代价不高，易于理解和实现\n缺点：容易欠拟合，分类精度可能不高\n适用数据类型：数值型和标称型数据\n\n首先我们需要一个阶跃函数（step function），Sigmoid函数，它的值域在0-1之间：\n\\sigma(z)=\\frac{1}{1+e^{-z}}\n下图给出了sigmoid函数在不同坐标尺度下的两条曲线图。\n为了实现logistic回归分类器，我们可以在每个特征上都乘以一个回归系数，然后把所有的结果值相加，将这个总和带入sigmoid函数中，进而得到一个范围在0~1之间的数值，任何大于0.5的数据被分入1类，小于0.5的即被归为0类，所以logistic回归也可以被看成一种概率估计。\n\n两种坐标尺度下的sigmoid函数图，上图的横坐标为-5~5，这时曲线变化较为平滑.\n基于最优化方法的最佳回归系数确定\nsigmoid函数的输入记为z，由下面的公式得出：\nz=w_0x_0+w_1x_1+w_2x_2+...+w_nx_n\n如果采用向量写法，上述公式可以写成Z=W^TX，它表示将这两个数值向量对应元素相乘然后全部加起来得到z的值。其中的向量x是分类器输入的数据，向量w是我们需要求得的最佳系数（weight）。\n梯度上升\n梯度上升算法的思想是：要找到某函数的最大值，最好的方法是沿着该函数的梯度方向探寻。如果梯度记为\\nabla，则函数f(x,y)的梯度由下式表示\n\n这个梯度意味着要沿x的方向移动 ，沿y的方向移动 ，其中f(x,y)必须要在待计算的点上由定义并且可微。\n\n梯度上升算法到达每个点后都会重新估计移动方向。从P0开始，计算完该点的梯度，函数就根据梯度移动到下一点P1，在P1点，梯度再次被重新计算，并且沿新的梯度方向移动到P2。如此循环迭代，直到满足停止条件。迭代的过程中，梯度算子总能保证我们选取到最佳的移动方向。\n梯度上升算法沿梯度方向移动了一步，可以看到，梯度算子总是指向函数增长最快的方向，这里所说的是移动方向，而未提到移动量的大小，该量值称为步长，记作\\alpha，用向量表示的话，梯度上升算法的迭代公式如下：\nw := w+\\alpha\\nabla_wf(W)\n该公式将一直被迭代执行，直到达到某个停止条件为止。\n梯度下降算法\n经常听到的应该是梯度下降算法，他与这里的梯度上升算法是一样的，只是公式中的加法需要变成减法，因此对应公式可以写成\n\nw := w-\\alpha\\nabla_wf(W)\n梯度上升算法用来求函数的最大值，而梯度下降用来求函数的最小值\n\n接下来我们将对下面的数据集使用梯度上升的算法来进行分类，求出最佳回归系数。\n\n训练算法：使用梯度上升找到最佳参数\n上图有100个样本点，每个点包含两个数值型特征X1和X2。在此数据集上，我们将通过使用梯度上升法找到最佳回归系数，也就是拟合出logistic回归模型的最佳参数。\n伪代码如下：\n每个回归系数初始化为1\n重复R次：\n    计算整个数据集的梯度\n    使用alpha × gradient更新回归系数的向量\n返回回归系数\n\nlogistic回归梯度上升优化算法\ndef loadDataSet():\n    dataMat = []\n    labelMat = []\n    fr = open(&#039;MLiA_SourceCode/machinelearninginaction/Ch05/testSet.txt&#039;)\n    for line in fr.readlines():\n        lineArr = line.strip().split()\n        dataMat.append([1.0, float(lineArr[0]), float(lineArr[1])])\n        labelMat.append(int(lineArr[2]))\n    return dataMat, labelMat\ndataArr, labelMat = loadDataSet()\nloadDataSet()函数的作用是打开testSet.txt文件，逐行读取，每行的前两个值是X1和X2，第三个值是对应的类别标签，为了方便计算将X0设置为1.0\n下面是sigmoid函数:\ndef sigmoid(inX):\n    return 1.0/(1+exp(-inX))\nfrom tqdm import trange\nfrom numpy import *\ndef gradAscent(dataMat, classLabels):\n    # 转换为numpy矩阵的数据类型\n    dataMatrix = mat(dataMat)\n    labelMat = mat(classLabels).transpose() # 转置\n    m, n = shape(dataMatrix)\n    alpha = 0.001 # 步长\n    maxCycles = 500 # 迭代次数\n    weights = ones((n, 1))\n    \n    for k in trange(maxCycles):\n        h = sigmoid(dataMatrix * weights)\n        error = (labelMat-h)\n        weights = weights + alpha * dataMatrix.transpose() * error\n    \n    return weights\ngradAscent()函数用来完成梯度上升算法的实现。\nweights = gradAscent(dataArr, labelMat)\nweights\n100%|██████████| 500/500 [00:00&lt;00:00, 15757.40it/s]\n\nmatrix([[ 4.12414349],\n        [ 0.48007329],\n        [-0.6168482 ]])\n\n分析数据：画出决策边界\n上面已经解出一组回归系数，接下来画出分割线。\nimport matplotlib.pyplot as plt\nplt.rcParams[&#039;axes.unicode_minus&#039;]=False #用来正常显示负号\ndef plotBestFit(weights):\n    dataMat, labelMat = loadDataSet()\n    dataArr = array(dataMat)\n    n = shape(dataArr)[0]\n    xcord1 = []\n    ycord1 = []\n    xcord2 = []\n    ycord2 = []\n    \n    for i in range(n):\n        if int(labelMat[i]) == 1:\n            xcord1.append(dataArr[i, 1])\n            ycord1.append(dataArr[i, 2])\n        else:\n            xcord2.append(dataArr[i, 1])\n            ycord2.append(dataArr[i, 2])\n        \n    \n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ax.scatter(xcord1, ycord1, s=30, c=&#039;red&#039;, marker=&#039;s&#039;)\n    ax.scatter(xcord2, ycord2, s=30, c=&#039;green&#039;)\n    x = arange(-3.0, 3.0, 0.1)\n    y = (-weights[0]-weights[1]*x)/weights[2]\n    ax.plot(x, y.T) # y转置是为了xy矩阵结构相同x.shape(60,) y.T.shape(60, 1)\n    plt.xlabel(&#039;X1&#039;)\n    plt.ylabel(&#039;X2&#039;)\n    plt.show()\n我们设定0=w_0x_0+w_1x_1+w_2x_2，然后用解出X1和X2的关系式（其中X0=1），画出线段\nplotBestFit(weights)\n\n这个分类结果相当不错，看图可知只分错了两个点。\n训练算法：随机梯度上升\n梯度上升算法每次迭代都需要遍历整个数据集，如果有十亿样本和上千万特征，那么改方法的计算复杂度就太高了，一种改进方法是一次仅用一个样本来更新回归系数，该方法称为随机梯度上升算法。由于可以在新样本到来时对分类器进行增量式更新，因而随机梯度上升算法是一个在线学习算法。与在线学习相对应，一次处理所有数据被称作是“批处理”。\n随机梯度上升算法可以写成如下的伪代码：\n所有回归系数初始化为1\n对数据集中每个样本\n    计算该样本的梯度\n    使用alpha×gradient更新回归系数\n然会回归系数\n\ndef stocGradAscent0(dataMatrix, classLabels):\n    m, n = shape(dataMatrix)\n    alpha = 0.01\n    weights = ones(n)\n    for i in range(m):\n        h = sigmoid(sum(dataMatrix[i]*weights))\n        error = classLabels[i] - h\n        weights = weights + alpha * error * dataMatrix[i]\n    return weights\n随机梯度上升算法和梯度上升算法代码很相似，有两点区别，第一，后者的变量h和error都是向量，前者全是数值；第二，前者没有矩阵转置过程，所有变量的数据类型都是numpy数组。\nweights = stocGradAscent0(array(dataArr), labelMat)\nplotBestFit(weights)\n\n观察上图发现有些欠拟合，梯度上升的算法迭代了500次，而这个结果只迭代了200次，所以还算不错了。\n下图展示了随机梯度上升算法在二百次迭代过程中回归系数的变换情况。\n%run MLiA_SourceCode/machinelearninginaction/Ch05/plotSDerror.py\n\n和书上画的不一样，略过。\n改进的随机梯度上升算法\ndef stocGradAscent1(dataMatrix, classLabels, numIter=150):\n    m, n = shape(dataMatrix)\n    weights = ones(n)\n    for j in range(numIter):\n        dataIndex = list(range(m))\n        for i in range(m):\n            # 1.alpha每次迭代需要调整\n            alpha = 4 / (1.0 + j + i) + 0.0001\n            # 2.随机选取更新\n            randIndex = int(random.uniform(0, len(dataIndex)))\n            h = sigmoid(sum(dataMatrix[randIndex]*weights))\n            error = classLabels[randIndex] - h\n            weights = weights + alpha * error * dataMatrix[randIndex]\n            del(dataIndex[randIndex])\n    return weights\n上面的程序改进有两处，1处改进会缓解数据波动或高频波动，虽然alpha会随迭代次数不断减小，但永远不会减小到0，这样做的原因是保证多次迭代后，新数据任然具有一定的影响。\n2处改进通过随机选取样本来更新回归系数，这种方法将减小周期波动。\n改进算法还增加了迭代次数作为第三个参数。\n%run MLiA_SourceCode/machinelearninginaction/Ch05/plotSDerror.py\n\n使用随机样本选择和alpha动态减少机制的随机梯度上升算法stocGradAscent1()所生成的系数收敛示意图。该方法比采用固定alpha的方法收敛速度更快。\nweights = stocGradAscent1(array(dataArr), labelMat)\nplotBestFit(weights)\n\n程序执行结果与gradAscent()差不多，但是计算量更少。\n实例：从疝气病症预测马的死亡率\n使用logistic回归预测患有疝病的马的存活问题。提供的数据有368个样本28个特征。\n\n收集数据\n准备数据：用python解析文本并填充缺失值\n分析数据：可视化观察数据\n训练算法：使用优化算法，找到最佳系数\n测试算法：为了量化回归效果，需要观察错误率。根据错误率决定是否回退到训练阶段，通过改变迭代的次数和步长等参数来得到更好的回归系数\n使用算法：实现简单的命令程序来收集马的病症并输出预测结果\n\n准备数据：处理数据中的缺失值\n当数据缺失时，可以用以下方法来解决这个问题：\n使用可用特征的均值来填补缺失值\n使用特殊值来填补缺失值，如-1\n忽略有缺失值的样本\n使用相似样本的均值来填补缺失值\n使用另外的机器学习算预测缺失值\n\n现在为了可以使用算法，我们要做两件事\n第一，选择实数0来替换所有的缺失值，修改回归系数的更新公式\nweights = weights + alpha × error × dataMatrix[randIndex]\n这样做是为了当randIndex对应的特征值为0时，weights将不会更新\n第二，如果在测试数据集中发现一条数据的类别标签已经缺失，简单的做法是将该条数据丢弃。\n测试算法：用logistic回归进行分类\ndef classifyVector(inX, weights):\n    prob = sigmoid(sum(inX*weights))\n    if prob &gt; 0.5:\n        return 1.0\n    return 0.0\n \ndef colicTest():\n    frTrain = open(&#039;MLiA_SourceCode/machinelearninginaction/Ch05/horseColicTraining.txt&#039;)\n    frTest = open(&#039;MLiA_SourceCode/machinelearninginaction/Ch05/horseColicTest.txt&#039;)\n    trainingSet = []\n    trainingLabels = []\n    for line in frTrain.readlines():\n        currLine = line.strip().split(&#039;\\t&#039;)\n        lineArr = []\n        for i in range(21):\n            lineArr.append(float(currLine[i]))\n        trainingSet.append(lineArr)\n        trainingLabels.append(float(currLine[21]))\n    trainWeights = stocGradAscent1(array(trainingSet), trainingLabels, 500)\n    errorCount = 0\n    numTestVec = 0.0\n    for line in frTest.readlines():\n        numTestVec += 1.0\n        currLine = line.strip().split(&#039;\\t&#039;)\n        lineArr = []\n        for i in range(21):\n            lineArr.append(float(currLine[i]))\n        if int(classifyVector(array(lineArr), trainWeights)) != int(currLine[21]):\n            errorCount += 1\n    errorRate = (float(errorCount)/numTestVec)\n    print(&quot;the error tate of this test is %f&quot; % errorRate)\n    return errorRate\n \ndef multiTest():\n    numTests = 10\n    errorSum = 0.0\n    for k in range(numTests):\n        errorSum += colicTest()\n    print(&quot;after %d iterations the average error rate is: %f&quot; % (numTests, errorSum/float(numTests)))\nmultiTest()\nthe error tate of this test is 0.268657\nthe error tate of this test is 0.373134\nthe error tate of this test is 0.343284\nthe error tate of this test is 0.328358\nthe error tate of this test is 0.313433\nthe error tate of this test is 0.298507\nthe error tate of this test is 0.417910\nthe error tate of this test is 0.328358\nthe error tate of this test is 0.283582\nthe error tate of this test is 0.373134\nafter 10 iterations the average error rate is: 0.332836\n\n十次迭代后的平均错误率为33%，还算不错\n总结\nLogistic回归的目的是寻找一个非线性函数sigmoid的最佳拟合参数，求解过程可以由最优化算法完成。在最优化算法中，最常用的就是梯度上升算法，而梯度上升算法又可以简化为随机梯度上升。\n随机梯度上升算法与梯度上升算法的效果相当，但占用更少的计算资源，并且，随机梯度上升算法是一个在线算法，它可以在新数据到来时就更新参数，而不需要重新读取整个数据集\n如何处理缺失值，取决于实际中的需求。"},"机器学习实战/6.-机器学习实战":{"slug":"机器学习实战/6.-机器学习实战","filePath":"机器学习实战/6. 机器学习实战.md","title":"6. 机器学习实战","links":[],"tags":["SVM","SMO","支持向量机"],"content":"这一章的内容非常多，在神经网络大火前，SVM是最优秀的机器学习算法，尽管现在已经很少用了，但作为一本七年前的书还是很详细的讲解了，所以这里简单的记录下。\n基于最大间隔分隔数据\n支持向量机\n优点：泛化错误率低，计算开销不大，结果易理解\n缺点：对参数调节和核函数选择敏感，原始分类器不加修改仅适用于处理二分类问题\n适用数据类型：数值型和标称型数据\n\n\n观察上图图发现我们不能画出一条线或者圆把圆形和方形的数据分割开，而下图可以画出一条直线将两组数据分开。所以下图的数据称为线性可分（linearly separable）。\n\n将数据集分开的直线称为分割超平面（separating hyperplane）。上面给出的例子数据都在二维平面上，所以分割超平面是一条直线，如果所给的数据是三维的，那么分割数据的就是一个平面。如果数据集是一个1024维的，那就需要一个1023维的对象对数据分割。如果一个数据集是N维的，需要一个N-1维的超平面分割。\n支持向量（support vector）就是离分割超平面最近的那些点。\n寻找最大间隔\nHow can we measure the line that best separates the data? To start with, look at figure 6.3. Our separating hyperplane has the form wTx+b. If we want to find the distance from A to the separating plane, we must measure normal or perpendicular to the line. This is given by |wTA+b|/||w||. The constant b is just an offset like w0 in logistic regression. All this w and b stuff describes the separating line, or hyperplane, for our data. Now, let’s talk about the classifier.\n\n分类器求解的优化问题\nSVM应用的一般框架\nSVM的一般流程\n\n收集数据\n准备数据：需要数值型数据\n分析数据：有助于可视化分割超平面\n训练算法：SVM大大部分时间都源自训练，该过程主要实现两个参数的调优\n测试算法：十分简单的计算过程就可以实现\n适用算法：几乎所有分类问题都可以适用SVM，值得一提的是，SVM本身是一个二分类分类器，对多分类问题SVM需要对代码修改\n\nSMO高效优化算法\nSMO表示序列最小优化（Sequential Minimal Optimization）\nSMO算法的目标是求出一系列alpha和b，一旦求出了这些alpha，就很容易计算处权重向量w并得到分割超平面。\nSMO算法的工作原理是：每次循环中选择两个alpha进行优化处理。一旦找到一\n对合适的alpha，那么就增大其中一个同时减小另一个。这里所谓的合适们就是指两个alpha必须要符合两个条件，一，两个alpha必须要在间隔边界之外，二，两个alpha还没有进行过区间化处理或者不在边界上。\n应用简化版SMO算法处理小规模数据集\n\\Sigma\\alpha*label^{i} = 0\ndef loadDataSet(fileName):\n    dataMat = []\n    labelMat = []\n    fr = open(fileName)\n    for line in fr.readlines():\n        lineArr = line.strip().split(&#039;\\t&#039;)\n        dataMat.append([float(lineArr[0]), float(lineArr[1])])\n        labelMat.append(float(lineArr[2]))\n    fr.close()\n    return dataMat, labelMat\n \ndef selectJrand(i, m):\n    j = i\n    while (j == i):\n        j = int(random.uniform(0, m))\n    return j\n \ndef clipAlpha(aj, H, L):\n    if aj &gt; H:\n        aj = H\n    if L &gt; aj:\n        aj = L\n    return aj\ndataArr, labelArr = loadDataSet(&#039;MLiA_SourceCode/machinelearninginaction/Ch06/testSet.txt&#039;)\nlabelArr[:10]\n[-1.0, -1.0, 1.0, -1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0]\n\nselectJrand()有两个参数，i是第一个alpha的下表，m是所有alpha的数目，只要函数值不等于输入值i，函数就会随机选择\nclipAlpha()的作用是调整alpha的值在H和L之间。\nSMO伪代码大致如下：\n创建一个alpha向量并将其初始化为0向量\n当迭代次数小于最大迭代次数时（外循环）\n    对数据集中的每个数据向量（内循环）：\n        如果给数据向量可以被优化：\n            随机选择另外一个数据向量\n            如果优化这两个向量\n            如果两个向量都不能被优化，退出内循环\n    如果所有向量都没被优化，增加迭代数目，继续下一次循环\n\nfrom numpy import *\ndef smoSimple(dataMatIn, classLabels, C, toler, maxIter):\n    dataMatrix = mat(dataMatIn)\n    labelMat = mat(classLabels).transpose()\n    b = 0\n    m,n = shape(dataMatrix)\n    alphas = mat(zeros((m,1)))\n    iter = 0\n    \n    while (iter &lt; maxIter):\n        alphaPairsChanged = 0\n        for i in range(m):\n            fXi = float(multiply(alphas,labelMat).T*(dataMatrix*dataMatrix[i,:].T)) + b\n            Ei = fXi - float(labelMat[i]) #if checks if an example violates KKT conditions\n            if ((labelMat[i]*Ei &lt; -toler) and (alphas[i] &lt; C)) or ((labelMat[i]*Ei &gt; toler) and (alphas[i] &gt; 0)):\n                j = selectJrand(i,m)\n                fXj = float(multiply(alphas,labelMat).T*(dataMatrix*dataMatrix[j,:].T)) + b\n                Ej = fXj - float(labelMat[j])\n                alphaIold = alphas[i].copy(); alphaJold = alphas[j].copy();\n                if (labelMat[i] != labelMat[j]):\n                    L = max(0, alphas[j] - alphas[i])\n                    H = min(C, C + alphas[j] - alphas[i])\n                else:\n                    L = max(0, alphas[j] + alphas[i] - C)\n                    H = min(C, alphas[j] + alphas[i])\n                    \n                if L==H: \n                    print(&quot;L==H&quot;)\n                    continue\n \n                eta = 2.0 * dataMatrix[i,:]*dataMatrix[j,:].T - dataMatrix[i,:]*dataMatrix[i,:].T - dataMatrix[j,:]*dataMatrix[j,:].T\n \n                if eta &gt;= 0:\n                    print(&quot;eta&gt;=0&quot;)\n                    continue\n \n                alphas[j] -= labelMat[j]*(Ei - Ej)/eta\n                alphas[j] = clipAlpha(alphas[j],H,L)\n                if (abs(alphas[j] - alphaJold) &lt; 0.00001):\n                    print(&quot;j not moving enough&quot;)\n                    continue\n \n                alphas[i] += labelMat[j]*labelMat[i]*(alphaJold - alphas[j]) # update i by the same amount as j\n                                                                             # the update is in the oppostie direction\n                b1 = b - Ei- labelMat[i]*(alphas[i]-alphaIold)*dataMatrix[i,:]*dataMatrix[i,:].T - labelMat[j]*(alphas[j]-alphaJold)*dataMatrix[i,:]*dataMatrix[j,:].T\n                b2 = b - Ej- labelMat[i]*(alphas[i]-alphaIold)*dataMatrix[i,:]*dataMatrix[j,:].T - labelMat[j]*(alphas[j]-alphaJold)*dataMatrix[j,:]*dataMatrix[j,:].T\n                if (0 &lt; alphas[i]) and (C &gt; alphas[i]): \n                    b = b1\n                elif (0 &lt; alphas[j]) and (C &gt; alphas[j]): \n                    b = b2\n                else: \n                    b = (b1 + b2)/2.0\n                alphaPairsChanged += 1\n \n                print(&quot;iter: %d i:%d, pairs changed %d&quot; % (iter,i,alphaPairsChanged))\n \n        if (alphaPairsChanged == 0):\n            iter += 1\n        else:\n            iter = 0\n        print(&quot;iteration number: %d&quot; % iter)\n \n    return b,alphas\nb, alphas = smoSimple(dataArr, labelArr, 0.6, 0.001, 40)\nL==H\nL==H\niter: 0 i:2, pairs changed 1\niter: 0 i:3, pairs changed 2\nL==H\n...\n...\niteration number: 39\nj not moving enough\nj not moving enough\nj not moving enough\niteration number: 40\n\nb\nmatrix([[-3.79661253]])\n\nalphas[alphas&gt;0]\nmatrix([[0.12629181, 0.24169497, 0.36797683]])\n\nshape(alphas[alphas&gt;0])\n(1, 3)\n\nsupportVectors = []\nfor i in range(100):\n    if alphas[i] &gt; 0.0:\n        supportVectors.append(dataArr[i])\n        print(dataArr[i], labelArr[i])\n[4.658191, 3.507396] -1.0\n[3.457096, -0.082216] -1.0\n[6.080573, 0.418886] 1.0\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Circle\nplt.rcParams[&#039;axes.unicode_minus&#039;]=False # 用来正常显示负号\n \ndef plotSupportVectors(supportVectors):\n    xcord0 = []\n    ycord0 = []\n    xcord1 = []\n    ycord1 = []\n    markers =[]\n    colors =[]\n    fr = open(&#039;MLiA_SourceCode/machinelearninginaction/Ch06/testSet.txt&#039;)\n    for line in fr.readlines():\n        lineSplit = line.strip().split(&#039;\\t&#039;)\n        xPt = float(lineSplit[0])\n        yPt = float(lineSplit[1])\n        label = int(lineSplit[2])\n        if (label == -1):\n            xcord0.append(xPt)\n            ycord0.append(yPt)\n        else:\n            xcord1.append(xPt)\n            ycord1.append(yPt)\n \n    fr.close()\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ax.scatter(xcord0,ycord0, marker=&#039;s&#039;, s=90)\n    ax.scatter(xcord1,ycord1, marker=&#039;o&#039;, s=50, c=&#039;red&#039;)\n    plt.title(&#039;Support Vectors Circled&#039;)\n    for vector in supportVectors:\n        circle = Circle(vector, 0.5, facecolor=&#039;none&#039;, edgecolor=(0,0.8,0.8), linewidth=2, alpha=0.5)\n        ax.add_patch(circle)\n \n    #plt.plot([2.3,8.5], [-6,6]) #seperating hyperplane\n    b = -3.75567; w0=0.8065; w1=-0.2761\n    x = arange(-2.0, 12.0, 0.1)\n    y = (-w0*x - b)/w1\n    ax.plot(x,y)\n    ax.axis([-2,12,-8,6])\n    plt.show()\n圈出支持向量\nplotSupportVectors(supportVectors)\n\n利用完整的SMO算法加速优化\nclass optStruct:\n    def __init__(self,dataMatIn, classLabels, C, toler, kTup):  # Initialize the structure with the parameters \n        self.X = dataMatIn\n        self.labelMat = classLabels\n        self.C = C\n        self.tol = toler\n        self.m = shape(dataMatIn)[0]\n        self.alphas = mat(zeros((self.m,1)))\n        self.b = 0\n        self.eCache = mat(zeros((self.m,2))) #first column is valid flag\n        \ndef calcEk(oS, k):\n    fXk = float(multiply(oS.alphas, oS.labelMat).T*(oS.X*oS.X[k, :].T)) + oS.b\n    Ek = fXk - float(oS.labelMat[k])\n    return Ek\n        \ndef selectJ(i, oS, Ei):         #this is the second choice -heurstic, and calcs Ej\n    maxK = -1; maxDeltaE = 0; Ej = 0\n    oS.eCache[i] = [1,Ei]  #set valid #choose the alpha that gives the maximum delta E\n    validEcacheList = nonzero(oS.eCache[:,0].A)[0]\n    if (len(validEcacheList)) &gt; 1:\n        for k in validEcacheList:   #loop through valid Ecache values and find the one that maximizes delta E\n            if k == i: continue #don&#039;t calc for i, waste of time\n            Ek = calcEk(oS, k)\n            deltaE = abs(Ei - Ek)\n            if (deltaE &gt; maxDeltaE):\n                maxK = k; maxDeltaE = deltaE; Ej = Ek\n        return maxK, Ej\n    else:   #in this case (first time around) we don&#039;t have any valid eCache values\n        j = selectJrand(i, oS.m)\n        Ej = calcEk(oS, j)\n    return j, Ej\n \ndef updateEk(oS, k):#after any alpha has changed update the new value in the cache\n    Ek = calcEk(oS, k)\n    oS.eCache[k] = [1,Ek]\ndef innerL(i, oS, istraces=True):\n    Ei = calcEk(oS, i)\n    if ((oS.labelMat[i]*Ei &lt; -oS.tol) and (oS.alphas[i] &lt; oS.C)) or ((oS.labelMat[i]*Ei &gt; oS.tol) and (oS.alphas[i] &gt; 0)):\n        j,Ej = selectJ(i, oS, Ei) #this has been changed from selectJrand\n        alphaIold = oS.alphas[i].copy(); alphaJold = oS.alphas[j].copy();\n        if (oS.labelMat[i] != oS.labelMat[j]):\n            L = max(0, oS.alphas[j] - oS.alphas[i])\n            H = min(oS.C, oS.C + oS.alphas[j] - oS.alphas[i])\n        else:\n            L = max(0, oS.alphas[j] + oS.alphas[i] - oS.C)\n            H = min(oS.C, oS.alphas[j] + oS.alphas[i])\n \n        if L==H:\n            if istraces: \n                print(&quot;L==H&quot;)\n            return 0\n \n        eta = 2.0 * oS.X[i,:]*oS.X[j,:].T - oS.X[i,:]*oS.X[i,:].T - oS.X[j,:]*oS.X[j,:].T\n        if eta &gt;= 0:\n            if istraces: \n                print(&quot;eta&gt;=0&quot;)\n            return 0\n \n        oS.alphas[j] -= oS.labelMat[j]*(Ei - Ej)/eta\n        oS.alphas[j] = clipAlpha(oS.alphas[j],H,L)\n        updateEk(oS, j) # added this for the Ecache\n        \n        if (abs(oS.alphas[j] - alphaJold) &lt; 0.00001):\n            if istraces: \n                print(&quot;j not moving enough&quot;)\n            return 0\n \n        oS.alphas[i] += oS.labelMat[j]*oS.labelMat[i]*(alphaJold - oS.alphas[j])#update i by the same amount as j\n        updateEk(oS, i) #added this for the Ecache                    #the update is in the oppostie direction\n        b1 = oS.b - Ei - oS.labelMat[i] * (oS.alphas[i]-alphaIold)*oS.X[i,:]*oS.X[i,:].T - oS.labelMat[j]*(oS.alphas[j] - alphaJold)*oS.X[i,:]*oS.X[j,:].T\n        b2 = oS.b - Ei - oS.labelMat[i] * (oS.alphas[i]-alphaIold)*oS.X[i,:]*oS.X[i,:].T - oS.labelMat[j]*(oS.alphas[j] - alphaJold)*oS.X[j,:]*oS.X[j,:].T\n        \n        \n        if (0 &lt; oS.alphas[i]) and (oS.C &gt; oS.alphas[i]):\n            oS.b = b1\n        elif (0 &lt; oS.alphas[j]) and (oS.C &gt; oS.alphas[j]):\n            oS.b = b2\n        else:\n            oS.b = (b1 + b2)/2.0\n        return 1\n    \n    else: \n        return 0\ndef smoP(dataMatIn, classLabels, C, toler, maxIter, kTup=(&#039;lin&#039;, 0), istraces=True):    #full Platt SMO\n    oS = optStruct(mat(dataMatIn),mat(classLabels).transpose(),C,toler, kTup)\n    iter = 0\n    entireSet = True; alphaPairsChanged = 0\n    while (iter &lt; maxIter) and ((alphaPairsChanged &gt; 0) or (entireSet)):\n        alphaPairsChanged = 0\n        if entireSet:   # go over all\n            for i in range(oS.m):        \n                alphaPairsChanged += innerL(i, oS)\n                if istraces:\n                    print(&quot;fullSet, iter: %d i:%d, pairs changed %d&quot; % (iter,i,alphaPairsChanged))\n            iter += 1\n        else: # go over non-bound (railed) alphas\n            nonBoundIs = nonzero((oS.alphas.A &gt; 0) * (oS.alphas.A &lt; C))[0]\n            for i in nonBoundIs:\n                alphaPairsChanged += innerL(i, oS, istraces)\n                if istraces:\n                    print(&quot;non-bound, iter: %d i:%d, pairs changed %d&quot; % (iter,i,alphaPairsChanged))\n            iter += 1\n        if entireSet:\n            entireSet = False #toggle entire set loop\n        elif (alphaPairsChanged == 0):\n            entireSet = True  \n        print(&quot;iteration number: %d&quot; % iter)\n    return oS.b, oS.alphas\ndataArr, labelArr = loadDataSet(&#039;MLiA_SourceCode/machinelearninginaction/Ch06/testSet.txt&#039;)\nb, alphas = smoP(dataArr, labelArr, 0.6, 0.001, 40)\nfullSet, iter: 0 i:0, pairs changed 1\nfullSet, iter: 0 i:1, pairs changed 1\nfullSet, iter: 0 i:2, pairs changed 2\nj not moving enough\n...\n...\nfullSet, iter: 2 i:97, pairs changed 0\nfullSet, iter: 2 i:98, pairs changed 0\nfullSet, iter: 2 i:99, pairs changed 0\niteration number: 3\n\nsupportVectors = []\nfor i in range(100):\n    if alphas[i] &gt; 0.0:\n        supportVectors.append(dataArr[i])\n        print(dataArr[i], labelArr[i])\n[3.542485, 1.977398] -1.0\n[7.55151, -1.58003] 1.0\n[8.127113, 1.274372] 1.0\n[7.108772, -0.986906] 1.0\n[6.080573, 0.418886] 1.0\n[3.107511, 0.758367] -1.0\n\nplotSupportVectors(supportVectors)\n\n如何用上面得到的alpha值来进行分类？首先必须基于alpha值得到超平面，计算w。\ndef calcWs(alphas, dataArr, classLabels):\n    X = mat(dataArr)\n    labelMat = mat(classLabels).transpose()\n    m, n = shape(X)\n    w = zeros((n ,1))\n    for i in range(m):\n        w += multiply(alphas[i]*labelMat[i], X[i,:].T)\n    return w\nws = calcWs(alphas, dataArr, labelArr)\nws\narray([[ 0.65139219],\n       [-0.18666913]])\n\ndatMat = mat(dataArr)\ndatMat[0]*mat(ws)+b\nmatrix([[-0.94421679]])\n\n如果该值大于0那么其属于1类，小于0则属于-1类。对于dataMat[0]点应该时类别-1，验证检查：\nlabelArr[0]\n-1.0\n\n写个函数全部检查一遍看看\ndef checkResult(alphas, b, dataArr, labelArr):\n    ws = calcWs(alphas, dataArr, labelArr)\n    datMat = mat(dataArr)\n    n = len(datMat)\n    errorCount = 0.0\n    for i in range(n):\n        result = datMat[i]*mat(ws)+b\n        result = 1.0 if float(result) &gt; 0 else -1.0\n        if result != labelArr[i]:\n            errorCount += 1.0\n    print(&quot;the error tate of this test is %f&quot; % float(errorCount/n))\ncheckResult(alphas, b, dataArr, labelArr)\nthe error tate of this test is 0.000000\n\n测试结果全部都分类正确\n在复杂的数据上应用核函数\n利用核函数将数据映射到高维空间\n径向基核函数\n%run MLiA_SourceCode/machinelearninginaction/Ch06/plotRBF.py\n\ndef kernelTrans(X, A, kTup): #calc the kernel or transform data to a higher dimensional space\n    m,n = shape(X)\n    K = mat(zeros((m,1)))\n    if kTup[0]==&#039;lin&#039;:\n        K = X * A.T   #linear kernel\n    elif kTup[0]==&#039;rbf&#039;:\n        for j in range(m):\n            deltaRow = X[j,:] - A\n            K[j] = deltaRow*deltaRow.T\n        K = exp(K/(-1*kTup[1]**2)) #divide in NumPy is element-wise not matrix like Matlab\n    else: raise NameError(&#039;Houston We Have a Problem -- \\\n    That Kernel is not recognized&#039;)\n    return K\n \nclass optStruct:\n    def __init__(self,dataMatIn, classLabels, C, toler, kTup):  # Initialize the structure with the parameters \n        self.X = dataMatIn\n        self.labelMat = classLabels\n        self.C = C\n        self.tol = toler\n        self.m = shape(dataMatIn)[0]\n        self.alphas = mat(zeros((self.m,1)))\n        self.b = 0\n        self.eCache = mat(zeros((self.m,2))) #first column is valid flag\n        self.K = mat(zeros((self.m,self.m)))\n        for i in range(self.m):\n            self.K[:,i] = kernelTrans(self.X, self.X[i,:], kTup)\ndef innerL(i, oS, istraces=False):\n    Ei = calcEk(oS, i)\n    if ((oS.labelMat[i]*Ei &lt; -oS.tol) and (oS.alphas[i] &lt; oS.C)) or ((oS.labelMat[i]*Ei &gt; oS.tol) and (oS.alphas[i] &gt; 0)):\n        j,Ej = selectJ(i, oS, Ei) #this has been changed from selectJrand\n        alphaIold = oS.alphas[i].copy(); alphaJold = oS.alphas[j].copy();\n        if (oS.labelMat[i] != oS.labelMat[j]):\n            L = max(0, oS.alphas[j] - oS.alphas[i])\n            H = min(oS.C, oS.C + oS.alphas[j] - oS.alphas[i])\n        else:\n            L = max(0, oS.alphas[j] + oS.alphas[i] - oS.C)\n            H = min(oS.C, oS.alphas[j] + oS.alphas[i])\n \n        if L==H:\n            if istraces: \n                print(&quot;L==H&quot;)\n            return 0\n \n        eta = 2.0 * oS.K[i,j] - oS.K[i,i] - oS.K[j,j] #changed for kernel\n        if eta &gt;= 0:\n            if istraces: \n                print(&quot;eta&gt;=0&quot;)\n            return 0\n \n        oS.alphas[j] -= oS.labelMat[j]*(Ei - Ej)/eta\n        oS.alphas[j] = clipAlpha(oS.alphas[j],H,L)\n        updateEk(oS, j) # added this for the Ecache\n        \n        if (abs(oS.alphas[j] - alphaJold) &lt; 0.00001):\n            if istraces: \n                print(&quot;j not moving enough&quot;)\n            return 0\n \n        oS.alphas[i] += oS.labelMat[j]*oS.labelMat[i]*(alphaJold - oS.alphas[j])#update i by the same amount as j\n        updateEk(oS, i) #added this for the Ecache                    #the update is in the oppostie direction\n        b1 = oS.b - Ei- oS.labelMat[i]*(oS.alphas[i]-alphaIold)*oS.K[i,i] - oS.labelMat[j]*(oS.alphas[j]-alphaJold)*oS.K[i,j]\n        b2 = oS.b - Ej- oS.labelMat[i]*(oS.alphas[i]-alphaIold)*oS.K[i,j]- oS.labelMat[j]*(oS.alphas[j]-alphaJold)*oS.K[j,j]\n        \n        if (0 &lt; oS.alphas[i]) and (oS.C &gt; oS.alphas[i]):\n            oS.b = b1\n        elif (0 &lt; oS.alphas[j]) and (oS.C &gt; oS.alphas[j]):\n            oS.b = b2\n        else:\n            oS.b = (b1 + b2)/2.0\n        return 1\n    \n    else: \n        return 0\ndef calcEk(oS, k):\n    fXk = float(multiply(oS.alphas,oS.labelMat).T*oS.K[:,k] + oS.b)\n    Ek = fXk - float(oS.labelMat[k])\n    return Ek\ndef testRbf(k1=1.3):\n    dataArr,labelArr = loadDataSet(&#039;MLiA_SourceCode/machinelearninginaction/Ch06/testSetRBF.txt&#039;)\n    b,alphas = smoP(dataArr, labelArr, 200, 0.0001, 10000, (&#039;rbf&#039;, k1)) #C=200 important\n    datMat=mat(dataArr); labelMat = mat(labelArr).transpose()\n    svInd=nonzero(alphas.A&gt;0)[0]\n    sVs=datMat[svInd] #get matrix of only support vectors\n    labelSV = labelMat[svInd];\n    print(&quot;there are %d Support Vectors&quot; % shape(sVs)[0])\n    m,n = shape(datMat)\n    errorCount = 0\n    for i in range(m):\n        kernelEval = kernelTrans(sVs,datMat[i,:],(&#039;rbf&#039;, k1))\n        predict=kernelEval.T * multiply(labelSV,alphas[svInd]) + b\n        if sign(predict)!=sign(labelArr[i]): errorCount += 1\n    print(&quot;the training error rate is: %f&quot; % (float(errorCount)/m))\n    dataArr,labelArr = loadDataSet(&#039;MLiA_SourceCode/machinelearninginaction/Ch06/testSetRBF2.txt&#039;)\n    errorCount = 0\n    datMat=mat(dataArr); labelMat = mat(labelArr).transpose()\n    m,n = shape(datMat)\n    for i in range(m):\n        kernelEval = kernelTrans(sVs,datMat[i,:],(&#039;rbf&#039;, k1))\n        predict=kernelEval.T * multiply(labelSV,alphas[svInd]) + b\n        if sign(predict)!=sign(labelArr[i]): errorCount += 1    \n    print(&quot;the test error rate is: %f&quot; % (float(errorCount)/m))\n在测试中使用核函数\n用加入了核函数的算法再次训练\ntestRbf()\nfullSet, iter: 0 i:0, pairs changed 1\nfullSet, iter: 0 i:1, pairs changed 1\n...\n...\nfullSet, iter: 6 i:99, pairs changed 0\niteration number: 7\nthere are 29 Support Vectors\nthe training error rate is: 0.070000\nthe test error rate is: 0.050000\n\ntestRbf(0.1)\nfullSet, iter: 0 i:0, pairs changed 1\nfullSet, iter: 0 i:1, pairs changed 2\n...\n...\niteration number: 7\nthere are 89 Support Vectors\nthe training error rate is: 0.000000\nthe test error rate is: 0.070000\n\n当k1=0.1时候，支持向量为89个，k1=1.3的时候时29个，当减小σ，训练错误率就会降低，但测试错误率就会上升。\n支持向量的数目存在一个最优值。SVM的优点在于它能对数据进行高效分类。如果支持向量太少，就可能会得到一个很差的决策边界；如果支持向量太多，也就相当于每次都利用整个数据集进行分类，这种分类法发称为K近邻。\n实例：手写识别问题回顾\n基于SVM的数字识别\n\n收集数据\n准备数据：基于二值图像构造向量\n分析数据：对图像向量进行目测\n训练算法：采用两种不同的核函数，并对径向（radial direction）基核函数采用不同的设置来运行SMO算法\n测试算法：编写一个函数来测试不同的核函数并计算错误率\n使用算法\n\n首先把第二章的img2vector()函数复制过来。\ndef img2vector(filename):\n    returnVect = zeros((1,1024))\n    fr = open(filename)\n    for i in range(32):\n        lineStr = fr.readline()\n        for j in range(32):\n            returnVect[0,32*i+j] = int(lineStr[j])\n    return returnVect\n \ndef loadImages(dirName):\n    from os import listdir\n    hwLabels = []\n    trainingFileList = listdir(dirName)         # load the training set\n    m = len(trainingFileList)\n    trainingMat = zeros((m,1024))\n    for i in range(m):\n        fileNameStr = trainingFileList[i]\n        fileStr = fileNameStr.split(&#039;.&#039;)[0]     # take off .txt\n        classNumStr = int(fileStr.split(&#039;_&#039;)[0])\n        if classNumStr == 9:\n            hwLabels.append(-1)\n        else:\n            hwLabels.append(1)\n        trainingMat[i,:] = img2vector(&#039;%s/%s&#039; % (dirName, fileNameStr))\n    return trainingMat, hwLabels\n \ndef testDigits(kTup=(&#039;rbf&#039;, 10), istrances=False):\n    dataArr,labelArr = loadImages(&#039;MLiA_SourceCode/machinelearninginaction/Ch06/digits/trainingDigits&#039;)\n    b,alphas = smoP(dataArr, labelArr, 200, 0.0001, 10000, kTup, istrances)\n    datMat=mat(dataArr)\n    labelMat = mat(labelArr).transpose()\n    svInd=nonzero(alphas.A&gt;0)[0]\n    sVs=datMat[svInd] \n    labelSV = labelMat[svInd];\n    \n    print(&quot;there are %d Support Vectors&quot; % shape(sVs)[0])\n    supportVectors = shape(sVs)[0]\n    m,n = shape(datMat)\n    errorCount = 0\n    \n    for i in range(m):\n        kernelEval = kernelTrans(sVs,datMat[i,:],kTup)\n        predict = kernelEval.T * multiply(labelSV,alphas[svInd]) + b\n        if sign(predict) != sign(labelArr[i]):\n            errorCount += 1\n    trainErrorRate = float(errorCount)/m        \n    print(&quot;the training error rate is: %f&quot; % (float(errorCount)/m))\n    dataArr,labelArr = loadImages(&#039;MLiA_SourceCode/machinelearninginaction/Ch06/digits/testDigits&#039;)\n    errorCount = 0\n    datMat=mat(dataArr) \n    labelMat = mat(labelArr).transpose()\n    m,n = shape(datMat)\n    \n    for i in range(m):\n        kernelEval = kernelTrans(sVs,datMat[i,:],kTup)\n        predict=kernelEval.T * multiply(labelSV,alphas[svInd]) + b\n        if sign(predict) != sign(labelArr[i]):\n            errorCount += 1\n \n    print(&quot;the test error rate is: %f&quot; % (float(errorCount)/m))\n    testErrorTate = float(errorCount)/m\n    return trainErrorRate, testErrorTate, supportVectors\n尝试不同的参数和线性核函数来学习：\nparameters = [[&#039;rbf&#039;, 0.1],\n             [&#039;rbf&#039;, 5],\n             [&#039;rbf&#039;, 10],\n             [&#039;rbf&#039;, 50],\n             [&#039;rbf&#039;, 100],\n             [&#039;lin&#039;, 0]]\ntrainErrorRate = []\ntestErrorTate = []\nsupportVectors = []\n \nfor i, j in parameters:\n    result = testDigits(kTup=(i, j))\n    trainErrorRate.append(result[0])\n    testErrorTate.append(result[1])\n    supportVectors.append(result[2])\niteration number: 1\niteration number: 2\niteration number: 3\n...\n...\niteration number: 9\nthere are 39 Support Vectors\nthe training error rate is: 0.000000\nthe test error rate is: 0.021505\n\nprint(&quot;内核,设置\\t训练错误率\\t测试错误率\\t支持向量数&quot;)\nfor i in range(len(parameters)):\n    print(&quot;%s,%.1f \\t%.4f \\t\\t%.4f \\t\\t%d&quot; %(parameters[i][0], parameters[i][1], trainErrorRate[i], testErrorTate[i], supportVectors[i]))\n内核,设置\t训练错误率\t测试错误率\t支持向量数\nrbf,0.1 \t0.0000 \t\t0.5215 \t\t402\nrbf,5.0 \t0.0000 \t\t0.0323 \t\t402\nrbf,10.0 \t0.0000 \t\t0.0054 \t\t132\nrbf,50.0 \t0.0149 \t\t0.0269 \t\t31\nrbf,100.0 \t0.0050 \t\t0.0108 \t\t34\nlin,0.0 \t0.0000 \t\t0.0215 \t\t39\n\n观察发现，最小的训练错误率并不对应最小的支持向量数，线性核函数的效果并不是特别糟糕。可以牺牲线性核函数的错误率来换取分类速度的提高。\n总结\n支持向量机时一种分类器，之所以称为“机”时因为它会产生一个二值决策结果，即它是一种决策“机”，支持向量机的泛化错误率较低，也就是说它具有良好的学习能力，并且学到的结果具有很好的推广性。\n核函数从一个低纬空间映射到一个高纬空间，可以将一个低维空间中的非线性问题转换为高纬度空间下的线性问题来求解。\n支持向量机是一个二分类器。当解决多分类问题时，则需要额外的方法对其进行扩展，SVM的效果也对优化参数和所用核函数中的参数敏感。"},"机器学习实战/7.-机器学习实战":{"slug":"机器学习实战/7.-机器学习实战","filePath":"机器学习实战/7. 机器学习实战.md","title":"7. 机器学习实战","links":[],"tags":["AdaBoost","bagging","boosting","ROC"],"content":"利用AdaBoost元算法提高分类性能\n在做决定时，大家可能会吸取多个专家而不是一个人的意见，机器学习也有类似的算法，这就是元算法（meta-algorithm）。\n元算法是对其他算法进行组合的一种方式。\n基于数据集多重抽样的分类器\n前面已经学习了五种不同的分类算法，它们各有优缺点，我们可以将不同的分类器组合起来，这种组合结果则呗称为集成方法（ensemble method）或者元算法（meta-algorithm）。\n集成方法有多种形式：不同算法的集成，同一种算法不同设置的集成，数据集不同部分分配给不同分类器之后的集成，同一种分类器多个不同实例的两种计算方法。\nAdaBoost\n优点：泛化错误率低，易编码，可以应用在大部分分类器上，无参数调整\n缺点：对离群点敏感\n适用数据类型：数值型和标称型数据\n\nbagging：基于数据随机抽样的分类器构建方法\n自举汇聚法（bootstrap aggregating），也称为bagging方法，从原始集合随机选择一个样本，然后随机选择的样本来代替这个样本，意为有放回的取样得到的。\n在有放回抽样得到S个数据集后，将某个学习算法分别作用在每个数据集上，就得到了S个分类器，对新数据进行分类时，用这个S个分类器进行投票表决的方法决定分类结果。\n其他bagging的方法：随机森林（random forest）\nboosting\nboosting是一种类似bagging的方法。bagging的分类器时通过串行训练而获得的，每个新分类器都根据已训练处的分类器的性能来进行训练。boosting是通过集中关注被已有分类器错分的那些数据来获得新的分类器。\n由于boosting分类结果是基于所有分类器的加权求和结果的，因此boosting与bagging不一样。\nbagging中的分类权重是相等的，boosting中的分类器权重并不相等，每个权重代表的是其对应分类器在上一轮迭代中的成功度。\nAdaBoost的一般流程\n\n收集数据\n准备数据：依赖于所有使用的弱分类器类型，本章使用的是单层决策树，这种分类器可以处理任何数据类型。作为弱分类器，简单的分类器效果更好。\n分析数据\n训练算法：AdaBoost的大部分时间都在训练上，分类器将多次在同一数据集上训练弱分类器\n测试算法：计算分类的错误率\n使用算法：同SVM一样，AdaBoost预测两个类别中的一个，如果要应用在多分类问题，要进行修改。\n\n训练算法：基于错误提升分类器的性能\nAdaBoost是adaptive boosting（自适应boosting）的缩写，其运行过程如下：训练数据中的每个样本，并赋予其中一个权重，这些权重构成向量D。\n一开始，这些权重都初始化成相等值，首先在训练数据上训练出一个弱分类器并计算该分类器的错误率，然后在同一数据集上再次训练弱分类器。\n在分类器的第二次训练中，将重新调整每个样本的权重，其中第一次分对的样本权重将会降低，而第一次分错的样本权重将会提高。为了从所有弱分类器中得到最终的分类结果\nAdaBoost为每个分类器都分配一个权重值alpha，这些alpha值是基于每个弱分类器的错误率进行计算的。\n其中错误率ε的定义为：\n\\epsilon=\\frac{未正确分类的样本数目}{所有样本数目}\nalpha的计算公式如下：\n\\alpha=\\frac{1}{2}ln(\\frac{1-\\epsilon}{\\epsilon})\n计算流程图如下：\n\n左边是数据集，其中直方图的不同宽度表示样本的权重。在经过一个分类器后，加权的预测结果会通过三角形的alpha进行加权。每个三角形中输出的加权结果在圆形中求和，得到输出结果。\n计算出alpha值后，可以对权重向量D进行更新。\n如果样本被正确分类，权重降低\n\n如果样本被错分，权重增加\n\n在计算出D之后，AdaBoost又开始进行下一轮迭代。AdaBoost算法会不断的重复训练和调整权重，直到训练错误率为0或者达到指定次数。\n基于单层决策树构建弱分类器\n单层决策树（decision stump，也称决策树桩）是一种简单的决策树。先构建一个简单的数据集。\nfrom numpy import *\ndef loadSimpData():\n    datMat = matrix([[1., 2.1],\n                     [2., 1.1],\n                     [1.3, 1.],\n                     [1., 1.],\n                     [2., 1.]])\n    classLabels = [1.0, 1.0, -1.0, -1.0, 1.0]\n    return datMat, classLabels\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Circle\nplt.rcParams[&#039;axes.unicode_minus&#039;]=False # 用来正常显示负号\n \ndef plotSupportVectors():\n    xcord0 = []\n    ycord0 = []\n    xcord1 = []\n    ycord1 = []\n    \n    datMat, classLabels = loadSimpData()\n    for i in range(len(classLabels)):\n        if (classLabels[i] == -1):\n            xcord0.append(datMat[i, 0])\n            ycord0.append(datMat[i, 1])\n        else:\n            xcord1.append(datMat[i, 0])\n            ycord1.append(datMat[i, 1])\n \n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ax.scatter(xcord0, ycord0, marker=&#039;s&#039;, s=90)\n    ax.scatter(xcord1, ycord1, marker=&#039;o&#039;, s=50, c=&#039;red&#039;)\n    plt.title(&#039;单层决策树测试数据&#039;)\n \n    plt.show()\nplotSupportVectors()\n\n如果想要选择一个与坐标轴平行的线来把圆点和方形分开是不可能的，这就是单层决策树难以处理的一个著名问题。通过使用多棵单层决策树我们就可以构建出一个能够正确处理该数据集的分类器。\ndatMat, classLabels = loadSimpData()\n接下来构建单层决策树\n第一个函数用来测试是否有某个值小于或者大于我们正在测试的阈值。\n第二个函数是在一个加权数据集中循环，并找到具有低错误率的单层决策树\n伪代码：\n将最小错误率minError设备+∞\n对数据集中的每一个特征（第一层循环）：\n    对每个步长（第二层循环）：\n        对每个不等号（第三层循环）：\n            建立一棵单层决策树并利用加权数据集对它测试\n            如果错误率低于minError，则将当前单层决策树设为最佳单层决策树\n返回最佳单层决策树\n\ndef stumpClassify(dataMatrix, dimen, threshVal, threshIneq):\n    retArray = ones((shape(dataMatrix)[0], 1))\n    if threshIneq == &#039;lt&#039;:\n        retArray[dataMatrix[:, dimen] &lt;= threshVal] = -1.0\n    else:\n        retArray[dataMatrix[:, dimen] &gt; threshVal] = -1.0\n    return retArray\n \ndef buildStump(dataArr, classLabels, D):\n    dataMatrix = mat(dataArr)\n    labelMat = mat(classLabels).T\n    m, n = shape(dataMatrix)\n    numSteps = 10.0\n    bestStump = {}\n    bestClasEst = mat(zeros((m, 1)))\n    minError = inf\n    \n    for i in range(n):\n        rangeMin = dataMatrix[:, i].min()\n        rangeMax = dataMatrix[:, i].max()\n        stepSize = (rangeMax-rangeMin)/numSteps\n        for j in range(-1, int(numSteps)+1):\n            for inequal in [&#039;lt&#039;, &#039;gt&#039;]:\n                threshVal = (rangeMin + float(j) * stepSize)\n                predictedVals = stumpClassify(dataMatrix, i, threshVal, inequal)\n                errArr = mat(ones((m, 1)))\n                errArr[predictedVals == labelMat] = 0\n                weightedError = D.T*errArr\n                #print(&quot;split: dim %d, thresh %.2f, thresh ineqal: %s, the weighted error is %.3f&quot; % (i, threshVal, inequal, weightedError))\n                if weightedError &lt; minError:\n                    minError = weightedError\n                    bestClasEst = predictedVals.copy()\n                    bestStump[&#039;dim&#039;] = i\n                    bestStump[&#039;thresh&#039;] = threshVal\n                    bestStump[&#039;ineq&#039;] = inequal\n    \n    return bestStump, minError, bestClasEst\nD = mat(ones((5, 1))/5)\nbuildStump(datMat, classLabels, D)\n({&#039;dim&#039;: 0, &#039;thresh&#039;: 1.3, &#039;ineq&#039;: &#039;lt&#039;},\n matrix([[0.2]]),\n array([[-1.],\n        [ 1.],\n        [-1.],\n        [-1.],\n        [ 1.]]))\n\n第一个函数stumpClassify()是通过阈值比较对数据进行分类的。所有在阈值一边的数据会分到类别-1，而在另一边的数据分到类别+1。\n第二个函数buildStump()将会遍历stumpClassify()函数所有的可能性，并找到数据集上最佳的单层决策树。这里的最佳是基于数据权重向量D来定义的。\nbestStump的字典用于储存给定权重向量D时所得到的最佳单层决策树的相关信息。\n变量numSteps用于在特征的所有可能值上进行遍历。而变量minError则在开始初始化为无穷大，之后寻找可能的最小错误率。\n这个单层决策树的核心思想就是找到一个数，通过比较这个数，找到拟合正确率最大的。\n结果dim=0，代表用第一列比较，thresh=1.3，ineq=lt，代表小于等于1.3的结果\n第一列：\ndatMat[:,0]\nmatrix([[1. ],\n        [2. ],\n        [1.3],\n        [1. ],\n        [2. ]])\n\n这一列大于1.3的为1，小于等于1.3的为-1\ndatMat[:,0] &lt;= 1.3\nmatrix([[ True],\n        [False],\n        [ True],\n        [ True],\n        [False]])\n\n所以预测结果为：[-1,1-1,-1,1] 正确率为4/5，所以错误率为0.2。\n这就是一个单层决策树，只考虑一列数的影响，而非全局。\n完整AdaBoost\n我们利用上面的单层决策树来实现完整的Adaboost，伪代码如下：\n对每次迭代：\n    利用buildStump()函数找到最佳的单层决策树\n    将最佳单层决策树加入到单层决策树数组\n    计算alpha\n    计算新的权重向量D\n    更新累计类别估计值\n    如果错误率等于0.0则退出循环\n\ndef adaBoostTrainDS(dataArr, classLabels, numIt=40):\n    weakClassArr = []\n    m = shape(dataArr)[0]\n    D = mat(ones((m,1))/m)\n    aggClassEst = mat(zeros((m, 1)))\n    for i in range(numIt):\n        bestStump, error, classEst = buildStump(dataArr, classLabels, D)\n        #print(&quot;D:&quot;,D.T)\n        alpha = float(0.5*log((1.0-error)/max(error, 1e-16)))\n        bestStump[&#039;alpha&#039;] = alpha\n        weakClassArr.append(bestStump) \n        #print(&quot;classEst: &quot;, classEst.T)\n        expon = multiply(-1*alpha*mat(classLabels).T, classEst) # 混淆矩阵\n        D = multiply(D, exp(expon))\n        D = D/D.sum()\n        aggClassEst += alpha*classEst\n        #print(&quot;aggClassEst: &quot;, aggClassEst.T)\n        aggErrors = multiply(sign(aggClassEst) != mat(classLabels).T, ones((m, 1)))\n        #print(&quot;predictedVals: &quot;, sign(aggClassEst).T)\n        errorRate = aggErrors.sum()/m\n        #print(&quot;total error: &quot;, errorRate, &quot;\\n&quot;)\n        if errorRate == 0.0:\n            break\n    return weakClassArr, aggClassEst\nadaBoostTrainDS(datMat, classLabels, 9)\nD: [[0.2 0.2 0.2 0.2 0.2]]\nclassEst:  [[-1.  1. -1. -1.  1.]]\naggClassEst:  [[-0.69314718  0.69314718 -0.69314718 -0.69314718  0.69314718]]\npredictedVals:  [[-1.  1. -1. -1.  1.]]\ntotal error:  0.2 \n\nD: [[0.5   0.125 0.125 0.125 0.125]]\nclassEst:  [[ 1.  1. -1. -1. -1.]]\naggClassEst:  [[ 0.27980789  1.66610226 -1.66610226 -1.66610226 -0.27980789]]\npredictedVals:  [[ 1.  1. -1. -1. -1.]]\ntotal error:  0.2 \n\nD: [[0.28571429 0.07142857 0.07142857 0.07142857 0.5       ]]\nclassEst:  [[1. 1. 1. 1. 1.]]\naggClassEst:  [[ 1.17568763  2.56198199 -0.77022252 -0.77022252  0.61607184]]\npredictedVals:  [[ 1.  1. -1. -1.  1.]]\ntotal error:  0.0 \n\n[{&#039;dim&#039;: 0, &#039;thresh&#039;: 1.3, &#039;ineq&#039;: &#039;lt&#039;, &#039;alpha&#039;: 0.6931471805599453},\n {&#039;dim&#039;: 1, &#039;thresh&#039;: 1.0, &#039;ineq&#039;: &#039;lt&#039;, &#039;alpha&#039;: 0.9729550745276565},\n {&#039;dim&#039;: 0, &#039;thresh&#039;: 0.9, &#039;ineq&#039;: &#039;lt&#039;, &#039;alpha&#039;: 0.8958797346140273}]\n\n结果包含三个字典，其中包含了分类所需要的所有信息。\n测试算法：基于AdaBoost的分类\ndef adaClassify(datToClass, classifierArr):\n    dataMatrix = mat(datToClass)\n    m = shape(dataMatrix)[0]\n    aggClassEst = mat(zeros((m, 1)))\n    for i in range(len(classifierArr)):\n        classEst = stumpClassify(dataMatrix, classifierArr[i][&#039;dim&#039;], classifierArr[i][&#039;thresh&#039;], classifierArr[i][&#039;ineq&#039;])\n        aggClassEst += classifierArr[i][&#039;alpha&#039;]*classEst\n        #print(aggClassEst.T)\n    return sign(aggClassEst)\nclassifierArr = adaBoostTrainDS(datMat, classLabels, 30)\nadaClassify([0, 0], classifierArr)\nD: [[0.2 0.2 0.2 0.2 0.2]]\nclassEst:  [[-1.  1. -1. -1.  1.]]\naggClassEst:  [[-0.69314718  0.69314718 -0.69314718 -0.69314718  0.69314718]]\npredictedVals:  [[-1.  1. -1. -1.  1.]]\ntotal error:  0.2 \n\nD: [[0.5   0.125 0.125 0.125 0.125]]\nclassEst:  [[ 1.  1. -1. -1. -1.]]\naggClassEst:  [[ 0.27980789  1.66610226 -1.66610226 -1.66610226 -0.27980789]]\npredictedVals:  [[ 1.  1. -1. -1. -1.]]\ntotal error:  0.2 \n\nD: [[0.28571429 0.07142857 0.07142857 0.07142857 0.5       ]]\nclassEst:  [[1. 1. 1. 1. 1.]]\naggClassEst:  [[ 1.17568763  2.56198199 -0.77022252 -0.77022252  0.61607184]]\npredictedVals:  [[ 1.  1. -1. -1.  1.]]\ntotal error:  0.0 \n\n[[-0.69314718]]\n[[-1.66610226]]\n[[-2.56198199]]\n\n\nmatrix([[-1.]])\n\nadaClassify([[5, 5], [0, 0]], classifierArr)\n[[ 0.69314718 -0.69314718]]\n[[ 1.66610226 -1.66610226]]\n[[ 2.56198199 -2.56198199]]\n\n\nmatrix([[ 1.],\n        [-1.]])\n\n实例：在一个复杂数据集上应用AdaBoost\n\n收集数据\n准备数据：确保类别标签是+1和-1而非0和1\n分析数据\n训练算法：在数据上，利用adaBoostTrainDS()函数训练出一系列分类器\n测试算法：用AdaBoost和Logistic回归对比\n使用算法：观察该例子上的错误率\n\ndef loadDataSet(fileName):\n    numFeat = len(open(fileName).readline().split(&#039;\\t&#039;))\n    dataMat = []; labelMat = []\n    fr = open(fileName)\n    for line in fr.readlines():\n        lineArr =[]\n        curLine = line.strip().split(&#039;\\t&#039;)\n        for i in range(numFeat-1):\n            lineArr.append(float(curLine[i]))\n        dataMat.append(lineArr)\n        labelMat.append(float(curLine[-1]))\n    return dataMat,labelMat\ndataArr, labelArr = loadDataSet(&#039;MLiA_SourceCode/machinelearninginaction/Ch07/horseColicTraining2.txt&#039;)\nclassifierArray = adaBoostTrainDS(dataArr, labelArr, 10)\ntestArr, testlabelArr = loadDataSet(&#039;MLiA_SourceCode/machinelearninginaction/Ch07/horseColicTest2.txt&#039;)\nprediction10 = adaClassify(testArr, classifierArray)\nerrArr=mat(ones((67, 1)))\nerrArr[prediction10 != mat(testlabelArr).T].sum()\n16.0\n\n分类错误为16个，错误率23%，明显优于逻辑回归的33%。\nclassifierNum = [1, 10 , 50, 100, 500, 1000, 10000]\ntrainerrArr=mat(ones((299, 1)))\ntesterrArr=mat(ones((67, 1)))\nprint(&quot;分类器数目\\t训练错误率\\t测试错误率&quot;)\nfor n in classifierNum:\n    classifierArray = adaBoostTrainDS(dataArr, labelArr, n)\n    trainPrediction = adaClassify(dataArr, classifierArray)\n    testPrediction = adaClassify(testArr, classifierArray)\n    trainErrRate = trainerrArr[trainPrediction != mat(labelArr).T].sum()/299\n    testErrRate = testerrArr[testPrediction != mat(testlabelArr).T].sum()/67\n    print(&quot;%d\\t\\t%.2f\\t\\t%.2f&quot; % (n, trainErrRate, testErrRate))\n分类器数目\t训练错误率\t测试错误率\n1\t\t0.28\t\t0.27\n10\t\t0.23\t\t0.24\n50\t\t0.19\t\t0.21\n100\t\t0.19\t\t0.22\n500\t\t0.16\t\t0.25\n1000\t\t0.14\t\t0.31\n10000\t\t0.11\t\t0.33\n\n观察上表发现，随着分类器的增加训练错误率在减小，测试错误率在达到一个最小值后又开始上升，这种现象称为过拟合（overfitting）。\nAdaBoost和SVM有很多相似之处，我们可以把弱分类器想象成SVM的一个核函数，也可以按照最大化某个最小间隔的方式重写AdaBoost算法。而它们的不同之处在于其所定义的间隔计算方式有所不同，因此导致结果也不同。特别是在高纬度空间下，这两者之间的差异就更加明显。\n非均衡问题分类\n之前我们假设类别的分类代价是一样的，这样就会出现一系列问题，例如：我们预测马会死，人们就可能给马实施安乐死而不是通过治疗来避免死亡，我们的预测也许是错误的，马本来可以继续活着，毕竟我们的分类器只有80%的精确率。\n如果过滤垃圾邮件，合法的邮件也被认为是垃圾邮件呢，癌症检测情愿误判也不能漏判。\n在大多数情况下不同类别的分类代价并不相等。接下来讨论一种新的分类器度量方法。\n其他分类性能度量指标：正确率、召回率、及ROC曲线\n错误率指的是在所有测试样例中错分的样例比例，这样的度量错误掩盖了样例如何被分错的事实。\n在机器学习中有一个普遍适用的称为混淆矩阵（confusion matrix）的工具，它可以帮助人们更好的了解分类中的错误，有这样一个关于在房子周围可能发现的动物类型的预测。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n.预测结果真狗猫鼠实狗2425结猫2270果鼠4230\n利用混淆矩阵就可以更好地理解分类中的错误了。如果矩阵非对角元素均为0，就会得到一个完美的分类器。\n接下来考虑另外一个混淆矩阵。\n.  || 预测结果||\n---|---|---|---|---\n真 |  | +1 | -1\n实 | +1 | 真正例（TP） | 伪反例（FN）\n结 | -1 | 伪证例（FP） | 真反例（TN）\n果 |\n正确率（precision） = TP/(TP+FP)\n召回率（Recall） = TP/(TP+FN)\n我们可以构造一个高正确率或搞召回率的分类器，但是奋男保证两者都成立。\n另一种度量分类中的非均衡性的工具是ROC曲线（ROC curve）,ROC代表接收者操作特征（receiver operating characteristic），他最早在二战期间由电器工程师构建雷达系统时使用过。\ndef plotROC(predStrengths, classLabels):\n    import matplotlib.pyplot as plt\n    cur = (1.0,1.0) # cursor\n    ySum = 0.0 # variable to calculate AUC\n    numPosClas = sum(array(classLabels)==1.0)\n    yStep = 1/float(numPosClas); xStep = 1/float(len(classLabels)-numPosClas)\n    sortedIndicies = predStrengths.argsort() # get sorted index, it&#039;s reverse\n    fig = plt.figure()\n    fig.clf()\n    ax = plt.subplot(111)\n    # loop through all the values, drawing a line segment at each point\n    for index in sortedIndicies.tolist()[0]:\n        if classLabels[index] == 1.0:\n            delX = 0\n            delY = yStep\n        else:\n            delX = xStep\n            delY = 0\n            ySum += cur[1]\n        # draw line from cur to (cur[0]-delX,cur[1]-delY)\n        ax.plot([cur[0],cur[0]-delX],[cur[1],cur[1]-delY], c=&#039;b&#039;)\n        cur = (cur[0]-delX,cur[1]-delY)\n    ax.plot([0,1],[0,1],&#039;b--&#039;)\n    plt.xlabel(&#039;False positive rate&#039;)\n    plt.ylabel(&#039;True positive rate&#039;)\n    plt.title(&#039;ROC curve for AdaBoost horse colic detection system&#039;)\n    ax.axis([0,1,0,1])\n    plt.show()\n    print(&quot;the Area Under the Curve is: &quot;,ySum*xStep)\ndataArr, labelArr = loadDataSet(&#039;MLiA_SourceCode/machinelearninginaction/Ch07/horseColicTraining2.txt&#039;)\nclassifierArray, aggClassEst = adaBoostTrainDS(dataArr, labelArr, 10)\nplotROC(aggClassEst.T, labelArr)\n\nthe Area Under the Curve is:  0.8582969635063604\n\n上图给出两条线，一条实线一条虚线，图中的横轴是伪证例的比例（FP/(FP+TN)），纵轴是真正例的比例（TP/(TP+FN)）。ROC曲线给出的是当阈值变化时假阳率和真阳率的变化情况。\nROC曲线不但可以用于比较分类器，还可以基于成本效益（cost versus benefit）分析来做出决策。由于在不同的阈值下，不同的分类器的表现情况可能各不相同，因此一某种方式将它们组合起来获取更有意义。\n在理想情况下，最佳的分类器应该尽可能处于左上角，这意味着分类器在假阳率很低的同时获得了很高的真阳率。例如过滤了所有的垃圾邮件，没有将合法邮件误识别为垃圾邮件。\nROC曲线下下的面积（Are unser the curve，AUC）AUC给出的时分类器的平均性能值，一个完美的分类器AUC为1，随机猜测的AUC为0.5。\n基于代价函数的分类器决策控制\n处理非均衡问题的数据抽样方法\n欠抽样（undersampling）删除样例\n过抽样（oversampling）复制样例\n总结\n集成方法通过组合多个分类器的分类结果，获得了比简单的分类器更好的分类结果。\n多个分类器组合可能会进一步凸显出单分类器的不足，比如过拟合问题。如果分类器之间差别显著，那么多个分类器组合就可能会缓解这一问题。\n在bagging中，是通过随机抽样的替换方式得到与原始数据集规模一样的数据集，而boosting在数据集上顺序应用了多个不同的分类器。\n非均衡分类问题是指在分类器训练时正例数目和反例数目不相等（相差很大）。该问题在错分正例和反例的代价不同时也存在"},"机器学习实战/8.-机器学习实战":{"slug":"机器学习实战/8.-机器学习实战","filePath":"机器学习实战/8. 机器学习实战.md","title":"8. 机器学习实战","links":[],"tags":["线性回归","岭回归","最小二乘法"],"content":"预测数值型数据：回归\n分类的目标变量是标称型数据，而回归是对连续性数据做出预测。\n用线性回归找到最佳拟合直线\n线性回归\n优点：结果易于理解，计算上不复杂\n缺点：对非线性的数据拟合不好\n适用数据类型：数值型和标称型数据\n\n回归的目的是预测数值型的目标值。最直接的办法是依据输入写出一个目标值的计算公式。\nZ = 0.1*X + 0.2*Y\n\n这就是所谓的回归方程（regression equation）其中0.1和0.2称作回归系数（regression weights），求回归系数的过程就是回归。\n回归的一般方法：\n\n收集数据\n准备数据：回归需要数值型数据，标称型数据将被转换成二进制数据\n分析数据：绘出数据的可视化二维图将有助于对数据做出理解和分析，在采用缩减法求得新回归系数之后，可以将新拟合线绘制在图上作为对比\n训练算法：找到回归系数\n测试算法：适用R^2或者预测值和数据的拟合度，来分析模型的效果\n使用算法：使用回归，可以在给定输入的时候预测出一个数值，这是对分类方法的提升，因为这样可以预测连续性数据而不仅仅是离散的类别标签\n\n假如输入数据为矩阵X，回归系数为向量w，对于给定的数据X_1预测结果将会通过Y_1=X^T_1w得到。\n找到w的方法是找到使误差最小的w，误差指预测值和真实值之间的差值，使用该差值简单的累加将使正差值和负差值相互抵消，所以采用平方误差。\n平方误差可以写作：\n\\sum_{i=1}^m(y_i-x_i^Tw)^2\n用矩阵表示可以写作:\n(y-Xw)^T(y-Xw)\n对w求导，得到X^T(Y-Xw)，令其等于0，解出\nw=(X^TX)^{-1}X^Ty\n(X^TX)^{-1}表示矩阵的逆，矩阵的逆可能并不存在，因此要在代码中作出判断。\n上述方法也称作OLS“普通最小二乘法”（ordinary least squares）\nfrom numpy import *\n \ndef loadDataSet(fileName):      #general function to parse tab -delimited floats\n    numFeat = len(open(fileName).readline().split(&#039;\\t&#039;)) - 1 #get number of fields \n    dataMat = []; labelMat = []\n    fr = open(fileName)\n    for line in fr.readlines():\n        lineArr =[]\n        curLine = line.strip().split(&#039;\\t&#039;)\n        for i in range(numFeat):\n            lineArr.append(float(curLine[i]))\n        dataMat.append(lineArr)\n        labelMat.append(float(curLine[-1]))\n    return dataMat,labelMat\n \ndef standRegres(xArr, yArr):\n    xMat = mat(xArr)\n    yMat = mat(yArr).T\n    xTx = xMat.T*xMat\n    if linalg.det(xTx) == 0.0:\n        print(&quot;This matrix is singular, cannot do inverse&quot;)\n        return\n    ws = xTx.I * (xMat.T*yMat)\n    return ws\nxArr, yArr = loadDataSet(&#039;MLiA_SourceCode/Ch08/ex0.txt&#039;)\nxArr[0:2]\n[[1.0, 0.067732], [1.0, 0.42781]]\n\nws = standRegres(xArr, yArr)\nws\n    matrix([[3.00774324],\n            [1.69532264]])\nxMat = mat(xArr)\nyMat = mat(yArr)\nyHat = xMat*ws\n \nimport matplotlib.pyplot as plt\nfig = plt.figure()\nax = fig.add_subplot(111)\nax.scatter(xMat[:,1].flatten().A[0], yMat.T[:,0].flatten().A[0], 10)\nxCopy = xMat.copy()\nxCopy.sort(0)\nyHat=xCopy*ws\nax.plot(xCopy[:,1], yHat)\nplt.show()\n\n几乎任意数据都可以用上述法放建立模型，有一种方法可以计算预测值yHat序列和真实值y序列的相关系数来鉴别拟合效果。\n在python中可以用numpy库的corrcoef(yEstimate, yActual)来计算相关性。\nyHat = xMat*ws\ncorrcoef(yHat.T, yMat)\narray([[1.        , 0.98647356],\n       [0.98647356, 1.        ]])\n\n上面的矩阵包含所有两两组合的相关系数，对角线为1.0是因为yMat和自己匹配是完美的，yHat和yMat的相关系数为0.98。\n局部加权线性回归\n线性回归有可能出现欠拟合，因为求的具有最小均方误差的无偏估计。可以引入一些偏差，从而降低预测的均方误差。其中一个方法是局部加权线性回归（locally weighted linear regression， LWLR），在该算法中，我们给待预测点附近的每个点赋予一定的权重；然后在这个子集上基于最小均方差来进行普通的回归。与KNN一样，这种算法每次预测需要事先选取出对应的数据子集，该算法解出的回归系数w的形式如下\nw=(X^TWX)^{-1}X^TW_y\n其中w是一个矩阵，用来给每个数据点赋予权重。\nLWLR使用“核”（与支持向量机类似）来对附近的点赋予更高的权重。最常用的是高斯核，因为高斯核符合正态分布，高斯核对应的权重如下：\nw(i,i)=exp(\\frac{|x^{i}-x|}{-2k^2})\n这样就构建了一个只含对角元素的权重矩阵w，并且点x与x(i)越近，w(i,i)将会越大。\n上述公式包含了一个需要用户指定的参数k，它决定了对附近的点赋予多大的权重，这是使用LWLR时唯一要考虑的参数。\n下图可以看到参数k与权重的关系\n\n每个点的权重图（假定我们正预测的点是=0.5），最上面的图是原始数据集，第二个图显示了当k=0.5时，大部分的数据都用于训练回归模型，最下面的图显示的是当k=0.01时，仅有很少的局部点被用于训练回归模型。\ndef lwlr(testPoint, xArr, yArr, k=1.0):\n    xMat = mat(xArr)\n    yMat = mat(yArr).T\n    m = shape(xMat)[0]\n    # 创建对角矩阵\n    weights = mat(eye((m)))\n    for j in range(m):\n        diffMat = testPoint - xMat[j, :]\n        # 权重大小以指数级衰减\n        weights[j, j] = exp(diffMat*diffMat.T/(-2.0*k**2))\n    xTx = xMat.T * (weights * xMat)\n    if linalg.det(xTx) == 0.0:\n        print(&#039;This matrix is singular, cannot do inverse&#039;)\n        return\n    ws = xTx.I * (xMat.T * (weights * yMat))\n    return testPoint * ws\n \ndef lwlrTest(testArr, xArr, yArr, k=1.0):\n    m = shape(testArr)[0]\n    yHat = zeros(m)\n    for i in range(m):\n        yHat[i] = lwlr(testArr[i], xArr, yArr, k)\n    return yHat\nlwlr()的作用是，给定x空间中的任意一点，计算出对应的预测值yHat。权重矩阵是一个方阵，阶数等于样本点个数。也就是说，该矩阵为每个样本初始化了一个权重。接着遍历数据集，计算每个样本点对应的权重，随着样本点与待预测点距离的递增，权重将以指数级衰减。输入参数K控制衰减速度。\n测试算法，对单点进行估计：\nxArr[:2], yArr[:2]\n([[1.0, 0.067732], [1.0, 0.42781]], [3.176513, 3.816464])\n\nlwlr(xArr[0], xArr, yArr, 1.0)\nmatrix([[3.12204471]])\n\nlwlr(xArr[0], xArr, yArr, 0.001)\nmatrix([[3.20175729]])\n\n对所有数据点估计，并绘图：\nyHat1 = lwlrTest(xArr, xArr, yArr, k=1.0)\nyHat2 = lwlrTest(xArr, xArr, yArr, k=0.01)\nyHat3 = lwlrTest(xArr, xArr, yArr, k=0.003)\ndef plotLwlr(xArr, yHat):\n    xMat = mat(xArr)\n    srtInd = xMat[:, 1].argsort(0)\n    xSort = xMat[srtInd][:, 0, :]\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ax.plot(xSort[:, 1], yHat[srtInd])\n    ax.scatter(xMat[:, 1].flatten().A[0], mat(yArr).T.flatten().A[0], 2, c=&#039;red&#039;)\n    plt.show()\nplotLwlr(xArr, yHat1)\nplotLwlr(xArr, yHat2)\nplotLwlr(xArr, yHat3)\n\n\n\n当k=1.0时，权重很大，如同将所有数据视为等权重，得到的最佳拟合直线和标准回归一致\n当k=0.01时，得到了非常好的效果，抓住了数据的潜在模式\n当k=0.003时，拟合的直线与数据点过于贴近，过拟合\n局部加权线性回归也存在一个问题，即增加了计算量，因为它对每个点做预测时都必须使用整个数据集。\n实例：预测鲍鱼的年龄\n鲍鱼年龄可以从鲍鱼壳的层数来推算得到。\ndef rssError(yArr, yHatArr):\n    return ((yArr-yHatArr)**2).sum()\nabX, abY = loadDataSet(&#039;MLiA_SourceCode/Ch08/abalone.txt&#039;)\nyHat01 = lwlrTest(abX[: 99], abX[: 99], abY[: 99], 0.1)\nyHat1 = lwlrTest(abX[: 99], abX[: 99], abY[: 99], 1)\nyHat10 = lwlrTest(abX[: 99], abX[: 99], abY[: 99], 10)\n分析误差大小\nrssError(abY[: 99], yHat01.T)\n56.78868743048742\n\nrssError(abY[: 99], yHat1.T)\n429.8905618704059\n\nrssError(abY[: 99], yHat10.T)\n549.1181708828803\n\n可以看出，使用较小的核，可以得到较低的误差，但对新的数据不一定能达到很好的预测效果。\nyHat01 = lwlrTest(abX[100: 199], abX[: 99], abY[: 99], 0.1)\nyHat1 = lwlrTest(abX[100: 199], abX[: 99], abY[: 99], 1)\nyHat10 = lwlrTest(abX[100: 199], abX[: 99], abY[: 99], 10)\nrssError(abY[100: 199], yHat01.T)\n57913.51550155909\n\nrssError(abY[100: 199], yHat1.T)\n573.5261441894984\n\nrssError(abY[100: 199], yHat10.T)\n517.5711905381573\n\n从上面的结果可以看出，核的大小为10的时候测试误差最小，但在训练集上误差却最大，接下来和简单的线性回归比较：\nws = standRegres(abX[0: 99], abY[0: 99])\nyHat = mat(abX[100: 199])*ws\nrssError(abY[100: 199], yHat.T.A)\n518.6363153245542\n\n简单的线性回归和局部加权线性回归的结果类似。\n缩减系数来“理解”数据\n如果数据的特征比样本点还多（m&gt;n），说明输入矩阵X不是满秩矩阵，求逆时会出错，也就是在计算(X^TX)^{-1}的时候会出错。为了解决这个问题，统计学引入了岭回归（ridge regression）。\n岭回归\n简单来说，岭回归就是在矩阵(X^TX)上加一个\\lambda I从而使矩阵非奇异，进而对(X^TX)+\\lambda I求逆。其中I是一个mxm的单位矩阵，对角线上的元素全为1，其它元素全为0，岭回归公式为：\nw = (X^TX + \\lambda I)^{-1}X^Ty\n岭回归最先用于处理特征数多于样本数的情况，现在也用于在估计中加入偏差。这里通过引入λ来限制所有w之和，通过引入该惩罚项，能够减少不重要的参数，这个技术在统计学中也叫作缩减（shrinkage）。\n岭回归中的岭是什么？\n岭回归使用了单位矩阵乘以向量λ，我们观察其中的单位矩阵I，可以看到I贯穿整个对角线，其余元素全是0。形象地，在0构成的平面上有一条1组成的“岭”，这就是岭回归中岭的由来。\n\ndef ridgeRegres(xMat, yMat, lam=0.2):\n    xTx = xMat.T*xMat\n    denom = xTx + eye(shape(xMat)[1]) * lam\n    if linalg.det(denom) == 0:\n        print(&quot;This matrix is singular, cannot do invers&quot;)\n        return\n    ws = denom.I * (xMat.T * yMat)\n    return ws\n \ndef ridgeTest(xArr, yArr):\n    xMat = mat(xArr)\n    yMat = mat(yArr).T\n    # 数据归一化\n    yMean = mean(yMat, 0)\n    yMat = yMat - yMean\n    xMeans = mean(xMat, 0)\n    xVar = var(xMat, 0)\n    xMat = (xMat - xMeans)/xVar\n    \n    numTestPts = 30\n    wMat = zeros((numTestPts, shape(xMat)[1]))\n    for i in range(numTestPts):\n        ws = ridgeRegres(xMat, yMat, exp(i-10))\n        wMat[i, :] = ws.T\n    return wMat\nridgeRegres()用于计算回归系数，而ridgeTest()用于在一组λ上测试结果。为了使用岭回归和缩减技术，首先需要对特征做标准化处理。\nridgeWeights = ridgeTest(abX, abY)\nplt.rcParams[&#039;axes.unicode_minus&#039;]=False # 用来正常显示负号\nfig = plt.figure()\nax = fig.add_subplot(111)\nax.plot(ridgeWeights)\nplt.show()\n\n上图绘制了回归系数log(λ)的关系。在最左边，即λ最小时，可以得到所有系数的原始值；而在右边，系数全部缩减成0；在中间的某值将可以取得最好的预测效果。为了定量的找到最佳参数值，还需要进行交叉验证。\n还有一些其它的缩减方法，例如lasso，LAR，PCA回归以及子集选择等，与岭回归一样，这些方法不仅可以提高预测精确率，而且可以解释回归系数。\n向前逐步回归\n向前逐步算法它属于一种贪心算法，即每一步都尽可能减小误差，一开始，权重都设为1，然后每一步所做的决策是对某个权重增加或减少一个很小的值。\n伪代码如下：\n数据标准化，使其分布满足0均值和单位方差\n在每轮迭代过程中：\n    设置当前最小误差lowestError为正无穷\n    对每个特征：\n        增大或减小：\n            改变一个系数得到一个新的W\n            计算新W下的误差\n            如果误差Error小于当前最小误差lowestError：设置Wbest等于当前的W\n         将W设置为最新的Wbest\n\ndef regularize(xMat):#regularize by columns\n    inMat = xMat.copy()\n    inMeans = mean(inMat,0)   #calc mean then subtract it off\n    inVar = var(inMat,0)      #calc variance of Xi then divide by it\n    inMat = (inMat - inMeans)/inVar\n    return inMat\n \ndef stageWise(xArr, yArr, eps=0.01, numIt=100):\n    xMat = mat(xArr)\n    yMat = mat(yArr).T\n    yMean = mean(yMat, 0)\n    yMat = yMat - yMean\n    xMat = regularize(xMat)\n    m, n = shape(xMat)\n    returnMat = zeros((numIt, n))\n    ws = zeros((n, 1))\n    wsTest = ws.copy()\n    wsMax = ws.copy()\n    \n    for i in range(numIt):\n        #print(ws.T)\n        lowestError = inf\n        for j in range(n):\n            for sign in [-1, 1]:\n                wsTest = ws.copy()\n                wsTest[j] += eps*sign\n                yTest = xMat*wsTest\n                rssE = rssError(yMat.A, yTest.A)\n                if rssE &lt; lowestError:\n                    lowestError = rssE\n                    wsMax = wsTest\n        ws = wsMax.copy()\n        returnMat[i, :] = ws.T\n    \n    return returnMat\nstageWise(abX, abY, 0.01, 200)\narray([[ 0.  ,  0.  ,  0.  , ...,  0.  ,  0.  ,  0.  ],\n       [ 0.  ,  0.  ,  0.  , ...,  0.  ,  0.  ,  0.  ],\n       [ 0.  ,  0.  ,  0.  , ...,  0.  ,  0.  ,  0.  ],\n       ...,\n       [ 0.05,  0.  ,  0.09, ..., -0.64,  0.  ,  0.36],\n       [ 0.04,  0.  ,  0.09, ..., -0.64,  0.  ,  0.36],\n       [ 0.05,  0.  ,  0.09, ..., -0.64,  0.  ,  0.36]])\n\n上述结果中w1和w6都是0，这表示他们不对目标值造成任何影响，也就是说这些特征可能是不需要的，另外，在参数eps设置为0.01的情况下，一段时间后，系数就已经饱和，并在特定值之间震荡，这是因为步长太大的缘故，这里看到第一个权重在0.04和0.05之间震荡。\nstageWise(abX, abY, 0.001, 5000)\narray([[ 0.   ,  0.   ,  0.   , ...,  0.   ,  0.   ,  0.   ],\n       [ 0.   ,  0.   ,  0.   , ...,  0.   ,  0.   ,  0.   ],\n       [ 0.   ,  0.   ,  0.   , ...,  0.   ,  0.   ,  0.   ],\n       ...,\n       [ 0.043, -0.011,  0.12 , ..., -0.963, -0.105,  0.187],\n       [ 0.044, -0.011,  0.12 , ..., -0.963, -0.105,  0.187],\n       [ 0.043, -0.011,  0.12 , ..., -0.963, -0.105,  0.187]])\n\n接着把这些结果与最小二乘法比较。\nxMat = mat(abX)\nyMat = mat(abY).T\nxMat = regularize(xMat)\nyM = mean(yMat, 0)\nyMat = yMat - yM\nweights = standRegres(xMat, yMat.T)\nweights.T\nmatrix([[ 0.0430442 , -0.02274163,  0.13214087,  0.02075182,  2.22403814,\n         -0.99895312, -0.11725427,  0.16622915]])\n\n可以看到在5000次迭代以后，逐步线性回归算法与常规的最小二乘法效果类似。\n使用0.005的epsilon值经过1000次迭代后的结果如图。和使用0.001迭代5000图像类似。\nfig = plt.figure()\nax = fig.add_subplot(111)\nax.plot(stageWise(abX, abY, 0.005, 1000))\nplt.show()\n\n逐步线性回归算法的主要优点在于它可以帮助人们理解现有的模型并做出改进。当构建了一个模型后，可以运行该算法找出重要特征，这样就有可能停止对那些不重要的特征收集。最后，如果用于测试，该算法每100次迭代后就可以构建出一个模型，可以使用类似于10折交叉验证的方法比较这些模型，最终选择误差最小的模型。\n当应用缩减方法时，模型也就增加了偏差（bias），于此同时减小模型的方差。\n权衡方差于偏差\n\n偏差方差折中与测试误差及训练误差的关系，上面的曲线就是测试误差，在中间部分最低。为了做出最好的预测，我们应该调整模型复杂度来达到测试误差的最小值。\n实例预测乐高玩具套装的价格\n用回归法预测乐高套装的价格\n\n收集数据：用Google Shopping的API收集数据\n准备数据：从返回的json数据中抽取价格\n分析数据：可视化观察数据\n训练算法：构建不同的模型，采用逐步线性回归和直接的线性回归模型\n测试算法：使用交叉验证来测试不同的模型，分析哪个效果最好\n使用算法：这次练习的目标就是生成数据模型\n\n收集数据\n因为Google api已经无法访问，所以使用以下code来收集数据。\ndef scrapePage(inFile, outFile, yr, numPce, origPrc):\n    from bs4 import BeautifulSoup \n    fr = open(inFile)\n    fw=open(outFile,&#039;a&#039;) # a is append mode writing\n    soup = BeautifulSoup(fr.read())\n    i=1\n    currentRow = soup.findAll(&#039;table&#039;, r=&quot;%d&quot; % i)\n    while(len(currentRow)!=0):\n        title = currentRow[0].findAll(&#039;a&#039;)[1].text\n        lwrTitle = title.lower()\n        if (lwrTitle.find(&#039;new&#039;) &gt; -1) or (lwrTitle.find(&#039;nisb&#039;) &gt; -1):\n            newFlag = 1.0\n        else:\n            newFlag = 0.0\n        soldUnicde = currentRow[0].findAll(&#039;td&#039;)[3].findAll(&#039;span&#039;)\n        if len(soldUnicde)==0:\n            print(&quot;item #%d did not sell&quot; % i)\n        else:\n            soldPrice = currentRow[0].findAll(&#039;td&#039;)[4]\n            priceStr = soldPrice.text\n            priceStr = priceStr.replace(&#039;$&#039;,&#039;&#039;) #strips out $\n            priceStr = priceStr.replace(&#039;,&#039;,&#039;&#039;) #strips out ,\n            if len(soldPrice)&gt;1:\n                priceStr = priceStr.replace(&#039;Free shipping&#039;, &#039;&#039;) #strips out Free Shipping\n            print(&quot;%s\\t%d\\t%s&quot; % (priceStr,newFlag,title))\n            fw.write(&quot;%d\\t%d\\t%d\\t%f\\t%s\\n&quot; % (yr,numPce,newFlag,origPrc,priceStr))\n        i += 1\n        currentRow = soup.findAll(&#039;table&#039;, r=&quot;%d&quot; % i)\n    fw.close()\n    \ndef setDataCollect(outFile):\n    scrapePage(&#039;MLiA_SourceCode/Ch08/setHtml/lego8288.html&#039;, outFile, 2006, 800, 49.99)\n    scrapePage(&#039;MLiA_SourceCode/Ch08/setHtml/lego10030.html&#039;, outFile, 2002, 3096, 269.99)\n    scrapePage(&#039;MLiA_SourceCode/Ch08/setHtml/lego10179.html&#039;, outFile, 2007, 5195, 499.99)\n    scrapePage(&#039;MLiA_SourceCode/Ch08/setHtml/lego10181.html&#039;, outFile, 2007, 3428, 199.99)\n    scrapePage(&#039;MLiA_SourceCode/Ch08/setHtml/lego10189.html&#039;, outFile, 2008, 5922, 299.99)\n    scrapePage(&#039;MLiA_SourceCode/Ch08/setHtml/lego10196.html&#039;, outFile, 2009, 3263, 249.99)\nsetDataCollect(&#039;result.txt&#039;)\n85.00\t0\tLego Technic 8288 Crawler crane\n102.50\t0\tLego Technic 8288 Crawler Crane USED SET \n77.00\t0\tLego Technic 8288 Crawler Crane\nitem #4 did not sell\n162.50\t0\tRARE Lego Technic 8288 Crawler Crane\n699.99\t0\tLego Star Wars Imperial Star Destroyer (10030)  Sealed!\n602.00\t0\tLego Star Wars UCS Imperial Star Destroyer #10030\n515.00\t0\tLego 10030 Imperial Star Destroyer \n510.00\t0\tLego Star Wars 10030 Ultimate Imperial Star Destroyer\n375.00\t0\tLego Star Wars Imperial Star Destroyer (10030)\n1050.00\t1\tLEGO STAR DESTROYER NEW IN SEALED BOX 10030 STAR WARS\n740.00\t0\tIMPERIAL STAR DESTROYER #10030 Lego Star Wars   SEALED \n759.00\t1\tLEGO STAR WARS 10030 UCS IMPERIAL DESTROYER NISB NEW\n730.00\t0\tLego 10030 Star Destroyer, MISB, Old Gray, Ships Free!\n750.00\t1\tNEW STAR WARS LEGO SET 10030 IMPERIAL STAR DESTROYER\nitem #11 did not sell\n910.00\t0\tLEGO star wars Millenium Falcon #10179 MISB\n1199.99\t1\tLego Star Wars - 10179 Ultimate Millennium Falcon - NEW\n811.88\t0\tLego Star Wars - 10179 Ultimate Millennium Falcon-USED\nitem #4 did not sell\n1324.79\t0\tLego Star Wars Millennium Falcon 10179\n850.00\t1\tNEW LEGO 10179 STAR WARS UC MILLENNIUM FALCON - NISB\n800.00\t1\tNEW LEGO 10179 STAR WARS UC MILLENNIUM FALCON - NISB\n810.00\t0\tlego star warsUltimateCol​lectors millennium falcon10179\n1075.00\t1\tLego Star Wars Ultimate Millenium Falcon 10179 NEW MISB\n1050.00\t0\tLEGO STAR WARS 10179 UCS MILLENIUM FALCON! MINT IN BOX\n1199.99\t1\tLEGO 10179 STAR WARS MILLENNIUM FALCON UCS NEW/SEALED\n1342.31\t0\tLego Star Wars 10179 Collectors Millennium Falcon\n1000.00\t1\tStar Wars - UCS Millennium Falcon - 10179 - New In Box\n1780.00\t0\tLEGO STAR WARS 10188 10179 DEATH STAR MILLENIUM FALCON\n750.00\t0\tSTAR WARS Lego 10179 Ultimate CS MILLENNIUM FALCON! \nitem #16 did not sell\n2204.99\t0\t HUGE LOT OF LEGOS 10179 FALCON &amp; MORE STARWARS &amp; MORE\nitem #18 did not sell\n925.00\t1\tLego #10179 BRAND NEW Star Wars UCS Millenium Falcon\n860.00\t0\tLEGO STAR WARS UCS MILLENNIUM FALCON #10179 WITH BOX\nitem #21 did not sell\nitem #22 did not sell\n1199.99\t1\tLego Star Wars 10179 UCS Millenium Falcon - NEW!\n1099.99\t1\tLego Star Wars 10179 UCS Millennium Falcon  NiSB  HUGE!\n1149.99\t1\tNEW LEGO 10179 STAR WARS MILLENNIUM FALCON NEW/SEALED\n800.00\t1\tNEW LEGO 10179 STAR WARS UC MILLENNIUM FALCON - NISB\n850.00\t1\tNEW LEGO 10179 STAR WARS UC MILLENNIUM FALCON - NISB\n469.95\t0\tLego Star Wars Death Star II 10143 MNIB SOLD OUT A++\n479.00\t0\tNIB Box Collectors Starwars Death Star II - 10143\n299.99\t0\tLego Star Wars Death Star II 10143 -Excellent Condition\n369.00\t0\tLego Star Wars Death Star ll # 10143\n424.95\t1\tLEGO Star Wars  Death Star II 10143 *Damaged Box* NEW\n380.00\t1\tNEW Lego Star Wars Death Star II #10143\n305.00\t0\tLEGO Star Wars Death Star II 10143 \n530.00\t1\tLEGO Taj Mahal NEW IN BOX MINT CONDITION! LAST ONE!\nitem #2 did not sell\n599.95\t1\tLEGO 10189 TAJ MAHAL - BRAND NEW - RARE &amp; SOLD OUT!\n510.00\t0\tLego~Taj Mahal~#10189~pu​t together once~EUC\n423.00\t0\tLego Taj Mahal 10189- Put together ONCE - perfect shape\nitem #6 did not sell\nitem #7 did not sell\n599.99\t1\tLego - Taj Mahal 10189  - NEW Sealed\nitem #9 did not sell\n589.99\t1\tLEGO 10189 TAJ MAHAL NEW SEALED IN BOX FAST SHIPPING\n569.99\t1\tLEGO 10189 TAJ MAHAL NEW SEALED MINT FREE SHIPPING \n529.99\t1\tLego 10189 Taj Mahal ***New &amp; Sealed***\n500.00\t0\tLEGO TAJ MAHAL \n549.95\t1\tLEGO 10189 TAJ MAHAL - BRAND NEW - RARE &amp; SOLD OUT!\n300.00\t0\tLego TAJ MAHAL 10189 100% Complete, No Box, Inst. Incl.\nitem #16 did not sell\n380.00\t1\tLego - Grand Carousel 10196 - NEW Sealed\n399.00\t1\tLego Grand Carousel 10196 - NIB Sealed, Brand New\n427.99\t1\tLego 10196 Grand Carousel ***New &amp; Sealed***\n360.00\t0\tGrand Carousel Lego 10196 Rare Used Extra Minifigs\nitem #5 did not sell\nitem #6 did not sell\n399.00\t1\tLego City 10196 Grand Carousel New In BOX! \n399.95\t1\tLEGO CREATOR CAROUSEL 10196 Box New *MISB*\n499.99\t1\tLego - Grand Carousel 10196  - NEW Sealed\nitem #10 did not sell\n399.95\t0\tLEGO Grand Carousel 10196 NIB\nitem #12 did not sell\n331.51\t1\tLego Carousel 10196, New Unopened Bags\n\nlgX, lgY = loadDataSet(&#039;result.txt&#039;)\n训练算法：建立模型\n首先需要添加对应常数项的特征X0(X0=1)，为此创建一个全为1的矩阵。\nshape(lgX)\n(126, 4)\n\nlgX1 = mat(ones((126, 5)))\nlgX1[:, 1:5] = mat(lgX)\nlgX[0]\n[2006.0, 800.0, 0.0, 49.99]\n\nlgX1[0]\nmatrix([[1.000e+00, 2.006e+03, 8.000e+02, 0.000e+00, 4.999e+01]])\n\nws = standRegres(lgX1, lgY)\nws\nmatrix([[ 5.53199701e+04],\n        [-2.75928219e+01],\n        [-2.68392234e-02],\n        [-1.12208481e+01],\n        [ 2.57604055e+00]])\n\n检查结果\nlgX1[0]*ws\nmatrix([[76.07418859]])\n\nlgX1[-1]*ws\nmatrix([[431.17797678]])\n\nlgX1[43]*ws\nmatrix([[516.20733111]])\n\nlgY[0]\n85.0\n\n交叉验证测试岭回归\nimport random\ndef crossValidation(xArr, yArr, numVal=10):\n    m = len(yArr)\n    indexList = list(range(m))\n    errorMat = zeros((numVal, 30))\n    for i in range(numVal):\n        trainX = []\n        trainY = []\n        testX = []\n        testY = []\n        random.shuffle(indexList)\n        for j in range(m):\n            if j &lt; m*0.9:\n                trainX.append(xArr[indexList[j]])\n                trainY.append(yArr[indexList[j]])\n            else:\n                testX.append(xArr[indexList[j]])\n                testY.append(yArr[indexList[j]])\n        wMat = ridgeTest(trainX, trainY)\n        for k in range(30):\n            matTestX = mat(testX)\n            matTrainX = mat(trainX)\n            meanTrain = mean(matTrainX, 0)\n            varTrain = var(matTrainX, 0)\n            matTestX = (matTestX-meanTrain)/varTrain\n            yEst = matTestX * mat(wMat[k, :]).T + mean(trainY)\n            errorMat[i, k] = rssError(yEst.T.A, array(testY))\n    meanErrors = mean(errorMat,0)#calc avg performance of the different ridge weight vectors\n    minMean = float(min(meanErrors))\n    bestWeights = wMat[nonzero(meanErrors==minMean)]\n    # 可以非正则化得到模型\n    # 正则化后，我们写了Xreg = (x- meanx)/var(x)\n    # 我们现在可以用x而不是Xreg来表示:x*w/var(x) - meanX/var(x) +mean\n    xMat = mat(xArr)\n    yMat = mat(yArr).T\n    meanX = mean(xMat,0)\n    varX = var(xMat,0)\n    unReg = bestWeights/varX\n    print(&quot;the best model from ridge regression is:\\n&quot;, unReg)\n    print(&quot;with constant term: &quot;, -1*sum(multiply(meanX, unReg))+mean(yMat))\ncrossValidation(lgX, lgY, numVal=10)\nthe best model from ridge regression is:\n [[-3.13000380e+01 -5.79216518e-04 -1.46976042e+01  2.33709492e+00]]\nwith constant term:  62728.39604629546\n\nridgeTest(lgX, lgY)\narray([[-1.42567890e+02, -1.59065167e+04, -3.32568485e+00,\n         4.50485291e+04],\n       [-1.46048534e+02, -5.88035105e+03, -3.20592314e+00,\n         4.38513111e+04],\n       [-1.46392961e+02, -7.53268167e+02, -2.49755326e+00,\n         4.21602572e+04],\n       [-1.42882929e+02,  1.24713671e+03, -5.78336771e-01,\n         3.87616551e+04],\n       [-1.34058297e+02,  1.65495690e+03,  3.46766980e+00,\n         3.19757189e+04],\n       [-1.20185790e+02,  1.28302796e+03,  9.65389591e+00,\n         2.16984122e+04],\n       [-1.06405098e+02,  7.16651262e+02,  1.57519175e+01,\n         1.15841453e+04],\n       [-9.74799755e+01,  3.21265653e+02,  1.96506716e+01,\n         5.10995216e+03],\n       [-9.29742820e+01,  1.28256118e+02,  2.14874762e+01,\n         2.02836160e+03],\n       [-9.04274807e+01,  4.86739495e+01,  2.21853397e+01,\n         7.68474711e+02],\n       [-8.75861154e+01,  1.80935644e+01,  2.23113328e+01,\n         2.85792138e+02],\n       [-8.19045992e+01,  6.66134743e+00,  2.20039861e+01,\n         1.05512059e+02],\n       [-6.99384526e+01,  2.43521001e+00,  2.11068607e+01,\n         3.88294842e+01],\n       [-5.00210980e+01,  8.84809679e-01,  1.94076129e+01,\n         1.42672332e+01],\n       [-2.79820303e+01,  3.21315572e-01,  1.69641166e+01,\n         5.24285549e+00],\n       [-1.24628930e+01,  1.17567465e-01,  1.37731297e+01,\n         1.93051917e+00],\n       [-4.77896841e+00,  4.34264276e-02,  9.63509521e+00,\n         7.12700367e-01],\n       [-1.71197823e+00,  1.61100639e-02,  5.40201378e+00,\n         2.63366078e-01],\n       [-6.08727885e-01,  5.96613258e-03,  2.47122138e+00,\n         9.72056597e-02],\n       [-2.19489801e-01,  2.20244723e-03,  9.99173868e-01,\n         3.58199566e-02],\n       [-8.00295532e-02,  8.11429705e-04,  3.81511828e-01,\n         1.31867450e-02],\n       [-2.93376909e-02,  2.98679351e-04,  1.42337367e-01,\n         4.85246297e-03],\n       [-1.07783727e-02,  1.09901635e-04,  5.26372259e-02,\n         1.78530510e-03],\n       [-3.96318049e-03,  4.04337769e-05,  1.94015377e-02,\n         6.56802092e-04],\n       [-1.45770631e-03,  1.48751929e-05,  7.14249987e-03,\n         2.41627386e-04],\n       [-5.36224096e-04,  5.47233696e-06,  2.62826610e-03,\n         8.88902084e-05],\n       [-1.97260935e-04,  2.01316829e-06,  9.66978104e-04,\n         3.27009426e-05],\n       [-7.25675812e-05,  7.40604313e-07,  3.55743958e-04,\n         1.20300129e-05],\n       [-2.66960317e-05,  2.72453248e-07,  1.30872593e-04,\n         4.42559557e-06],\n       [-9.82090911e-06,  1.00229968e-07,  4.81455670e-05,\n         1.62808578e-06]])\n\n总结\n与分类一样，回归也是预测目标值的过程。回归与分类的不同点在于，前者预测连续型变量而后者预测离散型变量。在回归方程里，求得特征对应的最佳回归系数的方法是最小化误差的平方和。\n当数据的样本比特征数还少的时候，矩阵X^TX的逆不能直接计算，这时可以考虑使用缩减法。\n缩减法还可以看做是对一个模型增加偏差的同时减小方差。"},"机器学习实战/9.-机器学习实战":{"slug":"机器学习实战/9.-机器学习实战","filePath":"机器学习实战/9. 机器学习实战.md","title":"9. 机器学习实战","links":[],"tags":["树回归","CSRT算法","树剪枝算法"],"content":"数回归\n分类回归树 Classification And Regression Trees 分类回归树。该算法既可以用于回归还可以用于分类。\n复杂数据的局部性建模\n数回归\n优点：可以对复杂和线性的数据建模\n缺点：结果不易理解\n适用数据类型：数值型和标称型数据\n\n第三章使用的树构建的算法是ID3。ID3的做法是每次选取当前最佳的特征来分割数据，并按照该特征的所有可能取值来切分。也就取值，那么数据将被切分成4份，一但按某种特征切分后，该特征在之后的算法执行过程中将不会再起作用，所以有观点认为这种切分方式过于迅速。另一种方法是二元切分发，即每次吧数据集切分成两份。如果数据的某个特征等于切分所要求的值，那么这些数据就进入树的左子树，反之则进入树的右子树。\n除了切分过于迅速外，ID3算法还存在另一个问题，它不能直接处理连续型特征。只有事先将连续型特征转换成离散型，才能在ID3算法中使用。但这种转换过程会破坏连续型变量的内在性质。而使用二元切分法则对于树构建过程进行调整以处理连续型特征。\n具体处理方法是：\n如果特征值大于给定值就走左子树，否则就走右子树。\n\n另外，二元切分法也节省了树的构建时间，但这点意义也不是特别大因为这些树的构建一般是离线完成的。\nCART是十分著名且广泛记载的树构建算法，它使用二元切分来处理连续型变量。对CART稍作修改就可以处理回归问题。\n回归树的一般方法：\n\n收集数据\n准备数据：需要数值型的数据，标称型数据应该映射成二值型数据\n分析数据：绘出数据的二维可视化显示结果，以字典方式生成树\n训练算法：大部分时间都花费在叶节点树模型的构建上\n测试算法：使用测试数据上的R^2值来分析模型的效果\n使用算法：使用训练出的树做预测## 连续和离散型特征的树的构建\n\n在树的构建过程中，需要解决多种类型数据的存储问题。这里将使用字典来存储树的数据结构，该字典将包含以下四种元素。\n待切分的特征\n待切分的特征值\n右子树。当不需要切分时，也可以是单值\n左子树。与右子树类似\n\nCART算法只做二元切分，所以这里可以固定树的数据结构。树包含左键和右键，可以存储另一颗树或者单个值。字典还包含特征和特征值这两个键，它们给出的切分算法所有的特征和特征值。\n接下来构建两种树，第一种是回归树（regression tree）其中每个叶节点包含单个值，第二种是是模型树（model tree）其中每个叶节点包含一个线性方程。\ncreateTree()的伪代码大致如下：\n找到最佳的待切分特征：\n    如果该节点不能再分，将该节点存为叶节点\n    执行二元切分\n    在右子树调用createTree()方法\n    在左子树调用createTree()方法\n    ```python\n\nfrom numpy import *\ndef loadDataSet(fileName):\ndataMat = []\nfr = open(fileName)\nfor line in fr.readlines():\ncurLine = line.strip().split(‘\\t’)\nfltLine = map(float, curLine)\nfltLine = list(fltLine)\ndataMat.append(fltLine)\nfr.close()\nreturn dataMat\ndef regLeaf(dataSet): # returns the value used for each leaf\nreturn mean(dataSet[:,-1])\ndef regErr(dataSet):\nreturn var(dataSet[:,-1]) * shape(dataSet)[0]\ndef createTree(dataSet, leafType=regLeaf, errType=regErr, ops=(1, 4)):\nfeat, val = chooseBestSplit(dataSet, leafType, errType, ops)\n# 满足条件时返回叶节点值\nif feat == None:\nreturn val\nretTree = {}\nretTree[‘spInd’] = feat\nretTree[‘spVal’] = val\nlSet, rSet = binSplitDataSet(dataSet, feat, val)\nretTree[‘left’] = createTree(lSet, leafType, errType, ops)\nretTree[‘right’] = createTree(rSet, leafType, errType, ops)\nreturn retTree\ndef binSplitDataSet(dataSet, feature, value):\nmat0 = dataSet[nonzero(dataSet[:, feature] &gt; value)[0], :]\nmat1 = dataSet[nonzero(dataSet[:, feature] ⇐ value)[0], :]\nreturn mat0, mat1\ntestMat = mat(eye(4))\ntestMat\nmatrix([[1., 0., 0., 0.],\n        [0., 1., 0., 0.],\n        [0., 0., 1., 0.],\n        [0., 0., 0., 1.]])\n\nmat0, mat1 = binSplitDataSet(testMat, 1, 0.5)\nmat0\n \nmatrix([[0., 1., 0., 0.]])\n\nmat1\n \nmatrix([[1., 0., 0., 0.],\n[0., 0., 1., 0.],\n[0., 0., 0., 1.]])\nnonzero(testMat[:, 1] &gt; 0.5)[0][0]\n1\n将CART算法用于回归\n要对数据的复杂关系建模，我们已经决定借用树结构来帮助切分数据，那么如何实现数据的切分呢？\n为成功构建以分段常数为叶节点的树，需要度量出数据的一致性。在给定的节点计算数据的混乱度，计算数据混乱度的方法，首先计算所有数据的均值，然后计算每条数据的值到均值的差值，为了对正负差值同等看待，一般使用绝对值或平方值来代替上述差值。类似方差的计算，唯一不同是方差是平方误差的均值，而这里需要的是平方误差的总值，总方差可以通过均方差乘以数据集中样本点的个数来得到。\n构建树\n构建回归树，需要补充一些新的代码，首先要做的就是实现chooseBestSplit()函数，给定某个误差计算方法，该函数会找到数据集上最佳的二元切分方式。另外该函数还要确定什么时候停止切分，一旦停止切分会生成一个叶节点。因此chooseBestSplit()函数需要完成两件事：用最佳方式切分数据集和生成相应的叶节点。\nleafType是对创建叶节点的函数的引用，errType是对前面介绍的总方差计算函数的引用，而ops是一个用户定义的参数构成的元组，用已完成树的构建。\n伪代码如下：\n对每个特征：\n    对每个特征值：\n        将数据集切分成两份\n        计算切分的误差\n        如果当前误差小于当前最小误差，那么将当前切分设定为最佳切分并更新最小误差\n返回最佳切分的特征和阈值\n\ndef chooseBestSplit(dataSet, leafType=regLeaf, errType=regErr, ops=(1, 4)):\n    tolS = ops[0]\n    tolN = ops[1]\n \n    if len(set(dataSet[:, -1].T.tolist()[0])) == 1: # 如果所有值相等则退出\n        return None, leafType(dataSet)\n    \n    m, n = shape(dataSet)\n    S = errType(dataSet)\n    bestS = inf\n    bestIndex = 0\n    bestValue = 0\n    \n    for featIndex in range(n-1):\n        for splitVal in set(dataSet[:, featIndex].tolist()[0]):\n            mat0, mat1 = binSplitDataSet(dataSet, featIndex, splitVal)\n            if (shape(mat0)[0] &lt; tolN) or (shape(mat1)[0] &lt; tolN) :\n                continue\n            newS = errType(mat0) + errType(mat1)\n            if newS &lt; bestS:\n                bestIndex = featIndex\n                bestValue = splitVal\n                bestS = newS\n \n    if (S - bestS) &lt; tolS: # 如果误差减小不大则退出\n        return None, leafType(dataSet)\n    \n    mat0, mat1 = binSplitDataSet(dataSet, bestIndex, bestValue)\n    \n    if (shape(mat0)[0] &lt; tolN) or (shape(mat1)[0] &lt; tolN): # 如果切分出的数据集很小则退出\n        return None, leafType(dataSet)\n    \n    return bestIndex, bestValue\n    \nregLeaf()它负责生成叶节点。当chooseBestSplit()函数确定不再对数据进行切分时，将调用该regLeaf()函数来得到叶节点的模型，在回归树中，该模型就是目标变量的均值。\nregErr()是误差估计函数，该函数在给定数据上计算目标变量的平方误差，当然也可以先计算出均值，然后计算每个差值再平方。因为这里需要总方差，所以用均方差函数var()的结果乘以数据集中的样本个数。\nchooseBestSplit()该函数的目的是找到数据的最佳二元切分方式。如果找不到一个好的二元切分，该函数返回None并同时调用createTree()方法来产生叶节点，叶节点的值也将返回None。ops设定了tolS和tolN两个值，tolS是容许的误差下降值，tolN是切分的最少样本数。\n运行代码```python\nmyDat = loadDataSet(‘MLiA_SourceCode/Ch09/ex00.txt’)\n\n```python\nmyMat = mat(myDat)\n\ncreateTree(myMat)\n{&#039;spInd&#039;: 0,\n &#039;spVal&#039;: 0.036098,\n &#039;left&#039;: 0.5878577680412371,\n &#039;right&#039;: 0.050698999999999994}\n\nimport matplotlib.pyplot as plt\ndef plotScatter(data):\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    #print(myMat)\n    ax.scatter(data[:, 0].T.tolist()[0], data[:, 1].T.tolist()[0], 5, c=&#039;red&#039;)\n    plt.show()\nplotScatter(myMat)\n\nmyDat = loadDataSet(&#039;MLiA_SourceCode/Ch09/ex0.txt&#039;)\nmyMat = mat(myDat)\ncreateTree(myMat)\n{&#039;spInd&#039;: 1,\n &#039;spVal&#039;: 0.409175,\n &#039;left&#039;: {&#039;spInd&#039;: 1,\n  &#039;spVal&#039;: 0.663687,\n  &#039;left&#039;: {&#039;spInd&#039;: 1,\n   &#039;spVal&#039;: 0.725426,\n   &#039;left&#039;: 3.7206952592592595,\n   &#039;right&#039;: 2.998615611111111},\n  &#039;right&#039;: 2.2076016800000002},\n &#039;right&#039;: 0.45470547435897446}\n\nplotScatter(myMat[:, 1:])\n# 树剪枝\n通过降低决策树的复杂度来避免过拟合的过程称为剪枝（pruning）。\n预剪枝\n树构建算法其实对输入的参数tolS和tolN非常敏感，对ops参数调整就是预剪枝。\n后剪枝\n利用测试集来对树进行剪枝，不需要用户指定参数，为后剪枝。\nprune()伪代码如下\n基于已有的树切分测试数据：\n    如果存在任一子集是一棵树，则在该子集递归剪枝过程\n    计算将当前两个叶节点合并后的误差\n    计算不合并的误差\n    如果合并降低误差，就将叶节点合并\n\ndef isTree(obj):\n    return (type(obj).__name__==&#039;dict&#039;)\n \ndef getMean(tree):\n    if isTree(tree[&#039;right&#039;]):\n        tree[&#039;right&#039;] = getMean(tree[&#039;right&#039;])\n    if isTree(tree[&#039;left&#039;]):\n        tree[&#039;left&#039;] = getMean(tree[&#039;left&#039;])\n    return (tree[&#039;left&#039;]+tree[&#039;right&#039;])/2.0\n    \ndef prune(tree, testData):\n    if shape(testData)[0] == 0:\n        return getMean(tree)  # if we have no test data collapse the tree\n    if (isTree(tree[&#039;right&#039;]) or isTree(tree[&#039;left&#039;])): # if the branches are not trees try to prune them\n        lSet, rSet = binSplitDataSet(testData, tree[&#039;spInd&#039;], tree[&#039;spVal&#039;])\n    if isTree(tree[&#039;left&#039;]): tree[&#039;left&#039;] = prune(tree[&#039;left&#039;], lSet)\n    if isTree(tree[&#039;right&#039;]): tree[&#039;right&#039;] =  prune(tree[&#039;right&#039;], rSet)\n    # if they are now both leafs, see if we can merge them\n    if not isTree(tree[&#039;left&#039;]) and not isTree(tree[&#039;right&#039;]):\n        lSet, rSet = binSplitDataSet(testData, tree[&#039;spInd&#039;], tree[&#039;spVal&#039;])\n        errorNoMerge = sum(power(lSet[:,-1] - tree[&#039;left&#039;],2)) +\\\n            sum(power(rSet[:,-1] - tree[&#039;right&#039;],2))\n        treeMean = (tree[&#039;left&#039;]+tree[&#039;right&#039;])/2.0\n        errorMerge = sum(power(testData[:,-1] - treeMean,2))\n        if errorMerge &lt; errorNoMerge: \n            return treeMean\n        else: \n            return tree\n    else: \n        return tree\nmyDat2 = loadDataSet(&#039;MLiA_SourceCode/Ch09/ex2.txt&#039;)\nmyMat2 = mat(myDat2)\nmyTree = createTree(myMat2, ops=(0, 1))\nmyDatTest = loadDataSet(&#039;MLiA_SourceCode/Ch09/ex2test.txt&#039;)\nmyMat2Test = mat(myDatTest)\nprune(myTree, myMat2Test)\n{&#039;spInd&#039;: 0,\n &#039;spVal&#039;: 0.228628,\n &#039;left&#039;: {&#039;spInd&#039;: 0,\n  &#039;spVal&#039;: 0.965969,\n  &#039;left&#039;: 92.5239915,\n  &#039;right&#039;: 65.53919801898735},\n &#039;right&#039;: -1.1055498250000002}\n\n模型树\n用树来对数据建模，除了吧叶节点简单地设定为常数值之外，还有一种方法是把叶节点设定为分段线性函数，这里所谓的分段性（piecewise linear）是指模型由多个线性片段组成。\n决策树相比其他机器学习算法的优势之一在于结果更易理解。很显然，两条直线比很多节点组成一棵大树更容易解释。模型树的可解释性是它优于回归树的特点之一。另外，模型树也具有更高的预测准确度。\ndef linearSolve(dataSet):\n    m, n = shape(dataSet)\n    X = mat(ones((m, n)))\n    Y = mat(ones((m, 1)))\n    X[:, 1:n] = dataSet[:, 0:n-1]\n    Y = dataSet[:, -1]\n    xTx = X.T*X\n    \n    if linalg.det(xTx) == 0.0:\n        raise NameError(&#039;This matrix is singular, cannot do inverse,\\\n                         try increasing the second value of ops&#039;)\n    \n    ws = xTx.I * (X.T * Y)\n    return ws, X, Y\n \ndef modelLeaf(dataSet):\n    ws, X, Y = linearSolve(dataSet)\n    \ndef modelErr(dataSet):\n    ws, X, Y = linearSolve(dataSet)\n    yHat = X * ws\n    return sum(power(Y - yHat, 2))\ncreateTree(myMat2, modelLeaf, modelErr, (1, 10))\n{&#039;spInd&#039;: 0, &#039;spVal&#039;: 0.228628, &#039;left&#039;: None, &#039;right&#039;: None}\n\n总结\n这一章提供的code有很多错误，修正后并不能得到书中的答案。如果要使用树算法，还是建议使用sklearn，而非自己编写。\nCART算法可以用于构建二元树并处理离散型或连续型数据的切分。若使用不同的误差准则，就可以通过CART算法构建模型树和回归树。该算法构建出的树会倾向于对数据过拟合。过拟合的树十分复杂，剪枝可以解决这个问题。"},"机器学习实战/机器学习实战中的函数学习记录":{"slug":"机器学习实战/机器学习实战中的函数学习记录","filePath":"机器学习实战/机器学习实战中的函数学习记录.md","title":"机器学习实战中的函数学习记录","links":[],"tags":["python函数"],"content":"记录机器学习实战中遇到的函数\nimport numpy as np\ntile()\ntile(A, reps)\ntile函数的作用是让某个数组或矩阵A，以reps的维度重复，构造出新的数组，所以返回值也是个数组。\na = array([0, 1])\nb = np.tile(a, 2)\nc = np.tile(a, (2,2))\nb,c\n(array([0, 1, 0, 1]),\n array([[0, 1, 0, 1],\n        [0, 1, 0, 1]]))\n\nargsort()\nx = array([1,4,3,-1,6,9])\nx.argsort()\narray([3, 0, 2, 1, 4, 5], dtype=int64)\n\nargsort()函数是将x中的元素从小到大排列，提取其对应的index(索引)，然后输出。例如：x[3]=-1最小，所以y[0]=3,x[5]=9最大，所以y[5]=5。\noperator.itemgetter函数\noperator模块提供的itemgetter函数用于获取对象的哪些维的数据，参数为一些序号。\n要注意，operator.itemgetter函数获取的不是值，而是定义了一个函数，通过该函数作用到对象上才能获取值。\nsorted函数用来排序，sorted(iterable[, cmp[, key[, reverse]]])\n其中key的参数为一个函数或者lambda函数。所以itemgetter可以用来当key的参数\nimport operator\na = [1,2,3]\nb=operator.itemgetter(1) \nb(a)\n2\n\nb=operator.itemgetter(1,0)  #定义函数b，获取对象的第1个域和第0个的值\nb(a) \n(2, 1)\n\nstudents = [(&#039;john&#039;, &#039;A&#039;, 15), (&#039;jane&#039;, &#039;B&#039;, 12), (&#039;dave&#039;, &#039;B&#039;, 10)]\n#根据第二个域进行排序\nsorted(students, key=operator.itemgetter(2))\n[(&#039;dave&#039;, &#039;B&#039;, 10), (&#039;jane&#039;, &#039;B&#039;, 12), (&#039;john&#039;, &#039;A&#039;, 15)]\n\npickle模块\n该pickle模块实现了用于序列化和反序列化Python对象结构的二进制协议。 “Pickling”是将Python对象层次结构转换为字节流的过程， “unpickling”是反向操作，从而将字节流（来自二进制文件或类似字节的对象）转换回对象层次结构。pickle模块对于错误或恶意构造的数据是不安全的。\npickle协议和JSON（JavaScript Object Notation）的区别 ：\n　　1. JSON是一种文本序列化格式（它输出unicode文本，虽然大部分时间它被编码utf-8），而pickle是二进制序列化格式;\n　　2. JSON是人类可读的，而pickle则不是;\n　　3. JSON是可互操作的，并且在Python生态系统之外广泛使用，而pickle是特定于Python的;\n默认情况下，JSON只能表示Python内置类型的子集，而不能表示自定义类; pickle可以表示极其庞大的Python类型（其中许多是自动的，通过巧妙地使用Python的内省工具;复杂的案例可以通过实现特定的对象API来解决）。\npickle 数据格式是特定于Python的。它的优点是没有外部标准强加的限制，例如JSON或XDR（不能代表指针共享）; 但是这意味着非Python程序可能无法重建pickled Python对象。\n默认情况下，pickle数据格式使用相对紧凑的二进制表示。如果您需要最佳尺寸特征，则可以有效地压缩数据。\n模块接口\n要序列化对象层次结构，只需调用该dumps()函数即可。同样，要对数据流进行反序列化，请调用该loads()函数。但是，如果您想要更多地控制序列化和反序列化，则可以分别创建一个Pickler或一个Unpickler对象。\npickle模块提供以下常量：\npickle.HIGHEST_PROTOCOL\n整数， 可用的最高协议版本。这个值可以作为一个被传递协议的价值函数 dump()和dumps()以及该Pickler 构造函数。\npickle.DEFAULT_PROTOCOL\n整数，用于编码的默认协议版本。可能不到HIGHEST_PROTOCOL。目前，默认协议是3，这是为Python 3设计的新协议。\npickle模块提供以下功能，使酸洗过程更加方便：\npickle.dump（obj，file，protocol = None，*，fix_imports = True ）\n将obj对象的编码pickle编码表示写入到文件对象中，相当于Pickler(file,protocol).dump(obj)\n可供选择的协议参数是一个整数，指定pickler使用的协议版本，支持的协议是0到HIGHEST_PROTOCOL。如果未指定，则默认为DEFAULT_PROTOCOL。如果指定为负数，则选择HIGHEST_PROTOCOL。\n文件参数必须具有接受单个字节的参数写方法。因此，它可以是为二进制写入打开的磁盘文件， io.BytesIO实例或满足此接口的任何其他自定义对象。\n如果fix_imports为true且protocol小于3，则pickle将尝试将新的Python 3名称映射到Python 2中使用的旧模块名称，以便使用Python 2可读取pickle数据流。\npickle.dumps（obj，protocol = None，*，fix_imports = True ）\n将对象的pickled表示作为bytes对象返回，而不是将其写入文件。\n参数protocol和fix_imports具有与in中相同的含义 dump()。\npickle.load（file，*，fix_imports = True，encoding =“ASCII”，errors =“strict” ）\n从打开的文件对象 文件中读取pickle对象表示，并返回其中指定的重构对象层次结构。这相当于Unpickler(file).load()。\npickle的协议版本是自动检测的，因此不需要协议参数。超过pickle对象的表示的字节将被忽略。\n参数文件必须有两个方法，一个采用整数参数的read()方法和一个不需要参数的readline()方法。两种方法都应返回字节。因此，文件可以是为二进制读取而打开的磁盘文件，io.BytesIO对象或满足此接口的任何其他自定义对象。\n可选的关键字参数是fix_imports，encoding和errors，用于控制Python 2生成的pickle流的兼容性支持。如果fix_imports为true，则pickle将尝试将旧的Python 2名称映射到Python 3中使用的新名称。编码和 错误告诉pickle如何解码Python 2编码的8位字符串实例; 这些默认分别为’ASCII’和’strict’。该编码可以是“字节”作为字节对象读取这些8位串的实例。使用encoding=‘latin1’所需的取储存NumPy的阵列和实例datetime，date并且time被Python 2解码。\npickle.loads（bytes_object，*，fix_imports = True，encoding =“ASCII”，errors =“strict” ）\n从bytes对象读取pickle对象层次结构并返回其中指定的重构对象层次结构。\npickle的协议版本是自动检测的，因此不需要协议参数。超过pickle对象的表示的字节将被忽略。\nimport numpy as np\nimport pickle\nimport io\n \npath = &#039;test&#039;\nf = open(path, &#039;wb&#039;)\ndata = {&#039;a&#039;:123, &#039;b&#039;:&#039;ads&#039;, &#039;c&#039;:[[1,2],[3,4]]}\npickle.dump(data, f)\nf.close()\n \nf1 = open(path, &#039;rb&#039;)\ndata1 = pickle.load(f1)\nprint(data1)\n{&#039;a&#039;: 123, &#039;b&#039;: &#039;ads&#039;, &#039;c&#039;: [[1, 2], [3, 4]]}\n\nfeedparser 模块\nfeedparser是一个Python的Feed解析库，可以处理RSS ，CDF，Atom 。使用它我们可从任何 RSS 或 Atom 订阅源得到标题、链接和文章的条目了。\nRSS(Really Simple Syndication,简易信息聚合)是一种描述和同步网站内容的格式你可以认为是一种定制个性化推送信息的服务。它能够解决你漫无目的的浏览网页的问题。它不会过时，信息越是过剩，它的意义也越加彰显。网络中充斥着大量的信息垃圾，每天摄入了太多自己根本不关心的信息。让自己关注的信息主动来找自己，且这些信息都是用户自己所需要的，这就是RSS的意义。\nparse() 方法\nfeedparser 最为核心的函数自然是 parse() 解析 URL 地址的函数。\n我们知道，每个RSS和Atom订阅源都包含一个标题（d.feed.title）和一组文章条目(d.entries)\n通常每个文章条目都有一段摘要（d.entries[i].summary）,或者是包含了条目中实际文本的描述性标签（d.entries[i].description）\nimport feedparser\nd=feedparser.parse(&#039;xvjie.wang/atom.xml&#039;)\nd.feed # 对应的值也是一个字典\n{&#039;title&#039;: &#039;Voidmort&#039;,\n &#039;title_detail&#039;: {&#039;type&#039;: &#039;text/plain&#039;,\n  &#039;language&#039;: None,\n  &#039;base&#039;: &#039;xvjie.wang/atom.xml&#039;,\n  &#039;value&#039;: &#039;Voidmort&#039;},\n &#039;links&#039;: [{&#039;href&#039;: &#039;xvjie.wang/atom.xml&#039;,\n   &#039;rel&#039;: &#039;self&#039;,\n   &#039;type&#039;: &#039;application/atom+xml&#039;},\n  {&#039;href&#039;: &#039;xvjie.wang/&#039;, &#039;rel&#039;: &#039;alternate&#039;, &#039;type&#039;: &#039;text/html&#039;}],\n &#039;link&#039;: &#039;xvjie.wang/&#039;,\n &#039;updated&#039;: &#039;2020-03-15T06:43:28.902Z&#039;,\n &#039;updated_parsed&#039;: time.struct_time(tm_year=2020, tm_mon=3, tm_mday=15, tm_hour=6, tm_min=43, tm_sec=28, tm_wday=6, tm_yday=75, tm_isdst=0),\n &#039;id&#039;: &#039;xvjie.wang/&#039;,\n &#039;guidislink&#039;: False,\n &#039;authors&#039;: [{&#039;name&#039;: &#039;Voidmort&#039;}],\n &#039;author_detail&#039;: {&#039;name&#039;: &#039;Voidmort&#039;},\n &#039;author&#039;: &#039;Voidmort&#039;,\n &#039;generator_detail&#039;: {&#039;href&#039;: &#039;hexo.io/&#039;, &#039;name&#039;: &#039;Hexo&#039;},\n &#039;generator&#039;: &#039;Hexo&#039;}\n\nd[&#039;feed&#039;][&#039;title&#039;]\n&#039;Voidmort&#039;\n\nd.feed.title    #通过属性的方式访问\n&#039;Voidmort&#039;\n\nd.feed.title_detail\n{&#039;type&#039;: &#039;text/plain&#039;,\n &#039;language&#039;: None,\n &#039;base&#039;: &#039;xvjie.wang/atom.xml&#039;,\n &#039;value&#039;: &#039;Voidmort&#039;}\n\nd.feed.link\n&#039;xvjie.wang/&#039;\n\n# 该属性类型为列表，表示一组文章的条目\nd.entries[:2]\n[{&#039;title&#039;: &#039;机器学习实战（三）&#039;,\n  &#039;title_detail&#039;: {&#039;type&#039;: &#039;text/plain&#039;,\n   &#039;language&#039;: None,\n   &#039;base&#039;: &#039;xvjie.wang/atom.xml&#039;,\n   &#039;value&#039;: &#039;机器学习实战（三）&#039;},\n  &#039;links&#039;: [{&#039;href&#039;: &#039;xvjie.wang/2020/03/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%89%EF%BC%89/&#039;,\n    &#039;rel&#039;: &#039;alternate&#039;,\n    &#039;type&#039;: &#039;text/html&#039;}],\n  &#039;link&#039;: &#039;xvjie.wang/2020/03/06/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98%EF%BC%88%E4%B8%89%EF%BC%89/&#039;,\n  &#039;id&#039;: &#039;xvjie.wang/2020/03/06/机器学习实战（三）/&#039;,\n  &#039;guidislink&#039;: False,\n  &#039;published&#039;: &#039;2020-03-06T02:15:50.000Z&#039;,\n  &#039;published_parsed&#039;: time.struct_time(tm_year=2020, tm_mon=3, tm_mday=6, tm_hour=2, tm_min=15, tm_sec=50, tm_wday=4, tm_yday=66, tm_isdst=0),\n  &#039;updated&#039;: &#039;2020-03-15T06:43:28.902Z&#039;,\n  &#039;updated_parsed&#039;: time.struct_time(tm_year=2020, tm_mon=3, tm_mday=15, tm_hour=6, tm_min=43, tm_sec=28, tm_wday=6, tm_yday=75, tm_isdst=0),\n  &#039;summary&#039;: &#039;&lt;h1 id=&quot;决策树的简介&quot;&gt;&lt;a href=&quot;#决策树的简介&quot; class=&quot;headerlink&quot;&#039;,\n  &#039;summary_detail&#039;: {&#039;type&#039;: &#039;text/html&#039;,\n   &#039;language&#039;: None,\n   &#039;base&#039;: &#039;xvjie.wang/atom.xml&#039;,\n   &#039;value&#039;: &#039;&lt;h1 id=&quot;决策树的简介&quot;&gt;&lt;a href=&quot;#决策树的简介&quot; class=&quot;headerlink&quot;&#039;},\n  &#039;tags&#039;: [{&#039;term&#039;: &#039;机器学习实战&#039;,\n    &#039;scheme&#039;: &#039;xvjie.wang/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E6%88%98/&#039;,\n    &#039;label&#039;: None},\n   {&#039;term&#039;: &#039;决策树&#039;,\n    &#039;scheme&#039;: &#039;xvjie.wang/tags/%E5%86%B3%E7%AD%96%E6%A0%91/&#039;,\n    &#039;label&#039;: None},\n   {&#039;term&#039;: &#039;ID3&#039;, &#039;scheme&#039;: &#039;xvjie.wang/tags/ID3/&#039;, &#039;label&#039;: None}]},\n {&#039;title&#039;: &#039;Python虚拟环境的搭建&#039;,\n  &#039;title_detail&#039;: {&#039;type&#039;: &#039;text/plain&#039;,\n   &#039;language&#039;: None,\n   &#039;base&#039;: &#039;xvjie.wang/atom.xml&#039;,\n   &#039;value&#039;: &#039;Python虚拟环境的搭建&#039;},\n  &#039;links&#039;: [{&#039;href&#039;: &#039;xvjie.wang/2020/02/19/Python%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83%E7%9A%84%E6%90%AD%E5%BB%BA/&#039;,\n    &#039;rel&#039;: &#039;alternate&#039;,\n    &#039;type&#039;: &#039;text/html&#039;}],\n  &#039;link&#039;: &#039;xvjie.wang/2020/02/19/Python%E8%99%9A%E6%8B%9F%E7%8E%AF%E5%A2%83%E7%9A%84%E6%90%AD%E5%BB%BA/&#039;,\n  &#039;id&#039;: &#039;xvjie.wang/2020/02/19/Python虚拟环境的搭建/&#039;,\n  &#039;guidislink&#039;: False,\n  &#039;published&#039;: &#039;2020-02-19T03:03:50.000Z&#039;,\n  &#039;published_parsed&#039;: time.struct_time(tm_year=2020, tm_mon=2, tm_mday=19, tm_hour=3, tm_min=3, tm_sec=50, tm_wday=2, tm_yday=50, tm_isdst=0),\n  &#039;updated&#039;: &#039;2020-03-06T09:01:06.144Z&#039;,\n  &#039;updated_parsed&#039;: time.struct_time(tm_year=2020, tm_mon=3, tm_mday=6, tm_hour=9, tm_min=1, tm_sec=6, tm_wday=4, tm_yday=66, tm_isdst=0),\n  &#039;summary&#039;: &#039;&lt;p&gt;我使用的Ubuntu18已经自带了pyhon3.6，现在我想用pip安装一些其它的应用的版本和现有的有冲突，为了防止冲突，我需要另一个python环境。&lt;/p&gt;\\n&lt;h1 id=&quot;python的安装&quot;&gt;&lt;a href=&quot;#python的安装&quot;&#039;,\n  &#039;summary_detail&#039;: {&#039;type&#039;: &#039;text/html&#039;,\n   &#039;language&#039;: None,\n   &#039;base&#039;: &#039;xvjie.wang/atom.xml&#039;,\n   &#039;value&#039;: &#039;&lt;p&gt;我使用的Ubuntu18已经自带了pyhon3.6，现在我想用pip安装一些其它的应用的版本和现有的有冲突，为了防止冲突，我需要另一个python环境。&lt;/p&gt;\\n&lt;h1 id=&quot;python的安装&quot;&gt;&lt;a href=&quot;#python的安装&quot;&#039;},\n  &#039;tags&#039;: [{&#039;term&#039;: &#039;virtualenv&#039;,\n    &#039;scheme&#039;: &#039;xvjie.wang/categories/virtualenv/&#039;,\n    &#039;label&#039;: None}]}]\n\nlen(d.entries)   #一共20篇文章\n20\n\n[e.title for e in d.entries][:5]         #列出前5篇文章的标题\n[&#039;机器学习实战（三）&#039;, &#039;Python虚拟环境的搭建&#039;, &#039;机器学习实战（二）&#039;, &#039;机器学习实战（一）&#039;, &#039;Django&#039;]\n\nd.entries[0].summary   #第一篇文章的摘要  和d.entries[0].description功能一样\n&#039;&lt;h1 id=&quot;决策树的简介&quot;&gt;&lt;a href=&quot;#决策树的简介&quot; class=&quot;headerlink&quot;&#039;\n\nsign()\nsign()是Python的Numpy中的取数字符号（数字前的正负号）的函数。\n\n#导入numpy库\nimport numpy as np\n \n#输入数据\ndataArr = [-0.2, -1.1, 0, 2.3, 4.5, 0.0]\nprint(&quot;输入数据为：&quot;)\nprint(dataArr)\n \n#使用numpy的sign(x)函数求输入数据的符号\nsignResult = np.sign(dataArr)\n \n#打印出sign()的输出结果\nprint(&quot;\\n使用sign函数的输出符号为：&quot;,signResult)\n输入数据为：\n[-0.2, -1.1, 0, 2.3, 4.5, 0.0]\n\n使用sign函数的输出符号为： [-1. -1.  0.  1.  1.  0.]\n\nnumpy.linalg\nnumpy.linalg模块包含线性代数的函数。使用这个模块，可以计算逆矩阵、求特征值、解线性方程组以及求解行列式等。\n求矩阵的逆\n注：矩阵必须是方阵且可逆，否则会抛出LinAlgError异常。\nimport numpy as np\n \nA = np.mat(&quot;0 1 2;1 0 3;4 -3 8&quot;)\nA\nmatrix([[ 0,  1,  2],\n        [ 1,  0,  3],\n        [ 4, -3,  8]])\n\n# 使用inv函数计算逆矩阵\ninv = np.linalg.inv(A)\ninv\nmatrix([[-4.5,  7. , -1.5],\n        [-2. ,  4. , -1. ],\n        [ 1.5, -2. ,  0.5]])\n\n# 检查原矩阵和求得的逆矩阵相乘的结果为单位矩阵\nA * inv\nmatrix([[1., 0., 0.],\n        [0., 1., 0.],\n        [0., 0., 1.]])\n\n求解线性方程组\nnumpy.linalg中的函数solve可以求解形如 Ax = b 的线性方程组，其中 A 为矩阵，b 为一维或二维的数组，x 是未知变量\n#创建矩阵和数组\nB = np.mat(&quot;1 -2 1;0 2 -8;-4 5 9&quot;)\nb = np.array([0,8,-9])\n \n# 调用solve函数求解线性方程\nx = np.linalg.solve(B,b)\nx\narray([29., 16.,  3.])\n\n# 使用dot函数检查求得的解是否正确\nnp.dot(B , x)\nmatrix([[ 0.,  8., -9.]])\n\n特征值和特征向量\n特征值（eigenvalue）即方程 Ax = ax 的根，是一个标量。\n其中，A 是一个二维矩阵，x 是一个一维向量。特征向量（eigenvector）是关于特征值的向量\nnumpy.linalg模块中，eigvals函数可以计算矩阵的特征值，而eig函数可以返回一个包含特征值和对应的特征向量的元组\n# 创建一个矩阵\nC = np.mat(&quot;3 -2;1 0&quot;)\n \n# 调用eigvals函数求解特征值\nc0 = np.linalg.eigvals(C)\nc0\narray([2., 1.])\n\n# 使用eig函数求解特征值和特征向量 \n#(该函数将返回一个元组，按列排放着特征值和对应的特征向量，其中第一列为特征值，第二列为特征向量)\nc1,c2 = np.linalg.eig(C)\nc1, c2\n(array([2., 1.]),\n matrix([[0.89442719, 0.70710678],\n         [0.4472136 , 0.70710678]]))\n\n# 使用dot函数验证求得的解是否正确\nfor i in range(len(c1)):\n    print (&quot;left:&quot;,np.dot(C,c2[:,i]))\n    print (&quot;right:&quot;,c1[i] * c2[:,i])\nleft: [[1.78885438]\n [0.89442719]]\nright: [[1.78885438]\n [0.89442719]]\nleft: [[0.70710678]\n [0.70710678]]\nright: [[0.70710678]\n [0.70710678]]\n\n奇异值分解\nSVD（Singular Value Decomposition，奇异值分解）是一种因子分解运算，将一个矩阵分解为3个矩阵的乘积\nnumpy.linalg模块中的svd函数可以对矩阵进行奇异值分解。该函数返回3个矩阵——U、Sigma和V，其中U和V是正交矩阵，Sigma包含输入矩阵的奇异值。\n# 分解矩阵\nD = np.mat(&quot;4 11 14;8 7 -2&quot;)\n# 使用svd函数分解矩阵\nU,Sigma,V = np.linalg.svd(D,full_matrices=False)\nU, Sigma, V\n(matrix([[-0.9486833 , -0.31622777],\n         [-0.31622777,  0.9486833 ]]),\n array([18.97366596,  9.48683298]),\n matrix([[-0.33333333, -0.66666667, -0.66666667],\n         [ 0.66666667,  0.33333333, -0.66666667]]))\n\n结果包含等式中左右两端的两个正交矩阵U和V，以及中间的奇异值矩阵Sigma\n# 使用diag函数生成完整的奇异值矩阵。将分解出的3个矩阵相乘\nU * np.diag(Sigma) * V\nmatrix([[ 4., 11., 14.],\n        [ 8.,  7., -2.]])\n\n广义逆矩阵\n使用numpy.linalg模块中的pinv函数进行求解,\n注：inv函数只接受方阵作为输入矩阵，而pinv函数则没有这个限制\n# 创建一个矩阵\nE = np.mat(&quot;4 11 14;8 7 -2&quot;)\n# 使用pinv函数计算广义逆矩阵\npseudoinv = np.linalg.pinv(E)\npseudoinv\nmatrix([[-0.00555556,  0.07222222],\n        [ 0.02222222,  0.04444444],\n        [ 0.05555556, -0.05555556]])\n\n# 将原矩阵和得到的广义逆矩阵相乘\nE * pseudoinv\nmatrix([[ 1.00000000e+00, -9.29811783e-16],\n        [-1.66533454e-16,  1.00000000e+00]])\n\n行列式\nnumpy.linalg模块中的det函数可以计算矩阵的行列式\n# 计算矩阵的行列式\nF = np.mat(&quot;3.0 4.0;5.0 6.0&quot;)\n# 使用det函数计算行列式\nnp.linalg.det(F)\n-1.9999999999999971\n\n3×6-4×5=-2 ?\nnp.eye((3))\narray([[1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.]])\n\nBeautifulSoup\nBeautiful Soup 是一个可以从HTML或XML文件中提取数据的Python库.它能够通过你喜欢的转换器实现惯用的文档导航,查找,修改文档的方式.Beautiful Soup会帮你节省数小时甚至数天的工作时间.\nbeautifulsoup.readthedocs.io/zh_CN/v4.4.0/\npower()\n函数解释：\npower(A,B) ：求A的B次方，数学等价于A^B\n其中A和B既可以是数字(标量),也可以是列表(向量)\na , b = 3, 4\nnp.power(a, b)\n81\n\nA, B = [1, 2, 3], 3\nnp.power(A, B)\narray([ 1,  8, 27], dtype=int32)\n\nA B都是列表(向量)时候，必须len(A)=len(B)\nA, B = [1, 2, 3], [4, 5, 6]\nnp.power(A, B)\narray([  1,  32, 729], dtype=int32)\n\nnonzero()\nnonzero(a)\nnonzero函数是numpy中用于得到数组array中非零元素的位置（数组索引）的函数。它的返回值是一个长度为a.ndim(数组a的轴数)的元组，元组的每个元素都是一个整数数组，其值为非零元素的下标在对应轴上的值。\n（1）只有a中非零元素才会有索引值，那些零值元素没有索引值；\n（2）返回的索引值数组是一个2维tuple数组，该tuple数组中包含一维的array数组。其中，一维array向量的个数与a的维数是一致的。\n（3）索引值数组的每一个array均是从一个维度上来描述其索引值。比如，如果a是一个二维数组，则索引值数组有两个array，第一个array从行维度来描述索引值；第二个array从列维度来描述索引值。\n（4）transpose(np.nonzero(x))函数能够描述出每一个非零元素在不同维度的索引值。\n（5）通过a[nonzero(a)]得到所有a中的非零值\na是一维数组\nimport numpy as np\na = [0,2,3]\nb = np.nonzero(a)\nb\n(array([1, 2], dtype=int64),)\n\nnp.array(b).ndim\n2\n\na是二维数组\na = np.array([[0,0,3],[0,0,0],[0,0,9]])\nb = np.nonzero(a)\na, b\n(array([[0, 0, 3],\n        [0, 0, 0],\n        [0, 0, 9]]),\n (array([0, 2], dtype=int64), array([2, 2], dtype=int64)))\n\nnp.array(b).ndim\n2\n\nnp.transpose(np.nonzero(a))\narray([[0, 2],\n       [2, 2]], dtype=int64)\n\nfrozenset()\n描述\nfrozenset() 返回一个冻结的集合，冻结后集合不能再添加或删除任何元素。\n语法\nfrozenset() 函数语法：\nclass frozenset([iterable])\n参数\niterable — 可迭代的对象，比如列表、字典、元组等等。\n返回值\n返回新的 frozenset 对象，如果不提供任何参数，默认会生成空集合。\n实例\n以下实例展示了 frozenset() 的使用方法：\na = frozenset(range(10))     # 生成一个新的不可变集合\na\nfrozenset({0, 1, 2, 3, 4, 5, 6, 7, 8, 9})\n\nb = frozenset(&#039;voidmort&#039;)\nb # 创建不可变集合\nfrozenset({&#039;d&#039;, &#039;i&#039;, &#039;m&#039;, &#039;o&#039;, &#039;r&#039;, &#039;t&#039;, &#039;v&#039;})\n\napriori 关联分析\n%pip install efficient-apriori\n  Downloading files.pythonhosted.org/packages/5a/c6/ecdf3a32d23cada466634c649cf4f50fefe76f56eae53ecceff688b306be/efficient_apriori-1.1.1-py3-none-any.whl\nInstalling collected packages: efficient-apriori\nSuccessfully installed efficient-apriori-1.1.1\n\nfrom efficient_apriori import apriori\ntransactions = [(&#039;eggs&#039;, &#039;bacon&#039;, &#039;soup&#039;),\n                (&#039;eggs&#039;, &#039;bacon&#039;, &#039;apple&#039;),\n                (&#039;soup&#039;, &#039;bacon&#039;, &#039;banana&#039;)]\nitemsets, rules = apriori(transactions, min_support=0.5, min_confidence=1)\nprint(rules) \n[{eggs} -&gt; {bacon}, {soup} -&gt; {bacon}]\n\nNumPy-corrcoef()\nnumpy.corrcoef(x, y=None, rowvar=True, bias=, ddof=)\n返回皮尔逊积矩相关系数。\n相关系数矩阵之间的关系，\n返回值r介于-1和1之间（含1）。r=0,没有相关性。\n参数:\nx : array_like\n包含多个变量和观测值的一维或二维数组。每行 x 表示一个变量，每列都是对所有这些变量的单个观察。也看到 rowvar 下面。\n\ny : 阵列式，可选\n一组附加的变量和观察值。 y 形状与 x .\n\n罗瓦尔 : 可选的布尔\n如果 rowvar 为真（默认值），则每行代表一个变量，列中包含观测值。否则，关系将被转置：每列表示一个变量，而行包含观测值。\n\n返回:\nR : 变量的相关系数矩阵。\n\nimport numpy as np\n \na = np.array([1,2,3])\nb = np.array([3,4,5])\n \ndef correlation(x, y):\n    return (((x-x.mean())/(x.std(ddof=0)))*((y-y.mean())/(y.std(ddof=0)))).mean() \n \ncorrelation(a,b)\n0.9999999999999999\n\nnp.corrcoef(a, b)\narray([[1., 1.],\n       [1., 1.]])\n"},"机器学习笔记/1.机器学习":{"slug":"机器学习笔记/1.机器学习","filePath":"机器学习笔记/1.机器学习.md","title":"1.机器学习","links":[],"tags":["机器学习"],"content":"这篇博文是对Andrew Ng的 机器学习入门的学习笔记，\n关于机器学习已经看了一段时间了，现在开始正式总结一下这段时间所学的东西，一边学习一边记录，希望能够更完这个博文。\n首先复习数学，哎，学校学的全还给老师了。。。\n线性代数\n\n线性代数是数学的一个分支，它的研究对象是向量，向量空间（或称线性空间），线性变换和有限维的线性方程组。向量空间是现代数学的一个重要课题；因而，线性代数被广泛地应用于抽象代数和泛函分析中；通过解析几何，线性代数得以被具体表示。线性代数的理论已被泛化为算子理论。由于科学研究中的非线性模型通常可以被近似为线性模型，使得线性代数被广泛地应用于自然科学和社会科学中。\n\n矩阵\n定义\n由m\\timesn个数排列成m行n列的矩阵，简称m\\times记作：\n\n矩阵加法\n\n矩阵的加法满足下列运算律(A，B，C都是同型矩阵)：\n\nA + B = B + A\n\n\n(A + B) + C = A + (B + C)\n\n只有行列相同的的矩阵才可以进行加法\n矩阵减法\n\n数乘\n\n矩阵的加减法和矩阵的数乘合称矩阵的线性运算\n转置\n把矩阵A的行和列互换产生的新矩阵称之为矩阵A的转置\n\n矩阵的转置满足一下定律：\n(A^T)^T = A\n(\\lambda A^T) = \\lambda A^T\n(AB)^T = B^TA^T\n矩阵乘法\n两个矩阵能够相乘，当且仅当第一个矩阵A的列数等于第二个矩阵B的行数时才能定义，如果A是m\\times n的矩阵B是n\\times p的矩阵，他们的乘积C将是一个m\\times p的矩阵C=(C_{ij}),它的每个元素是：\nc_{i,j} = a_{i,1}b_{1,j} + a_{i,2}b_{2,j} + ... + a_{i,n}b{n,j} = \\sum_{r=1}^n a_{i,r}b_{r,j}\n记作：C = AB\n例如：\n\n矩阵的乘法满足以下运算律：\n结合律，分配律，矩阵乘法不满足交换律。\n转置:(AB)^T=B^TA^T\n\n当矩阵A的列数等于矩阵B的行数时，A与B可以相乘。\n矩阵C的行数等于矩阵A的行数，C的列数等于B的列数。\n乘积C的第m行第n列的元素等于矩阵A的第m行的元素与矩阵B的第n列对应元素乘积之和。\n\n导数\n对于机器学习不需要理解的太深入，深入的自己也没学懂&gt;_&lt;,大概就是知道且会求偏导，知道斜率的意义就够了，其他部分太复杂就不记录了。。。好后悔当初没有好好学习微积分\n概率论\n先验概率和后验概率：\n后验概率是指在得到“结果”的信息后重新修正的概率，是“执果寻因”问题中的”果”。先验概率与后验概率有不可分割的联系，后验概率的计算要以先验概率为基础。\n事情还没有发生，要求这件事情发生的可能性的大小，是先验概率。事情已经发生，要求这件事情发生的原因是由某个因素引起的可能性的大小，是后验概率。\n先验概率不是根据有关自然状态的全部资料测定的，而只是利用现有的材料(主要是历史资料)计算的；后验概率使用了有关自然状态更加全面的资料，既有先验概率资料，也有补充资料；\n先验概率的计算比较简单，没有使用贝叶斯公式；而后验概率的计算，要使用贝叶斯公式，而且在利用样本资料计算逻辑概率时，还要使用理论概率分布，需要更多的数理统计知识。\n目前先知道这点就够了，具体以后再补充\n数学知识复习完毕 正式开始机器学习"},"机器学习笔记/10.机器学习":{"slug":"机器学习笔记/10.机器学习","filePath":"机器学习笔记/10.机器学习.md","title":"10.机器学习","links":[],"tags":["机器学习"],"content":"十三、聚类(Clustering)\n无监督学习：简介\n参考视频: 13 - 1 - Unsupervised Learning_ Introduction (3 min).mkv\n在这个视频中，我将开始介绍聚类算法。这将是一个激动人心的时刻，因为这是我们学习的第一个非监督学习算法。我们将要让计算机学习无标签数据，而不是此前的标签数据。\n那么，什么是非监督学习呢？在课程的一开始，我曾简单的介绍过非监督学习，然而，我们还是有必要将其与监督学习做一下比较。\n在一个典型的监督学习中，我们有一个有标签的训练集，我们的目标是找到能够区分正样本和负样本的决策边界，在这里的监督学习中，我们有一系列标签，我们需要据此拟合一个假设函数。与此不同的是，在非监督学习中，我们的数据没有附带任何标签，我们拿到的数据就是这样的：\n\n在这里我们有一系列点，却没有标签。因此，我们的训练集可以写成只有x^{(1)},x^{(2)}…..一直到x^{(m)}。我们没有任何标签y。因此，图上画的这些点没有标签信息。也就是说，在非监督学习中，我们需要将一系列无标签的训练数据，输入到一个算法中，然后我们告诉这个算法，快去为我们找找这个数据的内在结构给定数据。我们可能需要某种算法帮助我们寻找一种结构。图上的数据看起来可以分成两个分开的点集（称为簇），一个能够找到我圈出的这些点集的算法，就被称为聚类算法。\n\n这将是我们介绍的第一个非监督学习算法。当然，此后我们还将提到其他类型的非监督学习算法，它们可以为我们找到其他类型的结构或者其他的一些模式，而不只是簇。\n我们将先介绍聚类算法。此后，我们将陆续介绍其他算法。那么聚类算法一般用来做什么呢？\n\n在这门课程的早些时候，我曾经列举过一些应用：比如市场分割。也许你在数据库中存储了许多客户的信息，而你希望将他们分成不同的客户群，这样你可以对不同类型的客户分别销售产品或者分别提供更适合的服务。社交网络分析：事实上有许多研究人员正在研究这样一些内容，他们关注一群人，关注社交网络，例如Facebook，Google+，或者是其他的一些信息，比如说：你经常跟哪些人联系，而这些人又经常给哪些人发邮件，由此找到关系密切的人群。因此，这可能需要另一个聚类算法，你希望用它发现社交网络中关系密切的朋友。我有一个朋友正在研究这个问题，他希望使用聚类算法来更好的组织计算机集群，或者更好的管理数据中心。因为如果你知道数据中心中，那些计算机经常协作工作。那么，你可以重新分配资源，重新布局网络。由此优化数据中心，优化数据通信。\n最后，我实际上还在研究如何利用聚类算法了解星系的形成。然后用这个知识，了解一些天文学上的细节问题。好的，这就是聚类算法。这将是我们介绍的第一个非监督学习算法。在下一个视频中，我们将开始介绍一个具体的聚类算法。\nK-均值算法\n参考视频: 13 - 2 - K-Means Algorithm (13 min).mkv\nK-均值是最普及的聚类算法，算法接受一个未标记的数据集，然后将数据聚类成不同的组。\nK-均值是一个迭代算法，假设我们想要将数据聚类成n个组，其方法为:\n首先选择K个随机的点，称为聚类中心（cluster centroids）；\n对于数据集中的每一个数据，按照距离K个中心点的距离，将其与距离最近的中心点关联起来，与同一个中心点关联的所有点聚成一类。\n计算每一个组的平均值，将该组所关联的中心点移动到平均值的位置。\n重复步骤2-4直至中心点不再变化。\n下面是一个聚类示例：\n\n迭代 1 次\n\n迭代 3 次\n\n迭代 10 次\n用μ^1,μ^2,…,μ^k 来表示聚类中心，用c^{(1)},c^{(2)},…,c^{(m)}来存储与第i个实例数据最近的聚类中心的索引，K-均值算法的伪代码如下：\nRepeat {\n\nfor i = 1 to m\n\nc(i) := index (form 1 to K) of cluster centroid closest to x(i)\n\nfor k = 1 to K\n\nμk := average (mean) of points assigned to cluster k\n\n}\n\n算法分为两个步骤，第一个for循环是赋值步骤，即：对于每一个样例i，计算其应该属于的类。第二个for循环是聚类中心的移动，即：对于每一个类K，重新计算该类的质心。\nK-均值算法也可以很便利地用于将数据分为许多不同组，即使在没有非常明显区分的组群的情况下也可以。下图所示的数据集包含身高和体重两项特征构成的，利用K-均值算法将数据分为三类，用于帮助确定将要生产的T-恤衫的三种尺寸。\n\n优化目标\n参考视频: 13 - 3 - Optimization Objective (7 min).mkv\nK-均值最小化问题，是要最小化所有的数据点与其所关联的聚类中心点之间的距离之和，因此\nK-均值的代价函数（又称畸变函数 Distortion function）为\nJ(c^{(1)},...,c^{(m)},μ_1,...,μ_K)=\\dfrac {1}{m}\\sum^{m}_{i=1}\\left\\| X^{\\left( i\\right) }-\\mu_{c^{(i)}}\\right\\| ^{2}\n其中{{\\mu }_{{{c}^{(i)}}}}代表与{{x}^{(i)}}最近的聚类中心点。\n我们的的优化目标便是找出使得代价函数最小的 c^{(1)},c^{(2)},…,c^{(m)}和μ^1,μ^2,…,μ^k\n\n回顾刚才给出的:\nK-均值迭代算法，我们知道，第一个循环是用于减小c^{(i)}引起的代价，而第二个循环则是用于减小{{\\mu }_{i}}引起的代价。\n迭代的过程一定会是每一次迭代都在减小代价函数，不然便是出现了错误。\n随机初始化\n参考视频: 13 - 4 - Random Initialization (8 min).mkv\n在运行K-均值算法的之前，我们首先要随机初始化所有的聚类中心点，下面介绍怎样做：\n\n\n我们应该选择K&lt;m，即聚类中心点的个数要小于所有训练集实例的数量\n\n\n随机选择K个训练实例，然后令K个聚类中心分别与这K个训练实例相等\n\n\nK-均值的一个问题在于，它有可能会停留在一个局部最小值处，而这取决于初始化的情况。\n\n为了解决这个问题，我们通常需要多次运行K-均值算法，每一次都重新进行随机初始化，最后再比较多次运行K-均值的结果，选择代价函数最小的结果。\n这种方法在K较小的时候（2—10）还是可行的，但是如果K较大，这么做也可能不会有明显地改善。\n选择聚类数\n参考视频: 13 - 5 - Choosing the Number of Clusters (8 min).mkv\n没有所谓最好的选择聚类数的方法，通常是需要根据不同的问题，人工进行选择的。选择的时候思考我们运用K-均值算法聚类的动机是什么，然后选择能最好服务于该目的标聚类数。\n当人们在讨论，选择聚类数目的方法时，有一个可能会谈及的方法叫作“肘部法则”。关于“肘部法则”，我们所需要做的是改变K值，也就是聚类类别数目的总数。我们用一个聚类来运行K均值聚类方法。这就意味着，所有的数据都会分到一个聚类里，然后计算成本函数或者计算畸变函数J。K代表聚类数字。\n\n我们可能会得到一条类似于这样的曲线。像一个人的肘部。这就是“肘部法则”所做的，让我们来看这样一个图，看起来就好像有一个很清楚的肘在那儿。好像人的手臂，如果你伸出你的胳膊，那么这就是你的肩关节、肘关节、手。这就是“肘部法则”。你会发现这种模式，它的畸变值会迅速下降，从1到2，从2到3之后，你会在3的时候达到一个肘点。\n在此之后，畸变值就下降的非常慢，看起来就像使用3个聚类来进行聚类是正确的，这是因为那个点是曲线的肘点，畸变值下降得很快，K=3之后就下降得很慢，那么我们就选K=3。当你应用“肘部法则”的时候，如果你得到了一个像上面这样的图，那么这将是一种用来选择聚类个数的合理方法。\n例如，我们的 T-恤制造例子中，我们要将用户按照身材聚类，我们可以分成3个尺寸:S,M,L，也可以分成5个尺寸XS,S,M,L,XL，这样的选择是建立在回答“聚类后我们制造的T-恤是否能较好地适合我们的客户”这个问题的基础上作出的。\n聚类参考资料：\n1.相似度/距离计算方法总结\n(1). 闵可夫斯基距离Minkowski（其中欧式距离：p=2)\ndist(X,Y)={{\\left( {{\\sum\\limits_{i=1}^{n}{\\left| {{x}_{i}}-{{y}_{i}} \\right|}}^{p}} \\right)}^{\\frac{1}{p}}}\n(2). 杰卡德相似系数(Jaccard)：\nJ(A,B)=\\frac{\\left| A\\cap B \\right|}{\\left|A\\cup B \\right|}\n(3). 余弦相似度(cosine similarity)：\nn维向量x和y的夹角记做\\theta，根据余弦定理，其余弦值为：\ncos (\\theta )=\\frac{{{x}^{T}}y}{\\left|x \\right|\\cdot \\left| y \\right|}=\\frac{\\sum\\limits_{i=1}^{n}{{{x}_{i}}{{y}_{i}}}}{\\sqrt{\\sum\\limits_{i=1}^{n}{{{x}_{i}}^{2}}}\\sqrt{\\sum\\limits_{i=1}^{n}{{{y}_{i}}^{2}}}}\n(4). Pearson皮尔逊相关系数：\n{{\\rho }_{XY}}=\\frac{\\operatorname{cov}(X,Y)}{{{\\sigma }_{X}}{{\\sigma }_{Y}}}=\\frac{E[(X-{{\\mu }_{X}})(Y-{{\\mu }_{Y}})]}{{{\\sigma }_{X}}{{\\sigma }_{Y}}}=\\frac{\\sum\\limits_{i=1}^{n}{(x-{{\\mu }_{X}})(y-{{\\mu }_{Y}})}}{\\sqrt{\\sum\\limits_{i=1}^{n}{{{(x-{{\\mu }_{X}})}^{2}}}}\\sqrt{\\sum\\limits_{i=1}^{n}{{{(y-{{\\mu }_{Y}})}^{2}}}}}\nPearson相关系数即将x、y坐标向量各自平移到原点后的夹角余弦。\n2.聚类的衡量指标\n(1). 均一性：p\n类似于精确率，一个簇中只包含一个类别的样本，则满足均一性。其实也可以认为就是正确率(每个 聚簇中正确分类的样本数占该聚簇总样本数的比例和)\n(2). 完整性：r\n类似于召回率，同类别样本被归类到相同簇中，则满足完整性;每个聚簇中正确分类的样本数占该\n类型的总样本数比例的和\n(3). V-measure:\n均一性和完整性的加权平均\nV = \\frac{(1+\\beta^2)*pr}{\\beta^2*p+r}\n(4). 轮廓系数\n样本i的轮廓系数：s(i)\n簇内不相似度:计算样本i到同簇其它样本的平均距离为a(i)，应尽可能小。\n簇间不相似度:计算样本i到其它簇C_j的所有样本的平均距离b_{ij}，应尽可能大。\n轮廓系数：s(i)值越接近1表示样本i聚类越合理，越接近-1，表示样本i应该分类到 另外的簇中，近似为0，表示样本i应该在边界上;所有样本的s(i)的均值被成为聚类结果的轮廓系数。\ns(i) = \\frac{b(i)-a(i)}{max\\{a(i),b(i)\\}}\n(5). ARI\n数据集S共有N个元素，  两个聚类结果分别是：\nX=\\{{{X}_{1}},{{X}_{2}},...,{{X}_{r}}\\},Y=\\{{{Y}_{1}},{{Y}_{2}},...,{{Y}_{s}}\\}\nX和Y的元素个数为：\na=\\{{{a}_{1}},{{a}_{2}},...,{{a}_{r}}\\},b=\\{{{b}_{1}},{{b}_{2}},...,{{b}_{s}}\\}\n记：{{n}_{ij}}=\\left| {{X}_{i}}\\cap {{Y}_{i}} \\right|\nARI=\\frac{\\sum\\limits_{i,j}{C_{{{n}_{ij}}}^{2}}-\\left[ \\left( \\sum\\limits_{i}{C_{{{a}_{i}}}^{2}} \\right)\\cdot \\left( \\sum\\limits_{i}{C_{{{b}_{i}}}^{2}} \\right) \\right]/C_{n}^{2}}{\\frac{1}{2}\\left[ \\left( \\sum\\limits_{i}{C_{{{a}_{i}}}^{2}} \\right)+\\left( \\sum\\limits_{i}{C_{{{b}_{i}}}^{2}} \\right) \\right]-\\left[ \\left( \\sum\\limits_{i}{C_{{{a}_{i}}}^{2}} \\right)\\cdot \\left( \\sum\\limits_{i}{C_{{{b}_{i}}}^{2}} \\right) \\right]/C_{n}^{2}}\n十四、降维(Dimensionality Reduction)\n动机一：数据压缩\n参考视频: 14 - 1 - Motivation I_ Data Compression (10 min).mkv\n这个视频，我想开始谈论第二种类型的无监督学习问题，称为降维。有几个不同的的原因使你可能想要做降维。一是数据压缩，后面我们会看了一些视频后，数据压缩不仅允许我们压缩数据，因而使用较少的计算机内存或磁盘空间，但它也让我们加快我们的学习算法。\n但首先，让我们谈论降维是什么。作为一种生动的例子，我们收集的数据集，有许多，许多特征，我绘制两个在这里。\n\n假设我们未知两个的特征：x_1:长度：用厘米表示；x_2：是用英寸表示同一物体的长度。\n所以，这给了我们高度冗余表示，也许不是两个分开的特征x_1和x_2，这两个基本的长度度量，也许我们想要做的是减少数据到一维，只有一个数测量这个长度。这个例子似乎有点做作，这里厘米英寸的例子实际上不是那么不切实际的，两者并没有什么不同。\n将数据从二维降至一维：\n假使我们要采用两种不同的仪器来测量一些东西的尺寸，其中一个仪器测量结果的单位是英寸，另一个仪器测量的结果是厘米，我们希望将测量的结果作为我们机器学习的特征。现在的问题的是，两种仪器对同一个东西测量的结果不完全相等（由于误差、精度等），而将两者都作为特征有些重复，因而，我们希望将这个二维的数据降至一维。\n从这件事情我看到的东西发生在工业上的事。如果你有几百个或成千上万的特征，它是它这往往容易失去你需要的特征。有时可能有几个不同的工程团队，也许一个工程队给你二百个特征，第二工程队给你另外三百个的特征，第三工程队给你五百个特征，一千多个特征都在一起，它实际上会变得非常困难，去跟踪你知道的那些特征，你从那些工程队得到的。其实不想有高度冗余的特征一样。\n\n多年我一直在研究直升飞机自动驾驶。诸如此类。如果你想测量——如果你想做，你知道，做一个调查或做这些不同飞行员的测试——你可能有一个特征：x_1，这也许是他们的技能（直升机飞行员），也许x_2可能是飞行员的爱好。这是表示他们是否喜欢飞行，也许这两个特征将高度相关。你真正关心的可能是这条红线的方向，不同的特征，决定飞行员的能力。\n\n将数据从三维降至二维：\n这个例子中我们要将一个三维的特征向量降至一个二维的特征向量。过程是与上面类似的，我们将三维向量投射到一个二维的平面上，强迫使得所有的数据都在同一个平面上，降至二维的特征向量。\n\n这样的处理过程可以被用于把任何维度的数据降到任何想要的维度，例如将1000维的特征降至100维。\n正如我们所看到的，最后，这将使我们能够使我们的一些学习算法运行也较晚，但我们会在以后的视频提到它。\n动机二：数据可视化\n参考视频: 14 - 2 - Motivation II_ Visualization (6 min).mkv\n在许多及其学习问题中，如果我们能将数据可视化，我们便能寻找到一个更好的解决方案，降维可以帮助我们。\n\n假使我们有有关于许多不同国家的数据，每一个特征向量都有50个特征（如GDP，人均GDP，平均寿命等）。如果要将这个50维的数据可视化是不可能的。使用降维的方法将其降至2维，我们便可以将其可视化了。\n\n这样做的问题在于，降维的算法只负责减少维数，新产生的特征的意义就必须由我们自己去发现了。\n主成分分析问题\n参考视频: 14 - 3 - Principal Component Analysis Problem Formulation (9 min). mkv\n主成分分析(PCA)是最常见的降维算法。\n在PCA中，我们要做的是找到一个方向向量（Vector direction），当我们把所有的数据都投射到该向量上时，我们希望投射平均均方误差能尽可能地小。方向向量是一个经过原点的向量，而投射误差是从特征向量向该方向向量作垂线的长度。\n\n下面给出主成分分析问题的描述：\n问题是要将n维数据降至k维，目标是找到向量u^{(1)},u^{(2)},…,u^{(k)}使得总的投射误差最小。主成分分析与线性回顾的比较：\n主成分分析与线性回归是两种不同的算法。主成分分析最小化的是投射误差（Projected Error），而线性回归尝试的是最小化预测误差。线性回归的目的是预测结果，而主成分分析不作任何预测。\n\n上图中，左边的是线性回归的误差（垂直于横轴投影），右边则是主要成分分析的误差（垂直于红线投影）。\nPCA将n个特征降维到k个，可以用来进行数据压缩，如果100维的向量最后可以用10维来表示，那么压缩率为90%。同样图像处理领域的KL变换使用PCA做图像压缩。但PCA 要保证降维后，还要保证数据的特性损失最小。\nPCA技术的一大好处是对数据进行降维的处理。我们可以对新求出的“主元”向量的重要性进行排序，根据需要取前面最重要的部分，将后面的维数省去，可以达到降维从而简化模型或是对数据进行压缩的效果。同时最大程度的保持了原有数据的信息。\nPCA技术的一个很大的优点是，它是完全无参数限制的。在PCA的计算过程中完全不需要人为的设定参数或是根据任何经验模型对计算进行干预，最后的结果只与数据相关，与用户是独立的。\n但是，这一点同时也可以看作是缺点。如果用户对观测对象有一定的先验知识，掌握了数据的一些特征，却无法通过参数化等方法对处理过程进行干预，可能会得不到预期的效果，效率也不高。\n主成分分析算法\n参考视频: 14 - 4 - Principal Component Analysis Algorithm (15 min).mkv\nPCA 减少n维到k维：\n第一步是均值归一化。我们需要计算出所有特征的均值，然后令 x_j= x_j-μ_j。如果特征是在不同的数量级上，我们还需要将其除以标准差 σ^2。\n第二步是计算协方差矩阵（covariance matrix）Σ：\n\\sum=\\dfrac {1}{m}\\sum^{n}_{i=1}\\left( x^{(i)}\\right) \\left( x^{(i)}\\right) ^{T}\n第三步是计算协方差矩阵Σ的特征向量（eigenvectors）:\n在 Octave 里我们可以利用奇异值分解（singular value decomposition）来求解，[U, S, V]= svd(sigma)。\n\nSigma=\\dfrac {1}{m}\\sum^{n}_{i=1}\\left( x^{(i)}\\right) \\left( x^{(i)}\\right) ^{T}\n\n对于一个 n×n维度的矩阵，上式中的U是一个具有与数据之间最小投射误差的方向向量构成的矩阵。如果我们希望将数据从n维降至k维，我们只需要从U中选取前k个向量，获得一个n×k维度的矩阵，我们用U_{reduce}表示，然后通过如下计算获得要求的新特征向量z^{(i)}:\nz^{(i)}=U^{T}_{reduce}*x^{(i)}\n其中x是n×1维的，因此结果为k×1维度。注，我们不对方差特征进行处理。\n选择主成分的数量\n参考视频: 14 - 5 - Choosing The Number Of Principal Components (13 min).mkv\n主要成分分析是减少投射的平均均方误差：\n训练集的方差为：\\dfrac {1}{m}\\sum^{m}_{i=1}\\left\\| x^{\\left( i\\right) }\\right\\| ^{2}\n我们希望在平均均方误差与训练集方差的比例尽可能小的情况下选择尽可能小的k值。\n如果我们希望这个比例小于1%，就意味着原本数据的偏差有99%都保留下来了，如果我们选择保留95%的偏差，便能非常显著地降低模型中特征的维度了。\n我们可以先令k=1，然后进行主要成分分析，获得U_{reduce}和z，然后计算比例是否小于1%。如果不是的话再令k=2，如此类推，直到找到可以使得比例小于1%的最小k 值（原因是各个特征之间通常情况存在某种相关性）。\n还有一些更好的方式来选择k，当我们在Octave中调用“svd”函数的时候，我们获得三个参数：[U, S, V] = svd(sigma)。\n\n其中的S是一个n×n的矩阵，只有对角线上有值，而其它单元都是0，我们可以使用这个矩阵来计算平均均方误差与训练集方差的比例：\n\\dfrac {\\dfrac {1}{m}\\sum^{m}_{i=1}\\left\\| x^{\\left( i\\right) }-x^{\\left( i\\right) }_{approx}\\right\\| ^{2}}{\\dfrac {1}{m}\\sum^{m}_{i=1}\\left\\| x^{(i)}\\right\\| ^{2}}=1-\\dfrac {\\Sigma^{k}_{i=1}S_{ii}}{\\Sigma^{m}_{i=1}S_{ii}}\\leq 1\\%\n也就是：\\frac {\\Sigma^{k}_{i=1}s_{ii}}{\\Sigma^{n}_{i=1}s_{ii}}\\geq0.99\n在压缩过数据后，我们可以采用如下方法来近似地获得原有的特征：x^{\\left( i\\right) }_{approx}=U_{reduce}z^{(i)}\n重建的压缩表示\n参考视频: 14 - 6 - Reconstruction from Compressed Representation (4 min).mkv\n在以前的视频中，我谈论PCA作为压缩算法。在那里你可能需要把1000维的数据压缩100维特征，或具有三维数据压缩到一二维表示。所以，如果这是一个压缩算法，应该能回到这个压缩表示，回到你原有的高维数据的一种近似。\n所以，给定的z^{(i)}，这可能100维，怎么回到你原来的表示x^{(i)}，这可能是1000维的数组？\n\nPCA算法，我们可能有一个这样的样本。如图中样本x^{(1)},x^{(2)}。我们做的是，我们把这些样本投射到图中这个一维平面。然后现在我们需要只使用一个实数，比如z^{(1)}，指定这些点的位置后他们被投射到这一个三维曲面。给定一个点z^{(1)}，我们怎么能回去这个原始的二维空间呢？x为2维，z为1维，z=U^{T}_{reduce}x，相反的方程为：x_{appox}=U_{reduce}\\cdot z,x_{appox}\\approx x。如图：\n\n如你所知，这是一个漂亮的与原始数据相当相似。所以，这就是你从低维表示z回到未压缩的表示。我们得到的数据的一个之间你的原始数据 x，我们也把这个过程称为重建原始数据。\n当我们认为试图重建从压缩表示 x 的初始值。所以，给定未标记的数据集，您现在知道如何应用PCA，你的带高维特征x和映射到这的低维表示z。这个视频，希望你现在也知道如何采取这些低维表示z，映射到备份到一个近似你原有的高维数据。\n现在你知道如何实施应用PCA，我们将要做的事是谈论一些技术在实际使用PCA很好，特别是，在接下来的视频中，我想谈一谈关于如何选择k。\n主成分分析法的应用建议\n参考视频: 14 - 7 - Advice for Applying PCA (13 min).mkv\n假使我们正在针对一张 100×100像素的图片进行某个计算机视觉的机器学习，即总共有10000 个特征。\n\n\n第一步是运用主要成分分析将数据压缩至1000个特征\n\n\n然后对训练集运行学习算法\n\n在预测时，采用之前学习而来的U_{reduce}将输入的特征x转换成特征向量z，然后再进行预测\n\n\n\n注：如果我们有交叉验证集合测试集，也采用对训练集学习而来的U_{reduce}。\n错误的主要成分分析情况：一个常见错误使用主要成分分析的情况是，将其用于减少过拟合（减少了特征的数量）。这样做非常不好，不如尝试正则化处理。原因在于主要成分分析只是近似地丢弃掉一些特征，它并不考虑任何与结果变量有关的信息，因此可能会丢失非常重要的特征。然而当我们进行正则化处理时，会考虑到结果变量，不会丢掉重要的数据。\n另一个常见的错误是，默认地将主要成分分析作为学习过程中的一部分，这虽然很多时候有效果，最好还是从所有原始特征开始，只在有必要的时候（算法运行太慢或者占用太多内存）才考虑采用主要成分分析。"},"机器学习笔记/11.机器学习":{"slug":"机器学习笔记/11.机器学习","filePath":"机器学习笔记/11.机器学习.md","title":"11.机器学习","links":[],"tags":["机器学习"],"content":"十五、异常检测(Anomaly Detection)\n15.1 问题的动机\n参考文档: 15 - 1 - Problem Motivation (8 min).mkv\n在接下来的一系列视频中，我将向大家介绍异常检测(Anomaly detection)问题。这是机器学习算法的一个常见应用。这种算法的一个有趣之处在于：它虽然主要用于非监督学习问题，但从某些角度看，它又类似于一些监督学习问题。\n什么是异常检测呢？为了解释这个概念，让我举一个例子吧：\n假想你是一个飞机引擎制造商，当你生产的飞机引擎从生产线上流出时，你需要进行QA(质量控制测试)，而作为这个测试的一部分，你测量了飞机引擎的一些特征变量，比如引擎运转时产生的热量，或者引擎的振动等等。\n\n这样一来，你就有了一个数据集，从x^{(1)}到x^{(m)}，如果你生产了m个引擎的话，你将这些数据绘制成图表，看起来就是这个样子：\n\n这里的每个点、每个叉，都是你的无标签数据。这样，异常检测问题可以定义如下：我们假设后来有一天，你有一个新的飞机引擎从生产线上流出，而你的新飞机引擎有特征变量x_{test}。所谓的异常检测问题就是：我们希望知道这个新的飞机引擎是否有某种异常，或者说，我们希望判断这个引擎是否需要进一步测试。因为，如果它看起来像一个正常的引擎，那么我们可以直接将它运送到客户那里，而不需要进一步的测试。\n给定数据集 x^{(1)},x^{(2)},..,x^{(m)}，我们假使数据集是正常的，我们希望知道新的数据 x_{test} 是不是异常的，即这个测试数据不属于该组数据的几率如何。我们所构建的模型应该能根据该测试数据的位置告诉我们其属于一组数据的可能性 p(x)。\n\n上图中，在蓝色圈内的数据属于该组数据的可能性较高，而越是偏远的数据，其属于该组数据的可能性就越低。\n这种方法称为密度估计，表达如下：\nif \\quad p(x)\n\\begin{cases}\n&lt; \\varepsilon &amp; anomaly \\\\\n&gt; =\\varepsilon &amp; normal\n\\end{cases}\n欺诈检测：\nx^{(i)} = {用户的第i个活动特征}\n模型p(x) 为我们其属于一组数据的可能性，通过p(x) &lt; \\varepsilon检测非正常用户。\n异常检测主要用来识别欺骗。例如在线采集而来的有关用户的数据，一个特征向量中可能会包含如：用户多久登录一次，访问过的页面，在论坛发布的帖子数量，甚至是打字速度等。尝试根据这些特征构建一个模型，可以用这个模型来识别那些不符合该模式的用户。\n再一个例子是检测一个数据中心，特征可能包含：内存使用情况，被访问的磁盘数量，CPU的负载，网络的通信量等。根据这些特征可以构建一个模型，用来判断某些计算机是不是有可能出错了。\n15.2 高斯分布\n参考视频: 15 - 2 - Gaussian Distribution (10 min).mkv\n在这个视频中，我将介绍高斯分布，也称为正态分布。回顾高斯分布的基本知识。\n通常如果我们认为变量 x 符合高斯分布 x \\sim N(\\mu, \\sigma^2)则其概率密度函数为：\np(x,\\mu,\\sigma^2)=\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)\n我们可以利用已有的数据来预测总体中的μ和σ^2的计算方法如下：\n\\mu=\\frac{1}{m}\\sum\\limits_{i=1}^{m}x^{(i)}\n\\sigma^2=\\frac{1}{m}\\sum\\limits_{i=1}^{m}(x^{(i)}-\\mu)^2\n高斯分布样例：\n\n注：机器学习中对于方差我们通常只除以m而非统计学中的(m-1)。这里顺便提一下，在实际使用中，到底是选择使用1/m还是1/(m-1)其实区别很小，只要你有一个还算大的训练集，在机器学习领域大部分人更习惯使用1/m这个版本的公式。这两个版本的公式在理论特性和数学特性上稍有不同，但是在实际使用中，他们的区别甚小，几乎可以忽略不计。\n15.3 算法\n参考视频: 15 - 3 - Algorithm (12 min).mkv\n在本节视频中，我将应用高斯分布开发异常检测算法。\n异常检测算法：\n对于给定的数据集 x^{(1)},x^{(2)},...,x^{(m)}，我们要针对每一个特征计算 \\mu 和 \\sigma^2 的估计值。\n\\mu_j=\\frac{1}{m}\\sum\\limits_{i=1}^{m}x_j^{(i)}\n\\sigma_j^2=\\frac{1}{m}\\sum\\limits_{i=1}^m(x_j^{(i)}-\\mu_j)^2\n一旦我们获得了平均值和方差的估计值，给定新的一个训练实例，根据模型计算 p(x)：\np(x)=\\prod\\limits_{j=1}^np(x_j;\\mu_j,\\sigma_j^2)=\\prod\\limits_{j=1}^1\\frac{1}{\\sqrt{2\\pi}\\sigma_j}exp(-\\frac{(x_j-\\mu_j)^2}{2\\sigma_j^2})\n当p(x) &lt; \\varepsilon时，为异常。\n下图是一个由两个特征的训练集，以及特征的分布情况：\n\n下面的三维图表表示的是密度估计函数，z轴为根据两个特征的值所估计p(x)值：\n\n我们选择一个\\varepsilon，将p(x) = \\varepsilon作为我们的判定边界，当p(x) &gt; \\varepsilon时预测数据为正常数据，否则为异常。\n在这段视频中，我们介绍了如何拟合p(x)，也就是 x的概率值，以开发出一种异常检测算法。同时，在这节课中，我们也给出了通过给出的数据集拟合参数，进行参数估计，得到参数 \\mu 和 \\sigma，然后检测新的样本，确定新样本是否是异常。\n在接下来的课程中，我们将深入研究这一算法，同时更深入地介绍，怎样让算法工作地更加有效。\n15.4 开发和评价一个异常检测系统\n参考视频: 15 - 4 - Developing and Evaluating an Anomaly Detection System (13 min). mkv\n异常检测算法是一个非监督学习算法，意味着我们无法根据结果变量  y 的值来告诉我们数据是否真的是异常的。我们需要另一种方法来帮助检验算法是否有效。当我们开发一个异常检测系统时，我们从带标记（异常或正常）的数据着手，我们从其中选择一部分正常数据用于构建训练集，然后用剩下的正常数据和异常数据混合的数据构成交叉检验集和测试集。\n例如：我们有10000台正常引擎的数据，有20台异常引擎的数据。 我们这样分配数据：\n6000台正常引擎的数据作为训练集\n2000台正常引擎和10台异常引擎的数据作为交叉检验集\n2000台正常引擎和10台异常引擎的数据作为测试集\n具体的评价方法如下：\n\n\n根据测试集数据，我们估计特征的平均值和方差并构建p(x)函数\n\n\n对交叉检验集，我们尝试使用不同的\\varepsilon值作为阀值，并预测数据是否异常，根据F1值或者查准率与查全率的比例来选择 \\varepsilon\n\n\n选出 \\varepsilon 后，针对测试集进行预测，计算异常检验系统的F1值，或者查准率与查全率之比\n\n\n15.5 异常检测与监督学习对比\n参考视频: 15 - 5 - Anomaly Detection vs. Supervised Learning (8 min).mkv\n之前我们构建的异常检测系统也使用了带标记的数据，与监督学习有些相似，下面的对比有助于选择采用监督学习还是异常检测：\n两者比较：\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n异常检测监督学习非常少量的正向类（异常数据 y=1）, 大量的负向类（y=0）同时有大量的正向类和负向类许多不同种类的异常，非常难。根据非常 少量的正向类数据来训练算法。有足够多的正向类实例，足够用于训练 算法，未来遇到的正向类实例可能与训练集中的非常近似。未来遇到的异常可能与已掌握的异常、非常的不同。例如： 欺诈行为检测 生产（例如飞机引擎）检测数据中心的计算机运行状况例如：邮件过滤器 天气预报 肿瘤分类\n希望这节课能让你明白一个学习问题的什么样的特征，能让你把这个问题当做是一个异常检测，或者是一个监督学习的问题。另外，对于很多技术公司可能会遇到的一些问题，通常来说，正样本的数量很少，甚至有时候是0，也就是说，出现了太多没见过的不同的异常类型，那么对于这些问题，通常应该使用的算法就是异常检测算法。\n15.6 选择特征\n参考视频: 15 - 6 - Choosing What Features to Use (12 min).mkv\n对于异常检测算法，我们使用的特征是至关重要的，下面谈谈如何选择特征：\n异常检测假设特征符合高斯分布，如果数据的分布不是高斯分布，异常检测算法也能够工作，但是最好还是将数据转换成高斯分布，例如使用对数函数：x= log(x+c)，其中 c 为非负常数； 或者 x=x^c，c为 0-1 之间的一个分数，等方法。(编者注：在python中，通常用np.log1p()函数，log1p就是 log(x+1)，可以避免出现负数结果，反向函数就是np.expm1())\n\n误差分析：\n一个常见的问题是一些异常的数据可能也会有较高的p(x)值，因而被算法认为是正常的。这种情况下误差分析能够帮助我们，我们可以分析那些被算法错误预测为正常的数据，观察能否找出一些问题。我们可能能从问题中发现我们需要增加一些新的特征，增加这些新特征后获得的新算法能够帮助我们更好地进行异常检测。\n异常检测误差分析：\n\n我们通常可以通过将一些相关的特征进行组合，来获得一些新的更好的特征（异常数据的该特征值异常地大或小），例如，在检测数据中心的计算机状况的例子中，我们可以用CPU负载与网络通信量的比例作为一个新的特征，如果该值异常地大，便有可能意味着该服务器是陷入了一些问题中。\n在这段视频中，我们介绍了如何选择特征，以及对特征进行一些小小的转换，让数据更像正态分布，然后再把数据输入异常检测算法。同时也介绍了建立特征时，进行的误差分析方法，来捕捉各种异常的可能。希望你通过这些方法，能够了解如何选择好的特征变量，从而帮助你的异常检测算法，捕捉到各种不同的异常情况。\n15.7 多元高斯分布（选修）\n参考视频: 15 - 7 - Multivariate Gaussian Distribution (Optional) (14 min).mkv\n假使我们有两个相关的特征，而且这两个特征的值域范围比较宽，这种情况下，一般的高斯分布模型可能不能很好地识别异常数据。其原因在于，一般的高斯分布模型尝试的是去同时抓住两个特征的偏差，因此创造出一个比较大的判定边界。\n下图中是两个相关特征，洋红色的线（根据ε的不同其范围可大可小）是一般的高斯分布模型获得的判定边界，很明显绿色的X所代表的数据点很可能是异常值，但是其p(x)值却仍然在正常范围内。多元高斯分布将创建像图中蓝色曲线所示的判定边界。\n\n在一般的高斯分布模型中，我们计算 p(x) 的方法是：\n通过分别计算每个特征对应的几率然后将其累乘起来，在多元高斯分布模型中，我们将构建特征的协方差矩阵，用所有的特征一起来计算 p(x)。\n我们首先计算所有特征的平均值，然后再计算协方差矩阵：\np(x)=\\prod_{j=1}^np(x_j;\\mu,\\sigma_j^2)=\\prod_{j=1}^n\\frac{1}{\\sqrt{2\\pi}\\sigma_j}exp(-\\frac{(x_j-\\mu_j)^2}{2\\sigma_j^2})\n\\mu=\\frac{1}{m}\\sum_{i=1}^mx^{(i)}\n\\Sigma = \\frac{1}{m}\\sum_{i=1}^m(x^{(i)}-\\mu)(x^{(i)}-\\mu)^T=\\frac{1}{m}(X-\\mu)^T(X-\\mu)\n注:其中\\mu  是一个向量，其每一个单元都是原特征矩阵中一行数据的均值。最后我们计算多元高斯分布的p\\left( x \\right):\np(x)=\\frac{1}{(2\\pi)^{\\frac{n}{2}}|\\Sigma|^{\\frac{1}{2}}}exp\\left(-\\frac{1}{2}(x-\\mu)^T\\Sigma^{-1}(x-\\mu)\\right)\n其中：\n|\\Sigma|是定矩阵，在 Octave 中用 det(sigma)计算\n\\Sigma1 是逆矩阵，下面我们来看看协方差矩阵是如何影响模型的：\n\n上图是5个不同的模型，从左往右依次分析：\n\n\n是一个一般的高斯分布模型\n\n\n通过协方差矩阵，令特征1拥有较小的偏差，同时保持特征2的偏差\n\n\n通过协方差矩阵，令特征2拥有较大的偏差，同时保持特征1的偏差\n\n\n通过协方差矩阵，在不改变两个特征的原有偏差的基础上，增加两者之间的正相关性\n\n\n通过协方差矩阵，在不改变两个特征的原有偏差的基础上，增加两者之间的负相关性\n\n\n多元高斯分布模型与原高斯分布模型的关系：\n可以证明的是，原本的高斯分布模型是多元高斯分布模型的一个子集，即像上图中的第1、2、3，3个例子所示，如果协方差矩阵只在对角线的单位上有非零的值时，即为原本的高斯分布模型了。\n原高斯分布模型和多元高斯分布模型的比较：\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n原高斯分布模型多元高斯分布模型不能捕捉特征之间的相关性 但可以通过将特征进行组合的方法来解决自动捕捉特征之间的相关性计算代价低，能适应大规模的特征计算代价较高 训练集较小时也同样适用必须要有 m&gt;n，不然的话协方差矩阵 不可逆的，通常需要 m&gt;10n 另外特征冗余也会导致协方差矩阵不可逆\n原高斯分布模型被广泛使用着，如果特征之间在某种程度上存在相互关联的情况，我们可以通过构造新新特征的方法来捕捉这些相关性。\n如果训练集不是太大，并且没有太多的特征，我们可以使用多元高斯分布模型。\n15.8 使用多元高斯分布进行异常检测（可选）\n参考视频: 15 - 8 - Anomaly Detection using the Multivariate Gaussian Distribution (Optional) (14 min).mkv\n在我们谈到的最后一个视频，关于多元高斯分布，看到的一些建立的各种分布模型，当你改变参数，\\mu 和 \\Sigma。在这段视频中，让我们用这些想法，并应用它们制定一个不同的异常检测算法。\n要回顾一下多元高斯分布和多元正态分布：\n\n分布有两个参数， \\mu 和 \\Sigma。其中\\mu这一个n维向量和 \\Sigma 的协方差矩阵，是一种n\\times n的矩阵。而这里的公式x的概率，如按 \\mu 和参数化 \\Sigma，和你的变量 \\mu 和 \\Sigma，你可以得到一个范围的不同分布一样，你知道的，这些都是三个样本，那些我们在以前的视频看过了。\n因此，让我们谈谈参数拟合或参数估计问题：\n我有一组样本{{{ x^{(1)},x^{(2)},...,x^{(m)}} }}是一个n维向量，我想我的样本来自一个多元高斯分布。我如何尝试估计我的参数 \\mu 和 \\Sigma 以及标准公式？\n估计他们是你设置 \\mu 是你的训练样本的平均值。\n\\mu=\\frac{1}{m}\\sum_{i=1}^{m}x^{(i)}\n并设置\\Sigma：\n\\Sigma=\\frac{1}{m}\\sum_{i=1}^{m}(x^{(i)}-\\mu)(x^{(i)}-\\mu)^T\n这其实只是当我们使用PCA算法时候，有 \\Sigma 时写出来。所以你只需插入上述两个公式，这会给你你估计的参数 \\mu 和你估计的参数 \\Sigma。所以，这里给出的数据集是你如何估计 \\mu 和 \\Sigma。让我们以这种方法而只需将其插入到异常检测算法。那么，我们如何把所有这一切共同开发一个异常检测算法？\n\n首先，我们把我们的训练集，和我们的拟合模型，我们计算p(x)，要知道，设定\\mu和描述的一样\\Sigma。\n\n如图，该分布在中央最多，越到外面的圈的范围越小。\n并在该点是出路这里的概率非常低。\n原始模型与多元高斯模型的关系如图：\n其中：协方差矩阵\\Sigma为：\n\n原始模型和多元高斯分布比较如图：\n\n十六、推荐系统(Recommender Systems)\n16.1 问题形式化\n参考视频: 16 - 1 - Problem Formulation (8 min).mkv\n在接下来的视频中，我想讲一下推荐系统。我想讲推荐系统有两个原因：\n第一、仅仅因为它是机器学习中的一个重要的应用。在过去几年，我偶尔访问硅谷不同的技术公司，我常和工作在这儿致力于机器学习应用的人们聊天，我常问他们，最重要的机器学习的应用是什么，或者，你最想改进的机器学习应用有哪些。我最常听到的答案是推荐系统。现在，在硅谷有很多团体试图建立很好的推荐系统。因此，如果你考虑网站像亚马逊，或网飞公司或易趣，或iTunes Genius，有很多的网站或系统试图推荐新产品给用户。如，亚马逊推荐新书给你，网飞公司试图推荐新电影给你，等等。这些推荐系统，根据浏览你过去买过什么书，或过去评价过什么电影来判断。这些系统会带来很大一部分收入，比如为亚马逊和像网飞这样的公司。因此，对推荐系统性能的改善，将对这些企业的有实质性和直接的影响。\n推荐系统是个有趣的问题，在学术机器学习中因此，我们可以去参加一个学术机器学习会议，推荐系统问题实际上受到很少的关注，或者，至少在学术界它占了很小的份额。但是，如果你看正在发生的事情，许多有能力构建这些系统的科技企业，他们似乎在很多企业中占据很高的优先级。这是我为什么在这节课讨论它的原因之一。\n我想讨论推荐系统地第二个原因是：这个班视频的最后几集我想讨论机器学习中的一些大思想，并和大家分享。这节课我们也看到了，对机器学习来说，特征是很重要的，你所选择的特征，将对你学习算法的性能有很大的影响。因此，在机器学习中有一种大思想，它针对一些问题，可能并不是所有的问题，而是一些问题，有算法可以为你自动学习一套好的特征。因此，不要试图手动设计，而手写代码这是目前为止我们常干的。有一些设置，你可以有一个算法，仅仅学习其使用的特征，推荐系统就是类型设置的一个例子。还有很多其它的，但是通过推荐系统，我们将领略一小部分特征学习的思想，至少，你将能够了解到这方面的一个例子，我认为，机器学习中的大思想也是这样。因此，让我们开始讨论推荐系统问题形式化。\n我们从一个例子开始定义推荐系统的问题。\n假使我们是一个电影供应商，我们有 5 部电影和 4 个用户，我们要求用户为电影打分。\n\n前三部电影是爱情片，后两部则是动作片，我们可以看出Alice和Bob似乎更倾向与爱情片， 而 Carol 和 Dave 似乎更倾向与动作片。并且没有一个用户给所有的电影都打过分。我们希望构建一个算法来预测他们每个人可能会给他们没看过的电影打多少分，并以此作为推荐的依据。\n下面引入一些标记：\nn_u 代表用户的数量\nn_m 代表电影的数量\nr(i, j) 如果用户j给电影 i 评过分则 r(i,j)=1\ny^{(i, j)} 代表用户 j 给电影i的评分\nm_j代表用户 j 评过分的电影的总数\n16.2 基于内容的推荐系统\n参考视频: 16 - 2 - Content Based Recommendations (15 min).mkv\n在一个基于内容的推荐系统算法中，我们假设对于我们希望推荐的东西有一些数据，这些数据是有关这些东西的特征。\n在我们的例子中，我们可以假设每部电影都有两个特征，如x_1代表电影的浪漫程度，x_2 代表电影的动作程度。\n\n则每部电影都有一个特征向量，如x^{(1)}是第一部电影的特征向量为[0.9 0]。\n下面我们要基于这些特征来构建一个推荐系统算法。\n假设我们采用线性回归模型，我们可以针对每一个用户都训练一个线性回归模型，如{{\\theta }^{(1)}}是第一个用户的模型的参数。\n于是，我们有：\n\\theta^{(j)}用户 j 的参数向量\nx^{(i)}电影 i 的特征向量\n对于用户 j 和电影 i，我们预测评分为：(\\theta^{(j)})^T x^{(i)}\n代价函数\n针对用户 j，该线性回归模型的代价为预测误差的平方和，加上正则化项：\n\\min_{\\theta (j)}\\frac{1}{2}\\sum_{i:r(i,j)=1}\\left((\\theta^{(j)})^Tx^{(i)}-y^{(i,j)}\\right)^2+\\frac{\\lambda}{2}\\left(\\theta_{k}^{(j)}\\right)^2\n其中 i:r(i,j)表示我们只计算那些用户 j 评过分的电影。在一般的线性回归模型中，误差项和正则项应该都是乘以1/2m，在这里我们将m去掉。并且我们不对方差项\\theta_0进行正则化处理。\n上面的代价函数只是针对一个用户的，为了学习所有用户，我们将所有用户的代价函数求和：\n\\min_{\\theta^{(1)},...,\\theta^{(n_u)}} \\frac{1}{2}\\sum_{j=1}^{n_u}\\sum_{i:r(i,j)=1}\\left((\\theta^{(j)})^Tx^{(i)}-y^{(i,j)}\\right)^2+\\frac{\\lambda}{2}\\sum_{j=1}^{n_u}\\sum_{k=1}^{n}(\\theta_k^{(j)})^2\n如果我们要用梯度下降法来求解最优解，我们计算代价函数的偏导数后得到梯度下降的更新公式为：\n\\theta_k^{(j)}:=\\theta_k^{(j)}-\\alpha\\sum_{i:r(i,j)=1}((\\theta^{(j)})^Tx^{(i)}-y^{(i,j)})x_{k}^{(i)} \\quad (\\text{for} \\, k = 0)\n\\theta_k^{(j)}:=\\theta_k^{(j)}-\\alpha\\left(\\sum_{i:r(i,j)=1}((\\theta^{(j)})^Tx^{(i)}-y^{(i,j)})x_{k}^{(i)}+\\lambda\\theta_k^{(j)}\\right) \\quad (\\text{for} \\, k\\neq 0)\n16.3 协同过滤\n参考视频: 16 - 3 - Collaborative Filtering (10 min).mkv\n在之前的基于内容的推荐系统中，对于每一部电影，我们都掌握了可用的特征，使用这些特征训练出了每一个用户的参数。相反地，如果我们拥有用户的参数，我们可以学习得出电影的特征。\n\\mathop{min}\\limits_{x^{(1)},...,x^{(n_m)}}\\frac{1}{2}\\sum_{i=1}^{n_m}\\sum_{j{r(i,j)=1}}((\\theta^{(j)})^Tx^{(i)}-y^{(i,j)})^2+\\frac{\\lambda}{2}\\sum_{i=1}^{n_m}\\sum_{k=1}^{n}(x_k^{(i)})^2\n但是如果我们既没有用户的参数，也没有电影的特征，这两种方法都不可行了。协同过滤算法可以同时学习这两者。\n我们的优化目标便改为同时针对x和\\theta进行。\nJ(x^{(1)},...x^{(n_m)},\\theta^{(1)},...,\\theta^{(n_u)})=\\frac{1}{2}\\sum_{(i:j):r(i,j)=1}((\\theta^{(j)})^Tx^{(i)}-y^{(i,j)})^2+\\frac{\\lambda}{2}\\sum_{i=1}^{n_m}\\sum_{k=1}^{n}(x_k^{(j)})^2+\\frac{\\lambda}{2}\\sum_{j=1}^{n_u}\\sum_{k=1}^{n}(\\theta_k^{(j)})^2\n对代价函数求偏导数的结果如下：\nx_k^{(i)}:=x_k^{(i)}-\\alpha\\left(\\sum_{j:r(i,j)=1}((\\theta^{(j)})^Tx^{(i)}-y^{(i,j)}\\theta_k^{j}+\\lambda x_k^{(i)}\\right)\n\\theta_k^{(i)}:=\\theta_k^{(i)}-\\alpha\\left(\\sum_{i:r(i,j)=1}((\\theta^{(j)})^Tx^{(i)}-y^{(i,j)}x_k^{(i)}+\\lambda \\theta_k^{(j)}\\right)\n注：在协同过滤从算法中，我们通常不使用方差项，如果需要的话，算法会自动学得。\n协同过滤算法使用步骤如下：\n\n\n初始 x^{(1)},x^{(1)},...x^{(nm)},\\ \\theta^{(1)},\\theta^{(2)},...,\\theta^{(n_u)}为一些随机小值\n\n\n使用梯度下降算法最小化代价函数\n\n\n在训练完算法后，我们预测(\\theta^{(j)})^Tx^{(i)}为用户 j 给电影 i 的评分\n\n\n通过这个学习过程获得的特征矩阵包含了有关电影的重要数据，这些数据不总是人能读懂的，但是我们可以用这些数据作为给用户推荐电影的依据。\n例如，如果一位用户正在观看电影 x^{(i)}，我们可以寻找另一部电影x^{(j)}，依据两部电影的特征向量之间的距离\\left\\| {{x}^{(i)}}-{{x}^{(j)}} \\right\\|的大小。\n16.4 协同过滤算法\n参考视频: 16 - 4 - Collaborative Filtering Algorithm (9 min).mkv\n协同过滤优化目标：\n给定x^{(1)},...,x^{(n_m)}，估计\\theta^{(1)},...,\\theta^{(n_u)}：\n\\min_{\\theta^{(1)},...,\\theta^{(n_u)}}\\frac{1}{2}\\sum_{j=1}^{n_u}\\sum_{i:r(i,j)=1}((\\theta^{(j)})^Tx^{(i)}-y^{(i,j)})^2+\\frac{\\lambda}{2}\\sum_{j=1}^{n_u}\\sum_{k=1}^{n}(\\theta_k^{(j)})^2\n给定\\theta^{(1)},...,\\theta^{(n_u)}，估计x^{(1)},...,x^{(n_m)}：\n同时最小化x^{(1)},...,x^{(n_m)}和\\theta^{(1)},...,\\theta^{(n_u)}：\nJ(x^{(1)},...,x^{(n_m)},\\theta^{(1)},...,\\theta^{(n_u)})=\\frac{1}{2}\\sum_{(i,j):r(i,j)=1}((\\theta^{(j)})^Tx^{(i)}-y^{(i,j)})^2+\\frac{\\lambda}{2}\\sum_{i=1}^{n_m}\\sum_{k=1}^{n}(x_k^{(i)})^2+\\frac{\\lambda}{2}\\sum_{j=1}^{n_u}\\sum_{k=1}^{n}(\\theta_k^{(j)})^2\n\\min_{x^{(1)},...,x^{(n_m)} \\\\\\ \\theta^{(1)},...,\\theta^{(n_u)}}J(x^{(1)},...,x^{(n_m)},\\theta^{(1)},...,\\theta^{(n_u)})\n16.5 向量化：低秩矩阵分解\n参考视频: 16 - 5 - Vectorization_ Low Rank Matrix Factorization (8 min).mkv\n在上几节视频中，我们谈到了协同过滤算法，本节视频中我将会讲到有关该算法的向量化实现，以及说说有关该算法你可以做的其他事情。\n举例子：\n\n\n当给出一件产品时，你能否找到与之相关的其它产品。\n\n\n一位用户最近看上一件产品，有没有其它相关的产品，你可以推荐给他。\n\n\n我将要做的是：实现一种选择的方法，写出协同过滤算法的预测情况。\n我们有关于五部电影的数据集，我将要做的是，将这些用户的电影评分，进行分组并存到一个矩阵中。\n我们有五部电影，以及四位用户，那么 这个矩阵 Y 就是一个5行4列的矩阵，它将这些电影的用户评分数据都存在矩阵里：\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMovieAlice (1)Bob (2)Carol (3)Dave (4)Love at last5500Romance forever5??0Cute puppies of love?40?Nonstop car chases0054Swords vs. karate005?\n\n推出评分：\n\n找到相关影片：\n\n现在既然你已经对特征参数向量进行了学习，那么我们就会有一个很方便的方法来度量两部电影之间的相似性。例如说：电影 i 有一个特征向量x^{(i)}，你是否能找到一部不同的电影 j，保证两部电影的特征向量之间的距离x^{(i)}和x^{(j)}很小，那就能很有力地表明电影i和电影 j 在某种程度上有相似，至少在某种意义上，某些人喜欢电影 i，或许更有可能也对电影 j 感兴趣。总结一下，当用户在看某部电影 i 的时候，如果你想找5部与电影非常相似的电影，为了能给用户推荐5部新电影，你需要做的是找出电影 j，在这些不同的电影中与我们要找的电影 i 的距离最小，这样你就能给你的用户推荐几部不同的电影了。\n通过这个方法，希望你能知道，如何进行一个向量化的计算来对所有的用户和所有的电影进行评分计算。同时希望你也能掌握，通过学习特征参数，来找到相关电影和产品的方法。\n16.6 推行工作上的细节：均值归一化\n参考视频: 16 - 6 - Implementational Detail_ Mean Normalization (9 min).mkv\n让我们来看下面的用户评分数据：\n\n如果我们新增一个用户 Eve，并且 Eve 没有为任何电影评分，那么我们以什么为依据为Eve推荐电影呢？\n我们首先需要对结果 Y 矩阵进行均值归一化处理，将每一个用户对某一部电影的评分减去所有用户对该电影评分的平均值：\n\n然后我们利用这个新的 Y 矩阵来训练算法。\n如果我们要用新训练出的算法来预测评分，则需要将平均值重新加回去，预测(\\theta^{(j)})^T x^{(i)}+\\mu_i，对于Eve，我们的新模型会认为她给每部电影的评分都是该电影的平均分。"},"机器学习笔记/12.机器学习":{"slug":"机器学习笔记/12.机器学习","filePath":"机器学习笔记/12.机器学习.md","title":"12.机器学习","links":[],"tags":["机器学习"],"content":"十七、大规模机器学习(Large Scale Machine Learning)\n17.1 大型数据集的学习\n参考视频: 17 - 1 - Learning With Large Datasets (6 min).mkv\n如果我们有一个低方差的模型，增加数据集的规模可以帮助你获得更好的结果。我们应该怎样应对一个有100万条记录的训练集？\n以线性回归模型为例，每一次梯度下降迭代，我们都需要计算训练集的误差的平方和，如果我们的学习算法需要有20次迭代，这便已经是非常大的计算代价。\n首先应该做的事是去检查一个这么大规模的训练集是否真的必要，也许我们只用1000个训练集也能获得较好的效果，我们可以绘制学习曲线来帮助判断。\n\n17.2 随机梯度下降法\n参考视频: 17 - 2 - Stochastic Gradient Descent (13 min).mkv\n如果我们一定需要一个大规模的训练集，我们可以尝试使用随机梯度下降法来代替批量梯度下降法。\n在随机梯度下降法中，我们定义代价函数为一个单一训练实例的代价：\n​                                                           cost\\left(  \\theta, \\left( {x}^{(i)} , {y}^{(i)} \\right)  \\right) = \\frac{1}{2}\\left( {h}_{\\theta}\\left({x}^{(i)}\\right)-{y}^{{(i)}} \\right)^{2}\n随机梯度下降算法为：首先对训练集随机“洗牌”，然后：\nRepeat (usually anywhere between1-10){\nfor i = 1:m{\n​       \\theta:={\\theta}_{j}-\\alpha\\left( {h}_{\\theta}\\left({x}^{(i)}\\right)-{y}^{(i)} \\right){{x}_{j}}^{(i)}\n​        (for j=0:n)\n​    }\n}\n随机梯度下降算法在每一次计算之后便更新参数 {{\\theta }} ，而不需要首先将所有的训练集求和，在梯度下降算法还没有完成一次迭代时，随机梯度下降算法便已经走出了很远。但是这样的算法存在的问题是，不是每一步都是朝着”正确”的方向迈出的。因此算法虽然会逐渐走向全局最小值的位置，但是可能无法站到那个最小值的那一点，而是在最小值点附近徘徊。\n\n17.3 小批量梯度下降\n参考视频: 17 - 3 - Mini-Batch Gradient Descent (6 min).mkv\n小批量梯度下降算法是介于批量梯度下降算法和随机梯度下降算法之间的算法，每计算常数b次训练实例，便更新一次参数  {{\\theta }} 。\nRepeat {\nfor i = 1:m{\n​       \\theta:={\\theta}_{j}-\\alpha\\frac{1}{b}\\sum_\\limits{k=i}^{i+b-1}\\left( {h}_{\\theta}\\left({x}^{(k)}\\right)-{y}^{(k)} \\right){{x}_{j}}^{(k)}\n​       (for j=0:n)\n​      i +=10\n​     }\n}\n通常我们会令 b 在 2-100 之间。这样做的好处在于，我们可以用向量化的方式来循环 b个训练实例，如果我们用的线性代数函数库比较好，能够支持平行处理，那么算法的总体表现将不受影响（与随机梯度下降相同）。\n17.4 随机梯度下降收敛\n参考视频: 17 - 4 - Stochastic Gradient Descent Convergence (12 min). mkv\n现在我们介绍随机梯度下降算法的调试，以及学习率 α 的选取。\n在批量梯度下降中，我们可以令代价函数J为迭代次数的函数，绘制图表，根据图表来判断梯度下降是否收敛。但是，在大规模的训练集的情况下，这是不现实的，因为计算代价太大了。\n在随机梯度下降中，我们在每一次更新 {{\\theta }} 之前都计算一次代价，然后每x次迭代后，求出这x次对训练实例计算代价的平均值，然后绘制这些平均值与x次迭代的次数之间的函数图表。\n\n当我们绘制这样的图表时，可能会得到一个颠簸不平但是不会明显减少的函数图像（如上面左下图中蓝线所示）。我们可以增加α来使得函数更加平缓，也许便能看出下降的趋势了（如上面左下图中红线所示）；或者可能函数图表仍然是颠簸不平且不下降的（如洋红色线所示），那么我们的模型本身可能存在一些错误。\n如果我们得到的曲线如上面右下方所示，不断地上升，那么我们可能会需要选择一个较小的学习率α。\n我们也可以令学习率随着迭代次数的增加而减小，例如令：\n​\t\t\t\t\t\t\t\\alpha = \\frac{const1}{iterationNumber + const2}\n随着我们不断地靠近全局最小值，通过减小学习率，我们迫使算法收敛而非在最小值附近徘徊。\n但是通常我们不需要这样做便能有非常好的效果了，对α进行调整所耗费的计算通常不值得\n\n总结下，这段视频中，我们介绍了一种方法，近似地监测出随机梯度下降算法在最优化代价函数中的表现，这种方法不需要定时地扫描整个训练集，来算出整个样本集的代价函数，而是只需要每次对最后1000个，或者多少个样本，求一下平均值。应用这种方法，你既可以保证随机梯度下降法正在正常运转和收敛，也可以用它来调整学习速率α的大小。\n17.5 在线学习\n参考视频: 17 - 5 - Online Learning (13 min).mkv\n在这个视频中，讨论一种新的大规模的机器学习机制，叫做在线学习机制。在线学习机制让我们可以模型化问题。\n今天，许多大型网站或者许多大型网络公司，使用不同版本的在线学习机制算法，从大批的涌入又离开网站的用户身上进行学习。特别要提及的是，如果你有一个由连续的用户流引发的连续的数据流，进入你的网站，你能做的是使用一个在线学习机制，从数据流中学习用户的偏好，然后使用这些信息来优化一些关于网站的决策。\n假定你有一个提供运输服务的公司，用户们来向你询问把包裹从A地运到B地的服务，同时假定你有一个网站，让用户们可多次登陆，然后他们告诉你，他们想从哪里寄出包裹，以及包裹要寄到哪里去，也就是出发地与目的地，然后你的网站开出运输包裹的的服务价格。比如，我会收取$50来运输你的包裹，我会收取$20之类的，然后根据你开给用户的这个价格，用户有时会接受这个运输服务，那么这就是个正样本，有时他们会走掉，然后他们拒绝购买你的运输服务，所以，让我们假定我们想要一个学习算法来帮助我们，优化我们想给用户开出的价格。\n一个算法来从中学习的时候来模型化问题在线学习算法指的是对数据流而非离线的静态数据集的学习。许多在线网站都有持续不断的用户流，对于每一个用户，网站希望能在不将数据存储到数据库中便顺利地进行算法学习。\n假使我们正在经营一家物流公司，每当一个用户询问从地点A至地点B的快递费用时，我们给用户一个报价，该用户可能选择接受（y=1）或不接受（y=0）。\n现在，我们希望构建一个模型，来预测用户接受报价使用我们的物流服务的可能性。因此报价\n是我们的一个特征，其他特征为距离，起始地点，目标地点以及特定的用户数据。模型的输出是:p(y=1)。\n在线学习的算法与随机梯度下降算法有些类似，我们对单一的实例进行学习，而非对一个提前定义的训练集进行循环。\nRepeat forever (as long as the website is running) {\nGet \\left(x,y\\right) corresponding to the current user\n​        \\theta:={\\theta}_{j}-\\alpha\\left( {h}_{\\theta}\\left({x}\\right)-{y} \\right){{x}_{j}}\n​       (for j=0:n)\n}\n一旦对一个数据的学习完成了，我们便可以丢弃该数据，不需要再存储它了。这种方式的好处在于，我们的算法可以很好的适应用户的倾向性，算法可以针对用户的当前行为不断地更新模型以适应该用户。\n每次交互事件并不只产生一个数据集，例如，我们一次给用户提供3个物流选项，用户选择2项，我们实际上可以获得3个新的训练实例，因而我们的算法可以一次从3个实例中学习并更新模型。\n这些问题中的任何一个都可以被归类到标准的，拥有一个固定的样本集的机器学习问题中。或许，你可以运行一个你自己的网站，尝试运行几天，然后保存一个数据集，一个固定的数据集，然后对其运行一个学习算法。但是这些是实际的问题，在这些问题里，你会看到大公司会获取如此多的数据，真的没有必要来保存一个固定的数据集，取而代之的是你可以使用一个在线学习算法来连续的学习，从这些用户不断产生的数据中来学习。这就是在线学习机制，然后就像我们所看到的，我们所使用的这个算法与随机梯度下降算法非常类似，唯一的区别的是，我们不会使用一个固定的数据集，我们会做的是获取一个用户样本，从那个样本中学习，然后丢弃那个样本并继续下去，而且如果你对某一种应用有一个连续的数据流，这样的算法可能会非常值得考虑。当然，在线学习的一个优点就是，如果你有一个变化的用户群，又或者你在尝试预测的事情，在缓慢变化，就像你的用户的品味在缓慢变化，这个在线学习算法，可以慢慢地调试你所学习到的假设，将其调节更新到最新的用户行为。\n17.6 映射化简和数据并行\n参考视频: 17 - 6 - Map Reduce and Data Parallelism (14 min).mkv\n映射化简和数据并行对于大规模机器学习问题而言是非常重要的概念。之前提到，如果我们用批量梯度下降算法来求解大规模数据集的最优解，我们需要对整个训练集进行循环，计算偏导数和代价，再求和，计算代价非常大。如果我们能够将我们的数据集分配给不多台计算机，让每一台计算机处理数据集的一个子集，然后我们将计所的结果汇总在求和。这样的方法叫做映射简化。\n具体而言，如果任何学习算法能够表达为，对训练集的函数的求和，那么便能将这个任务分配给多台计算机（或者同一台计算机的不同CPU 核心），以达到加速处理的目的。\n例如，我们有400个训练实例，我们可以将批量梯度下降的求和任务分配给4台计算机进行处理：\n\n很多高级的线性代数函数库已经能够利用多核CPU的多个核心来并行地处理矩阵运算，这也是算法的向量化实现如此重要的缘故（比调用循环快）。\n十八、应用实例：图片文字识别(Application Example: Photo OCR)\n18.1 问题描述和流程图\n参考视频: 18 - 1 - Problem Description and Pipeline (7 min).mkv\n图像文字识别应用所作的事是，从一张给定的图片中识别文字。这比从一份扫描文档中识别文字要复杂的多。\n\n为了完成这样的工作，需要采取如下步骤：\n\n\n文字侦测（Text detection）——将图片上的文字与其他环境对象分离开来\n\n\n字符切分（Character segmentation）——将文字分割成一个个单一的字符\n\n\n字符分类（Character classification）——确定每一个字符是什么\n可以用任务流程图来表达这个问题，每一项任务可以由一个单独的小队来负责解决：\n\n\n\n18.2 滑动窗口\n参考视频: 18 - 2 - Sliding Windows (15 min).mkv\n滑动窗口是一项用来从图像中抽取对象的技术。假使我们需要在一张图片中识别行人，首先要做的是用许多固定尺寸的图片来训练一个能够准确识别行人的模型。然后我们用之前训练识别行人的模型时所采用的图片尺寸在我们要进行行人识别的图片上进行剪裁，然后将剪裁得到的切片交给模型，让模型判断是否为行人，然后在图片上滑动剪裁区域重新进行剪裁，将新剪裁的切片也交给模型进行判断，如此循环直至将图片全部检测完。\n一旦完成后，我们按比例放大剪裁的区域，再以新的尺寸对图片进行剪裁，将新剪裁的切片按比例缩小至模型所采纳的尺寸，交给模型进行判断，如此循环。\n\n滑动窗口技术也被用于文字识别，首先训练模型能够区分字符与非字符，然后，运用滑动窗口技术识别字符，一旦完成了字符的识别，我们将识别得出的区域进行一些扩展，然后将重叠的区域进行合并。接着我们以宽高比作为过滤条件，过滤掉高度比宽度更大的区域（认为单词的长度通常比高度要大）。下图中绿色的区域是经过这些步骤后被认为是文字的区域，而红色的区域是被忽略的。\n\n以上便是文字侦测阶段。\n下一步是训练一个模型来完成将文字分割成一个个字符的任务，需要的训练集由单个字符的图片和两个相连字符之间的图片来训练模型。\n\n\n模型训练完后，我们仍然是使用滑动窗口技术来进行字符识别。\n以上便是字符切分阶段。\n最后一个阶段是字符分类阶段，利用神经网络、支持向量机或者逻辑回归算法训练一个分类器即可。\n18.3 获取大量数据和人工数据\n参考视频: 18 - 3 - Getting Lots of Data and Artificial Data (16 min).mkv\n如果我们的模型是低方差的，那么获得更多的数据用于训练模型，是能够有更好的效果的。问题在于，我们怎样获得数据，数据不总是可以直接获得的，我们有可能需要人工地创造一些数据。\n以我们的文字识别应用为例，我们可以字体网站下载各种字体，然后利用这些不同的字体配上各种不同的随机背景图片创造出一些用于训练的实例，这让我们能够获得一个无限大的训练集。这是从零开始创造实例。\n另一种方法是，利用已有的数据，然后对其进行修改，例如将已有的字符图片进行一些扭曲、旋转、模糊处理。只要我们认为实际数据有可能和经过这样处理后的数据类似，我们便可以用这样的方法来创造大量的数据。\n有关获得更多数据的几种方法：\n\n\n人工数据合成\n\n\n手动收集、标记数据\n\n\n众包\n\n\n18.4 上限分析：哪部分管道的接下去做\n参考视频: 18 - 4 - Ceiling Analysis_ What Part of the Pipeline to Work on Next\n(14 min).mkv\n在机器学习的应用中，我们通常需要通过几个步骤才能进行最终的预测，我们如何能够知道哪一部分最值得我们花时间和精力去改善呢？这个问题可以通过上限分析来回答。\n回到我们的文字识别应用中，我们的流程图如下：\n\n流程图中每一部分的输出都是下一部分的输入，上限分析中，我们选取一部分，手工提供100%正确的输出结果，然后看应用的整体效果提升了多少。假使我们的例子中总体效果为72%的正确率。\n如果我们令文字侦测部分输出的结果100%正确，发现系统的总体效果从72%提高到了89%。这意味着我们很可能会希望投入时间精力来提高我们的文字侦测部分。\n接着我们手动选择数据，让字符切分输出的结果100%正确，发现系统的总体效果只提升了1%，这意味着，我们的字符切分部分可能已经足够好了。\n最后我们手工选择数据，让字符分类输出的结果100%正确，系统的总体效果又提升了10%，这意味着我们可能也会应该投入更多的时间和精力来提高应用的总体表现。\n\n十九、总结(Conclusion)\n19.1 总结和致谢\n参考视频: 19 - 1 - Summary and Thank You (5 min).mkv\n欢迎来到《机器学习》课的最后一段视频。我们已经一起学习很长一段时间了。在最后这段视频中，我想快速地回顾一下这门课的主要内容，然后简单说几句想说的话。\n作为这门课的结束时间，那么我们学到了些什么呢？在这门课中，我们花了大量的时间介绍了诸如线性回归、逻辑回归、神经网络、支持向量机等等一些监督学习算法，这类算法具有带标签的数据和样本，比如{{x}^{\\left( i \\right)}}、{{y}^{\\left( i \\right)}}。\n然后我们也花了很多时间介绍无监督学习。例如 K-均值聚类、用于降维的主成分分析，以及当你只有一系列无标签数据 {{x}^{\\left( i \\right)}} 时的异常检测算法。\n当然，有时带标签的数据，也可以用于异常检测算法的评估。此外，我们也花时间讨论了一些特别的应用或者特别的话题，比如说推荐系统。以及大规模机器学习系统，包括并行系统和映射化简方法，还有其他一些特别的应用。比如，用于计算机视觉技术的滑动窗口分类算法。\n最后，我们还提到了很多关于构建机器学习系统的实用建议。这包括了怎样理解某个机器学习算法是否正常工作的原因，所以我们谈到了偏差和方差的问题，也谈到了解决方差问题的正则化，同时我们也讨论了怎样决定接下来怎么做的问题，也就是说当你在开发一个机器学习系统时，什么工作才是接下来应该优先考虑的问题。因此我们讨论了学习算法的评价法。介绍了评价矩阵，比如：查准率、召回率以及F1分数，还有评价学习算法比较实用的训练集、交叉验证集和测试集。我们也介绍了学习算法的调试，以及如何确保学习算法的正常运行，于是我们介绍了一些诊断法，比如学习曲线，同时也讨论了误差分析、上限分析等等内容。\n所有这些工具都能有效地指引你决定接下来应该怎样做，让你把宝贵的时间用在刀刃上。现在你已经掌握了很多机器学习的工具，包括监督学习算法和无监督学习算法等等。\n但除了这些以外，我更希望你现在不仅仅只是认识这些工具，更重要的是掌握怎样有效地利用这些工具来建立强大的机器学习系统。所以，以上就是这门课的全部内容。如果你跟着我们的课程一路走来，到现在，你应该已经感觉到自己已经成为机器学习方面的专家了吧？\n我们都知道，机器学习是一门对科技、工业产生深远影响的重要学科，而现在，你已经完全具备了应用这些机器学习工具来创造伟大成就的能力。我希望你们中的很多人都能在相应的领域，应用所学的机器学习工具，构建出完美的机器学习系统，开发出无与伦比的产品和应用。并且我也希望你们通过应用机器学习，不仅仅改变自己的生活，有朝一日，还要让更多的人生活得更加美好！\n我也想告诉大家，教这门课对我来讲是一种享受。所以，谢谢大家！\n最后，在结束之前，我还想再多说一点：那就是，也许不久以前我也是一个学生，即使是现在，我也尽可能挤出时间听一些课，学一些新的东西。所以，我深知要坚持学完这门课是很需要花一些时间的，我知道，也许你是一个很忙的人，生活中有很多很多事情要处理。正因如此，你依然挤出时间来观看这些课程视频。我知道，很多视频的时间都长达数小时，你依然花了好多时间来做这些复习题。你们中好多人，还愿意花时间来研究那些编程练习，那些又长又复杂的编程练习。我对你们表示衷心的感谢！我知道你们很多人在这门课中都非常努力，很多人都在这门课上花了很多时间，很多人都为这门课贡献了自己的很多精力。所以，我衷心地希望你们能从这门课中有所收获！\n最后我想说！再次感谢你们选修这门课程！\nAndew Ng"},"机器学习笔记/2.机器学习":{"slug":"机器学习笔记/2.机器学习","filePath":"机器学习笔记/2.机器学习.md","title":"2.机器学习","links":[],"tags":["机器学习"],"content":"记得当年背英语单词的书永远都翻在第一页，abandon背了一遍又一遍，还是没有记住，现如今周志华老师的书买了好久没有翻，吴恩达的视频看了一遍又一遍，只能希望这是最后一次看了。\nMachine Learning 学习视屏地址\n什么是机器学习\n\n不借助特定的程序使电脑学习的科学\n\n监督学习（supervised learning）\n例如：房价预测（回归问题），肿瘤预测（分类问题）\n监督学习就是给出一组特征值，同时也给出这组特征所对应的结果。比如通过某一地区房子的面积和卧室数来预测房子的价格。\n无监督学习（unsupervised learning）\n无监督学习则是只给出一些特征值，但是并没有这些特征所对应的结果，通过这些特征值来寻找他们之间的关系，例如聚类问题，把同一事件的新闻划分为一类。\n模型表示（Model Representation）\n符号定义\nm：样本数量（training examples）\nx：输入值，又叫特征（input variables/features）\ny：输出值，又叫目标值（output variables/target variables）\n（x，y）：训练样本\n第i个训练样本：(x^i,y^i)\n\n监督学习的工作方式\n\n预测函数的表示\nh_\\theta(x)=\\theta_0+\\theta_1x\n关于x单变量的线性回归方程\n代价函数（cost function）\n预测函数h_\\theta(x)的线性意义：\n\n预测函数h_\\theta(x)是关于x的函数,而代价函数是一个关于(\\theta_0,\\theta_1)的函数\nJ(\\theta_0,\\theta_1) = \\frac{1}{2m} \\sum^m_{i=1}(h_\\theta(x^i)-y^i)^2\n优化目标：minimize J(\\theta_0,\\theta_1)\n\n教授讲的很详细，这里记一下自己的见解吧：\n预测函数是根据已知特征向量和结果所描述的一个线性方程，根据改变线段的斜率来观察匹配到的特征吻合度达，当预测函数可以匹配到最多特征时则这个预测函数是最优解，如何获取最优解引入了代价函数，所谓代价函数就是预测函数的积分，\n\n梯度下降\n在微积分里面，对多元函数的参数求∂偏导数，把求得的各个参数的偏导数以向量的形式写出来，就是梯度\n梯度下降是迭代法的一种,可以用于求解最小二乘问题(线性和非线性都可以)。在求解机器学习算法的模型参数，即无约束优化问题时，梯度下降（Gradient Descent）是最常采用的方法之一，另一种常用的方法是最小二乘法。在求解损失函数的最小值时，可以通过梯度下降法来一步步的迭代求解，得到最小化的损失函数和模型参数值。反过来，如果我们需要求解损失函数的最大值，这时就需要用梯度上升法来迭代了。在机器学习中，基于基本的梯度下降法发展了两种梯度下降方法，分别为随机梯度下降法和批量梯度下降法。\n通过梯度下降的方法来寻找代价函数的最优解\n\n符号解释\n\n：= 赋值符，把右边的值赋值给左边\n\n\n\\alpha 学习速度，步长，过小收敛时间过长，过大超过最小值无法收敛\n\n\n\\theta_0和\\theta_1是同时更新的\n梯度下降的缺点：\n靠近极小值时收敛速度减慢。\n直线搜索时可能会产生一些问题。\n可能会“之字形”地下降。\n会产生局部最优解而非全局。\n总结\n这里基本简单的记录了视频前三章的内容，梳理一下知识点。\n在机器学习中首先需要有样本，也叫训练集，然后是一个机器学习算法，把训练集扔进这个算法中，通过迭代之类的方法计算机会发现其中的规律而给出统一的模型从而做到预测分析。\n当训练集既有输入内容又有输出结果，就是监督学习（比如回归问题），当样本里没有结果时是无监督学习（比如聚类问题）\n梯度下降就是寻找最佳的预测模型的方式，当我们要建立一个准确的预测模型需要不断改变参数（\\theta_0,\\theta_1）,于是建立一个关于\\theta_0,\\theta_1的方程，这个方程叫代价方程（cost function），其实这个方程就是度量预测函数的结果和实际结果的方差，当方差最小就是最佳\\theta_0,\\theta_1，方法就是计算所有预测函数的结果减实际结果和的平方，方差最小就代表拟合度最佳。可视化观察就向是在一个弯曲的山谷中寻找最低点。"},"机器学习笔记/3.机器学习":{"slug":"机器学习笔记/3.机器学习","filePath":"机器学习笔记/3.机器学习.md","title":"3.机器学习","links":["python笔记/Python实现梯度下降"],"tags":["机器学习"],"content":"特征缩放 （feature scaling）\n确保不同的特征值在同一个范围内，这样能保证梯度下降能够更快的收敛。\n例如：\nx1是房屋大小，非常的大（0-2000）\nx2是房间卧室数（1-5）\n参数适当的缩放，使收敛的更快\n\n建议把特征缩放到-1到1的范围，可以偏差，但不能偏差太大，特征值不需要太精确，只是希望梯度下降收敛更快。\n学习速率的选取\n如何选择梯度下降学习速率\\alpha，可以画出代价函数随迭代步数J(\\theta)增加的函数曲线，观察曲线来判断梯度下降算法是否收敛，下面这个曲线中，当迭代达到三百时基本已经停止下降，所以这个曲线中三百是最佳的迭代次数\n\n一个典型例子来的判断是否收敛，比如代价函数J(\\theta)已经小于一个ε，比如设置ε为0.001，选择一个阈值来告诉算法已经收敛。\n\\alpha过小会收敛太慢，过大会导致震荡无法收敛，通常会尝试多个α来找到最佳的学习速率，比如从0.001到1，每扩大三倍选取一个alpha来确定学习速率。\n多特征值的线性回归问题\n\n\n和前一章一样，对每个参数θ求J的偏导数，然后把它们全部置零，然后求出θ1到θn的值，这样就能求出最小的代价函数的所有θ的值。这是一个非常复杂的微分方程，用线性代数的方法可以快速解决。\nNormal Equation\n构建两个矩阵，矩阵X由x0（全部为1），x1，x2…xn构成，y是结果矩阵，X和y矩阵是以列排的\n\n简单的说就是:\n\n通过计算X的转置乘以X的逆乘以X的转置乘以Y来得到θ\n\n\n就是这个公式，这里懒得写为什么了，这个也是最小二乘法的公式。\n优点是不需要选取学习速率，不需要迭代，但是当特征值大于百万级别求矩阵的逆会非常慢，这时则应该选择梯度下降而不是标准方程。\n当特征值存在线性关系时，会导致矩阵不可逆，但是可以通过求伪逆来获取结果刚，对结果影响不大。\nPython代码实现\nPython实现梯度下降"},"机器学习笔记/4.机器学习":{"slug":"机器学习笔记/4.机器学习","filePath":"机器学习笔记/4.机器学习.md","title":"4.机器学习","links":[],"tags":["机器学习","octave"],"content":"前四章的内容学习完毕，第五章讲了Octave这个软件的使用，类似于matlab，大学有过学习matlab经验所以这个学起来想对比较轻松，不论是在Ubuntu还是windows安装都很简单，这个的界面布局都和matlab基本一模一样。\n虽然用python都可以实现，但Octave开源免费，比numpy更简单的实现算法，所以有必要学习一下。\n其实关于Octave的东西并不想记录，和matlab一样，但为了这个博客的完整性还是简单的记录一下，我使用的是windows版的直接打开GUI就能使用了。\n基本操作\n%基本四则运算\n&gt;&gt; 1+2\nans =  3\n&gt;&gt; 6-1\nans =  5\n&gt;&gt; 5*8\nans =  40\n&gt;&gt; 1/5\nans =  0.20000\n&gt;&gt; 3^6\nans =  729\n\n%不等号是~而不是！\n&gt;&gt; 1==2\nans = 0\n&gt;&gt; 1~=2\nans = 1\n\n%与 或 异或\n&gt;&gt; 8 &gt; 1 &amp;&amp; 0\nans = 0\n&gt;&gt; 9 &gt; 1 || 0\nans = 1\n&gt;&gt; xor(1, 0)\nans = 1\n\n%如果你想分配一个变量，但不希望在屏幕上显示结果，你可以在命令后加一个分号，可以抑制打印输出，敲入回车后，不打印任何东西。\n&gt;&gt; a = 3\na =  3\n&gt;&gt; a = 3;\n&gt;&gt; b = &#039;hello word&#039;;\n&gt;&gt; b\nb = hello word\n%设置A等于圆周率π，如果我要打印该值，那么只需键入A像这样就打印出来了。\n&gt;&gt; a = pi;\n&gt;&gt; pi\nans =  3.1416\n&gt;&gt; a\na =  3.1416\n&gt;&gt; disp(sprintf(&#039;2 decimals: %0.12f&#039;, a))\n2 decimals: 3.141592653590\n这是一种，旧风格的C语言语法，对于之前就学过C语言的同学来说，你可以使用这种基本的语法来将结果打印到屏幕。\n\n例如 sprintf命令的六个小数：0.6%f ,a，这应该打印π的6位小数形式。\n\n也有一些控制输出长短格式的快捷命令：\n&gt;&gt; format long\n&gt;&gt; a\na =  3.14159265358979\n&gt;&gt; format short\n&gt;&gt; a\na =  3.1416\n\n简单的运算符就是这些，重点是关于矩阵的\n简单矩阵的创建\n简单矩阵的创建\n&gt;&gt; A = [1 2; 3 4; 5 6]\nA =\n\n   1   2\n   3   4\n   5   6\n\n&gt;&gt; A = [2 2;\n3 3;\n4 4]\nA =\n\n   2   2\n   3   3\n   4   4\n\n&gt;&gt; B = [1 2 3]\nB =\n\n   1   2   3\n\n&gt;&gt; B = [1; 2; 3]\nB =\n\n   1\n   2\n   3\n\n&gt;&gt;\n这个集合V是一组值，从数值1开始，增量或说是步长为0.1，直到增加到2，按照这样的方法对向量V操作，可以得到一个行向量，这是一个1行11列的矩阵，其矩阵的元素是1 1.1 1.2 1.3，依此类推，直到数值2。\n\n我也可以建立一个集合V并用命令“1:6”进行赋值，这样V就被赋值了1至6的六个整数。\n&gt;&gt; v = 1:6\nv =\n\n   1   2   3   4   5   6\n\n这里还有一些其他的方法来生成矩阵\n例如“ones(2,3)”，也可以用来生成矩阵：\n&gt;&gt; ones(2,3)\nans =\n\n   1   1   1\n   1   1   1\n\n元素都为2，两行三列的矩阵，就可以使用这个命令：\n&gt;&gt; C = 2*ones(2,3)\nC =\n\n   2   2   2\n   2   2   2\n\n你可以把这个方法当成一个生成矩阵的快速方法。\nw为一个一行三列的零矩阵，一行三列的A矩阵里的元素全部是零：\n&gt;&gt; W = zeros(1,3)\nW =\n\n   0   0   0\n\n如果我对W进行赋值，用Rand命令建立一个一行三列的矩阵，因为使用了Rand命令，则其一行三列的元素均为随机值，如“rand(3, 3)”命令，这就生成了一个3×3的矩阵，并且其所有元素均为随机。\n&gt;&gt; rand(3,3)\nans =\n\n   0.60790   0.22000   0.10036\n   0.61343   0.58981   0.17660\n   0.22697   0.88276   0.42049\n\n&gt;&gt;\n你知道什么是高斯随机变量，或者，你知道什么是正态分布的随机变量，你可以设置集合W，使其等于一个一行三列的\nN矩阵，并且，来自三个值，一个平均值为0的高斯分布，方差或者等于1的标准偏差。\n&gt;&gt; w = randn(1,3)\nw =\n\n  -1.24688   1.87417  -0.70878\n\n并用hist命令绘制直方图。\n&gt;&gt; w = -9 + sqrt(10)*(randn(1, 10000));\n&gt;&gt; hist(w)\n&gt;&gt; hist(w,50)\n\n绘制单位矩阵：\n&gt;&gt; I = eye(6)\nI =\n\nDiagonal Matrix\n\n   1   0   0   0   0   0\n   0   1   0   0   0   0\n   0   0   1   0   0   0\n   0   0   0   1   0   0\n   0   0   0   0   1   0\n   0   0   0   0   0   1\n\n对命令不清楚可以通过help命令查询\n\nsize函数\n&gt;&gt; A = [1: 2; 3 4; 5 6]\nA =\n\n   1   2\n   3   4\n   5   6\n\n&gt;&gt; size(A) %输出[行数 列数]\nans =\n\n   3   2\n\n&gt;&gt; size(A, 1) %行数\nans =  3\n&gt;&gt; size(A, 2) %列数\nans =  2\n&gt;&gt; length(A) %行数和列数中最大值\nans =  3\n\n导入与导出数据\nload 文件名\nwhos %将当前的变量都显示出来\nclear A %将变量A删除\n\nsave hello.mat A; %将变量A存入hello.mat文件\nsave hello.txt A -ascii; %将A存为ascii\n\n取矩阵中的值\n&gt;&gt; A\nA =\n\n   1   2\n   3   4\n   5   6\n\n&gt;&gt; A(3,2) %矩阵A第三行第二列的数\nans =  6\n\n&gt;&gt; A(2,:) %第二行的数\nans =\n\n   3   4\n\n&gt;&gt; A(:,2) %第二列的数\nans =\n\n   2\n   4\n   6\n\n&gt;&gt; A([1 3],:) %第一行和第三行的数\nans =\n\n   1   2\n   5   6\n\n&gt;&gt; A(:,2) = [10;11;12] %修改第二列的数\nA =\n\n    1   10\n    3   11\n    5   12\n\n&gt;&gt; A = [A,[100;200;300]] %增加一列数据\nA =\n\n     1    10   100\n     3    11   200\n     5    12   300\n\n&gt;&gt; A(:) %修改为一列向量\nans =\n\n     1\n     3\n     5\n    10\n    11\n    12\n   100\n   200\n   300\n\n\n拼接矩阵\n&gt;&gt; A = [1 2; 3 4; 5 6]\nA =\n\n   1   2\n   3   4\n   5   6\n\n&gt;&gt; B = [11 12; 13 14; 15 16]\nB =\n\n   11   12\n   13   14\n   15   16\n\n&gt;&gt; C = [A B] %将矩阵A和B并列拼接\nC =\n\n    1    2   11   12\n    3    4   13   14\n    5    6   15   16\n\n&gt;&gt; C = [A;B] %加分号是将B矩阵拼接到A下面\nC =\n\n    1    2\n    3    4\n    5    6\n   11   12\n   13   14\n   15   16\n&gt;&gt;\n\n矩阵计算\n&gt;&gt; a = [1 2; 3 4; 5 6]\na =\n\n   1   2\n   3   4\n   5   6\n\n&gt;&gt; B = [11 22; 33 44; 55 66]\nB =\n\n   11   22\n   33   44\n   55   66\n\n&gt;&gt; C = [1 1; 2 2]\nC =\n\n   1   1\n   2   2\n\n&gt;&gt; V = [1; 2; 3]\nV =\n\n   1\n   2\n   3\n\n&gt;&gt; A*C %矩阵相乘\nans =\n\n    5    5\n   11   11\n   17   17\n\n&gt;&gt; A*B %相乘条件必须是A矩阵的列等于B矩阵的行，否则报错\nerror: operator *: nonconformant arguments (op1 is 3x2, op2 is 3x2)\n\n&gt;&gt; A.*2 %矩阵中的每个元素都乘二\nans =\n\n    2    4\n    6    8\n   10   12\n\n&gt;&gt; A.^2 %每个元素的平方\nans =\n\n    1    4\n    9   16\n   25   36\n\n\n&gt;&gt; 1./V %每个元素的倒数\nans =\n\n   1.00000\n   0.50000\n   0.33333\n   \n&gt;&gt; V + ones(length(V), 1) %每个元素都加一\nans =\n\n   2\n   3\n   4\n\n&gt;&gt; A&#039; %A的转置\nans =\n\n   1   3   5\n   2   4   6\n\n&gt;&gt;\n\n矩阵的索引\n&gt;&gt; a = [1 15 2 0.5]\na =\n\n    1.00000   15.00000    2.00000    0.50000\n\n&gt;&gt; [val,ind] = max(a) % val 矩阵中的最大元素，ind 最大值的index\nval =  15\nind =  2\n&gt;&gt; val = max(A) %矩阵每列的最大值\nval =\n\n   5   6\n\n&gt;&gt; a &lt; 3 %检查矩阵中比3小的元素，返回布尔型\nans =\n\n  1  0  1  1\n\n&gt;&gt; find(a&lt;3) %比3小的元素的位置\nans =\n\n   1   3   4\n\n&gt;&gt; A = magic(3) %创建一个幻方 （行，列，对角线相加想等）\nA =\n\n   8   1   6\n   3   5   7\n   4   9   2\n\n&gt;&gt; [r c] = find(A&gt;=7) % 符合A&gt;=7元素的行列坐标\nr =\n\n   1\n   3\n   2\n\nc =\n\n   1\n   2\n   3\n\n\n&gt;&gt; sum(a) %求所有元素的和\nans =  18.500\n&gt;&gt; prod(A) %求每列的乘积\nans =\n\n   96   45   84\n\n&gt;&gt; sum(A) %求每列的和\nans =\n\n   15   15   15\n\n&gt;&gt; floor(a) %返回小于元素的最小整数\nans =\n\n    1   15    2    0\n\n&gt;&gt; ceil(a) %返回大于元素的最大整数\nans =\n\n    1   15    2    1\n\n&gt;&gt; max(rand(3), rand(3)) %比较两个矩阵返回最大值\nans =\n\n   0.65329   0.32803   0.23948\n   0.56627   0.37716   0.64170\n   0.17771   0.81867   0.73937\n\n&gt;&gt; max(A, [], 1) %返回每一列的最大值\nans =\n\n   8   9   7\n\n&gt;&gt; max(A, [], 2) %返回每一行的最大值\nans =\n\n   8\n   7\n   9\n\n&gt;&gt; A = magic(9)\nA =\n\n   47   58   69   80    1   12   23   34   45\n   57   68   79    9   11   22   33   44   46\n   67   78    8   10   21   32   43   54   56\n   77    7   18   20   31   42   53   55   66\n    6   17   19   30   41   52   63   65   76\n   16   27   29   40   51   62   64   75    5\n   26   28   39   50   61   72   74    4   15\n   36   38   49   60   71   73    3   14   25\n   37   48   59   70   81    2   13   24   35\n\n&gt;&gt; sum(A,2) %行的和\nans =\n\n   369\n   369\n   369\n   369\n   369\n   369\n   369\n   369\n   369\n\n&gt;&gt; sum(A,1) %列的和\nans =\n\n   369   369   369   369   369   369   369   369   369\n\n&gt;&gt; sum(sum(A.* eye(9))) %对角线的和\nans =  369\n&gt;&gt; pinv(A) %伪逆矩阵\n\n画图\n&gt; t = [0 : 0.01 : 0.98];\n&gt;&gt; y1 = sin(2*pi*4*t);\n&gt;&gt; plot(t, y1,&#039;r&#039;)\n\n\n在一个画布上画两副如图\n&gt;&gt; y1 = sin(2*pi*4*t);\n&gt;&gt; y2 = cos(2*pi*4*t);\n&gt;&gt; plot(t,y2)\n&gt;&gt; hold on;\n&gt;&gt; plot(t, y1,&#039;r&#039;)\n\n\n并列显示两个图\n&gt;&gt; subplot(1,2,1)\n&gt;&gt; plot(t,y1)\n&gt;&gt; subplot(1,2,2)\n&gt;&gt; plot(t,y2)\n&gt;&gt; axis([0.5 1 -1 1])\n\n\n绘制矩阵\n&gt;&gt; A = magic(5)\nA =\n\n   17   24    1    8   15\n   23    5    7   14   16\n    4    6   13   20   22\n   10   12   19   21    3\n   11   18   25    2    9\n\n&gt;&gt; imagesc(A)\n\n\n&gt;&gt; imagesc(A),colorbar,colormap gray;\n\n\n控制语句\n%for循环语句\n&gt;&gt; v = zeros(1,10)\nv =\n\n   0   0   0   0   0   0   0   0   0   0\n\n&gt;&gt; for i = 1: 10\nv(i) = 2^i;\nend;\n&gt;&gt; v\nv =\n\n      2      4      8     16     32     64    128    256    512   1024\n\n&gt;&gt;\n\n%while语句\n&gt;&gt; while i &lt; 5,\nv(i) = 100;\ni = i+1\nend\n&gt;&gt; v\nv =\n\n      2      4      8     16     32     64    128    256    512   1024\n\n%if break语句\n&gt;&gt; i = 1;\n&gt;&gt; while true;\nv(i) = 999;\ni = i+1\nif i==6,\nbreak;\nend\nend\ni =  2\ni =  3\ni =  4\ni =  5\ni =  6\n&gt;&gt; v\nv =\n\n    999    999    999    999    999     64    128    256    512   1024\n\n&gt;&gt;\n\n&gt;&gt; if v(1) == 1,\ndisp(&#039;The value is one!&#039;)\nelseif v(1) == 2,\ndisp(&#039;The value is two!&#039;)\nelse\ndisp(&#039;The value is not one or two!&#039;)\nend\nThe value is not one or two!\n&gt;&gt;\n\n定义函数\n将函数定义写在文件中，并把文件名命名为‘函数名.m’，将文件放在当前路径下，或者用 addpath 将文件目录加入当前会话\n\n本章学习结束\n"},"机器学习笔记/5.机器学习":{"slug":"机器学习笔记/5.机器学习","filePath":"机器学习笔记/5.机器学习.md","title":"5.机器学习","links":[],"tags":["机器学习"],"content":"最近事情比较多，又懒惰了，继续学习。\n分类问题\n什么是分类问题，例如：\n垃圾邮件分类，恶性肿瘤预测。\n\n在分类问题中一般结果是0和1，1称为正样本或正类，0称为负样本或负类。\n首先讲解的是简单的两变量分类问题\n使用线性回归的方式解决分类问题如何？\n\n如果是上图这样的例子来看，使用线性回归的方式貌似可以解决分类问题，但是如果存在一个严重偏差的特征时，使用线性回归拟合分类问题就会出现严重的偏差，在分类问题中最终的结果只有0和1，但是在线性回归中会出现小于1和大于0的结果。\n\n所以使用线性回归的方式不能很好的处理分类问题，于是引出了另一种模型，逻辑回归（逻辑回归的叫法是历史原因，和回归并没有什么关系）\n逻辑回归\n什么样的数学模型适合回归问题呢？一个只会在0与1中间震荡的函数模型：\n\n其中：\nx: 表示的是特征向量\ng: 代表逻辑函数(Logistic function)是一个常用的曲线函数(Sigmoid function),表达式为：\n\n函数的图像就如上图所示。\nh,表示的就是逻辑回归，带入到函数g中，最终得到的表达式就是\n\n函数h表示的就是当输入特征X时，根据输入的特征计算输出变量Y=1的可能性。假设h(x)=0.7,表示的就是患有恶性肿瘤的概率为0.7\n判定边界(Decision Boundary)\n判定边界能够让我们更好的理解逻辑回归和假设函数在计算什么\n\n上图就是逻辑回归的函数和图像，看一下数学意义：\n当h &gt;= 0.5时,预测结果 y = 1，\n当h &lt; 0.5时，预测结果 y = 0,\n所以：\n当 y = 1 时，h(x) = g(z) &gt;= 0.5 \n那么 z &gt;= 0,也就是θtX&gt;=0;\n当y=0时，最后得到θtX&lt;0。\n\n具体看下面这个例子\n\n其中的theta的参数分别为-3,1,1\n存在如上图所示的数据以及表示函数,如果要预测y=1的概率，最后得到的表达式为：\n\n最后得到的结果很明显是一个过（0,3）（3,0）的直线：\n\n其中的方程就是一个判定边界，通过这条线就可以分辨出正样本和负样本了。\n除了这种线性的判定边界之外，还有一些其他形状的判定边界，如圆形。\n\n逻辑回归中的代价函数\n\n上面就是之前讲过的线性回归中的代价函数，这个代价函数在线性回归中能够很好地使用，但是在逻辑回归中却会出现问题，因为将逻辑回归的表达式带入到h函数中得到的是一个非凸函数的图像，那么就会存在多个局部最优解，无法像凸函数一样得到全局最优解。示例如下。\n\n所以在逻辑回归中需要重新定义代价函数：\n\n最后得到的函数h和Cost函数之前的关系如下：\n\n构建一个这样的函数的好处是在于，当y=1时，h=1，如果h不为1时误差随着h的变小而增大；同样，当y=0时，h=0，如果h不为0时误差随着h的变大而增大。\n代价函数中的梯度下降\n在上一节中的逻辑回归中的代价函数中给出了代价函数的定义\n\n最后可以简化为:\n\n最终的求解问题就是要求回归函数的值最小，那么同样可以使用在线性回归中所用到的梯度函数。\n\n上图就是逻辑回归的梯度求解过程，虽然看起来和线性回归相似，但实则是完全不同的。在线性回归中，h函数为theta的转置与X的乘积，但是在逻辑回归中则不是。这样就导致了两者在运算方面和优化方面是完全不同的。但是在运行梯度下降算法之前，进行特征缩放依旧是非常重要的。\n高级优化\n优化算法除了讲到的梯度下降算法之外，还有一些叫做共轭梯度下降算法(BFGS,L-BFGS)。使用这些共轭梯度下降算法的好处在于，不需要手动地选择学习率a，这些算法会自行尝试选择a;比梯度下降算法运算更快。\n一般情况下，在常见的机器学习算法库中都带有这些算法，不需要程序员手动实现这些算法。\n多类别分类问题\n现实世界中除了二元的分类问题还有多元的分类问题，例如邮件的类型有工作，朋友，家人，爱好等多种，分类到不同的文件夹下，如对天气的分类，是晴天、多云、小雨等等天气。\n多元分类问题与二元分类问题的区别如下:\n\n多元分类的思路与二元分类问题的解决思路是类似的。可以将多元问题变为两元问题，具体如下：\n\n这样n元的分类问题，就会进行n次的机器学习的分类算法。对每一次的分类结果即为h(x)。那么经过n此分类之后，最后得到的结果为:\n\n那么当输入新的训练集或者是变量X，只需要按照上面的思路进行分类，其中的h(x)的最大值就是对应的最后的分类结果。"},"机器学习笔记/6.机器学习":{"slug":"机器学习笔记/6.机器学习","filePath":"机器学习笔记/6.机器学习.md","title":"6.机器学习","links":[],"tags":["机器学习","正则化"],"content":"欠拟合(under fitting)和过拟合(over fitting)\n现在已经学习了一些不同的机器学习算法，包括线性回归和逻辑回归，它们能够有效的解决许多问题，但是将它们应用到某些特定的机器学习中时，就会出现欠拟合或者过拟合的问题，导致效果很差，通过正则化的方法可以改善算法，下面介绍什么是过拟合与欠拟合。\n\n继续用线性回归预测房价的例子：\n\n首先看第一幅图，使用一条直线函数来拟合数据，很显然随着房子面积的增大，房屋价格的变化越稳定或者说是越像右越趋于平滑，因此线性回归并没有很好拟合训练数据。\n\n我们把此类情况称为欠拟合(underfitting)，或者叫作叫做高偏差(bias)。\n这两种说法大致相似，都表示没有很好地拟合训练数据。高偏差这个词是 machine learning 的研究初期传下来的一个专业名词，具体到这个问题，意思就是说如果用线性回归这个算法去拟合训练数据，那么该算法实际上会产生一个非常大的偏差或者说存在一个很强的偏见。\n2. 在第二幅图我们用了一个二次函数去拟合数据，可以拟合出一条合理的曲线，事实证明这是一个很好的拟合效果。\n3. 第三幅图，在这里我们有五个训练数据，所以使用了五个参数θ0到θ4的一个四次多项式去拟合数据，它似乎是一个很好的拟合，因为它成功的通过了我们的所有训练数据，但是它非常的扭曲，在上下波动，所以事实上我们并不认为它是一个预测房价的好模型。\n我们把这类情况叫做过拟合(overfitting)，也叫高方差(variance)。\n与高偏差一样，高方差同样也是一个历史上的叫法。从第一印象上来说，如果我们拟合一个高阶多项式，那么这个函数能很好的拟合训练集（能拟合几乎所有的训练数据），但这也就面临函数可能太过庞大的问题，变量太多。\n同时如果我们没有足够的数据集（训练集）去约束这个变量过多的模型，那么就会发生过拟合。\n为什么足够多的多项式可以完美的拟合数据？\n泰勒公式展开式。\n\n概括的说，过拟合将会发生在变量（特征）过多的时候，这时候训练出的方程总能够很好的拟合训练数据，我们的代价函数无限趋于0或者就是0，但是这样千方百计拟合训练数据的曲线无法泛化到新的样本数据中，以至于无法预测新的样本价格。术语泛化指的是一个假设模型能够应用到新样本的能力，新样本指的是不在训练集中的样本数据。\n过拟合和欠拟合的情况不仅出现在线性回归也会出现在逻辑回归的问题\n\n过多的变量（特征），同时只有非常少的训练数据，会导致出现过度拟合的问题\n如何避免过拟合呢？有以下两个方式来避免过拟合\n\n\n减少选取变量的数量，保留更加重要的特征\n具体而言，我们可以人工检查每一项变量，并以此来确定哪些变量更为重要，然后，保留那些更为重要的特征变量。至于，哪些变量应该舍弃，我们以后在讨论，这会涉及到模型选择算法，这种算法是可以自动选择采用哪些特征变量，自动舍弃不需要的变量。这类做法非常有效，但是其缺点是当你舍弃一部分特征变量时，你也舍弃了问题中的一些信息。例如，也许所有的特征变量对于预测房价都是有用的，我们实际上并不想舍弃一些信息或者说舍弃这些特征变量。\n正则化\n正则化中我们将保留所有的特征变量，但是会减小特征变量的数量级（参数数值的大小θ(j)）。\n\n这个方法非常有效，当我们有很多特征变量时，其中每一个变量都能对预测产生一点影响。正如我们在房价预测的例子中看到的那样，我们可以有很多特征变量，其中每一个变量都是有用的，因此我们不希望把它们删掉，这就导致了正则化概念的发生。\n接下来我们会讨论怎样应用正则化和什么叫做正则化均值，然后将开始讨论怎样使用正则化来使学习算法正常工作，并避免过拟合。\n机器学习的正则化\n\n在前面介绍了用二次函数去拟合这些数据，他的拟合效果是很好的，然而我们用更高次的多项式去拟合，最终到的一个曲线，尽管他很好的拟合了训练集，但并不是一个好的结果，因为他过度拟合了数据，所以一般性不好。\n\n然后我们考虑下面的假设，我们想要加上惩罚项从而使参数 θ3 和 θ4 足够小，上面函数是我们的优化目标，也就是说我们要尽量减少代价函数的均方差，对于这个函数我们对它添加一些项，加上 1000 乘以 θ3 的平方，再加上 1000 乘以 θ4 的平方，于是出现了下面的式子：\n\n1000 只是我随便写的某个较大的数字而已。现在，如果我们要最小化这个函数，那么为了最小化这个新的代价函数，我们要让 θ3 和 θ4 尽可能小。因为，如果你在原有代价函数的基础上加上 1000 乘以 θ3 这一项 ，那么这个新的代价函数将变得很大，所以，当我们最小化这个新的代价函数时， 我们将使 θ3 的值接近于 0，同样 θ4 的值也接近于 0，就像我们忽略了这两个值一样。如果我们做到这一点（ θ3 和 θ4 接近 0 ），那么我们将得到一个近似的二次函数。\n\n因此，我们最终恰当地拟合了数据，我们所使用的正是二次函数加上一些非常小，贡献很小项（因为这些项的 θ3、 θ4 非常接近于0）。显然，这是一个更好的假设。\n\n正则化背后的思路，如果我们的参数值对应一个较小的值的（参数值较小），那么我们会道道一个形式更简单的假设。\n在上面的例子中，我们的惩罚的只是θ3和θ4，使这两个值均接近于零，从而得到了一个更简单的假设，实际上这个假设类似一个二次函数。\n更简单的讲，如果我们像惩罚θ3和θ4这样惩罚其他参数，那么我们往往可以得到一个相对简单的假设函数。\n实际上，这些参数的值越小，对应的函数曲线越平滑，也就是更加简单的函数，因此，就不易发生过拟合的问题。\n为什么越小的参数对应一个相对简单的假设函数，具体看下面这个例子。\n对于房屋价格的预测我们可能又上百个特征，与刚刚所说的多项式例子不同，我们并不知道θ3和θ4是高阶多项式的项，所以，我们有一百个特征，但是我们那并不知道如何选择关联度更好的参数，如何缩小参数的数目等等。\n因此在正则化里，我们要做的事情，就是减小我们的代价函数所有的参数值，因为我们并不知道哪一个或几个要去缩小，所以我们要修改代价函数，在后面添加一项，就像我们在方括号里的这项，当我们添加一个额外的正则化项的时候，我们缩小了每一个参数。\n\n顺便说一下，按照惯例，我们没有去惩罚 θ0，因此 θ0的值是大的。这就是一个约定从 1 到 n 的求和，而不是从 0 到 n 的求和。但其实在实践中这只会有非常小的差异，无论你是否包括这θ0这项。但是按照惯例，通常情况下我们还是只从 θ1 到 θn 进行正则化。\n带λ的的这项就是一个正则化项，并且λ在这里我们称做正则化参数。\nλ 要做的就是控制在两个不同的目标中的平衡关系。\n第一个目标就是我们想要使假设函数更好的拟合训练数据\n第二个目标是要保持我们的参数较小通过正则化\n而λ这个正则化参数需要控制的是两者之间的平衡，既平衡拟合训练的目标和保持参数值较小的目标。从而保持假设的形式相对简单，来避免过拟合。\n对于房屋价格预测来说，我们之前所用的非常多的高阶多项式来拟合，我们将会得到一个非常弯曲和复杂的曲线函数，现在只需要通过正则化的优化，就可以得到一个更加合适的曲线，这个曲线不是一个真正的二次函数曲线，而是更加的流畅和简单的一个曲线。这样就得到了对于这个数据集更好的假设函数。\n再一次说明下，这部分内容的确有些难以明白，为什么加上参数的影响可以具有这种效果？但如果你亲自实现了正规化，你将能够看到这种影响的最直观的感受。\n\n在正则化线性回归中，如果正则化参数值λ被设定的非常大，那么会发生什么呢？我们非常大的惩罚参数θ1 θ2 θ3 θ4 … 也就是说，我们最终惩罚θ1 θ2 θ3 θ4 …  在一个非常大的程度，那么我们会使所有这些参数接近于零。\n\n如果我们这么做，那么就意味着我们的假设相当于去掉了这些项，并且使我们只留下了一个简单的假设，这个假设只能表明房屋价格等于θ0的值，那就是类似与一条水平的直线，对于数据来说就是一个欠拟合。这是一个失败的假设直线，对于训练集来说这就是一条平滑的直线，它没有任何趋势，它不会去趋向大部分的训练样本的任何值。\n另一个说法就是这种假设有过于强烈的偏见或者说使高偏差，认为预测的价格只等于θ0，对于数据来说只是一条水平线。\n因此，为了使正则化运行良好，我们应当注意一些方面，应该去选择一个不错的正则化参数λ，当我们以后讲到多重选择时我们将讨论一种方法来自动选择正则化参数 λ，为了使用正则化，接下来我们将把这些概念应用到到线性回归和逻辑回归中去，那么我们就可以让他们避免过度拟合了。\n正则化的线性回归 （Regularized Linear Regression）\n之前写过线性回归的代价函数如下：\n\n对于线性回归的求解，我们之前运用了两种学习算法，一种基于梯度下降，一种基于正规方程。\n梯度下降\n\n正规方程\n\n不可逆情况\n当出现样本数量M比特征数N少或等于时，矩阵XTX将出现不可逆或者奇异（singluar）矩阵，用另一个说法就是矩阵的退化（degenerate），这时我们就没办法用正规方程来求 θ。\n\n正则化可以解决这个问题，具体的说只要正则参数是严格大于零，实际上，可以证明上图的蓝括号部分是可逆的（invertable），因此正则化可以解决任何XTX不可逆的问题。\n所以现在可以实现线性回归避免过度拟合的问题，即使是一个相对较小的训练集合里面有很多的特征值。\n正则化的逻辑回归（Regularized Logistic Regression）\n逻辑回归的正则化实际上和线性回归的正则化十分的相似。\n\n同样使用梯度下降：\n\n如果在高级优化算法中，使用正则化技术的话，那么对于这类算法我们需要自己定义costfunction\n\n这个我们自定义的 costFunction 的输入为向量 θ ，返回值有两项，分别是代价函数 jVal 以及 梯度gradient。\n总之我们需要的就是这个自定义函数costFunction，针对Octave而言，我们可以将这个函数作为参数传入到 fminunc 系统函数中（fminunc 用来求函数的最小值，将@costFunction作为参数代进去，注意 @costFunction 类似于C语言中的函数指针），fminunc返回的是函数 costFunction 在无约束条件下的最小值，即我们提供的代价函数 jVal 的最小值，当然也会返回向量 θ 的解。\n上述方法显然对正则化逻辑回归是适用的。\n总结\n从这里开始感觉课程已经有些难度了，关于正则化我也是查阅了其他相关资料才得以明白，尝试写个关于正则化的程序吧。"},"机器学习笔记/7.机器学习":{"slug":"机器学习笔记/7.机器学习","filePath":"机器学习笔记/7.机器学习.md","title":"7.机器学习","links":[],"tags":["机器学习","神经网络"],"content":"非线性假设（Non linear Hypotheses）\n神经网络实际上是一个相对古老的算法，是20世纪80年代时期出现的，但是没有成为发展的热点，随着现代计算机计算能力的提升，近年来，神经网络又成为机器学习算法中的一个热点。\n之前已经学习过线性回归和逻辑回归算法，为什么还要研究神经网络呢？为了阐述研究神经网络算法目的，我们首先来看几个机器学习的问题作为例子，这几个例子都依赖于复杂的非线性分类器\n考虑这个监督学习的分类问题，我们已经有了对应的训练集，如果利用逻辑回归算法来解决这个问题，首先需要构造一个包含很多非线性项的逻辑回归函数，这里g仍是s型函数（即f(x)=1/(1+e^-x)）。我们能让函数包含许多像这样的多项式，当多项式的项数足够的的时候你能够得到一个分开正样本和负样本的判定边界。\n例如当只有两项时比如x1和x2，这种方法确实能够得到不错的结果，因为你可以把x1和x2的所有组合都包含到多项式中，但是对于许多复杂的机器学习问题而言，设计的多项式往往多于两项。\n\n例如我们之前讨论过的房价预测问题，假设现在处理的是关于房屋的分类问题而不是一个回归问题。假设你对一栋房子的多方面特点都有所了解，你想预测房屋在未来半年内能够被卖出去的概率，这是一个分类问题。\n我们可以想到许多特征，对于不同的房子有可能有上百个特征，对于这类问题如果要包含所有的二次项，即使只包含二项式或多项式的计算，最终的多项式也可能有很多项，比如x1^2 ,x1x2 ,x1x3 ,x1x4直到x1x100,接着还有x2^2, x2x3等等很多项。因此即使只考虑二阶项，也就是说两个项的乘积x1乘以x1等等类似于此的项，那么，在n=100的情况下最终也有5050个二次项。\n而且随着特征个数n的增加，二次项的个数大约以 n^2 的量级增长，其中n是原始项的个数，即我们之前说过的x1到x100这些项。事实上二次项的个数大约是（n^2）/2个，因此要包含所有的二次项是很困难的，所以这可能不是一个好的做法。\n而且由于项数的过多，最后的结果可能是过拟合的，此外，在处理这么多项时也存在运算量过大的问题。当然，我们也可以试试只包含上边这些二次项的子集，例如，我们只考虑x1^2， x2^2， x3^3直到 x100^2这些项，这样就可以将二次项的数量大幅度减少，减少到只有100个二次项。但是由于忽略了太多项，在处理类似左上角的数据时，不太可能得到理想的结果。\n实际上如果只考虑x1的平方到x100的平方这一百个二次项，那么你可能会拟合出一些特别的假设，比如可能拟合出一个椭圆状的曲线，但是肯定不会拟合出左上角这个数据集的分界线，所以5000个二次项看起来已经很多了。\n而现在的假设还包括三次项， 例如x1x2x3, x1^2x2, x10x11x17等等，类似的三次项有很多很多，事实上，三次项的个数是n^3的量级增加。当n=100时，可以计算出来最后能得到大概17000个三次项。\n所以，当初始特征个数n增大时，这些高阶多项式将以几何级数递增，特征空间也随之急剧膨胀。当特征值个数n很大时，如果找出附加项来建立一些分类器，这并不是一个好做法。对于许多实际的机器学习问题，特征个数n是很大的。\n我们看看下边这个例子，这是关于计算机视觉中的一个问题。假设你想要使用机器学习算法来训练一个分类器，使他检测一个图像是否为一辆汽车。很多人可能会好奇，觉得这对计算器视觉来说有什么难的？\n\n当我们自己看这幅图像时里面有什么事一目了然的事情，你肯定会奇怪，为什么学算法会不知道图像是什么。\n为了解答这个问题，我们取出这幅图像的一部分，将其放大，比如这幅图中，汽车的门把手，红框中的部分，人肉眼看到一辆车时，计算机看到的是一个这样的数据矩阵。\n它们表示了像素强度值，告诉我们图像中每个像素的亮度值。因此，对于计算机视觉来说问题就变成了，根据这个像素点亮度矩阵来告诉我们这些数值是否代表一个汽车门把手。\n\n具体而言，当机器学习算法构造一个汽车识别器时，我们想出一个带标签的样本集，其中一些样本是各类汽车，而另一部分样本是其他任何东西。将这个样本输入给学习算法以训练出一个分类器，当训练完毕后，我们输入一副新的图片，让分类器判别“这是什么东西？”理想情况下，分类器能识别出这是一辆汽车。\n\n为了理解引入非线性分类器的必要性，我们从学习算法的训练样本中挑选出一些汽车的图片和非汽车的图片。让我们从其中每幅图片中挑出一组像素电，例如上图像素点1的位置和像素2的位置。\n在坐标系中标出这幅汽车的位置，其他坐标系中的位置取决于像素点1和像素点2的亮度。让我们用同样的方法在坐标系中标出其他图片中汽车的位置。接着我们在坐标系中继续画上两个非汽车样本。\n然后我们继续在坐标系中画上更多新样本，用“+”表示汽车图片，用“-”表示非汽车图片，我们将发现汽车样本和非汽车样本分布在坐标系中的不同区域，因此我们现在需要一个非线性分类器，来尽量分开这两类样本。\n这个分类问题中特征空间的维度是多少？\n显然在真实情况下，我们不可能只取两个像素点来做特征。假设我们用50*50像素的图片，注意，我们的图片已经足够小了，长宽只各有50个像素，但这依然是25000个像素点，因此，我们的特征向量的元素数量 n=2500。特征向量X包含了所有像素点的亮度值。\n对于典型的计算机图片表示方法，如果储存的每个像素点灰度值（色彩的强烈程度），那么每个元素的值应该在0 到255之间。因此，这个问题中n=2500\n但是这只是使用灰度图片的情况，如果我们用的是RGB彩色图像，每个像素点包含红，绿，蓝三个子像素，那么n=7500。\n因此，如果我们非要通过包含所有的二次项来解决这个非线性问题，那么仅仅二次项 xi * xj总共有大约300万个（2500^2/2），这个数字大的有点离谱了。对于每个样本来说，要发现并表示所有这300万个项，这个计算成本太高。因此，只是简单的增加二次项或者三次项之类的逻辑回归算法并不是一个解决复杂线性问题的好办法。因为n很大时，将会产生非常多的特征项。\n接下来，我们会讨论神经网络，他在解决复杂的非线性分类问题上，被证明是一种好的多的算法，及时你输入的特征空间或输入的特征维度n很大，也能轻松搞定。\n神经元和大脑（Neurons and the brain）\n神经网络是一种很古老的算法，他最初产生的目的是制造模拟大脑的机器。我们将会讨论神经网络，因为他能很好的解决不同的机器学习问题，而不是只因为他们在逻辑上行的通。\n神经网络产生的原因是人们想尝试设计出模拟大脑的计算。从某种意义上说，如果我们想要建立学习系统那为什么不去模拟我们所认识的最神奇的学习机器—人类的大脑的？\n神经网络逐渐兴起于二十世纪八九十年代，应用的非常广泛。但由于各种原因在90年代的后期应用减少，其中一个原因是神经网络是一种计算量有些偏大的算法，但是最近神经网络又东山再起了，大概 由于近年来计算机的运行速度变快，才足以真正运行起大规模的神经网络。\n正式由于这个原因和其他一些我们后面会讨论的技术因素，如今的神经网络对于许多应用来说是最先进的技术。\n当你模拟大脑时，是指想制造出于人类大脑效果相同的机器。大脑可以学会去看而不是听的方式处理图像，学会处理我们的触觉。我们能学习数学，学习计算微积分，而且大脑能处理各种不同的令人惊奇的事情。似乎如果你想要模仿它，你需要写许多不同的软件来模拟所有大脑告诉我们这些五花八门的奇妙的事情。\n如果假设大脑处理所有这些不同事情不需要上千个程序去实现他，相反，大脑只需要一个简单的学习算法就可以了呢？\n尽管这只是一个假设，不过让我和你分享一些这方面的证据。\n\n如图大脑这个部分，这一小片红色区域是你的听觉皮层，如果你通过我说的话来理解我表达的内容，那么是靠耳朵接收到声音信号并把声音信号传递给你的听觉表皮层，正因如此，你才能明白我的话。\n神经系统科学家做了一个有趣的实验，把耳朵到听觉表皮的神经切断。在这种情况下，将其重新接到一个动物的大脑上，这样从眼睛看到的视觉神经的信号最终将传到听觉表皮层，结果表明，听觉表皮层将会学会“看”。\n所以，如果你对动物这样做那么动物就可以完成视觉辨别任务，他们可以看图像，并根据图像做出适当的决定。它们正是通过脑组织中的这个部分完成的。\n\n下面在举另一个例子，这块红色的脑组织是你的躯体感觉皮层，这是你用来处理触觉的。如果你做一个和刚才类似的重接实验，那么躯体感觉皮层也能会“看”，这个实验和其他一些类似的实验被称为神经重接实验。从这个意义上说，如果人体有同一块脑组织可以处理光、声或触觉信号，那么也许存在一种学习方法可以同时处理视觉，听觉和触觉，而不是需要成千个不同的程序或者算法来做这些。\n大脑能够完成的成千上万的事，我们需要做的就是找出一些近似的或实际的大脑学习算法，然后实现它，大脑通过自学掌握如何处理这些不同类型的数据，在很大程度上可以猜想，如果我们把任何一种传感器接到大脑的任何一个部位，大脑就会学会处理它。\n\n再看上图的几个例子，左上角这张图是用舌头学会“看”的一个例子。这实际上是一个名为BrainPort的系统，他现在正在FDA（美国食品药物管理局）的临床试验阶段，他帮助失明人士看见事物。他的原理是，在你前额戴上一个灰度摄像头，他能够获取你面前的事物的低分辨率的灰度图像。你连接一根线到舌头上安装的电极阵列上，那么每个像素都被映射到你舌头上的某个位置。可能电压值高的点对应一个暗像素，电压值的点对应亮像素。\n即使依靠它现在的功能，使用这种系统就能够让人在几十分钟里学会用我们的舌头看东西。\n下面是第二个例子，关于人体回声定位或者说人体声纳。你有两种方法可以实现，你可以弹响指或者咂舌头。现在有失明人士确实在学校里接受这样的培训，并学会解读从环境反弹回来的声波模式—这就是声纳。\n如果你搜索 YouTube 之后，就会发现有些视频讲述了一个令人称奇的孩子，他因为癌症眼球惨遭移除，虽然失去了眼球，但是通过打响指他可以四处走动而不撞到任何东西。他能滑滑板，他可以将篮球投入篮框中。\n第三个例子是触觉皮带，如果你把它戴在腰上，蜂鸣器会响，而且总是朝向北时发出嗡嗡声。它可以使人拥有方向感，用类似于鸟类感知方向的方式。\n还有一些离奇的例子，如果你在青蛙身上插入第三只眼，青蛙也能学会使用那只眼睛。\n因此，这将会非常令人惊奇。如果你能把几乎任何传感器接入到大脑中，大脑的学习算法就能找出学习数据的方法并处理这些数据。从某种意义上来说如果我们能找出大脑的学习算法，然后在计算机上执行大脑学习算法或与之相似的算法，也许这将是我们向人工智能迈进做出的最好的尝试。\n模型表示（Model Representation）\n神经网络是在模仿大脑中的神经元或者神经网络时发明的。因此，要解释如何表示模型假设，我们不妨先来看单个神经元在大脑中是什么样的。\n\n我们的大脑中充满了如上图所示的这样的的神经元，神经元是大脑中俄细胞，其中有两点值得我们注意，一是神经元有像这样的细胞主题（Nucleus），二是神经元有一定数量的输入神经和输出神经。这些输入神经叫做树突（dendrite），可以把他们想象成输入电线，他们接受来自其他神经元的信息。神经元的输出神经焦作轴突（Axon），这些输出神经是用来给其他神经元传递信号或者传递信息的。\n简而言之，神经元是一个计算单元，他从输入神经接受一定数目的信息，并做一些计算，然后将结果通过他的轴突传送到其他节点或者大脑中的其他神经元。\n下面是神经元的示意图：\n\n神经元利用微弱的电流进行沟通。这些弱电流也称作动作电位，其实就是微弱的电流。所以如果神经元想要传递一个消息，他就会通过它的轴突发送一段微弱电流给其他神经元。\n\n上图中，黄色的圆圈就代表了一个神经元，X为输入向量，θ代表神经元的权重（就是我们之前所说的模型参数），hθ(x)代表激励函数（在神经网络术语中，激励函数只对类似非线性函数（g(z)的另一个术语称呼，g(z)等于1除以1加e的-z次方）。\n实际上你可以这样理解，神经元就是权重θ。\n当将输入送进神经元后，经计算（就是X^Tθ）会有一个输出，这个输出再送进激励函数中，便得到了神经元的真实输出。\n注意：当我们绘制一个神经网络时，通常我会只绘制节点x1,x2,x3等等，但有时可以增加一个额外的节点x0，这个x0节点有时被称作偏置神经元。但因为x0总是等于1，去哦们会画出它，有时我们不会画出，这要看画出他是否对例子有利。\n\n神经网络就是不同的神经元组合在一起，第一层为输入层，最后一层为输出层，而且中间的所有层均为隐藏层。\n注意：输入单元x1，x2，x3，再说一次，有时也可以画上额外的节点x0.同时，这里有三个神经元，我在里面写了a1(2) 、 a2(2) 和a3(2) ,然后再次说明这里我可以添加一个a0(2) ，这和x0一样，代表一个额外的偏度单位，它的值永远是1，注意：a1(2) 、 a2(2) 和a3(2)中计算的是g（X^Tθ）的值，而a0(2)中存放的就是偏置1。\n\n如果一个网络在第j层有sj个单元，在j+1层有sj+1个单元，那么矩阵θ(j)即控制第j层到第j+1层的映射。\n矩阵θ(j)的维度是s(j+1)*(sj+1),s(j+1)行，(sj+1)列\n总之，上面的图展示了是怎样定义一个人工网络的。这个神经网络定义了函数h:从输入x到输出y的映射。我将这些假设的参数记为大写的θ，这样一来不同的θ对应不同的假设，所以我们有不同的函数，比如说从x到y的映射。\n以上就是我们怎么从数学上定义神经网络的假设\n下面将讲解如何高效的进行计算，并展示一个向量化的实现方法，更重要的是让你明白这样表示神经网络是一个好方法，并且明白它们怎样帮助我们学习复杂的非线性假设\n\n以前我们说过计算出假设输出的步骤，通过左边的这些方程计算出三个隐藏的单元的激励值，然后利用这些值来计算假设函数h(x)的最终输出，接下来我要定义一些额外的项，因此，上图中蓝色线的项把他定义为z上标（2）下标1，这样一来就有了a(2)1 这个项，等于g(z(2)1)(上标2的意思是与第二层相关，即神经网络的隐藏层有关)接下来画红线的项同样定义为z(2)2，最后一项定义为z(2)3，这样我们就有a(2)3=g(z(2)3)，所以这些Z的值是线性组合，是输入值x1,x2,x3的加权线性组合，他将进入一个特定的神经元，类似于矩阵向量的乘法。\n现在看一线灰色框里的一维数组，你可能会注意到这一块对应了矩阵向量的运算x1乘以向量x，观察到这一点我就能够将神经网络的运算向量化，具体而言我们定义特征向量x为x0,x1,x2,x3组成的向量，其中x0=1，并定义z^2为\n这些值组成的向量，注意：这里的Z(2)是一个三维向量。\n下面我们可以这样向量化1(2) 、 a2(2) 和a3(2)的计算我们只用两个步骤z(2)等于θ(1)乘以x，然后a(2)等于g(z(2))，需要明白的是这里的z(2)是一个三维向量，并且a(2)也是一个三维向量因此这里的激励将s函数逐元素作用于z(2)中的每个元素z(2)就等于θ(1)乘以a(1)。当然x也有偏置单元x0，\n顺便说一下，为了让我们的符号和接下来的工作一致，在输入层，虽然我们有输入x但是我们还可以把这些想成是第一层的激励，所以我们可以定义第一层的激励a(1)=x,因此a(1)就是一个向量了，我们可以把这里的x替换成a(1)\n现在我们得到了a1，a2，a3的值，但是我们同样需要一个值a0，他对应隐藏层得到这个输出的偏置单元，这时a(2)就是一个四维的特征向量。\n为了计算假设的实际输出值h，我们只需要计算z(3),z(3)等于绿色框框中的项目，最后假设函数h(x)输出他等于a(3),a(3)是输出层唯一的单元，他是一个实数。\n这个h(x)的计算过程也成为向前传播(forward propagation),这样的命名是因为我们是从输入层的激励开始，然后进行向前传播给隐藏层，并计算隐藏层，然后我们继续向前传播，计算输出层的激励，这个从输入层到隐藏层再到输出层一次计算激励的过程叫向前传播。\n我们刚刚得到了这一过程的向量化实现方法，如果用右边的公式计算，会得到一个有效的计算h(x)的方法\n这种向前传播的角度，可以帮助我们了解神经网络的原理，帮助我们学习非线性假设\n\n看上面这幅图，我们先盖住图片左边的部分，如果只看右边，这看起来很像逻辑回归，在逻辑回归中我们用最后一个节点，也就是最后一个逻辑回归单元来预测h(x)的值，具体来说，假设输出的h(x)等于s型激励函数g(Θa1+Θa2…)。其中a由那三个单元一样，为了和我们之前的定义保持一致，需要添加红色的上标和下标1，因为我们只有一个输出单元，但如果你只观察蓝色的部分，这看起来非常像标准的逻辑回归模型，不同之处在于，我现在用的是大写的Θ，而不是小写的Θ，这样做完我们只得到了逻辑回归，但是逻辑回归输入特征值是通过隐藏层计算的。\n再说一遍，神经网络所做的就像逻辑回归，但是它不是使用x1，x2，x3作为输入特征，而是用a1，a2，a3作为新的输入特征，同样的我们需要把上标加上来和之前的记号保持一致，有趣的是特征值a1，a2，a2是当做输入函数来学习的，具体来说，就是从第一层映射到第二层的函数，这个函数由其他一组参数θ(1)决定，而在神经网络上，他没有用输入特征x1，x2，x3，来训练逻辑回归而是自己训练逻辑回归的输入a1，a2，a3，可以想象，如果在θ1中选择不同的参数，可以学习一些很有趣和复杂的特征，就可以得到一个更好的假设，比使用原始输入x1，x2或x3时得到的假设更好。\n你也可以x1，x2，x3等作为输入项，但是这个算法可以灵活的快速学习任意的特征项，把这些a1，a2，a3,输入这个最后的单元，实际上他是逻辑回归。\n\n还可以用其他类型图表示神经网络，神经网络中神经元相连接的方式，称为神经网络的架构，所以架构是指，不同的神经元是如何相互连接的，这里有个一不同的神经网络架构的例子，你可以意识到这个第二层是如何工作的，我们有三个隐藏单元，它们根据输入层计算一个复杂的函数，然后到第三层，我们可以将第二层训练出的特征项作为输入，并在第三层计算一些更复杂的函数，这样你在第四次，即输出层时，就可以利用第三层训练出的更复杂的特征项作为输入，以此得到非常有趣的非线性假设。顺便说下，在这个网络中，第一层被称为输入层，第四层仍然是我们的输出层，这个网络有两个隐藏层，所以都被称为隐藏层任何一个不是输入层或者输出层的。\n示例和直觉（Examples and Intuitions）\n接下来讲解两个例子来说明神经网络是如何计算的。\n关于输入的复杂的非线性函数，希望这个例子可以让你了解，神经网络可以用来学习复杂的非线性假设\n\n我们有x1，x2要么取0要么取1，所以x1和x2只能有两种取值，在这个例子中，我只画出了，两个正样本和两个负样本，你可以认为这是一个复杂样本的简单版本，咋这个复杂问题中，我们可能在右上角有一堆正样本，和左上角一堆用圈圈表示的负样本，我们想要学习一种非线性的决策边界来区分正负样本。\n我们用左边的例子来说明，具体来讲我们需要计算的是目标函数y等于x1异或x2，或者y也可以等于x1异或非x2，其中异或非表示x1异或x2后取反，x1异或x2为真当且仅当这两个值，x1或者x2中有且仅有一个为1，如果我用xNOR的例子比用NOT作为例子结果会好一些，但这两个其实是相同的，这就意味着在x1异或x2后再取反，即他们同时为真或者同时为假的时候，我们将会获得y等于1，y为0的结果是，如果他们中仅有一个为真，则y为0。\n我们能否找到一个神经网络模型来拟合这种训练集，为了建立能够拟合XNOR运算，我们先拟合一个简单的神经网络，它拟合了“且运算”。\n\n假设我们有输入x1，x2都是二进制，即要么是0要么是1，我们的目标函数y等于x1且x2，一个逻辑与运算，那么我们怎样得到一个具有单个神经元的神经网络来计算这个逻辑与呢？\n我们给这个网络分配一些权重或参数，-30，+20，+20，即我们给x的前面系数赋值，所以我们的h(x)=g(-30+20x1+20x2),右上角的图就是我们的s型函数，然后我们看四种输入的可能性，就是与运算的结果。\n\n同样我们用神经网络实现或运算然后讲解更为赋值的神经网络。\n\n我们只要在x1前面放入一个很大的负数，就可以实现非的功能。\n\n我们现在把这三个功能放在一起，就可以实现x1 XNOR x2的功能。\n当层数很多的时候，你有一个相对简单的输入量的函数作为第二层，而第三层可以建立在此基础上建立一些更加复杂的函数，然后在下一层又在计算一个稍微复杂的函数，我们可以运用更深层的函数计算更加复杂的函数。\n神经网络还可以用于识别手写数字。\n它使用的输入是不同的图像或者说就是一些原始的像素点。第一层计算出一些特征，然后下一层再计算出一些稍复杂的特征，然后是更复杂的特征，然后这些特征实际上被最终传递给最后一层逻辑回归分类器上，使其准确地预测出神经网络“看”到的数字。\n多类分类（Multiclass Classification）\n在多分类问题中我们如何处理？\n\n\n和处理逻辑回归的多分类问题一样。"},"机器学习笔记/8.机器学习":{"slug":"机器学习笔记/8.机器学习","filePath":"机器学习笔记/8.机器学习.md","title":"8.机器学习","links":[],"tags":["机器学习"],"content":"十、应用机器学习的建议(Advice for Applying Machine Learning)\n10.1 决定下一步做什么\n参考视频: 10 - 1 - Deciding What to Try Next (6 min).mkv\n​\t到目前为止，我们已经介绍了许多不同的学习算法，如果你一直跟着这些视频的进度学习，你会发现自己已经不知不觉地成为一个了解许多先进机器学习技术的专家了。\n​\t然而，在懂机器学习的人当中依然存在着很大的差距，一部分人确实掌握了怎样高效有力地运用这些学习算法。而另一些人他们可能对我马上要讲的东西，就不是那么熟悉了。他们可能没有完全理解怎样运用这些算法。因此总是把时间浪费在毫无意义的尝试上。我想做的是确保你在设计机器学习的系统时，你能够明白怎样选择一条最合适、最正确的道路。因此，在这节视频和之后的几段视频中，我将向你介绍一些实用的建议和指导，帮助你明白怎样进行选择。具体来讲，我将重点关注的问题是假如你在开发一个机器学习系统，或者想试着改进一个机器学习系统的性能，你应如何决定接下来应该选择哪条道路？为了解释这一问题，我想仍然使用预测房价的学习例子，假如你已经完成了正则化线性回归，也就是最小化代价函数J的值，假如，在你得到你的学习参数以后，如果你要将你的假设函数放到一组新的房屋样本上进行测试，假如说你发现在预测房价时产生了巨大的误差，现在你的问题是要想改进这个算法，接下来应该怎么办？\n​\t实际上你可以想出很多种方法来改进这个算法的性能，其中一种办法是使用更多的训练样本。具体来讲，也许你能想到通过电话调查或上门调查来获取更多的不同的房屋出售数据。遗憾的是，我看到好多人花费了好多时间想收集更多的训练样本。他们总认为，要是我有两倍甚至十倍数量的训练数据，那就一定会解决问题的是吧？但有时候获得更多的训练数据实际上并没有作用。在接下来的几段视频中，我们将解释原因。\n​\t我们也将知道怎样避免把过多的时间浪费在收集更多的训练数据上，这实际上是于事无补的。另一个方法，你也许能想到的是尝试选用更少的特征集。因此如果你有一系列特征比如x_1,x_2,x_3等等。也许有很多特征，也许你可以花一点时间从这些特征中仔细挑选一小部分来防止过拟合。或者也许你需要用更多的特征，也许目前的特征集，对你来讲并不是很有帮助。你希望从获取更多特征的角度来收集更多的数据，同样地，你可以把这个问题扩展为一个很大的项目，比如使用电话调查来得到更多的房屋案例，或者再进行土地测量来获得更多有关，这块土地的信息等等，因此这是一个复杂的问题。同样的道理，我们非常希望在花费大量时间完成这些工作之前，我们就能知道其效果如何。我们也可以尝试增加多项式特征的方法，比如x_1的平方，x_2的平方，x_1,x_2的乘积，我们可以花很多时间来考虑这一方法，我们也可以考虑其他方法减小或增大正则化参数\\lambda的值。我们列出的这个单子，上面的很多方法都可以扩展开来扩展成一个六个月或更长时间的项目。遗憾的是，大多数人用来选择这些方法的标准是凭感觉的，也就是说，大多数人的选择方法是随便从这些方法中选择一种，比如他们会说“噢，我们来多找点数据吧”，然后花上六个月的时间收集了一大堆数据，然后也许另一个人说：“好吧，让我们来从这些房子的数据中多找点特征吧”。我很遗憾不止一次地看到很多人花了至少六个月时间来完成他们随便选择的一种方法，而在六个月或者更长时间后，他们很遗憾地发现自己选择的是一条不归路。幸运的是，有一系列简单的方法能让你事半功倍，排除掉单子上的至少一半的方法，留下那些确实有前途的方法，同时也有一种很简单的方法，只要你使用，就能很轻松地排除掉很多选择，从而为你节省大量不必要花费的时间。最终达到改进机器学习系统性能的目的假设我们需要用一个线性回归模型来预测房价，当我们运用训练好了的模型来预测未知数据的时候发现有较大的误差，我们下一步可以做什么？\n\n\n获得更多的训练实例——通常是有效的，但代价较大，下面的方法也可能有效，可考虑先采用下面的几种方法。\n\n\n尝试减少特征的数量\n\n\n尝试获得更多的特征\n\n\n尝试增加多项式特征\n\n\n尝试减少正则化程度\\lambda\n\n\n尝试增加正则化程度\\lambda\n\n\n​        我们不应该随机选择上面的某种方法来改进我们的算法，而是运用一些机器学习诊断法来帮助我们知道上面哪些方法对我们的算法是有效的。\n​\t在接下来的两段视频中，我首先介绍怎样评估机器学习算法的性能，然后在之后的几段视频中，我将开始讨论这些方法，它们也被称为”机器学习诊断法”。“诊断法”的意思是：这是一种测试法，你通过执行这种测试，能够深入了解某种算法到底是否有用。这通常也能够告诉你，要想改进一种算法的效果，什么样的尝试，才是有意义的。在这一系列的视频中我们将介绍具体的诊断法，但我要提前说明一点的是，这些诊断法的执行和实现，是需要花些时间的，有时候确实需要花很多时间来理解和实现，但这样做的确是把时间用在了刀刃上，因为这些方法让你在开发学习算法时，节省了几个月的时间，因此，在接下来几节课中，我将先来介绍如何评价你的学习算法。在此之后，我将介绍一些诊断法，希望能让你更清楚。在接下来的尝试中，如何选择更有意义的方法。\n10.2 评估一个假设\n参考视频: 10 - 2 - Evaluating a Hypothesis (8 min).mkv\n​\t在本节视频中我想介绍一下怎样用你学过的算法来评估假设函数。在之后的课程中，我们将以此为基础来讨论如何避免过拟合和欠拟合的问题。\n\n​\t当我们确定学习算法的参数的时候，我们考虑的是选择参量来使训练误差最小化，有人认为得到一个非常小的训练误差一定是一件好事，但我们已经知道，仅仅是因为这个假设具有很小的训练误差，并不能说明它就一定是一个好的假设函数。而且我们也学习了过拟合假设函数的例子，所以这推广到新的训练集上是不适用的。\n​\t那么，你该如何判断一个假设函数是过拟合的呢？对于这个简单的例子，我们可以对假设函数h(x)进行画图，然后观察图形趋势，但对于特征变量不止一个的这种一般情况，还有像有很多特征变量的问题，想要通过画出假设函数来进行观察，就会变得很难甚至是不可能实现。\n​\t因此，我们需要另一种方法来评估我们的假设函数过拟合检验。\n​\t为了检验算法是否过拟合，我们将数据分成训练集和测试集，通常用70%的数据作为训练集，用剩下30%的数据作为测试集。很重要的一点是训练集和测试集均要含有各种类型的数据，通常我们要对数据进行“洗牌”，然后再分成训练集和测试集。\n\n​\t测试集评估在通过训练集让我们的模型学习得出其参数后，对测试集运用该模型，我们有两种方式计算误差：\n\n对于线性回归模型，我们利用测试集数据计算代价函数J\n对于逻辑回归模型，我们除了可以利用测试数据集来计算代价函数外：\n\n J_{test}{(\\theta)} = -\\frac{1}{{m}_{test}}\\sum_\\limits{i=1}^{m_{test}}\\log{h_{\\theta}(x^{(i)}_{test})}+(1-{y^{(i)}_{test}})\\log{h_{\\theta}(x^{(i)}_{test})}\n误分类的比率，对于每一个测试集实例，计算：\n\n然后对计算结果求平均。\n10.3 模型选择和交叉验证集\n参考视频: 10 - 3 - Model Selection and Train_Validation_Test Sets (12 min).mkv\n​\t假设我们要在10个不同次数的二项式模型之间进行选择：\n\n​\t显然越高次数的多项式模型越能够适应我们的训练数据集，但是适应训练数据集并不代表着能推广至一般情况，我们应该选择一个更能适应一般情况的模型。我们需要使用交叉验证集来帮助选择模型。\n​\t即：使用60%的数据作为训练集，使用 20%的数据作为交叉验证集，使用20%的数据作为测试集\n\n模型选择的方法为：\n\n\n使用训练集训练出10个模型\n\n\n用10个模型分别对交叉验证集计算得出交叉验证误差（代价函数的值）\n\n\n选取代价函数值最小的模型\n\n\n用步骤3中选出的模型对测试集计算得出推广误差（代价函数的值）\nTrain/validation/test error\nTraining error:\n​\t\t\t\tJ_{train}(\\theta) = \\frac{1}{2m}\\sum_\\limits{i=1}^{m}(h_{\\theta}(x^{(i)})-y^{(i)})^2\nCross Validation error:\n​\t\t\t\tJ_{cv}(\\theta) = \\frac{1}{2m_{cv}}\\sum_\\limits{i=1}^{m}(h_{\\theta}(x^{(i)}_{cv})-y^{(i)}_{cv})^2​\nTest error:\n​\t\t\t\tJ_{test}(\\theta)=\\frac{1}{2m_{test}}\\sum_\\limits{i=1}^{m_{test}}(h_{\\theta}(x^{(i)}_{cv})-y^{(i)}_{cv})^2\n\n\n10.4 诊断偏差和方差\n参考视频: 10 - 4 - Diagnosing Bias vs. Variance (8 min).mkv\n​\t当你运行一个学习算法时，如果这个算法的表现不理想，那么多半是出现两种情况：要么是偏差比较大，要么是方差比较大。换句话说，出现的情况要么是欠拟合，要么是过拟合问题。那么这两种情况，哪个和偏差有关，哪个和方差有关，或者是不是和两个都有关？搞清楚这一点非常重要，因为能判断出现的情况是这两种情况中的哪一种。其实是一个很有效的指示器，指引着可以改进算法的最有效的方法和途径。在这段视频中，我想更深入地探讨一下有关偏差和方差的问题，希望你能对它们有一个更深入的理解，并且也能弄清楚怎样评价一个学习算法，能够判断一个算法是偏差还是方差有问题，因为这个问题对于弄清如何改进学习算法的效果非常重要，高偏差和高方差的问题基本上来说是欠拟合和过拟合的问题。\n\n​\t我们通常会通过将训练集和交叉验证集的代价函数误差与多项式的次数绘制在同一张图表上来帮助分析：\n\nBias/variance\nTraining error:\t\t\t\t               J_{train}(\\theta) = \\frac{1}{2m}\\sum_\\limits{i=1}^{m}(h_{\\theta}(x^{(i)})-y^{(i)})^2\nCross Validation error:\t\t\t\tJ_{cv}(\\theta) = \\frac{1}{2m_{cv}}\\sum_\\limits{i=1}^{m}(h_{\\theta}(x^{(i)}_{cv})-y^{(i)}_{cv})^2\n\n​\t对于训练集，当 d 较小时，模型拟合程度更低，误差较大；随着 d 的增长，拟合程度提高，误差减小。\n​\t对于交叉验证集，当 d 较小时，模型拟合程度低，误差较大；但是随着 d 的增长，误差呈现先减小后增大的趋势，转折点是我们的模型开始过拟合训练数据集的时候。\n​\t如果我们的交叉验证集误差较大，我们如何判断是方差还是偏差呢？根据上面的图表，我们知道:\n\n​\t训练集误差和交叉验证集误差近似时：偏差/欠拟合\n​\t交叉验证集误差远大于训练集误差时：方差/过拟合\n10.5 正则化和偏差/方差\n参考视频: 10 - 5 - Regularization and Bias_Variance (11 min).mkv\n​\t在我们在训练模型的过程中，一般会使用一些正则化方法来防止过拟合。但是我们可能会正则化的程度太高或太小了，即我们在选择λ的值时也需要思考与刚才选择多项式模型次数类似的问题。\n\n​\t我们选择一系列的想要测试的 \\lambda 值，通常是 0-10之间的呈现2倍关系的值（如：0,0.01,0.02,0.04,0.08,0.15,0.32,0.64,1.28,2.56,5.12,10共12个）。 我们同样把数据分为训练集、交叉验证集和测试集。\n\n选择\\lambda的方法为：\n\n使用训练集训练出12个不同程度正则化的模型\n用12个模型分别对交叉验证集计算的出交叉验证误差\n选择得出交叉验证误差最小的模型\n运用步骤3中选出模型对测试集计算得出推广误差，我们也可以同时将训练集和交叉验证集模型的代价函数误差与λ的值绘制在一张图表上：\n\n\n​\t• 当 \\lambda 较小时，训练集误差较小（过拟合）而交叉验证集误差较大\n​\t• 随着 \\lambda 的增加，训练集误差不断增加（欠拟合），而交叉验证集误差则是先减小后增加\n10.6 学习曲线\n参考视频: 10 - 6 - Learning Curves (12 min).mkv\n​\t学习曲线就是一种很好的工具，我经常使用学习曲线来判断某一个学习算法是否处于偏差、方差问题。学习曲线是学习算法的一个很好的合理检验（sanity check）。学习曲线是将训练集误差和交叉验证集误差作为训练集实例数量（m）的函数绘制的图表。\n​\t即，如果我们有100行数据，我们从1行数据开始，逐渐学习更多行的数据。思想是：当训练较少行数据的时候，训练的模型将能够非常完美地适应较少的训练数据，但是训练出来的模型却不能很好地适应交叉验证集数据或测试集数据。\n\n\n​\t如何利用学习曲线识别高偏差/欠拟合：作为例子，我们尝试用一条直线来适应下面的数据，可以看出，无论训练集有多么大误差都不会有太大改观：\n\n​\t也就是说在高偏差/欠拟合的情况下，增加数据到训练集不一定能有帮助。\n​\t如何利用学习曲线识别高方差/过拟合：假设我们使用一个非常高次的多项式模型，并且正则化非常小，可以看出，当交叉验证集误差远大于训练集误差时，往训练集增加更多数据可以提高模型的效果。\n\n​\t也就是说在高方差/过拟合的情况下，增加更多数据到训练集可能可以提高算法效果。\n10.7 决定下一步做什么\n参考视频: 10 - 7 - Deciding What to Do Next Revisited (7 min).mkv\n​\t我们已经介绍了怎样评价一个学习算法，我们讨论了模型选择问题，偏差和方差的问题。那么这些诊断法则怎样帮助我们判断，哪些方法可能有助于改进学习算法的效果，而哪些可能是徒劳的呢？\n​\t让我们再次回到最开始的例子，在那里寻找答案，这就是我们之前的例子。回顾 1.1 中提出的六种可选的下一步，让我们来看一看我们在什么情况下应该怎样选择：\n\n\n获得更多的训练实例——解决高方差\n\n\n尝试减少特征的数量——解决高方差\n\n\n尝试获得更多的特征——解决高偏差\n\n\n尝试增加多项式特征——解决高偏差\n\n\n尝试减少正则化程度λ——解决高偏差\n\n\n尝试增加正则化程度λ——解决高方差\n\n\n神经网络的方差和偏差：\n\n​\t使用较小的神经网络，类似于参数较少的情况，容易导致高偏差和欠拟合，但计算代价较小使用较大的神经网络，类似于参数较多的情况，容易导致高方差和过拟合，虽然计算代价比较大，但是可以通过正则化手段来调整而更加适应数据。\n​\t通常选择较大的神经网络并采用正则化处理会比采用较小的神经网络效果要好。\n​\t对于神经网络中的隐藏层的层数的选择，通常从一层开始逐渐增加层数，为了更好地作选择，可以把数据分为训练集、交叉验证集和测试集，针对不同隐藏层层数的神经网络训练神经网络，\n然后选择交叉验证集代价最小的神经网络。\n​\t好的，以上就是我们介绍的偏差和方差问题，以及诊断该问题的学习曲线方法。在改进学习算法的表现时，你可以充分运用以上这些内容来判断哪些途径可能是有帮助的。而哪些方法可能是无意义的。如果你理解了以上几节视频中介绍的内容，并且懂得如何运用。那么你已经可以使用机器学习方法有效的解决实际问题了。你也能像硅谷的大部分机器学习从业者一样，他们每天的工作就是使用这些学习算法来解决众多实际问题。我希望这几节中提到的一些技巧，关于方差、偏差，以及学习曲线为代表的诊断法能够真正帮助你更有效率地应用机器学习，让它们高效地工作。\n十一、机器学习系统的设计(Machine Learning System Design)\n11.1 首先要做什么\n参考视频: 11 - 1 - Prioritizing What to Work On (10 min).mkv\n​\t在接下来的视频中，我将谈到机器学习系统的设计。这些视频将谈及在设计复杂的机器学习系统时，你将遇到的主要问题。同时我们会试着给出一些关于如何巧妙构建一个复杂的机器学习系统的建议。下面的课程的的数学性可能不是那么强，但是我认为我们将要讲到的这些东西是非常有用的，可能在构建大型的机器学习系统时，节省大量的时间。\n​\t本周以一个垃圾邮件分类器算法为例进行讨论。\n​\t为了解决这样一个问题，我们首先要做的决定是如何选择并表达特征向量x。我们可以选择一个由100个最常出现在垃圾邮件中的词所构成的列表，根据这些词是否有在邮件中出现，来获得我们的特征向量（出现为1，不出现为0），尺寸为100×1。\n为了构建这个分类器算法，我们可以做很多事，例如：\n\n\n收集更多的数据，让我们有更多的垃圾邮件和非垃圾邮件的样本\n\n\n基于邮件的路由信息开发一系列复杂的特征\n\n\n基于邮件的正文信息开发一系列复杂的特征，包括考虑截词的处理\n\n\n为探测刻意的拼写错误（把watch 写成w4tch）开发复杂的算法\n\n\n​       在上面这些选项中，非常难决定应该在哪一项上花费时间和精力，作出明智的选择，比随着感觉走要更好。当我们使用机器学习时，总是可以“头脑风暴”一下，想出一堆方法来试试。实际上，当你需要通过头脑风暴来想出不同方法来尝试去提高精度的时候，你可能已经超越了很多人了。大部分人并不尝试着列出可能的方法，他们做的只是某天早上醒来，因为某些原因有了一个突发奇想：“让我们来试试用Honey Pot项目收集大量的数据吧。”\n​       我们将在随后的课程中讲误差分析，我会告诉你怎样用一个更加系统性的方法，从一堆不同的方法中，选取合适的那一个。因此，你更有可能选择一个真正的好方法，能让你花上几天几周，甚至是几个月去进行深入的研究。\n11.2 误差分析\n参考视频: 11 - 2 - Error Analysis (13 min).mkv\n​\t在本次课程中，我们将会讲到误差分析（Error Analysis）的概念。这会帮助你更系统地做出决定。如果你准备研究机器学习的东西，或者构造机器学习应用程序，最好的实践方法不是建立一个非常复杂的系统，拥有多么复杂的变量；而是构建一个简单的算法，这样你可以很快地实现它。\n​\t每当我研究机器学习的问题时，我最多只会花一天的时间，就是字面意义上的24小时，来试图很快的把结果搞出来，即便效果不好。坦白的说，就是根本没有用复杂的系统，但是只是很快的得到的结果。即便运行得不完美，但是也把它运行一遍，最后通过交叉验证来检验数据。一旦做完，你可以画出学习曲线，通过画出学习曲线，以及检验误差，来找出你的算法是否有高偏差和高方差的问题，或者别的问题。在这样分析之后，再来决定用更多的数据训练，或者加入更多的特征变量是否有用。这么做的原因是：这在你刚接触机器学习问题时是一个很好的方法，你并不能提前知道你是否需要复杂的特征变量，或者你是否需要更多的数据，还是别的什么。提前知道你应该做什么，是非常难的，因为你缺少证据，缺少学习曲线。因此，你很难知道你应该把时间花在什么地方来提高算法的表现。但是当你实践一个非常简单即便不完美的方法时，你可以通过画出学习曲线来做出进一步的选择。你可以用这种方式来避免一种电脑编程里的过早优化问题，这种理念是：我们必须用证据来领导我们的决策，怎样分配自己的时间来优化算法，而不是仅仅凭直觉，凭直觉得出的东西一般总是错误的。除了画出学习曲线之外，一件非常有用的事是误差分析，我的意思是说：当我们在构造垃圾邮件分类器时，我会看一看我的交叉验证数据集，然后亲自看一看哪些邮件被算法错误地分类。因此，通过这些被算法错误分类的垃圾邮件与非垃圾邮件，你可以发现某些系统性的规律：什么类型的邮件总是被错误分类。经常地这样做之后，这个过程能启发你构造新的特征变量，或者告诉你：现在这个系统的短处，然后启发你如何去提高它。\n​\t构建一个学习算法的推荐方法为：\n​\t1. 从一个简单的能快速实现的算法开始，实现该算法并用交叉验证集数据测试这个算法\n​\t2.绘制学习曲线，决定是增加更多数据，或者添加更多特征，还是其他选择\n​\t3.进行误差分析：人工检查交叉验证集中我们算法中产生预测误差的实例，看看这些实例是否有某种系统化的趋势\n​\t以我们的垃圾邮件过滤器为例，误差分析要做的既是检验交叉验证集中我们的算法产生错误预测的所有邮件，看：是否能将这些邮件按照类分组。例如医药品垃圾邮件，仿冒品垃圾邮件或者密码窃取邮件等。然后看分类器对哪一组邮件的预测误差最大，并着手优化。\n​\t思考怎样能改进分类器。例如，发现是否缺少某些特征，记下这些特征出现的次数。\n​\t例如记录下错误拼写出现了多少次，异常的邮件路由情况出现了多少次等等，然后从出现次数最多的情况开始着手优化。\n​\t误差分析并不总能帮助我们判断应该采取怎样的行动。有时我们需要尝试不同的模型，然后进行比较，在模型比较时，用数值来判断哪一个模型更好更有效，通常我们是看交叉验证集的误差。\n​\t在我们的垃圾邮件分类器例子中，对于“我们是否应该将discount/discounts/discounted/discounting处理成同一个词？”如果这样做可以改善我们算法，我们会采用一些截词软件。误差分析不能帮助我们做出这类判断，我们只能尝试采用和不采用截词软件这两种不同方案，然后根据数值检验的结果来判断哪一种更好。\n​\t因此，当你在构造学习算法的时候，你总是会去尝试很多新的想法，实现出很多版本的学习算法，如果每一次你实践新想法的时候，你都要手动地检测这些例子，去看看是表现差还是表现好，那么这很难让你做出决定。到底是否使用词干提取，是否区分大小写。但是通过一个量化的数值评估，你可以看看这个数字，误差是变大还是变小了。你可以通过它更快地实践你的新想法，它基本上非常直观地告诉你：你的想法是提高了算法表现，还是让它变得更坏，这会大大提高你实践算法时的速度。所以我强烈推荐在交叉验证集上来实施误差分析，而不是在测试集上。但是，还是有一些人会在测试集上来做误差分析。即使这从数学上讲是不合适的。所以我还是推荐你在交叉验证向量上来做误差分析。\n​\t总结一下，当你在研究一个新的机器学习问题时，我总是推荐你实现一个较为简单快速、即便不是那么完美的算法。我几乎从未见过人们这样做。大家经常干的事情是：花费大量的时间在构造算法上，构造他们以为的简单的方法。因此，不要担心你的算法太简单，或者太不完美，而是尽可能快地实现你的算法。当你有了初始的实现之后，它会变成一个非常有力的工具，来帮助你决定下一步的做法。因为我们可以先看看算法造成的错误，通过误差分析，来看看他犯了什么错，然后来决定优化的方式。另一件事是：假设你有了一个快速而不完美的算法实现，又有一个数值的评估数据，这会帮助你尝试新的想法，快速地发现你尝试的这些想法是否能够提高算法的表现，从而你会更快地做出决定，在算法中放弃什么，吸收什么误差分析可以帮助我们系统化地选择该做什么。\n11.3 类偏斜的误差度量\n参考视频: 11 - 3 - Error Metrics for Skewed Classes (12 min).mkv\n​\t在前面的课程中，我提到了误差分析，以及设定误差度量值的重要性。那就是，设定某个实数来评估你的学习算法，并衡量它的表现，有了算法的评估和误差度量值。有一件重要的事情要注意，就是使用一个合适的误差度量值，这有时会对于你的学习算法造成非常微妙的影响，这件重要的事情就是偏斜类（skewed classes）的问题。类偏斜情况表现为我们的训练集中有非常多的同一种类的实例，只有很少或没有其他类的实例。\n​\t例如我们希望用算法来预测癌症是否是恶性的，在我们的训练集中，只有0.5%的实例是恶性肿瘤。假设我们编写一个非学习而来的算法，在所有情况下都预测肿瘤是良性的，那么误差只有0.5%。然而我们通过训练而得到的神经网络算法却有1%的误差。这时，误差的大小是不能视为评判算法效果的依据的。\n​\t查准率（Precision）和查全率（Recall） 我们将算法预测的结果分成四种情况：\n​\t1. 正确肯定（True Positive,TP）：预测为真，实际为真\n​\t2.正确否定（True Negative,TN）：预测为假，实际为假\n​\t3.错误肯定（False Positive,FP）：预测为真，实际为假\n​\t4.错误否定（False Negative,FN）：预测为假，实际为真\n​\t则：查准率=TP/(TP+FP)。例，在所有我们预测有恶性肿瘤的病人中，实际上有恶性肿瘤的病人的百分比，越高越好。\n​\t查全率=TP/(TP+FN)。例，在所有实际上有恶性肿瘤的病人中，成功预测有恶性肿瘤的病人的百分比，越高越好。\n​\t这样，对于我们刚才那个总是预测病人肿瘤为良性的算法，其查全率是0。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n预测值PositiveNegtive实际值PositiveTPFNNegtiveFPTN\n11.4 查准率和查全率之间的权衡\n参考视频: 11 - 4 - Trading Off Precision and Recall (14 min).mkv\n​\t在之前的课程中，我们谈到查准率和召回率，作为遇到偏斜类问题的评估度量值。在很多应用中，我们希望能够保证查准率和召回率的相对平衡。\n​\t在这节课中，我将告诉你应该怎么做，同时也向你展示一些查准率和召回率作为算法评估度量值的更有效的方式。继续沿用刚才预测肿瘤性质的例子。假使，我们的算法输出的结果在0-1 之间，我们使用阀值0.5 来预测真和假。\n\n​\t查准率**(Precision)=TP/(TP+FP)**\n例，在所有我们预测有恶性肿瘤的病人中，实际上有恶性肿瘤的病人的百分比，越高越好。\n​\t查全率**(Recall)=TP/(TP+FN)**例，在所有实际上有恶性肿瘤的病人中，成功预测有恶性肿瘤的病人的百分比，越高越好。\n​\t如果我们希望只在非常确信的情况下预测为真（肿瘤为恶性），即我们希望更高的查准率，我们可以使用比0.5更大的阀值，如0.7，0.9。这样做我们会减少错误预测病人为恶性肿瘤的情况，同时却会增加未能成功预测肿瘤为恶性的情况。\n​\t如果我们希望提高查全率，尽可能地让所有有可能是恶性肿瘤的病人都得到进一步地检查、诊断，我们可以使用比0.5更小的阀值，如0.3。\n​\t我们可以将不同阀值情况下，查全率与查准率的关系绘制成图表，曲线的形状根据数据的不同而不同：\n\n​\t我们希望有一个帮助我们选择这个阀值的方法。一种方法是计算F1 值（F1 Score），其计算公式为：\n{{F}_{1}}Score:2\\frac{PR}{P+R}\n我们选择使得F1值最高的阀值。\n11.5 机器学习的数据\n参考视频: 11 - 5 - Data For Machine Learning (11 min).mkv\n​\t在之前的视频中，我们讨论了评价指标。在这个视频中，我要稍微转换一下，讨论一下机器学习系统设计中另一个重要的方面，这往往涉及到用来训练的数据有多少。在之前的一些视频中，我曾告诫大家不要盲目地开始，而是花大量的时间来收集大量的数据，因为数据有时是唯一能实际起到作用的。但事实证明，在一定条件下，我会在这个视频里讲到这些条件是什么。得到大量的数据并在某种类型的学习算法中进行训练，可以是一种有效的方法来获得一个具有良好性能的学习算法。而这种情况往往出现在这些条件对于你的问题都成立。\n并且你能够得到大量数据的情况下。这可以是一个很好的方式来获得非常高性能的学习算法。因此，在这段视频中，让我们一起讨论一下这个问题。\n​\t很多很多年前，我认识的两位研究人员Michele Banko 和Eric Brill进行了一项有趣的研究，他们尝试通过机器学习算法来区分常见的易混淆的单词，他们尝试了许多种不同的算法，并发现数据量非常大时，这些不同类型的算法效果都很好。\n\n​\t比如，在这样的句子中：早餐我吃了__个鸡蛋(to,two,too)，在这个例子中，“早餐我吃了2个鸡蛋”，这是一个易混淆的单词的例子。于是他们把诸如这样的机器学习问题，当做一类监督学习问题，并尝试将其分类，什么样的词，在一个英文句子特定的位置，才是合适的。他们用了几种不同的学习算法，这些算法都是在他们2001年进行研究的时候，都已经被公认是比较领先的。因此他们使用了一个方差，用于逻辑回归上的一个方差，被称作”感知器”(perceptron)。他们也采取了一些过去常用，但是现在比较少用的算法，比如 Winnow算法，很类似于回归问题，但在一些方面又有所不同，过去用得比较多，但现在用得不太多。还有一种基于内存的学习算法，现在也用得比较少了，但是我稍后会讨论一点，而且他们用了一个朴素算法。这些具体算法的细节不那么重要，我们下面希望探讨，什么时候我们会希望获得更多数据，而非修改算法。他们所做的就是改变了训练数据集的大小，并尝试将这些学习算法用于不同大小的训练数据集中，这就是他们得到的结果。\n\n​\t这些趋势非常明显，首先大部分算法，都具有相似的性能，其次，随着训练数据集的增大，在横轴上代表以百万为单位的训练集大小，从0.1个百万到1000百万，也就是到了10亿规模的训练集的样本，这些算法的性能也都对应地增强了。\n​\t事实上，如果你选择任意一个算法，可能是选择了一个”劣等的”算法，如果你给这个劣等算法更多的数据，那么从这些例子中看起来的话，它看上去很有可能会其他算法更好，甚至会比”优等算法”更好。由于这项原始的研究非常具有影响力，因此已经有一系列许多不同的研究显示了类似的结果。这些结果表明，许多不同的学习算法有时倾向于表现出非常相似的表现，这还取决于一些细节，但是真正能提高性能的，是你能够给一个算法大量的训练数据。像这样的结果，引起了一种在机器学习中的普遍共识：“取得成功的人不是拥有最好算法的人，而是拥有最多数据的人”。\n​\t那么这种说法在什么时候是真，什么时候是假呢？因为如果我们有一个学习算法，并且如果这种说法是真的，那么得到大量的数据通常是保证我们具有一个高性能算法的最佳方式，而不是去争辩应该用什么样的算法。\n​\t假如有这样一些假设，在这些假设下有大量我们认为有用的训练集，我们假设在我们的机器学习问题中，特征值x包含了足够的信息，这些信息可以帮助我们用来准确地预测y，例如，如果我们采用了一些容易混淆的词，如：two、to、too，假如说它能够描述x，捕捉到需要填写的空白处周围的词语，那么特征捕捉到之后，我们就希望有对于“早饭我吃了__鸡蛋”，那么这就有大量的信息来告诉我中间我需要填的词是“两个”(two)，而不是单词 to 或too，因此特征捕捉，哪怕是周围词语中的一个词，就能够给我足够的信息来确定出标签 y是什么。换句话说，从这三组易混淆的词中，我应该选什么词来填空。\n​\t那么让我们来看一看，大量的数据是有帮助的情况。假设特征值有足够的信息来预测y值，假设我们使用一种需要大量参数的学习算法，比如有很多特征的逻辑回归或线性回归，或者用带有许多隐藏单元的神经网络，那又是另外一种带有很多参数的学习算法，这些都是非常强大的学习算法，它们有很多参数，这些参数可以拟合非常复杂的函数，因此我要调用这些，我将把这些算法想象成低偏差算法，因为我们能够拟合非常复杂的函数，而且因为我们有非常强大的学习算法，这些学习算法能够拟合非常复杂的函数。很有可能，如果我们用这些数据运行这些算法，这种算法能很好地拟合训练集，因此，训练误差就会很低了。\n​\t现在假设我们使用了非常非常大的训练集，在这种情况下，尽管我们希望有很多参数，但是如果训练集比参数的数量还大，甚至是更多，那么这些算法就不太可能会过度拟合。也就是说训练误差有希望接近测试误差。\n​\t另一种考虑这个问题的角度是为了有一个高性能的学习算法，我们希望它不要有高的偏差和方差。\n​\t因此偏差问题，我么将通过确保有一个具有很多参数的学习算法来解决，以便我们能够得到一个较低偏差的算法，并且通过用非常大的训练集来保证。\n\n​\t我们在此没有方差问题，我们的算法将没有方差，并且通过将这两个值放在一起，我们最终可以得到一个低误差和低方差的学习算法。这使得我们能够很好地测试测试数据集。从根本上来说，这是一个关键的假设：特征值有足够的信息量，且我们有一类很好的函数，这是为什么能保证低误差的关键所在。它有大量的训练数据集，这能保证得到更多的方差值，因此这给我们提出了一些可能的条件，如果你有大量的数据，而且你训练了一种带有很多参数的学习算法，那么这将会是一个很好的方式，来提供一个高性能的学习算法。\n​\t我觉得关键的测试：首先，一个人类专家看到了特征值 x，能很有信心的预测出y值吗？因为这可以证明  y 可以根据特征值x被准确地预测出来。其次，我们实际上能得到一组庞大的训练集，并且在这个训练集中训练一个有很多参数的学习算法吗？如果你不能做到这两者，那么更多时候，你会得到一个性能很好的学习算法。"},"机器学习笔记/9.机器学习":{"slug":"机器学习笔记/9.机器学习","filePath":"机器学习笔记/9.机器学习.md","title":"9.机器学习","links":[],"tags":["机器学习","SVM"],"content":"十二、支持向量机(Support Vector Machines)\n12.1 优化目标\n参考视频: 12 - 1 - Optimization Objective (15 min).mkv\n到目前为止,你已经见过一系列不同的学习算法。在监督学习中，许多学习算法的性能都非常类似，因此，重要的不是你该选择使用学习算法A还是学习算法B，而更重要的是，应用这些算法时，所创建的大量数据在应用这些算法时，表现情况通常依赖于你的水平。比如：你为学习算法所设计的特征量的选择，以及如何选择正则化参数，诸如此类的事。还有一个更加强大的算法广泛的应用于工业界和学术界，它被称为支持向量机(Support Vector Machine)。与逻辑回归和神经网络相比，支持向量机，或者简称SVM，在学习复杂的非线性方程时提供了一种更为清晰，更加强大的方式。因此，在接下来的视频中，我会探讨这一算法。在稍后的课程中，我也会对监督学习算法进行简要的总结。当然，仅仅是作简要描述。但对于支持向量机，鉴于该算法的强大和受欢迎度，在本课中，我会花许多时间来讲解它。它也是我们所介绍的最后一个监督学习算法。\n正如我们之前开发的学习算法，我们从优化目标开始。那么，我们开始学习这个算法。为了描述支持向量机，事实上，我将会从逻辑回归开始展示我们如何一点一点修改来得到本质上的支持向量机。\n\n那么，在逻辑回归中我们已经熟悉了这里的假设函数形式，和右边的S型激励函数。然而，为了解释一些数学知识.我将用z 表示\\theta^Tx。\n现在考虑下我们想要逻辑回归做什么：如果有一个 y=1的样本，我的意思是不管是在训练集中或是在测试集中，又或者在交叉验证集中，总之是 y=1，现在我们希望{{h}_{\\theta }}\\left( x \\right) 趋近1。因为我们想要正确地将此样本分类，这就意味着当 {{h}_{\\theta }}\\left( x \\right)趋近于1时，\\theta^Tx 应当远大于0，这里的&gt;&gt;意思是远远大于0。这是因为由于 z 表示 \\theta^Tx，当 z远大于0时，即到了该图的右边，你不难发现此时逻辑回归的输出将趋近于1。相反地，如果我们有另一个样本，即y=0。我们希望假设函数的输出值将趋近于0，这对应于\\theta^Tx，或者就是 z 会远小于0，因为对应的假设函数的输出值趋近0。\n\n如果你进一步观察逻辑回归的代价函数，你会发现每个样本 (x,y)都会为总代价函数，增加这里的一项，因此，对于总代价函数通常会有对所有的训练样本求和，并且这里还有一个1/m项，但是，在逻辑回归中，这里的这一项就是表示一个训练样本所对应的表达式。现在，如果我将完整定义的假设函数代入这里。那么，我们就会得到每一个训练样本都影响这一项。\n现在，先忽略 1/m 这一项，但是这一项是影响整个总代价函数中的这一项的。\n现在，一起来考虑两种情况：\n一种是y等于1的情况；另一种是 y 等于0的情况。\n在第一种情况中，假设 y=1 ，此时在目标函数中只需有第一项起作用，因为y=1时，(1-y)项将等于0。因此，当在 y=1 的样本中时，即在 (x, y) 中 ，我们得到 y=1 -\\log(1-\\frac{1}{1+e^{-z}})这样一项，这里同上一张幻灯片一致。\n我用 z 表示\\theta^Tx，即： z= \\theta^Tx。当然，在代价函数中，y 前面有负号。我们只是这样表示，如果 y=1 代价函数中，这一项也等于1。这样做是为了简化此处的表达式。如果画出关于z 的函数，你会看到左下角的这条曲线，我们同样可以看到，当z 增大时，也就是相当于\\theta^Tx增大时，z 对应的值会变的非常小。对整个代价函数而言，影响也非常小。这也就解释了，为什么逻辑回归在观察到正样本y=1时，试图将\\theta^Tx设置得非常大。因为，在代价函数中的这一项会变的非常小。\n现在开始建立支持向量机，我们从这里开始：\n我们会从这个代价函数开始，也就是-\\log(1-\\frac{1}{1+e^{-z}})一点一点修改，让我取这里的z=1 点，我先画出将要用的代价函数。\n\n新的代价函数将会水平的从这里到右边(图外)，然后我再画一条同逻辑回归非常相似的直线，但是，在这里是一条直线，也就是我用紫红色画的曲线，就是这条紫红色的曲线。那么，到了这里已经非常接近逻辑回归中使用的代价函数了。只是这里是由两条线段组成，即位于右边的水平部分和位于左边的直线部分，先别过多的考虑左边直线部分的斜率，这并不是很重要。但是，这里我们将使用的新的代价函数，是在y=1的前提下的。你也许能想到，这应该能做同逻辑回归中类似的事情，但事实上，在之后的优化问题中，这会变得更坚定，并且为支持向量机，带来计算上的优势。例如，更容易计算股票交易的问题等等。\n目前，我们只是讨论了y=1的情况，另外一种情况是当y=0时，此时如果你仔细观察代价函数只留下了第二项，因为第一项被消除了。如果当y=0时，那么这一项也就是0了。所以上述表达式只留下了第二项。因此，这个样本的代价或是代价函数的贡献。将会由这一项表示。并且，如果你将这一项作为z的函数，那么，这里就会得到横轴z。现在，你完成了支持向量机中的部分内容，同样地，我们要替代这一条蓝色的线，用相似的方法。\n\n如果我们用一个新的代价函数来代替，即这条从0点开始的水平直线，然后是一条斜线，像上图。那么，现在让我给这两个方程命名，左边的函数，我称之为{\\cos}t_1{(z)}，同时，右边函数我称它为{\\cos}t_0{(z)}。这里的下标是指在代价函数中，对应的 y=1 和 y=0 的情况，拥有了这些定义后，现在，我们就开始构建支持向量机。\n\n这是我们在逻辑回归中使用代价函数J(\\theta)。也许这个方程看起来不是非常熟悉。这是因为之前有个负号在方程外面，但是，这里我所做的是，将负号移到了表达式的里面，这样做使得方程看起来有些不同。对于支持向量机而言，实质上我们要将这替换为{\\cos}t_1{(z)}，也就是{\\cos}t_1{(\\theta^Tx)}，同样地，我也将这一项替换为{\\cos}t_0{(z)}，也就是代价{\\cos}t_0{(\\theta^Tx)}。这里的代价函数{\\cos}t_1，就是之前所提到的那条线。此外，代价函数{\\cos}t_0，也是上面所介绍过的那条线。因此，对于支持向量机，我们得到了这里的最小化问题，即:\n\n然后，再加上正则化参数。现在，按照支持向量机的惯例，事实上，我们的书写会稍微有些不同，代价函数的参数表示也会稍微有些不同。\n首先，我们要除去1/m这一项，当然，这仅仅是由于人们使用支持向量机时，对比于逻辑回归而言，不同的习惯所致，但这里我所说的意思是：你知道，我将要做的是仅仅除去1/m这一项，但是，这也会得出同样的 \\theta 最优值，好的，因为1/m 仅是个常量，因此，你知道在这个最小化问题中，无论前面是否有1/m 这一项，最终我所得到的最优值\\theta都是一样的。这里我的意思是，先给你举一个实例，假定有一最小化问题：即要求当(u-5)^2+1取得最小值时的u值，这时最小值为：当u=5时取得最小值。\n现在，如果我们想要将这个目标函数乘上常数10，这里我的最小化问题就变成了：求使得10×(u-5)^2+10最小的值u，然而，使得这里最小的u值仍为5。因此将一些常数乘以你的最小化项，这并不会改变最小化该方程时得到u值。因此，这里我所做的是删去常量m。也相同的，我将目标函数乘上一个常量m，并不会改变取得最小值时的\\theta值。\n第二点概念上的变化，我们只是指在使用支持向量机时，一些如下的标准惯例，而不是逻辑回归。因此，对于逻辑回归，在目标函数中，我们有两项：第一个是训练样本的代价，第二个是我们的正则化项，我们不得不去用这一项来平衡。这就相当于我们想要最小化A加上正则化参数\\lambda，然后乘以其他项B对吧？这里的A表示这里的第一项，同时我用B表示第二项，但不包括\\lambda，我们不是优化这里的A+\\lambda\\times B。我们所做的是通过设置不同正则参数\\lambda达到优化目的。这样，我们就能够权衡对应的项，是使得训练样本拟合的更好。即最小化A。还是保证正则参数足够小，也即是对于B项而言，但对于支持向量机，按照惯例，我们将使用一个不同的参数替换这里使用的\\lambda来权衡这两项。你知道，就是第一项和第二项我们依照惯例使用一个不同的参数称为C，同时改为优化目标，C×A+B因此，在逻辑回归中，如果给定\\lambda，一个非常大的值，意味着给予B更大的权重。而这里，就对应于将C 设定为非常小的值，那么，相应的将会给B比给A更大的权重。因此，这只是一种不同的方式来控制这种权衡或者一种不同的方法，即用参数来决定是更关心第一项的优化，还是更关心第二项的优化。当然你也可以把这里的参数C 考虑成1/\\lambda，同 1/\\lambda所扮演的角色相同，并且这两个方程或这两个表达式并不相同，因为C=1/\\lambda，但是也并不全是这样，如果当C=1/\\lambda时，这两个优化目标应当得到相同的值，相同的最优值 \\theta。因此，就用它们来代替。那么，我现在删掉这里的\\lambda，并且用常数C来代替。因此，这就得到了在支持向量机中我们的整个优化目标函数。然后最小化这个目标函数，得到SVM 学习到的参数C。\n\n最后有别于逻辑回归输出的概率。在这里，我们的代价函数，当最小化代价函数，获得参数\\theta时，支持向量机所做的是它来直接预测y的值等于1，还是等于0。因此，这个假设函数会预测1。当\\theta^Tx大于或者等于0时，或者等于0时，所以学习参数\\theta就是支持向量机假设函数的形式。那么，这就是支持向量机数学上的定义。\n在接下来的视频中，让我们再回去从直观的角度看看优化目标，实际上是在做什么，以及SVM的假设函数将会学习什么，同时也会谈谈如何做些许修改，学习更加复杂、非线性的函数。\n12.2 大边界的直观理解\n参考视频: 12 - 2 - Large Margin Intuition (11 min).mkv\n人们有时将支持向量机看作是大间距分类器。在这一部分，我将介绍其中的含义，这有助于我们直观理解SVM模型的假设是什么样的。\n\n这是我的支持向量机模型的代价函数，在左边这里我画出了关于z的代价函数{\\cos}t_1{(z)}，此函数用于正样本，而在右边这里我画出了关于z的代价函数{\\cos}t_0{(z)}，横轴表示z，现在让我们考虑一下，最小化这些代价函数的必要条件是什么。如果你有一个正样本，y=1，则只有在z&gt;=1时，代价函数{\\cos}t_1{(z)}才等于0。\n换句话说，如果你有一个正样本，我们会希望\\theta^Tx&gt;=1，反之，如果y=0，我们观察一下，函数{\\cos}t_0{(z)}，它只有在z&lt;=1的区间里函数值为0。这是支持向量机的一个有趣性质。事实上，如果你有一个正样本y=1，则其实我们仅仅要求\\theta^Tx大于等于0，就能将该样本恰当分出，这是因为如果\\theta^Tx&gt;0大的话，我们的模型代价函数值为0，类似地，如果你有一个负样本，则仅需要\\theta^Tx⇐0就会将负例正确分离，但是，支持向量机的要求更高，不仅仅要能正确分开输入的样本，即不仅仅要求\\theta^Tx&gt;0，我们需要的是比0值大很多，比如大于等于1，我也想这个比0小很多，比如我希望它小于等于-1，这就相当于在支持向量机中嵌入了一个额外的安全因子，或者说安全的间距因子。\n当然，逻辑回归做了类似的事情。但是让我们看一下，在支持向量机中，这个因子会导致什么结果。具体而言，我接下来会考虑一个特例。我们将这个常数C设置成一个非常大的值。比如我们假设C的值为100000或者其它非常大的数，然后来观察支持向量机会给出什么结果？\n\n如果 C非常大，则最小化代价函数的时候，我们将会很希望找到一个使第一项为0的最优解。因此，让我们尝试在代价项的第一项为0的情形下理解该优化问题。比如我们可以把C设置成了非常大的常数，这将给我们一些关于支持向量机模型的直观感受。\n​\t\t\t\t\\min_\\limits{\\theta}C\\sum_\\limits{i=1}^{m}\\left[y^{(i)}{\\cos}t_{1}\\left(\\theta^{T}x^{(i)}\\right)+\\left(1-y^{(i)}\\right){\\cos}t\\left(\\theta^{T}x^{(i)}\\right)\\right]+\\frac{1}{2}\\sum_\\limits{i=1}^{n}\\theta^{2}_{j}\n我们已经看到输入一个训练样本标签为y=1，你想令第一项为0，你需要做的是找到一个\\theta，使得\\theta^Tx&gt;=1，类似地，对于一个训练样本，标签为y=0，为了使{\\cos}t_0{(z)} 函数的值为0，我们需要\\theta^Tx&lt;=-1。因此，现在考虑我们的优化问题。选择参数，使得第一项等于0，就会导致下面的优化问题，因为我们将选择参数使第一项为0，因此这个函数的第一项为0，因此是C乘以0加上二分之一乘以第二项。这里第一项是C乘以0，因此可以将其删去，因为我知道它是0。\n这将遵从以下的约束：\\theta^Tx^{(i)}&gt;=1，如果 y^{(i)}是等于1 的，\\theta^Tx^{(i)}&lt;=-1，如果样本i是一个负样本，这样当你求解这个优化问题的时候，当你最小化这个关于变量\\theta的函数的时候，你会得到一个非常有趣的决策边界。\n\n具体而言，如果你考察这样一个数据集，其中有正样本，也有负样本，可以看到这个数据集是线性可分的。我的意思是，存在一条直线把正负样本分开。当然有多条不同的直线，可以把正样本和负样本完全分开。\n\n比如，这就是一个决策边界可以把正样本和负样本分开。但是多多少少这个看起来并不是非常自然是么?\n或者我们可以画一条更差的决策界，这是另一条决策边界，可以将正样本和负样本分开，但仅仅是勉强分开，这些决策边界看起来都不是特别好的选择，支持向量机将会选择这个黑色的决策边界，相较于之前我用粉色或者绿色画的决策界。这条黑色的看起来好得多，黑线看起来是更稳健的决策界。在分离正样本和负样本上它显得的更好。数学上来讲，这是什么意思呢？这条黑线有更大的距离，这个距离叫做间距(margin)。\n\n当画出这两条额外的蓝线，我们看到黑色的决策界和训练样本之间有更大的最短距离。然而粉线和蓝线离训练样本就非常近，在分离样本的时候就会比黑线表现差。因此，这个距离叫做支持向量机的间距，而这是支持向量机具有鲁棒性的原因，因为它努力用一个最大间距来分离样本。因此支持向量机有时被称为大间距分类器，而这其实是求解上一页幻灯片上优化问题的结果。\n我知道你也许想知道求解上一页幻灯片中的优化问题为什么会产生这个结果？它是如何产生这个大间距分类器的呢？我知道我还没有解释这一点。\n我将会从直观上略述为什么这个优化问题会产生大间距分类器。总之这个图示有助于你理解支持向量机模型的做法，即努力将正样本和负样本用最大的间距分开。\n\n在本节课中关于大间距分类器，我想讲最后一点：我们将这个大间距分类器中的正则化因子常数C设置的非常大，我记得我将其设置为了100000，因此对这样的一个数据集，也许我们将选择这样的决策界，从而最大间距地分离开正样本和负样本。那么在让代价函数最小化的过程中，我们希望找出在y=1和y=0两种情况下都使得代价函数中左边的这一项尽量为零的参数。如果我们找到了这样的参数，则我们的最小化问题便转变成：\n\n事实上，支持向量机现在要比这个大间距分类器所体现得更成熟，尤其是当你使用大间距分类器的时候，你的学习算法会受异常点(outlier) 的影响。比如我们加入一个额外的正样本。\n\n在这里，如果你加了这个样本，为了将样本用最大间距分开，也许我最终会得到一条类似这样的决策界，对么？就是这条粉色的线，仅仅基于一个异常值，仅仅基于一个样本，就将我的决策界从这条黑线变到这条粉线，这实在是不明智的。而如果正则化参数C，设置的非常大，这事实上正是支持向量机将会做的。它将决策界，从黑线变到了粉线，但是如果C 设置的小一点，**如果你将C设置的不要太大，则你最终会得到这条黑线，**当然数据如果不是线性可分的，如果你在这里有一些正样本或者你在这里有一些负样本，则支持向量机也会将它们恰当分开。因此，大间距分类器的描述，仅仅是从直观上给出了正则化参数C非常大的情形，同时，要提醒你C的作用类似于1/\\lambda，\\lambda是我们之前使用过的正则化参数。这只是C非常大的情形，或者等价地 \\lambda 非常小的情形。你最终会得到类似粉线这样的决策界，但是实际上应用支持向量机的时候，**当C不是非常非常大的时候，它可以忽略掉一些异常点的影响，得到更好的决策界。**甚至当你的数据不是线性可分的时候，支持向量机也可以给出好的结果。\n回顾 C=1/\\lambda，因此：\nC 较大时，相当于 \\lambda 较小，可能会导致过拟合，高方差。\nC 较小时，相当于\\lambda较大，可能会导致低拟合，高偏差。\n我们稍后会介绍支持向量机的偏差和方差，希望在那时候关于如何处理参数的这种平衡会变得更加清晰。我希望，这节课给出了一些关于为什么支持向量机被看做大间距分类器的直观理解。它用最大间距将样本区分开，尽管从技术上讲，这只有当参数C是非常大的时候是真的，但是它对于理解支持向量机是有益的。\n本节课中我们略去了一步，那就是我们在幻灯片中给出的优化问题。为什么会是这样的？它是如何得出大间距分类器的？我在本节中没有讲解，在下一节课中，我将略述这些问题背后的数学原理，来解释这个优化问题是如何得到一个大间距分类器的。\n12.3 数学背后的大边界分类（选修）\n参考视频: 12 - 3 - Mathematics Behind Large Margin Classification (Optional) (20 min).mkv\n在本节课中，我将介绍一些大间隔分类背后的数学原理。本节为选学部分，你完全可以跳过它，但是听听这节课可能让你对支持向量机中的优化问题，以及如何得到大间距分类器，产生更好的直观理解。\n\n首先，让我来给大家复习一下关于向量内积的知识。假设我有两个向量，u和v，我将它们写在这里。两个都是二维向量，我们看一下，u^T v的结果。u^T v也叫做向量u和v之间的内积。由于是二维向量，我可以将它们画在这个图上。我们说，这就是向量u即在横轴上，取值为某个{{u}_{1}}，而在纵轴上，高度是某个{{u}_{2}}作为u的第二个分量。现在，很容易计算的一个量就是向量u的范数。\\left\\| u \\right\\|表示u的范数，即u的长度，即向量u的欧几里得长度。根据毕达哥拉斯定理，\\left\\| u \\right\\|=\\sqrt{u_{1}^{2}+u_{2}^{2}}，这是向量u的长度，它是一个实数。现在你知道了这个的长度是多少了。我刚刚画的这个向量的长度就知道了。\n现在让我们回头来看向量v ，因为我们想计算内积。v是另一个向量，它的两个分量{{v}_{1}}和{{v}_{2}}是已知的。向量v可以画在这里，现在让我们来看看如何计算u和v之间的内积。这就是具体做法，我们将向量v投影到向量u上，我们做一个直角投影，或者说一个90度投影将其投影到u上，接下来我度量这条红线的长度。我称这条红线的长度为p，因此p就是长度，或者说是向量v投影到向量u上的量，我将它写下来，p是v投影到向量u上的长度，因此可以将{{u}^{T}}v=p\\centerdot \\left\\| u \\right\\|，或者说u的长度。这是计算内积的一种方法。如果你从几何上画出p的值，同时画出u的范数，你也会同样地计算出内积，答案是一样的。另一个计算公式是：u^T v就是\\left[ {{u}_{1}}\\text{ }{{u}_{2}} \\right] 这个一行两列的矩阵乘以v。因此可以得到{{u}_{1}}\\times {{v}_{1}}+{{u}_{2}}\\times {{v}_{2}}。根据线性代数的知识，这两个公式会给出同样的结果。顺便说一句，u^Tv=v^Tu。因此如果你将u和v交换位置，将u投影到v上，而不是将v投影到u上，然后做同样地计算，只是把u和v的位置交换一下，你事实上可以得到同样的结果。申明一点，在这个等式中u的范数是一个实数，p也是一个实数，因此u^T v就是两个实数正常相乘。\n\n最后一点，需要注意的就是p值，p事实上是有符号的，即它可能是正值，也可能是负值。我的意思是说，如果u是一个类似这样的向量，v是一个类似这样的向量，u和v之间的夹角大于90度，则如果将v投影到u上，会得到这样的一个投影，这是p的长度，在这个情形下我们仍然有{{u}^{T}}v是等于p乘以u的范数。唯一一点不同的是p在这里是负的。在内积计算中，如果u和v之间的夹角小于90度，那么那条红线的长度p是正值。然而如果这个夹角大于90度，则p将会是负的。就是这个小线段的长度是负的。如果它们之间的夹角大于90度，两个向量之间的内积也是负的。这就是关于向量内积的知识。我们接下来将会使用这些关于向量内积的性质试图来理解支持向量机中的目标函数。\n\n这就是我们先前给出的支持向量机模型中的目标函数。为了讲解方便，我做一点简化，仅仅是为了让目标函数更容易被分析。\n\n我接下来忽略掉截距，令{{\\theta }_{0}}=0，这样更容易画示意图。我将特征数n置为2，因此我们仅有两个特征{{x}_{1}},{{x}_{2}}，现在我们来看一下目标函数，支持向量机的优化目标函数。当我们仅有两个特征，即n=2时，这个式子可以写作：\\frac{1}{2}\\left({\\theta_1^2+\\theta_2^2}\\right)=\\frac{1}{2}\\left(\\sqrt{\\theta_1^2+\\theta_2^2}\\right)^2，我们只有两个参数{{\\theta }_{1}},{{\\theta }_{2}}。你可能注意到括号里面的这一项是向量\\theta的范数，或者说是向量\\theta的长度。我的意思是如果我们将向量\\theta写出来，那么我刚刚画红线的这一项就是向量\\theta的长度或范数。这里我们用的是之前学过的向量范数的定义，事实上这就等于向量\\theta的长度。\n当然你可以将其写作{{\\theta }_{0}}\\text{,}{{\\theta }_{1}},{{\\theta }_{2}}，如果{{\\theta }_{0}}=0，那就是{{\\theta }_{1}},{{\\theta }_{2}}的长度。在这里我将忽略{{\\theta }_{0}}，这样来写\\theta的范数，它仅仅和{{\\theta }_{1}},{{\\theta }_{2}}有关。但是，数学上不管你是否包含，其实并没有差别，因此在我们接下来的推导中去掉{{\\theta }_{0}}不会有影响这意味着我们的目标函数是等于\\frac{1}{2}\\left\\| \\theta \\right\\|^2。因此支持向量机做的全部事情，就是极小化参数向量\\theta范数的平方，或者说长度的平方。\n现在我将要看看这些项：\\theta^{T}x更深入地理解它们的含义。给定参数向量\\theta 给定一个样本x，这等于什么呢?在前一页幻灯片上，我们画出了在不同情形下，u^Tv的示意图，我们将会使用这些概念，\\theta 和x^{(i)}就类似于u和v 。\n\n让我们看一下示意图：们考察一个单一的训练样本，我有一个正样本在这里，用一个叉来表示这个样本x^{(i)}​，意思是在水平轴上取值为x_1^{(i)}​，在竖直轴上取值为x_2^{(i)}​。这就是我画出的训练样本。尽管我没有将其真的看做向量。它事实上就是一个始于原点，终点位置在这个训练样本点的向量。现在，我们有一个参数向量我会将它也画成向量。我将θ_1​画在横轴这里，将θ_2​ 画在纵轴这里，那么内积θ^T x^{(i)} 将会是什么呢？\n使用我们之前的方法，我们计算的方式就是我将训练样本投影到参数向量\\theta，然后我来看一看这个线段的长度，我将它画成红色。我将它称为p^{(i)}用来表示这是第 i个训练样本在参数向量\\theta上的投影。根据我们之前幻灯片的内容，我们知道的是θ^Tx^{(i)}将会等于p 乘以向量 θ 的长度或范数。这就等于\\theta_1\\cdot{x_1^{(i)}}+\\theta_2\\cdot{x_2^{(i)}}。这两种方式是等价的，都可以用来计算θ和x^{(i)}之间的内积。\n这告诉了我们什么呢？这里表达的意思是：这个θ^Tx^{(i)}&gt;=1  或者θ^Tx^{(i)}&lt;-1的,约束是可以被p^{(i)}\\cdot{x}&gt;=1这个约束所代替的。因为θ^Tx^{(i)}=p^{(i)}\\cdot{\\left\\| \\theta \\right\\|} ，将其写入我们的优化目标。我们将会得到没有了约束，θ^Tx^{(i)}而变成了p^{(i)}\\cdot{\\left\\| \\theta \\right\\|}。\n\n需要提醒一点，我们之前曾讲过这个优化目标函数可以被写成等于\\frac{1}{2}\\left\\| \\theta \\right\\|^2。\n现在让我们考虑下面这里的训练样本。现在，继续使用之前的简化，即{{\\theta }_{0}}=0，我们来看一下支持向量机会选择什么样的决策界。这是一种选择，我们假设支持向量机会选择这个决策边界。这不是一个非常好的选择，因为它的间距很小。这个决策界离训练样本的距离很近。我们来看一下为什么支持向量机不会选择它。\n对于这样选择的参数\\theta，可以看到参数向量\\theta事实上是和决策界是90度正交的，因此这个绿色的决策界对应着一个参数向量\\theta这个方向,顺便提一句{{\\theta }_{0}}=0的简化仅仅意味着决策界必须通过原点(0,0)。现在让我们看一下这对于优化目标函数意味着什么。\n\n比如这个样本，我们假设它是我的第一个样本x^{(1)}，如果我考察这个样本到参数\\theta的投影，投影是这个短的红线段，就等于p^{(1)}，它非常短。类似地，这个样本如果它恰好是x^{(2)}，我的第二个训练样本，则它到\\theta的投影在这里。我将它画成粉色，这个短的粉色线段是p^{(2)}，即第二个样本到我的参数向量\\theta的投影。因此，这个投影非常短。p^{(2)}事实上是一个负值，p^{(2)}是在相反的方向，这个向量和参数向量\\theta的夹角大于90度，p^{(2)}的值小于0。\n我们会发现这些p^{(i)}将会是非常小的数，因此当我们考察优化目标函数的时候，对于正样本而言，我们需要p^{(i)}\\cdot{\\left\\| \\theta \\right\\|}&gt;=1,但是如果 p^{(i)}在这里非常小,那就意味着我们需要\\theta的范数非常大.因为如果 p^{(1)} 很小,而我们希望p^{(1)}\\cdot{\\left\\| \\theta \\right\\|}&gt;=1,令其实现的唯一的办法就是这两个数较大。如果 p^{(1)} 小，我们就希望\\theta的范数大。类似地，对于负样本而言我们需要p^{(2)}\\cdot{\\left\\|\\theta \\right\\|}&lt;=-1。我们已经在这个样本中看到p^{(2)}会是一个非常小的数，因此唯一的办法就是\\theta的范数变大。但是我们的目标函数是希望找到一个参数\\theta，它的范数是小的。因此，这看起来不像是一个好的参数向量\\theta的选择。\n\n相反的，来看一个不同的决策边界。比如说，支持向量机选择了这个决策界，现在状况会有很大不同。如果这是决策界，这就是相对应的参数\\theta的方向，因此，在这个决策界之下，垂直线是决策界。使用线性代数的知识，可以说明，这个绿色的决策界有一个垂直于它的向量\\theta。现在如果你考察你的数据在横轴x上的投影，比如这个我之前提到的样本，我的样本x^{(1)}，当我将它投影到横轴x上，或说投影到\\theta上，就会得到这样p^{(1)}。它的长度是p^{(1)}，另一个样本，那个样本是x^{(2)}。我做同样的投影，我会发现，p^{(2)}的长度是负值。你会注意到现在p^{(1)} 和p^{(2)}这些投影长度是长多了。如果我们仍然要满足这些约束，P^{(i)}\\cdot{\\left\\| \\theta \\right\\|}&gt;1，则因为p^{(1)}变大了，\\theta的范数就可以变小了。因此这意味着通过选择右边的决策界，而不是左边的那个，支持向量机可以使参数\\theta的范数变小很多。因此，如果我们想令\\theta的范数变小，从而令\\theta范数的平方变小，就能让支持向量机选择右边的决策界。这就是支持向量机如何能有效地产生大间距分类的原因。\n看这条绿线，这个绿色的决策界。我们希望正样本和负样本投影到\\theta的值大。要做到这一点的唯一方式就是选择这条绿线做决策界。这是大间距决策界来区分开正样本和负样本这个间距的值。这个间距的值就是p^{(1)},p^{(2)},p^{(3)}等等的值。通过让间距变大，即通过这些p^{(1)},p^{(2)},p^{(3)}等等的值，支持向量机最终可以找到一个较小的\\theta范数。这正是支持向量机中最小化目标函数的目的。\n以上就是为什么支持向量机最终会找到大间距分类器的原因。因为它试图极大化这些p^{(i)}的范数，它们是训练样本到决策边界的距离。最后一点，我们的推导自始至终使用了这个简化假设，就是参数θ_0=0。\n\n就像我之前提到的。这个的作用是：θ_0=0的意思是我们让决策界通过原点。如果你令θ_0不是0的话，含义就是你希望决策界不通过原点。我将不会做全部的推导。实际上，支持向量机产生大间距分类器的结论，会被证明同样成立，证明方式是非常类似的，是我们刚刚做的证明的推广。\n之前视频中说过，即便θ_0不等于0，支持向量机要做的事情都是优化这个目标函数对应着C值非常大的情况，但是可以说明的是，即便θ_0不等于0，支持向量机仍然会找到正样本和负样本之间的大间距分隔。\n总之，我们解释了为什么支持向量机是一个大间距分类器。在下一节我们，将开始讨论如何利用支持向量机的原理，应用它们建立一个复杂的非线性分类器。\n12.4 核函数1\n参考视频: 12 - 4 - Kernels I (16 min).mkv\n回顾我们之前讨论过可以使用高级数的多项式模型来解决无法用直线进行分隔的分类问题：\n\n为了获得上图所示的判定边界，我们的模型可能是{{\\theta }_{0}}+{{\\theta }_{1}}{{x}_{1}}+{{\\theta }_{2}}{{x}_{2}}+{{\\theta }_{3}}{{x}_{1}}{{x}_{2}}+{{\\theta }_{4}}x_{1}^{2}+{{\\theta }_{5}}x_{2}^{2}+\\cdots 的形式。\n我们可以用一系列的新的特征f来替换模型中的每一项。例如令：\n{{f}_{1}}={{x}_{1}},{{f}_{2}}={{x}_{2}},{{f}_{3}}={{x}_{1}}{{x}_{2}},{{f}_{4}}=x_{1}^{2},{{f}_{5}}=x_{2}^{2}\n…得到h_θ(x)=f_1+f_2+...+f_n。然而，除了对原有的特征进行组合以外，有没有更好的方法来构造f_1,f_2,f_3？我们可以利用核函数来计算出新的特征。\n给定一个训练实例x，我们利用x的各个特征与我们预先选定的地标(landmarks)l^{(1)},l^{(2)},l^{(3)}的近似程度来选取新的特征f_1,f_2,f_3。\n\n例如：\n{{f}_{1}}=similarity(x,{{l}^{(1)}})=e(-\\frac{{{\\left\\| x-{{l}^{(1)}} \\right\\|}^{2}}}{2{{\\sigma }^{2}}})\n其中：{{\\left\\| x-{{l}^{(1)}} \\right\\|}^{2}}=\\sum{_{j=1}^{n}}{{({{x}_{j}}-l_{j}^{(1)})}^{2}}，为实例x中所有特征与地标l^{(1)}之间的距离的和。上例中的similarity(x,{{l}^{(1)}})就是核函数，具体而言，这里是一个高斯核函数(Gaussian Kernel)。 注：这个函数与正态分布没什么实际上的关系，只是看上去像而已。\n这些地标的作用是什么？\n如果一个训练实例x与地标L之间的距离近似于0，则新特征 f近似于e^{-0}=1，如果训练实例x与地标L之间距离较远，则f近似于e^{-(一个较大的数)}=0。\n假设我们的训练实例含有两个特征[x_{1} x{_2}]，给定地标l^{(1)}与不同的\\sigma值，见下图：\n\n图中水平面的坐标为 x_{1}，x_{2}而垂直坐标轴代表f。可以看出，只有当x与l^{(1)}重合时f才具有最大值。随着x的改变f值改变的速率受到\\sigma^2的控制。\n在下图中，当实例处于洋红色的点位置处，因为其离l^{(1)}更近，但是离l^{(2)}和l^{(3)}较远，因此f_1接近1，而f_2,f_3接近0。因此h_θ(x)=θ_0+θ_1f_1+θ_2f_2+θ_1f_3&gt;0，因此预测y=1。同理可以求出，对于离l^{(2)}较近的绿色点，也预测y=1，但是对于蓝绿色的点，因为其离三个地标都较远，预测y=0。\n\n这样，图中红色的封闭曲线所表示的范围，便是我们依据一个单一的训练实例和我们选取的地标所得出的判定边界，在预测时，我们采用的特征不是训练实例本身的特征，而是通过核函数计算出的新特征f_1,f_2,f_3。\n12.5 核函数2\n参考视频: 12 - 5 - Kernels II (16 min).mkv\n在上一节视频里，我们讨论了核函数这个想法，以及怎样利用它去实现支持向量机的一些新特性。在这一节视频中，我将补充一些缺失的细节，并简单的介绍一下怎么在实际中使用应用这些想法。\n如何选择地标？\n我们通常是根据训练集的数量选择地标的数量，即如果训练集中有m个实例，则我们选取m个地标，并且令:l^{(1)}=x^{(1)},l^{(2)}=x^{(2)},.....,l^{(m)}=x^{(m)}。这样做的好处在于：现在我们得到的新特征是建立在原有特征与训练集中所有其他特征之间距离的基础之上的，即：\n\n\n下面我们将核函数运用到支持向量机中，修改我们的支持向量机假设为：\n• 给定x，计算新特征f，当θ^Tf&gt;=0 时，预测 y=1，否则反之。\n相应地修改代价函数为：\\sum{_{j=1}^{n=m}}\\theta _{j}^{2}={{\\theta}^{T}}\\theta ，\nmin C\\sum\\limits_{i=1}^{m}{[{{y}^{(i)}}cos {{t}_{1}}}( {{\\theta }^{T}}{{f}^{(i)}})+(1-{{y}^{(i)}})cos {{t}_{0}}( {{\\theta }^{T}}{{f}^{(i)}})]+\\frac{1}{2}\\sum\\limits_{j=1}^{n=m}{\\theta _{j}^{2}}.\n在具体实施过程中，我们还需要对最后的正则化项进行些微调整，在计算\\sum{_{j=1}^{n=m}}\\theta _{j}^{2}={{\\theta}^{T}}\\theta 时，我们用θ^TMθ代替θ^Tθ，其中M是根据我们选择的核函数而不同的一个矩阵。这样做的原因是为了简化计算。\n理论上讲，我们也可以在逻辑回归中使用核函数，但是上面使用 M来简化计算的方法不适用与逻辑回归，因此计算将非常耗费时间。\n在此，我们不介绍最小化支持向量机的代价函数的方法，你可以使用现有的软件包（如liblinear,libsvm等）。在使用这些软件包最小化我们的代价函数之前，我们通常需要编写核函数，并且如果我们使用高斯核函数，那么在使用之前进行特征缩放是非常必要的。\n另外，支持向量机也可以不使用核函数，不使用核函数又称为线性核函数(linear kernel)，当我们不采用非常复杂的函数，或者我们的训练集特征非常多而实例非常少的时候，可以采用这种不带核函数的支持向量机。\n下面是支持向量机的两个参数C和\\sigma的影响：\nC=1/\\lambda\nC 较大时，相当于\\lambda较小，可能会导致过拟合，高方差；\nC 较小时，相当于λ较大，可能会导致低拟合，高偏差；\n\\sigma较大时，可能会导致低方差，高偏差；\n\\sigma较小时，可能会导致低偏差，高方差。\n如果你看了本周的编程作业，你就能亲自实现这些想法，并亲眼看到这些效果。这就是利用核函数的支持向量机算法，希望这些关于偏差和方差的讨论，能给你一些对于算法结果预期的直观印象。\n12.6 使用支持向量机\n参考视频: 12 - 6 - Using An SVM (21 min).mkv\n目前为止，我们已经讨论了SVM比较抽象的层面，在这个视频中我将要讨论到为了运行或者运用SVM。你实际上所需要的一些东西：支持向量机算法，提出了一个特别优化的问题。但是就如在之前的视频中我简单提到的，我真的不建议你自己写软件来求解参数\\theta，因此由于今天我们中的很少人，或者其实没有人考虑过自己写代码来转换矩阵，或求一个数的平方根等我们只是知道如何去调用库函数来实现这些功能。同样的，用以解决SVM最优化问题的软件很复杂，且已经有研究者做了很多年数值优化了。因此你提出好的软件库和好的软件包来做这样一些事儿。然后强烈建议使用高优化软件库中的一个，而不是尝试自己落实一些数据。有许多好的软件库，我正好用得最多的两个是liblinear和libsvm，但是真的有很多软件库可以用来做这件事儿。你可以连接许多你可能会用来编写学习算法的主要编程语言。\n在高斯核函数之外我们还有其他一些选择，如：\n多项式核函数（Polynomial Kernel）\n字符串核函数（String kernel）\n卡方核函数（ chi-square kernel）\n直方图交集核函数（histogram intersection kernel）\n等等…\n这些核函数的目标也都是根据训练集和地标之间的距离来构建新特征，这些核函数需要满足Mercer’s定理，才能被支持向量机的优化软件正确处理。\n多类分类问题\n假设我们利用之前介绍的一对多方法来解决一个多类分类问题。如果一共有k个类，则我们需要k个模型，以及k个参数向量\\theta 。我们同样也可以训练k个支持向量机来解决多类分类问题。但是大多数支持向量机软件包都有内置的多类分类功能，我们只要直接使用即可。\n尽管你不去写你自己的SVM的优化软件，但是你也需要做几件事：\n1、是提出参数C的选择。我们在之前的视频中讨论过误差/方差在这方面的性质。\n2、你也需要选择内核参数或你想要使用的相似函数，其中一个选择是：我们选择不需要任何内核参数，没有内核参数的理念，也叫线性核函数。因此，如果有人说他使用了线性核的SVM（支持向量机），这就意味这他使用了不带有核函数的SVM（支持向量机）。\n从逻辑回归模型，我们得到了支持向量机模型，在两者之间，我们应该如何选择呢？\n下面是一些普遍使用的准则：\nn为特征数，m为训练样本数。\n(1)如果相较于m而言，n要大许多，即训练集数据量不够支持我们训练一个复杂的非线性模型，我们选用逻辑回归模型或者不带核函数的支持向量机。\n(2)如果n较小，而且m大小中等，例如n在 1-1000 之间，而m在10-10000之间，使用高斯核函数的支持向量机。\n(3)如果n较小，而m较大，例如n在1-1000之间，而m大于50000，则使用支持向量机会非常慢，解决方案是创造、增加更多的特征，然后使用逻辑回归或不带核函数的支持向量机。\n值得一提的是，神经网络在以上三种情况下都可能会有较好的表现，但是训练神经网络可能非常慢，选择支持向量机的原因主要在于它的代价函数是凸函数，不存在局部最小值。\n今天的SVM包会工作得很好，但是它们仍然会有一些慢。当你有非常非常大的训练集，且用高斯核函数是在这种情况下，我经常会做的是尝试手动地创建，拥有更多的特征变量，然后用逻辑回归或者不带核函数的支持向量机。如果你看到这个幻灯片，看到了逻辑回归，或者不带核函数的支持向量机。在这个两个地方，我把它们放在一起是有原因的。原因是：逻辑回归和不带核函数的支持向量机它们都是非常相似的算法，不管是逻辑回归还是不带核函数的SVM，通常都会做相似的事情，并给出相似的结果。但是根据你实现的情况，其中一个可能会比另一个更加有效。但是在其中一个算法应用的地方，逻辑回归或不带核函数的SVM另一个也很有可能很有效。但是随着SVM的复杂度增加，当你使用不同的内核函数来学习复杂的非线性函数时，这个体系，你知道的，当你有多达1万（10,000）的样本时，也可能是5万（50,000），你的特征变量的数量这是相当大的。那是一个非常常见的体系，也许在这个体系里，不带核函数的支持向量机就会表现得相当突出。你可以做比这困难得多需要逻辑回归的事情。\n最后，神经网络使用于什么时候呢？ 对于所有的这些问题，对于所有的这些不同体系一个设计得很好的神经网络也很有可能会非常有效。有一个缺点是，或者说是有时可能不会使用神经网络的原因是：对于许多这样的问题，神经网络训练起来可能会特别慢，但是如果你有一个非常好的SVM实现包，它可能会运行得比较快比神经网络快很多，尽管我们在此之前没有展示，但是事实证明，SVM具有的优化问题，是一种凸优化问题。因此，好的SVM优化软件包总是会找到全局最小值，或者接近它的值。对于SVM你不需要担心局部最优。在实际应用中，局部最优不是神经网络所需要解决的一个重大问题，所以这是你在使用SVM的时候不需要太去担心的一个问题。根据你的问题，神经网络可能会比SVM慢，尤其是在这样一个体系中，至于这里给出的参考，看上去有些模糊，如果你在考虑一些问题，这些参考会有一些模糊，但是我仍然不能完全确定，我是该用这个算法还是改用那个算法，这个没有太大关系，当我遇到机器学习问题的时候，有时它确实不清楚这是否是最好的算法，但是就如在之前的视频中看到的算法确实很重要。但是通常更加重要的是：你有多少数据，你有多熟练是否擅长做误差分析和排除学习算法，指出如何设定新的特征变量和找出其他能决定你学习算法的变量等方面，通常这些方面会比你使用逻辑回归还是SVM这方面更加重要。但是，已经说过了，SVM仍然被广泛认为是一种最强大的学习算法，这是一个体系，包含了什么时候一个有效的方法去学习复杂的非线性函数。因此，实际上与逻辑回归、神经网络、SVM一起使用这些方法来提高学习算法，我认为你会很好地建立很有技术的状态。（编者注：当时GPU计算比较慢，神经网络还不流行。）\n机器学习系统对于一个宽泛的应用领域来说，这是另一个在你军械库里非常强大的工具，你可以把它应用到很多地方，如硅谷、在工业、学术等领域建立许多高性能的机器学习系统。"},"机器学习笔记/EmojiAnalysis":{"slug":"机器学习笔记/EmojiAnalysis","filePath":"机器学习笔记/EmojiAnalysis.md","title":"文本转Emoji","links":[],"tags":["深度学习","LSTM"],"content":"分析微博中的emoji\n# 数据分析库\nimport pandas as pd\nimport numpy as np\nimport random\n \n# 数据可视化\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.style.use(&#039;ggplot&#039;)\n \nimport emoji\n处理收集到到微博评论\nimport os\ndef all_path(dirname):\n    result = []#所有的文件\n    for maindir, subdir, file_name_list in os.walk(dirname):\n        for filename in file_name_list:\n            apath = os.path.join(maindir, filename)#合并成一个完整路径\n            result.append(apath)\n    return result\nfilenames = all_path(&#039;WeiboData&#039;)\nfilenames = filter(lambda x: &#039;WeiBoDataRet&#039; in x, filenames)\ndata_list = []\nfor filename in filenames:\n    tmp = pd.read_csv(filename, sep=&#039;|&#039;, encoding=&quot;utf-8&quot;, names=[&#039;text&#039;, &#039;emoji&#039;])\n    data_list.append(tmp)\ndata = pd.concat(data_list) # 把所有数据拼接起来\ndata.head()\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      text\n      emoji\n    \n  \n  \n    \n      0\n      heartbroken\n      伤心\n    \n    \n      1\n      走好\n      悲伤\n    \n    \n      2\n      走好\n      心\n    \n    \n      3\n      走好\n      心\n    \n    \n      4\n      走好\n      心\n    \n  \n\n\ndata.describe()\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      text\n      emoji\n    \n  \n  \n    \n      count\n      1674858\n      1673955\n    \n    \n      unique\n      220772\n      1780\n    \n    \n      top\n      \n      心\n    \n    \n      freq\n      228436\n      194223\n    \n  \n\n\n数据清洗\n去重\n去除 空格\n去除非emoji\n构建emoji字典\ndata = data.drop_duplicates()\ndata.text = data.text.apply(lambda x: x.strip())\ndata.emoji = data.emoji.apply(lambda x: str(x).strip())\nimport json\ndef GetEmojiDict():\n    fr = open(&#039;ImgName.txt&#039;, &#039;r&#039;)\n    jsondata = fr.read()\n    return json.loads(jsondata)\nemojiDict = GetEmojiDict()\nlen(emojiDict)\n149\n\n去除非中文text\nimport re\nzhPattern=re.compile(u&#039;[\\u4e00-\\u9fa5]+&#039;)\ndef FilterEmoji(contents):\n    if contents in emojiDict:\n        return True\n    else:\n        if zhPattern.search(contents):\n            return False\n    return True\ndata.info()\n&lt;class &#039;pandas.core.frame.DataFrame&#039;&gt;\nInt64Index: 287976 entries, 0 to 703321\nData columns (total 2 columns):\n #   Column  Non-Null Count   Dtype \n---  ------  --------------   ----- \n 0   text    287976 non-null  object\n 1   emoji   287976 non-null  object\ndtypes: object(2)\nmemory usage: 6.6+ MB\n\nfilterReg = data.emoji.apply(lambda x: FilterEmoji(x))\nfilterIndex = filterReg[filterReg==0].index\nfilterIndex\nInt64Index([    61,    374,    653,    816,   1428,   1503,   1772,   2093,\n              2382,   2697,\n            ...\n            701750, 701882, 701894, 702185, 702186, 702380, 702501, 703077,\n            703095, 703096],\n           dtype=&#039;int64&#039;, length=4556)\n\ndata = data.drop(filterIndex)\nfilterReg = data.text.apply(lambda x: FilterEmoji(x))\nfilterIndex = filterReg[filterReg==1].index\nfilterIndex\nInt64Index([     0,     47,     56,     65,    144,    152,    159,    168,\n               174,    175,\n            ...\n            700205, 700206, 700207, 700266, 700352, 702191, 702192, 702193,\n            702194, 702195],\n           dtype=&#039;int64&#039;, length=3475)\n\nfilterIndex\nInt64Index([     0,     47,     56,     65,    144,    152,    159,    168,\n               174,    175,\n            ...\n            700205, 700206, 700207, 700266, 700352, 702191, 702192, 702193,\n            702194, 702195],\n           dtype=&#039;int64&#039;, length=3475)\n\ndata = data.drop(filterIndex)\ndata.describe()\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      text\n      emoji\n    \n  \n  \n    \n      count\n      277826\n      277826\n    \n    \n      unique\n      215349\n      1339\n    \n    \n      top\n      [加油]\n      允悲\n    \n    \n      freq\n      72\n      22486\n    \n  \n\n\nemoji_counts = data.emoji.value_counts()\nemoji_counts.describe()\ncount     1339.000000\nmean       207.487677\nstd       1196.566008\nmin          1.000000\n25%          3.000000\n50%          9.000000\n75%         41.000000\nmax      22486.000000\nName: emoji, dtype: float64\n\ndata\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      text\n      emoji\n    \n  \n  \n    \n      1\n      走好\n      悲伤\n    \n    \n      2\n      走好\n      心\n    \n    \n      5\n      走好\n      蜡烛\n    \n    \n      6\n      舒服了，着尼哥终于死了\n      笑哈哈\n    \n    \n      7\n      舒服了，着尼哥终于死了\n      给力\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      703299\n      期待蜜桃第二季，期待邓伦\n      :peach:\n    \n    \n      703303\n      期待伦伦\n      心\n    \n    \n      703315\n      川西真的是随便一个地方都是风景\n      鼓掌\n    \n    \n      703320\n      お疲れ様でした\n      跪了\n    \n    \n      703321\n      太快了！！！\n      赞\n    \n  \n\n277826 rows × 2 columns\n\n删除重复的text\ndupdata = data.text.duplicated()\ndupIndex = dupdata[dupdata==1].index\nlen(dupIndex)\n62477\n\ndata = data.drop(dupIndex)\ndata\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      text\n      emoji\n    \n  \n  \n    \n      1\n      走好\n      悲伤\n    \n    \n      6\n      舒服了，着尼哥终于死了\n      笑哈哈\n    \n    \n      9\n      老人家值得所有人尊重\n      赞\n    \n    \n      12\n      老百姓真好\n      心\n    \n    \n      14\n      虽然行为点赞，但是还是衷心希望老人，把自己过好了，有能力，再去帮助别人。\n      摊手\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      703298\n      期待蜜桃第二季，期待邓伦\n      心\n    \n    \n      703303\n      期待伦伦\n      心\n    \n    \n      703315\n      川西真的是随便一个地方都是风景\n      鼓掌\n    \n    \n      703320\n      お疲れ様でした\n      跪了\n    \n    \n      703321\n      太快了！！！\n      赞\n    \n  \n\n203185 rows × 2 columns\n\n单词向量\n下面这段code的功能是解析词组对应的向量的三个字典\nword_to_index 单词查索引\nindex_to_word 索引查单词\nword_to_vec_map 单词查向量\n\ndef read_word_vecs(file):\n    with open(file, &#039;r&#039;, encoding=&quot;utf-8&quot;) as f:\n        words = set()\n        word_to_vec_map = {}\n        for line in f:\n            line = line.strip().split()\n            curr_word = line[0]\n            words.add(curr_word)\n            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n        \n        i = 1\n        words_to_index = {}\n        index_to_words = {}\n        for w in sorted(words):\n            words_to_index[w] = i\n            index_to_words[i] = w\n            i = i + 1\n    return words_to_index, index_to_words, word_to_vec_map\nword_to_index, index_to_word, word_to_vec_map = read_word_vecs(&#039;sgns.weibo.word&#039;)\nword_to_vec_map[&#039;他们&#039;]\narray([-0.058985, -0.095981,  0.213523,  0.078806,  0.090758,  0.303698,\n       -0.080819, -0.070496,  0.100697, -0.014494,  0.105789, -0.081538,\n       -0.220132, -0.001089, -0.010554,  0.045872,  0.020978, -0.078958,\n        0.310522,  0.026538,  0.116843, -0.012077,  0.091833,  0.199016,\n       -0.253477,  0.105833, -0.079761, -0.114635,  0.437327,  0.003209,\n       -0.191938, -0.292937, -0.042434, -0.092598, -0.031424,  0.232141,\n        0.175102, -0.028322, -0.182089, -0.127304, -0.105405, -0.0155  ,\n       -0.105409,  0.128716,  0.271304, -0.258079,  0.294854, -0.225564,\n        0.041693,  0.122313, -0.10642 ,  0.218218, -0.061122,  0.032375,\n        0.061754,  0.060876,  0.177719,  0.080874,  0.040064,  0.028098,\n        0.181363, -0.073601, -0.009067, -0.031534,  0.190581,  0.175827,\n       -0.003394, -0.120093,  0.136633,  0.22353 , -0.286703, -0.083716,\n        0.07307 ,  0.290753, -0.073568, -0.146416,  0.287048,  0.177982,\n        0.159483,  0.033554, -0.113645,  0.086506,  0.182751,  0.222543,\n        0.069108, -0.005411, -0.117244,  0.278492,  0.292221, -0.277547,\n        0.035062,  0.05546 ,  0.043035,  0.118464, -0.03085 ,  0.163017,\n        0.032309,  0.238069,  0.164545, -0.162392, -0.093865,  0.358772,\n       -0.138829, -0.27499 , -0.190523, -0.198303, -0.228555,  0.02823 ,\n        0.12706 , -0.017478,  0.279601, -0.130354,  0.376413,  0.107592,\n        0.501358, -0.392651,  0.167826,  0.030806, -0.047537,  0.0542  ,\n       -0.027822, -0.177908,  0.436953, -0.139909, -0.205398, -0.069401,\n        0.210465, -0.09408 , -0.030155, -0.186514, -0.408763,  0.209337,\n        0.154496, -0.155053, -0.073264, -0.208221,  0.031705,  0.007868,\n        0.105028, -0.313043, -0.030095,  0.32314 ,  0.039472,  0.056924,\n       -0.029449, -0.18332 ,  0.329696, -0.20353 ,  0.079724,  0.005614,\n        0.033271,  0.129164,  0.06442 , -0.093268, -0.26306 ,  0.042632,\n       -0.066531, -0.25593 , -0.082908, -0.211791,  0.269096, -0.231714,\n        0.498682,  0.171995, -0.188561, -0.254678,  0.127424,  0.490121,\n       -0.002619, -0.270687, -0.062654, -0.009806,  0.068663, -0.131597,\n        0.157276, -0.118741,  0.362313, -0.107524, -0.043709,  0.051271,\n        0.016886, -0.303519, -0.131623, -0.103483,  0.090379,  0.071147,\n        0.132338, -0.146149, -0.366627, -0.351044, -0.063839,  0.082302,\n        0.385776,  0.158985,  0.224325, -0.116336, -0.247472, -0.500043,\n       -0.054399, -0.51975 , -0.165844,  0.067776, -0.311503,  0.160354,\n        0.310949, -0.158256, -0.13147 , -0.046553, -0.132425, -0.174187,\n        0.137154,  0.128941,  0.077095,  0.086764, -0.085013, -0.076975,\n        0.116672, -0.234487, -0.029225, -0.297913,  0.03733 ,  0.07142 ,\n       -0.333047,  0.250342,  0.071834, -0.360994,  0.160254, -0.085961,\n       -0.244442, -0.00217 ,  0.016221, -0.25117 ,  0.102826, -0.190794,\n       -0.163422,  0.067348, -0.066799, -0.105879,  0.281125, -0.092643,\n        0.014463, -0.040031, -0.047755, -0.192767,  0.166827, -0.210013,\n       -0.126185,  0.228651,  0.28803 ,  0.045921,  0.15332 ,  0.014357,\n       -0.149424, -0.235598, -0.137925, -0.333645,  0.114881,  0.25207 ,\n        0.046461,  0.00136 ,  0.089115, -0.182189, -0.200544,  0.175124,\n        0.069565, -0.055904,  0.05993 ,  0.067038,  0.119123,  0.143849,\n       -0.182774,  0.354611, -0.137333,  0.157642,  0.028673, -0.504065,\n       -0.006483, -0.056175,  0.131101, -0.106961, -0.07638 ,  0.294719,\n        0.003378,  0.096714, -0.157428, -0.032374, -0.244506,  0.012603,\n        0.202828,  0.080087,  0.06369 , -0.315489, -0.087886,  0.172018,\n       -0.135227, -0.168902,  0.25539 , -0.265512, -0.209118,  0.003291])\n\n文本分词\n使用jieba对text分词\nimport jieba\nwords = data.text.apply(lambda x: list(jieba.cut(x)))\nBuilding prefix dict from the default dictionary ...\nLoading model from cache /tmp/jieba.cache\nLoading model cost 0.805 seconds.\nPrefix dict has been built successfully.\n\nwords\n1                                                    [走, 好]\n6                                 [舒服, 了, ，, 着尼哥, 终于, 死, 了]\n9                                        [老人家, 值得, 所有人, 尊重]\n12                                              [老百姓, 真, 好]\n14        [虽然, 行为, 点赞, ，, 但是, 还是, 衷心希望, 老人, ，, 把, 自己, 过,...\n                                ...                        \n703298                             [期待, 蜜桃, 第二季, ，, 期待, 邓伦]\n703303                                             [期待, 伦伦]\n703315                    [川西, 真的, 是, 随便, 一个, 地方, 都, 是, 风景]\n703320                                [お, 疲, れ, 様, で, し, た]\n703321                                     [太快, 了, ！, ！, ！]\nName: text, Length: 203185, dtype: object\n\ndata[&#039;words&#039;] = words\n&lt;ipython-input-41-21ad536f0372&gt;:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  data[&#039;words&#039;] = words\n\ndata\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      text\n      emoji\n      words\n    \n  \n  \n    \n      1\n      走好\n      悲伤\n      [走, 好]\n    \n    \n      6\n      舒服了，着尼哥终于死了\n      笑哈哈\n      [舒服, 了, ，, 着尼哥, 终于, 死, 了]\n    \n    \n      9\n      老人家值得所有人尊重\n      赞\n      [老人家, 值得, 所有人, 尊重]\n    \n    \n      12\n      老百姓真好\n      心\n      [老百姓, 真, 好]\n    \n    \n      14\n      虽然行为点赞，但是还是衷心希望老人，把自己过好了，有能力，再去帮助别人。\n      摊手\n      [虽然, 行为, 点赞, ，, 但是, 还是, 衷心希望, 老人, ，, 把, 自己, 过,...\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      703298\n      期待蜜桃第二季，期待邓伦\n      心\n      [期待, 蜜桃, 第二季, ，, 期待, 邓伦]\n    \n    \n      703303\n      期待伦伦\n      心\n      [期待, 伦伦]\n    \n    \n      703315\n      川西真的是随便一个地方都是风景\n      鼓掌\n      [川西, 真的, 是, 随便, 一个, 地方, 都, 是, 风景]\n    \n    \n      703320\n      お疲れ様でした\n      跪了\n      [お, 疲, れ, 様, で, し, た]\n    \n    \n      703321\n      太快了！！！\n      赞\n      [太快, 了, ！, ！, ！]\n    \n  \n\n203185 rows × 3 columns\n\ndef ret_words_vector(words):\n    vector = np.zeros(300)\n    n = 0\n    for word in words:\n        v = word_to_vec_map.get(word, None)\n        if type(v) != type(None):\n            n += 1\n            vector += v\n    if not vector.all():\n        return None\n    return vector/n\nvectors = data.words.apply(ret_words_vector)\nvectors\n1        [-0.047105999999999995, 0.23246850000000002, 0...\n6        [-0.027774333333333328, -0.114836, 0.062758, 0...\n9        [-0.05153150000000001, 0.103688, 0.23323525, 0...\n12       [-0.16598533333333335, 0.16545333333333334, 0....\n13       [-0.045179, 0.132102, 0.237468, 0.255355, -0.0...\n                               ...                        \n53656    [-0.06892586111111111, -0.040252222222222224, ...\n53666    [-0.124204, -0.0818, -0.02715266666666667, 0.0...\n53669    [0.049611333333333334, 0.011862, 0.10510733333...\n53683    [-0.01563454166666667, -0.008308624999999997, ...\n53685    [-0.363667, -0.25396850000000004, 0.011313, -0...\nName: words, Length: 6638, dtype: object\n\ndata[&#039;vectors&#039;] = vectors\ndata\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      text\n      emoji\n      words\n      vectors\n    \n  \n  \n    \n      1\n      走好\n      悲伤\n      [走, 好]\n      [-0.047105999999999995, 0.23246850000000002, 0...\n    \n    \n      6\n      舒服了，着尼哥终于死了\n      笑哈哈\n      [舒服, 了, ，, 着尼哥, 终于, 死, 了]\n      [-0.027774333333333328, -0.114836, 0.062758, 0...\n    \n    \n      9\n      老人家值得所有人尊重\n      赞\n      [老人家, 值得, 所有人, 尊重]\n      [-0.05153150000000001, 0.103688, 0.23323525, 0...\n    \n    \n      12\n      老百姓真好\n      心\n      [老百姓, 真, 好]\n      [-0.16598533333333335, 0.16545333333333334, 0....\n    \n    \n      13\n      好人\n      赞\n      [好人]\n      [-0.045179, 0.132102, 0.237468, 0.255355, -0.0...\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      53656\n      还有我们sf9的李达渊他让我们有钱的话先孝敬父母然后有多余的钱再买专辑如果还有多余的钱再来看...\n      泪\n      [还有, 我们, sf9, 的, 李达渊, 他, 让, 我们, 有钱, 的话, 先, 孝敬父...\n      [-0.06892586111111111, -0.040252222222222224, ...\n    \n    \n      53666\n      全球的挑战\n      允悲\n      [全球, 的, 挑战]\n      [-0.124204, -0.0818, -0.02715266666666667, 0.0...\n    \n    \n      53669\n      [加油]\n      鲜花\n      [[, 加油, ]]\n      [0.049611333333333334, 0.011862, 0.10510733333...\n    \n    \n      53683\n      假设不离婚！各过各的！她的业务不再给他凭他的能力早晚被辞退然后没了生计！房子爱住你就住没人管...\n      doge\n      [假设, 不, 离婚, ！, 各过, 各, 的, ！, 她, 的, 业务, 不再, 给, 他...\n      [-0.01563454166666667, -0.008308624999999997, ...\n    \n    \n      53685\n      我不配\n      伤心\n      [我, 不配]\n      [-0.363667, -0.25396850000000004, 0.011313, -0...\n    \n  \n\n6638 rows × 4 columns\n\ndata = data.dropna()\ndata\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      text\n      emoji\n      words\n      vectors\n    \n  \n  \n    \n      1\n      走好\n      悲伤\n      [走, 好]\n      [-0.047105999999999995, 0.23246850000000002, 0...\n    \n    \n      6\n      舒服了，着尼哥终于死了\n      笑哈哈\n      [舒服, 了, ，, 着尼哥, 终于, 死, 了]\n      [-0.027774333333333328, -0.114836, 0.062758, 0...\n    \n    \n      9\n      老人家值得所有人尊重\n      赞\n      [老人家, 值得, 所有人, 尊重]\n      [-0.05153150000000001, 0.103688, 0.23323525, 0...\n    \n    \n      12\n      老百姓真好\n      心\n      [老百姓, 真, 好]\n      [-0.16598533333333335, 0.16545333333333334, 0....\n    \n    \n      13\n      好人\n      赞\n      [好人]\n      [-0.045179, 0.132102, 0.237468, 0.255355, -0.0...\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      53656\n      还有我们sf9的李达渊他让我们有钱的话先孝敬父母然后有多余的钱再买专辑如果还有多余的钱再来看...\n      泪\n      [还有, 我们, sf9, 的, 李达渊, 他, 让, 我们, 有钱, 的话, 先, 孝敬父...\n      [-0.06892586111111111, -0.040252222222222224, ...\n    \n    \n      53666\n      全球的挑战\n      允悲\n      [全球, 的, 挑战]\n      [-0.124204, -0.0818, -0.02715266666666667, 0.0...\n    \n    \n      53669\n      [加油]\n      鲜花\n      [[, 加油, ]]\n      [0.049611333333333334, 0.011862, 0.10510733333...\n    \n    \n      53683\n      假设不离婚！各过各的！她的业务不再给他凭他的能力早晚被辞退然后没了生计！房子爱住你就住没人管...\n      doge\n      [假设, 不, 离婚, ！, 各过, 各, 的, ！, 她, 的, 业务, 不再, 给, 他...\n      [-0.01563454166666667, -0.008308624999999997, ...\n    \n    \n      53685\n      我不配\n      伤心\n      [我, 不配]\n      [-0.363667, -0.25396850000000004, 0.011313, -0...\n    \n  \n\n6580 rows × 4 columns\n\nemoji映射为数字\nfrom sklearn.preprocessing import LabelEncoder\n \nclass NpEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, np.integer):\n            return int(obj)\n        elif isinstance(obj, np.floating):\n            return float(obj)\n        elif isinstance(obj, np.ndarray):\n            return obj.tolist()\n        else:\n            return super(NpEncoder, self).default(obj)\n \nclass EmojiMap:\n    def generate(emoji_data):\n        labelencoder = LabelEncoder()\n        emoji_set = list(set(data.emoji))\n        x = labelencoder.fit_transform(emoji_set)\n        emoji_to_index = dict(zip(emoji_set, x))\n        index_to_emoji = dict(zip(x, emoji_set))\n        return emoji_to_index, index_to_emoji\n    \n    def save(emoji_dictionary):\n        fw = open(&#039;emoji_dictionary.json&#039;, &#039;w&#039;)\n        data = json.dumps(emoji_dictionary, cls=NpEncoder)\n        fw.write(data)\n        fw.close()\n    \n    def read(filename):\n        fr = open(&#039;emoji_dictionary.json&#039;, &#039;r&#039;)\n        data = fr.read()\n        emoji_dictionary = json.loads(data)\n        fr.close()\n        return emoji_dictionary\nemoji_to_index, index_to_emoji = EmojiMap.generate(data.emoji)\nlen(emoji_to_index)\n1043\n\n保存emoji_dictionary\n#EmojiMap.save(emoji_to_index)\nemoji_vector = data.emoji.apply(lambda x: emoji_to_index[x])\ndata[&#039;emoji_vector&#039;] = emoji_vector\n&lt;ipython-input-52-2ca0af0b144e&gt;:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  data[&#039;emoji_vector&#039;] = emoji_vector\n\ndata\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      text\n      emoji\n      words\n      emoji_vector\n    \n  \n  \n    \n      1\n      走好\n      悲伤\n      [走, 好]\n      967\n    \n    \n      6\n      舒服了，着尼哥终于死了\n      笑哈哈\n      [舒服, 了, ，, 着尼哥, 终于, 死, 了]\n      1005\n    \n    \n      9\n      老人家值得所有人尊重\n      赞\n      [老人家, 值得, 所有人, 尊重]\n      1023\n    \n    \n      12\n      老百姓真好\n      心\n      [老百姓, 真, 好]\n      963\n    \n    \n      14\n      虽然行为点赞，但是还是衷心希望老人，把自己过好了，有能力，再去帮助别人。\n      摊手\n      [虽然, 行为, 点赞, ，, 但是, 还是, 衷心希望, 老人, ，, 把, 自己, 过,...\n      977\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      703298\n      期待蜜桃第二季，期待邓伦\n      心\n      [期待, 蜜桃, 第二季, ，, 期待, 邓伦]\n      963\n    \n    \n      703303\n      期待伦伦\n      心\n      [期待, 伦伦]\n      963\n    \n    \n      703315\n      川西真的是随便一个地方都是风景\n      鼓掌\n      [川西, 真的, 是, 随便, 一个, 地方, 都, 是, 风景]\n      1041\n    \n    \n      703320\n      お疲れ様でした\n      跪了\n      [お, 疲, れ, 様, で, し, た]\n      1025\n    \n    \n      703321\n      太快了！！！\n      赞\n      [太快, 了, ！, ！, ！]\n      1023\n    \n  \n\n203185 rows × 4 columns\n\n文本转向量与预测\ndef text_to_vector(txt):\n    words = jieba.cut(txt)\n    return ret_words_vector(words)\n \ndef predict_emoji(txt, alg):\n    X_test = text_to_vector(txt)\n    X_test = np.array([X_test])\n    Y_pred = alg.predict(X_test)\n    Y_pred = int(Y_pred)\n    return index_to_emoji[Y_pred]\n构建训练集\n# shuffle数据\ndata = data.sample(frac=1)\ndata.info()\n&lt;class &#039;pandas.core.frame.DataFrame&#039;&gt;\nInt64Index: 6580 entries, 30383 to 19525\nData columns (total 5 columns):\n #   Column        Non-Null Count  Dtype \n---  ------        --------------  ----- \n 0   text          6580 non-null   object\n 1   emoji         6580 non-null   object\n 2   words         6580 non-null   object\n 3   vectors       6580 non-null   object\n 4   emoji_vector  6580 non-null   int64 \ndtypes: int64(1), object(4)\nmemory usage: 308.4+ KB\n\nX_train = data.vectors.iloc[0: ]\nY_train = np.array(data.emoji_vector.iloc[0: ])\nX_train.shape, Y_train.shape\n((6580,), (6580,))\n\na = []\nfor i in X_train:\n    a.append(i)\na = np.array(a)\nX_train = a\na = []\nfor i in Y_train:\n    a.append([i])\na = np.array(a)\nY_train = a\nSVM\nfrom sklearn.svm import SVC, LinearSVC\nsvm = SVC(C=100, gamma=10, probability=True)\nsvm.fit(X_train, Y_train)\npredict_emoji(&#039;虽然行为点赞，但是还是衷心希望老人，把自己过好了，有能力，再去帮助别人。&#039;, svm)\n&#039;心&#039;\n\n随机森林\n# Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\nrandom_forest = RandomForestClassifier(max_depth=50)\nrandom_forest.fit(X_train, Y_train)\nrandom_forest.score(X_train, Y_train)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nacc_random_forest\n99.47\n\npredict_emoji(&#039;你是不是傻&#039;, random_forest)\n&#039;doge&#039;\n\n保存模型\nfrom sklearn.externals import joblib\n \njoblib.dump(random_forest, &#039;random_forest.model&#039;)\nsvm2 = joblib.load(&#039;random_forest.model&#039;)\n使用keras搭建STML模型\n构建Feature和Labe\ndata\n\n\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n.dataframe tbody tr th {\n    vertical-align: top;\n}\n\n.dataframe thead th {\n    text-align: right;\n}\n\n\n\n  \n    \n      \n      text\n      emoji\n    \n  \n  \n    \n      1\n      走好\n      悲伤\n    \n    \n      6\n      舒服了，着尼哥终于死了\n      笑哈哈\n    \n    \n      9\n      老人家值得所有人尊重\n      赞\n    \n    \n      12\n      老百姓真好\n      心\n    \n    \n      14\n      虽然行为点赞，但是还是衷心希望老人，把自己过好了，有能力，再去帮助别人。\n      摊手\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      703298\n      期待蜜桃第二季，期待邓伦\n      心\n    \n    \n      703303\n      期待伦伦\n      心\n    \n    \n      703315\n      川西真的是随便一个地方都是风景\n      鼓掌\n    \n    \n      703320\n      お疲れ様でした\n      跪了\n    \n    \n      703321\n      太快了！！！\n      赞\n    \n  \n\n203185 rows × 2 columns\n\ndf = data\nmax_len = df.words.map(lambda x: len(x)).max()\nmax_len\n215\n\n将一组句子(字符串)转换为与句子中的单词对应的索引数组。输出形状应该是这样的，它可以提供给’ Embedding() ’\ndef sentences_to_indices(X, word_to_index, max_len):\n    &quot;&quot;&quot;\n    Converts an array of sentences (strings) into an array of indices corresponding to words in the sentences.\n    The output shape should be such that it can be given to `Embedding()`\n    \n    Arguments:\n    X -- array of sentences (strings), of shape (m, 1)\n    word_to_index -- a dictionary containing the each word mapped to its index\n    max_len -- maximum number of words in a sentence. You can assume every sentence in X is no longer than this. \n    \n    Returns:\n    X_indices -- array of indices corresponding to words in the sentences from X, of shape (m, max_len)\n    &quot;&quot;&quot;\n    \n    m = X.shape[0]                                   # number of training examples\n \n    # Initialize X_indices as a numpy matrix of zeros and the correct shape (≈ 1 line)\n    X_indices = np.zeros((m, max_len))\n    \n    for i in range(m):                               # loop over training examples\n        \n        # Convert the ith training sentence in lower case and split is into words. You should get a list of words.\n        sentence_words = X[i]\n        \n        # Initialize j to 0\n        j = 0\n        \n        # Loop over the words of sentence_words\n        for w in sentence_words:\n            # Set the (i,j)th entry of X_indices to the index of the correct word.\n            X_indices[i, j] = word_to_index.get(w, 0)\n            # Increment j to j + 1\n            j = j + 1\n    \n    return X_indices\nX1_indices = sentences_to_indices(np.array(df.words), word_to_index, max_len)\nX_train = X1_indices\nY_train = np.array(df.emoji_vector)\n模型结构如下图\n\nimport numpy as np\nfrom keras.models import Model\nfrom keras.layers import Dense, Input, Dropout, LSTM, Activation\nfrom keras.layers.embeddings import Embedding\nfrom keras.preprocessing import sequence\nfrom keras.initializers import glorot_uniform\nUsing TensorFlow backend.\n\n实现pretrained_embedding_layer ()需要执行以下步骤:\n\n将嵌入矩阵初始化为具有正确形状的零数组。\n用从’ word_to_vec_map ‘中提取的所有单词嵌入填充嵌入矩阵。\n定义Keras嵌入层。使用嵌入()(keras.io/layers/embeddings/)。通过在调用“Embedding()”时设置“trainable = False”，确保这个层是不可训练的。如果您设置’ trainable = True ‘，那么它将允许优化算法修改单词embeddings的值。\n设嵌入权重等于嵌入矩阵\n\n# GRADED FUNCTION: 预处理一个embedding层\n \ndef pretrained_embedding_layer(word_to_vec_map, word_to_index):\n    &quot;&quot;&quot;\n    创建一个 Keras Embedding() 层\n    \n    Arguments:\n    word_to_vec_map -- dictionary mapping words to their GloVe vector representation.\n    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n \n    Returns:\n    embedding_layer -- pretrained layer Keras instance\n    &quot;&quot;&quot;\n    \n    vocab_len = len(word_to_index) + 1                  # 增加一层\n    emb_dim = word_to_vec_map[&quot;我&quot;].shape[0]      # 默认的向量维度\n    \n    # 将嵌入矩阵初始化为一个形状为零的numpy数组(vocab_len，单词向量的维数= emb_dim)\n    emb_matrix = np.zeros((vocab_len, emb_dim))\n    \n    # 将嵌入矩阵的每一行“index”设为词汇表中“index”第四个单词的单词向量表示\n    for word, index in word_to_index.items():\n        emb_matrix[index, :] = word_to_vec_map[word]\n \n    # 定义Keras嵌入层与正确的输出/输入大小，使其可训练。使用嵌入(…)。设置trainable=False。\n    embedding_layer = Embedding(vocab_len, emb_dim, trainable=False)\n \n    # 构建嵌入层，在设置嵌入层权重之前需要。不要修改“None”。\n    embedding_layer.build((None,))\n    \n    # 将嵌入层的权重设置为嵌入矩阵。现在是预先训练好的。\n    embedding_layer.set_weights([emb_matrix])\n    \n    return embedding_layer\nembedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\nprint(&quot;weights[0][1][3] =&quot;, embedding_layer.get_weights()[0][1][3])\nweights[0][1][3] = 0.475248\n\ndef Emojify(input_shape, word_to_vec_map, word_to_index):\n    &quot;&quot;&quot;\n    Function creating the Emojify-v2 model&#039;s graph.\n    \n    Arguments:\n    input_shape -- shape of the input, usually (max_len,)\n    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation\n    word_to_index -- dictionary mapping from words to their indices in the vocabulary\n \n    Returns:\n    model -- a model instance in Keras\n    &quot;&quot;&quot;\n    \n    ### START CODE HERE ###\n    # Define sentence_indices as the input of the graph, it should be of shape input_shape and dtype &#039;int32&#039; (as it contains indices).\n    sentence_indices = Input(shape=input_shape, dtype=&quot;int32&quot;)\n    \n    # Create the embedding layer pretrained with GloVe Vectors (≈1 line)\n    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n    \n    # Propagate sentence_indices through your embedding layer, you get back the embeddings\n    embeddings = embedding_layer(sentence_indices)\n    \n    # Propagate the embeddings through an LSTM layer with 128-dimensional hidden state\n    # Be careful, the returned output should be a batch of sequences.\n    X = LSTM(units=128, return_sequences=True)(embeddings)\n    # Add dropout with a probability of 0.5\n    X = Dropout(0.5)(X)\n    # Propagate X trough another LSTM layer with 128-dimensional hidden state\n    # Be careful, the returned output should be a single hidden state, not a batch of sequences.\n    X = LSTM(units=128, return_sequences=False)(X)\n    # Add dropout with a probability of 0.5\n    X = Dropout(0.5)(X)\n    # Propagate X through a Dense layer with softmax activation to get back a batch of 5-dimensional vectors.\n    X = Dense(1)(X)\n    # Add a softmax activation\n    X = Activation(&quot;softmax&quot;)(X)\n    \n    # Create Model instance which converts sentence_indices into X.\n    model = Model(inputs=sentence_indices, outputs=X)\n    \n    return model\nmaxLen = len(max(X_train, key=len))\nmodel = Emojify((maxLen,), word_to_vec_map, word_to_index)\nmodel.summary()\nmodel.compile(loss=&#039;binary_crossentropy&#039;, optimizer=&#039;adam&#039;, metrics=[&#039;accuracy&#039;])\nX_train[0], Y_train[0]\n(array([170081.,  75835.,      0.,      0.,      0.,      0.,      0.,\n             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n             0.,      0.,      0.,      0.,      0.]),\n 967)\n\nmodel.fit(X_train, Y_train, epochs = 5, batch_size = 32, shuffle=True)\nEpoch 1/5\n 33632/203185 [===&gt;..........................] - ETA: 32:43 - loss: -13765.9092 - accuracy: 0.0000e+00\n\n预测\nx = list(jieba.cut(&#039;今天天气不错&#039;))\nx = np.array([x])\nx = sentences_to_indices(x, word_to_index, maxLen)\np = model.predict(x)\nindex_to_emoji[int(p)]\n总结\n随机森林效果最好，但是训练速度最慢，占用内存太大，SVM稍快，占用空间小。"},"机器学习笔记/Keras学习笔记":{"slug":"机器学习笔记/Keras学习笔记","filePath":"机器学习笔记/Keras学习笔记.md","title":"Keras学习笔记","links":[],"tags":[],"content":"import tensorflow as tf\nfrom tensorflow  import keras\nfrom tensorflow.keras import layers\n使用Sequential模型\n一个Sequential模型适用于简单的层堆叠， 其中每一层正好有一个输入张量和一个输出张量。\nmodel = keras.Sequential(\n    [\n        layers.Dense(2, activation=&quot;relu&quot;, name=&quot;layer1&quot;),\n        layers.Dense(3, activation=&quot;relu&quot;, name=&quot;layer2&quot;),\n        layers.Dense(4, name=&quot;layer3&quot;),\n    ]\n)\nx = tf.ones((3, 3))\nx\n&lt;tf.Tensor: shape=(3, 3), dtype=float32, numpy=\narray([[1., 1., 1.],\n       [1., 1., 1.],\n       [1., 1., 1.]], dtype=float32)&gt;\n\ny = model(x)\ny\n&lt;tf.Tensor: shape=(3, 4), dtype=float32, numpy=\narray([[0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.]], dtype=float32)&gt;\n\n上述代码等效于一下代码：\n# Create 3 layers\nlayer1 = layers.Dense(2, activation=&quot;relu&quot;, name=&quot;layer1&quot;)\nlayer2 = layers.Dense(3, activation=&quot;relu&quot;, name=&quot;layer2&quot;)\nlayer3 = layers.Dense(4, name=&quot;layer3&quot;)\n \n# Call layers on a test input\nx = tf.ones((3, 3))\ny = layer3(layer2(layer1(x)))\ny\n&lt;tf.Tensor: shape=(3, 4), dtype=float32, numpy=\narray([[0., 0., 0., 0.],\n       [0., 0., 0., 0.],\n       [0., 0., 0., 0.]], dtype=float32)&gt;\n\nSequential不适用于以下情况：\n模型有多个输入或多个输出\n任何一层都有多个输入或多个输出\n需要进行图层共享\n需要非线性拓扑（例如，残余连接，多分支模型）\n\n可通过以下layers属性访问其图层\nmodel.layers\n[&lt;tensorflow.python.keras.layers.core.Dense at 0x1ee679d3cf8&gt;,\n &lt;tensorflow.python.keras.layers.core.Dense at 0x1ee679f3a90&gt;,\n &lt;tensorflow.python.keras.layers.core.Dense at 0x1ee67a0c208&gt;]\n\n还可以通过以下add()方法逐步创建一个顺序模型：\nmodel = keras.Sequential()\nmodel.add(layers.Dense(2, activation=&quot;relu&quot;))\nmodel.add(layers.Dense(3, activation=&quot;relu&quot;))\nmodel.add(layers.Dense(4))\n还有一种相应的pop()方法可以删除图层：顺序模型的行为非常类似于图层列表。\nprint(len(model.layers))\n3\n\nmodel.pop()\nprint(len(model.layers))\n2\n\nSequential构造函数接受name参数，就像Keras中的任何层或模型一样。这对于用语义上有意义的名称注释TensorBoard图很有用。\nmodel = keras.Sequential(name=&quot;my_sequential&quot;)\nmodel.add(layers.Dense(2, activation=&quot;relu&quot;, name=&quot;layer1&quot;))\nmodel.add(layers.Dense(3, activation=&quot;relu&quot;, name=&quot;layer2&quot;))\nmodel.add(layers.Dense(4, name=&quot;layer3&quot;))\n预先指定输入形状\nKeras中的所有图层都需要知道其输入的形状，以便能够创建其权重。因此，当创建这样的图层时，最初没有权重：\nlayer = layers.Dense(3)\nlayer.weights\n[]\n\n由于权重的形状取决于输入的形状，因此会在首次调用输入时创建其权重：\nx = tf.ones((1, 4))\ny = layer(x)\nlayer.weights\n[&lt;tf.Variable &#039;dense_3/kernel:0&#039; shape=(4, 3) dtype=float32, numpy=\n array([[-0.23496091, -0.42415935, -0.38969237],\n        [ 0.47878957,  0.6321573 ,  0.53070235],\n        [-0.57678986,  0.5862113 , -0.5439472 ],\n        [-0.8276289 ,  0.88936853, -0.6267946 ]], dtype=float32)&gt;,\n &lt;tf.Variable &#039;dense_3/bias:0&#039; shape=(3,) dtype=float32, numpy=array([0., 0., 0.], dtype=float32)&gt;]\n\n这也适用于顺序模型。当实例化没有输入形状的顺序模型时，它不是“构建”的：它没有权重（并且调用 model.weights结果仅说明了这一点）。权重是在模型首次看到一些输入数据时创建的：\nmodel = keras.Sequential(\n    [\n        layers.Dense(2, activation=&quot;relu&quot;),\n        layers.Dense(3, activation=&quot;relu&quot;),\n        layers.Dense(4),\n    ]\n)\n \nx = tf.ones((1, 4))\ny = model(x)\nprint(&quot;Number of weights after calling the model:&quot;, len(model.weights))  # 6\nNumber of weights after calling the model: 6\n\n一旦“构建”了模型，就可以调用其summary()方法以显示其内容：\nmodel.summary()\nModel: &quot;sequential_6&quot;\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_4 (Dense)              (1, 2)                    10        \n_________________________________________________________________\ndense_5 (Dense)              (1, 3)                    9         \n_________________________________________________________________\ndense_6 (Dense)              (1, 4)                    16        \n=================================================================\nTotal params: 35\nTrainable params: 35\nNon-trainable params: 0\n_________________________________________________________________\n\n但是，当逐步构建顺序模型时，能够显示到目前为止的模型摘要（包括当前输出形状）非常有用。在这种情况下，应该通过将一个Input 对象传递给模型来启动模型，以使它从一开始就知道其输入形状：\nmodel = keras.Sequential()\nmodel.add(keras.Input(shape=(4,)))\nmodel.add(layers.Dense(2, activation=&quot;relu&quot;))\n \nmodel.summary()\nModel: &quot;sequential_8&quot;\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_10 (Dense)             (None, 2)                 10        \n=================================================================\nTotal params: 10\nTrainable params: 10\nNon-trainable params: 0\n_________________________________________________________________\n\n由于该Input对象model.layers不是图层，因此不会显示为的一部分：\nmodel.layers\n[&lt;tensorflow.python.keras.layers.core.Dense at 0x1ee68ade4e0&gt;]\n\n一个简单的替代方法是将一个input_shape参数传递给第一层：\nmodel = keras.Sequential()\nmodel.add(layers.Dense(2, activation=&quot;relu&quot;, input_shape=(4,)))\n \nmodel.summary()\nModel: &quot;sequential_9&quot;\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_11 (Dense)             (None, 2)                 10        \n=================================================================\nTotal params: 10\nTrainable params: 10\nNon-trainable params: 0\n_________________________________________________________________\n\n使用这样的预定义输入形状构建的模型始终具有权重（甚至在查看任何数据之前），并且始终具有定义的输出形状。\n通常，建议的最佳做法是始终事先指定顺序模型的输入形状（如果预先知道它是什么）。\n常见的调试工作流程：add()+summary()\n在构建新的顺序体系结构时，以渐进方式堆叠层add()并经常打印模型摘要很有用。例如，可以监视堆栈Conv2D和MaxPooling2D图层如何对图像特征贴图进行下采样：\nmodel = keras.Sequential()\nmodel.add(keras.Input(shape=(250, 250, 3)))  # 250x250 RGB images\nmodel.add(layers.Conv2D(32, 5, strides=2, activation=&quot;relu&quot;))\nmodel.add(layers.Conv2D(32, 3, activation=&quot;relu&quot;))\nmodel.add(layers.MaxPooling2D(3))\n \nmodel.summary()\n \n# (40, 40, 32)\n \nmodel.add(layers.Conv2D(32, 3, activation=&quot;relu&quot;))\nmodel.add(layers.Conv2D(32, 3, activation=&quot;relu&quot;))\nmodel.add(layers.MaxPooling2D(3))\nmodel.add(layers.Conv2D(32, 3, activation=&quot;relu&quot;))\nmodel.add(layers.Conv2D(32, 3, activation=&quot;relu&quot;))\nmodel.add(layers.MaxPooling2D(2))\n \nmodel.summary()\n \n# 现在我们有了4x4的特征图，是时候应用MaxPooling了。\nmodel.add(layers.GlobalMaxPooling2D())\n \n# 最后，我们添加一个分类层。\nmodel.add(layers.Dense(10))\nModel: &quot;sequential_11&quot;\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_6 (Conv2D)            (None, 123, 123, 32)      2432      \n_________________________________________________________________\nconv2d_7 (Conv2D)            (None, 121, 121, 32)      9248      \n_________________________________________________________________\nmax_pooling2d_3 (MaxPooling2 (None, 40, 40, 32)        0         \n=================================================================\nTotal params: 11,680\nTrainable params: 11,680\nNon-trainable params: 0\n_________________________________________________________________\nModel: &quot;sequential_11&quot;\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_6 (Conv2D)            (None, 123, 123, 32)      2432      \n_________________________________________________________________\nconv2d_7 (Conv2D)            (None, 121, 121, 32)      9248      \n_________________________________________________________________\nmax_pooling2d_3 (MaxPooling2 (None, 40, 40, 32)        0         \n_________________________________________________________________\nconv2d_8 (Conv2D)            (None, 38, 38, 32)        9248      \n_________________________________________________________________\nconv2d_9 (Conv2D)            (None, 36, 36, 32)        9248      \n_________________________________________________________________\nmax_pooling2d_4 (MaxPooling2 (None, 12, 12, 32)        0         \n_________________________________________________________________\nconv2d_10 (Conv2D)           (None, 10, 10, 32)        9248      \n_________________________________________________________________\nconv2d_11 (Conv2D)           (None, 8, 8, 32)          9248      \n_________________________________________________________________\nmax_pooling2d_5 (MaxPooling2 (None, 4, 4, 32)          0         \n=================================================================\nTotal params: 48,672\nTrainable params: 48,672\nNon-trainable params: 0\n_________________________________________________________________\n\n拥有模型后该怎么办\n一旦模型架构准备就绪，将需要：\n训练模型，评估模型并进行推理。\n将模型保存到磁盘并还原。\n通过利用多个GPU来加速模型训练。\n\n使用顺序模型进行特征提取\n一旦建立了顺序模型，它的行为就类似于功能API模型。这意味着每个层都有一个input and output属性。这些属性可以做一些事情，例如快速创建一个模型，以提取顺序模型中所有中间层的输出：\ninitial_model = keras.Sequential(\n    [\n        keras.Input(shape=(250, 250, 3)),\n        layers.Conv2D(32, 5, strides=2, activation=&quot;relu&quot;),\n        layers.Conv2D(32, 3, activation=&quot;relu&quot;),\n        layers.Conv2D(32, 3, activation=&quot;relu&quot;),\n    ]\n)\nfeature_extractor = keras.Model(\n    inputs=initial_model.inputs,\n    outputs=[layer.output for layer in initial_model.layers],\n)\n \n# Call feature extractor on test input.\nx = tf.ones((1, 250, 250, 3))\nfeatures = feature_extractor(x)\n这是一个类似的示例，仅从一层中提取要素：\ninitial_model = keras.Sequential(\n    [\n        keras.Input(shape=(250, 250, 3)),\n        layers.Conv2D(32, 5, strides=2, activation=&quot;relu&quot;),\n        layers.Conv2D(32, 3, activation=&quot;relu&quot;, name=&quot;my_intermediate_layer&quot;),\n        layers.Conv2D(32, 3, activation=&quot;relu&quot;),\n    ]\n)\nfeature_extractor = keras.Model(\n    inputs=initial_model.inputs,\n    outputs=initial_model.get_layer(name=&quot;my_intermediate_layer&quot;).output,\n)\n# Call feature extractor on test input.\nx = tf.ones((1, 250, 250, 3))\nfeatures = feature_extractor(x)"},"机器学习笔记/【转】机器学习法则：ML工程的最佳实践":{"slug":"机器学习笔记/【转】机器学习法则：ML工程的最佳实践","filePath":"机器学习笔记/【转】机器学习法则：ML工程的最佳实践.md","title":"【转】机器学习法则：ML工程的最佳实践","links":[],"tags":["机器学习"],"content":"机器学习法则：ML工程的最佳实践\n作者无邪机器学习研究者，人工智障推进者。Martin Zinkevich 在2016年将 google 内容多年关于机器学习相关的经验分享了出来，这篇文章是对该分享的一些翻译+解读，如果想查看原文请参见：developers.google.com/machine-learning/rules-of-ml/ 。术语在说到具体的相关经验之前，先来了解下常用的术语。示例（Instance）：那些你要为其做出预测的事物称为示例。例如，示例可能是一个网页，你要将其归为“关于猫的”网页或者“不是关于猫的”网页。标签（Label）：预测任务的答案或结果称为标签。无论是机器学习系统的答案或结果，还是训练数据的答案或结果，都可以称为标签。例如，将网页标记为“关于猫的”。特征（Feature）：预测任务中示例的属性即为“特征”。例如，网页可以有“包含词汇‘猫’”的特征。特征栏（Feature Column）：特征栏是相关特征的集合，如用户所住地区存在的所有可能国籍的集合。在同一个样本的同一个特征栏中可能有一个或多个特征。特征栏相当于（雅虎或微软的）虚拟机系统的 “命名空间（namespace）”或“域（field）”。样本（Example）：样本包含示例（具有各种特征）和一个标签。模型（Model）：模型是预测任务的数学表达形式。先是通过样本训练模型，而后利用模型做出预测。指标（Metric）：指标是指一系列的数字，这些数字直接或间接的都被优化过。目标（Objective）：目标是指算法经过优化，努力要达到的度量标准。工作流（Pipeline）：工作流指的是围绕机器学习算法而存在的基础架构。从前端搜集数据、将搜集到的数据放入训练数据文件夹、训练一个或多个模型以及将模型用于生产等过程，都属于工作流。点击率（Click-through Rate）：用户浏览网页时对包含广告链接的点击次数占浏览次数的百分比。概述要想创造出优秀的产品：你需要以一位优秀工程师的身份去运用深度学习！记住！你不单单是一位机器学习的研究者！事实上，你所面临的大多数问题都是工程问题。即便拥有足以媲美机器学习专家的理论知识，要想有所突破，大多数情况下都在依赖示例的良好特征，而非优秀的机器学习算法。因此，基本方法如下：确保你的工作流各连接端十分可靠从树立合理的目标开始用简单的方式，添加符合常识的特征确保你的工作流始终可靠这种方法能带来相当多的盈利，也能在较长时间里令许多人都满意，甚至还可能实现双赢。只有在简单技巧不发挥任何作用的情况下，才考虑使用复杂的一些的方法。方法越复杂，产品最终输出速度慢。当所有的简单技巧用完后，很可能就要考虑最前沿机器学习术了。本文档主要由四部分组成：第一部分：帮助你明白是否到了需要构建一个机器学习系统第二部分：部署你的第一个工作流第三部分：往工作流增加新特征时的发布和迭代，以及如何评价模型和训练-服务倾斜（training-serving shew)第四部分：达到稳定阶段后该继续做什么。在机器学习之前Rule #1:Don’t be afraid to launch a product without machine learning.法则 1:不要害怕发布一款没有用到机器学习的产品机器学习是很酷，但它需要数据。如果你认为机器学习可以提高 100% 收益，那么启发式规则可以获得 50% 收益。Rule #2: First, design and implement metrics.法则2：首先需要设计和实现评估指标在构建具体的机器学习系统之前，首先在当前系统中记录尽量详细的历史信息。原因如下：在早期，更容易获得系统用户的权限许可（获得系统用户权限后，更容易收集各种数据）。如果你觉得某个问题以后会受到关注，最好是从现状开始就搜集历史数据。如果设计系统的时候考虑了评估指标，这对将来会大有益处。具体来说，这是为了让你以后不用在日志文件中寻找相关的字符串。你能够注意到什么（随着时间）改变了，什么（随着时间）没有改变。举个例子，假设你想要直接优化一天的活跃用户量。然而在早期对系统的处理中可能会发现用户体验的变化并没有显著改变活跃用户量的度量。Rule #3: Choose machine learning over a complex heuristic.法则3：优先选择机器学习而不是复杂的启发式规则简单的启发式方法可以轻松应用到产品上，而复杂的启发式方法却难以维护。一旦你拥有了足够的数据，并且对要实现的目标有了基本的概念，那就转向机器学习吧。在大多数软件工程中，不管使用的是启发方法还是机器学习模型，都需要经常更新算法。但是你会发现，使用机器学习的模型更容易更新和维护。机器学习阶段 1：第一条工作流构建第一个机器学习工作流时，一定要更多关注系统基础架构的建设。虽然机器学习的算法令人激动，但是因为基础架构不给力找不到问题时会令人抓狂。Rule #4: Keep the first model simple and get the infrastructure right.法则4：第一个模型要简单，但是基础架构要正确第一个模型对你的产品提高最大，因此它不需要有多花哨。相反，你会碰到比你想象的多的基础架构方面的问题。在别人使用你的的新机器学习系统前，你需要确定：如何为你的学习算法得到样本为你的系统初步定义“好”与“坏”的标准如何将模型集成到应用程序中。你可以直接将模型应用到在线应用程序中，也可以在离线样本的基础上对模型进行预计算（pre－compute），然后把与计算的结果储存在表格中。选择简单的特征，这样会更容易确保：特征正确应用到算法中模型能够学习到合理的权重特征正确应用到服务器模型（也就是生产环境的模型）中你的系统如果能够可靠地遵守这三点，你就完成了大多数工作。你的简单模型能够提供基准指标和基准行为，你可以用来测量更加复杂的模型。Rule #5: Test the infrastructure independently from the machine learning.法则5：独立于机器学习来测试架构流程不仅需要确保基础架构的可测试性，还需要确保系统的学习部分（learning part）是封装好的（encapsulated），这样才能测试所有与之相关的部件。具体来说：测试输入到算法中的数据。检查应该填充的特征栏是否正确填充。测试模型在训练算法之外的运行情况。确保模型的训练环境中和服务环境中的得分相同。机器学习的一个特点就是不可预测性。因此，你必须确保在训练和实际运行中创造样本的代码能被测试，并且在实际运行中始终使用同一个固定的模型。Rule #6: Be careful about dropped data when copying pipelines.法则6：复制工作流时留意丢失的数据我们有时候会通过复制已经存在的工作流来创建一个新的工作流。在新的工作流中需要的数据，很可能在旧的数据流就丢弃了。Rule #7: Turn heuristics into features, or handle them externally.法则 7: 将启发规则转化为特征，或者在外部处理它们机器学习系统解决的问题通常都不是新问题，而是对已有问题的进一步优化。这意味着有很多已有的规则或者启发式规则可供使用。这部分信息应该被充分利用。下面是几种启发式规则可以被使用的方式：用启发式规则进行预处理。 若特征相当完美，则可以采用这个方法。举个例子，在垃圾邮件过滤器中，如果发件人已经被加入黑名单了，则可以不用重新学习“黑名单”的概念。直接阻止该信息就可以！这种方法在二元分类（binary classification）任务中很有用。创建特征。 直接从启发式规则中创建特征会很便捷。举个例子，若要用启发式规则为某个查询结果计算相关度，你可以把分数纳入特征的值中。接下来，用机器学习的方法来处理这些值（例如，把这些值转化为由一系列离散值组成的有限集，或者也可以与其它特征相结合），但是要从启发式方法生成的原始数据入手。挖掘启发式方法的原始输入数据。 对于某款 app，若存在一个启发式方法，其包含安装量、文本字符数和当天日期等要素，可以考虑将这些原始信息单独作为特征使用。修改标签。当你发觉启发式方法捕捉了一些信息，而这些信息没有包含在标记中，这时可以考虑该选项。举个例子，如果你想让下载量达到最大，但同时对内容的质量有要求，那么可以用 app 的平均评级乘以标记来解决问题。监控一般来说，所有系统都要设置良好的警示程序，警报系统需要顺利执行，或者设置一个仪表板页面（dashboard page）。Rule #8: Know the freshness requirements of your system.法则 8: 了解你系统对新鲜度的要求如果你使用的是一天前的旧模型，运行状况会下降多少？如果是一周前的呢？或一个季度前的呢？知道何时该刷新系统能帮助你划分监控的优先级。如果你的模型一天没有更新，受益便下降 10%，因此有必要指派一名工程师时时关注它的动态。大多数广告服务系统每天都会有新的广告需要处理和更新。此外，要留意系统对新鲜度的要求会随着时间变化，特别是在添加或移除特征栏的时候，需要尤为注意。Rule #9: Detect problems before exporting models.法则 9: 输出（发布）模型前发现问题许多机器学习系统都存在这样一个阶段：直接把模型输出运行。如果问题出现在模型输出之后，那么这个问题就是用户所面临的问题。而如果问题出现在模型输出之前，就是训练过程中的问题，用户不会发现。输出模型之前请做好完整性检查（sanity check）。具体来讲，确保模型在留存数据上运行合理，例如AUC。Rule #10: Watch for silent failures.法则10：注意隐藏性故障比起其它系统，机器学习系统更容易出现潜在的问题。假设系统的某个特定的表格不再进行更新，整个系统通过调整仍会保持良好的运行水准，但是会慢慢衰减。有时有些表格几个月都不会刷新一次，而只需简单的刷新就能大幅度提升系统的运行水准，效果甚至超过该季度最新发布的那些模型！例如，由于系统实现（implementation）发生变化，特征的覆盖范围也会发生相应的变化：比如，某个特征栏刚开始可能包含 90%的样本，接下来却可能突然下降到 60％。解决方法是是对关键数据的统计信息进行监控，并且周期性对关键数据进行人工检查。Rule #11: Give feature columns owners and documentation.法则 11：为特征栏指定负责人并记录文档如果系统的规模比较大，并且特征栏比较多，那么必须清楚每个特征栏的创建者或者维护者。如果某个了解该特征栏的人离开了，一定要确保另外还有人了解这部分信息。虽然很多特征栏的名字非常直观，但最好还是使用更详尽的文档来描述这些特征的内容、来自哪里以及它们的作用。你的第一个目标（Objective）objective 是模型试图优化的值，而 metric 指的是任何用来评估系统的值。Rule #12: Don’t overthink which objective you choose to directly optimize.法则 12: 不要过于纠结该优化哪个目标你有成千上万关心的指标，这些指标也值得你去测试。但是，在机器学习过程的早期，你会发现，即使你并没有直接去优化，他们也都会上升。比如，你关心点击次数，停留时间以及每日活跃用户数。如果仅优化了点击次数，通常也会看到停留时间增加了。所以，当提高所有的指标都不难的时候，就没必要花心思来如何权衡不同的指标。不过过犹不及：不要混淆了你的目标和系统的整体健康度。Rule #13: Choose a simple, observable and attributable metric for your first objective.法则 13：选择一个简单、可观测并且可归类的评估指标（metric）作为你的第一个目标（objective）有时候你自以为你清楚真实的目标,但随着你对数据的观察，对老系统和新的机器学习系统的分析，你会发现你又想要调整。而且，不同的团队成员对于真实目标并不能达成一致。机器学习的目标必须是能很容易测量的，并且一定是“真实”目标的代言。因此，在简单的机器学习目标上训练，并创建一个“决策层”，以允许你在上面增加额外的逻辑（这些逻辑，越简单越好）来形成最后的排序。最容易建模的是那些可以直接观察并可归属到系统的某个动作的用户行为：排序的链接被点击了吗？排序的物品被下载了吗？排序的物品被转发/回复/邮件订阅了吗？排序的物品被评价了吗？展示的物品是否被标注为垃圾/色情/暴力？最开始要避免对间接效果建模：用户第二天会来访吗？用户访问时间是多长？每日活跃用户是什么样的？间接效果是非常重要的指标，在A/B test和发布决定的时候可以使用。最后，不要试图让机器学习来回答以下问题：用户使用你的产品是否开心用户是否有满意的体验产品是否提高了用户的整体幸福感这些是否影响了公司的整体健康度这些都很重要，但太难评估了。与其如此，不如考虑其他代替的：比如，用户如果高兴，那停留时间就应该更长。如果用户满意，他就会再次造访。Rule #14: Starting with an interpretable model makes debugging easier.法则 14：从容易解释的模型入手会让调试过程更加容易线性回归，逻辑回归和泊松回归直接由概率模型激发。每个预测可解释为概率或期望值。这使得他们比那些使用目标来直接优化分类准确性和排序性能的模型要更容易调试。比如，如果训练时的概率和预测时的概率，或者生产系统上的查看到的概率有偏差，那说明存在某种问题。Rule #15: Separate Spam Filtering and Quality Ranking in a Policy Layer.法则 15：在策略层将垃圾信息过滤和质量排名分开质量排名是一门艺术，而垃圾过滤是一场战争。那些使用你系统的人非常清楚你采用什么来评价一篇帖子的质量，所以他们会想尽办法来使得他们的帖子具有这些属性。因此，质量排序应该关注对哪些诚实发布的内容进行排序。如果将垃圾邮件排高名次，那质量排序学习器就大打折扣。同理也要将粗俗的内容从质量排序中拿出分开处理。垃圾过滤就是另外一回事。你必须考虑到要生成的特征会经常性的改变。你会输入很多明显的规则到系统中。至少要保证你的模型是每日更新的。同时，要重点考虑内容创建者的信誉问题。机器学习阶段 2：特征工程在机器学习系统研发周期的第一阶段，重点是把训练数据导入学习系统，得到感兴趣的评价指标，并创建基础架构。当你有了一个端对端的系统，并且该系统的单元和测试都仪表化之后，第二阶段便开始了。第二阶段需要纳入尽可能多的有效特征，并依据直观的感觉组合起来。在这个阶段，所有的评估指标仍然会上升。Rule #16: Plan to launch and iterate.法则16：做好持续迭代上线的准备不要期望现在发布的这个模型是最终的模型。因此，考虑你给当前这个模型增加的复杂度会不会减慢后续的发布。许多团队每季度推出一个模型或者更多年。之所以不断发布新模型，有三个基本原因：你会不断地想到新的特征。你会不断地调整并以新的方式组合旧的特征。你会不断调优目标。Rule #17: Start with directly observed and reported features as opposed to learned features.法则 17：优先使用直接观测或收集到的特征，而不是学习出来的特征（learned features）先描述一下什么是学习出来的特征（learned features）。学习出来的特征（learned features）是由外部系统（比如无监督聚类系统）或学习者本身（比如因子模型、深度学习）生成的特征。两种方式生成的特征都很有用，但也有很多问题，因此不应当用在第一个模型中。Rule #18: Explore with features of content that generalize across contexts.法则 18：探索使用可以跨场景的内容特征通常情况下，机器学习只占到一个大系统中的很小一部分，因此你必须要试着从不同角度审视一个用户行为。比如热门推荐这一场景，一般情况下论坛里“热门推荐”里的帖子都会有许多评论、分享和阅读量，如果利用这些统计数据对模型展开训练，然后对一个新帖子进行优化，就有可能使其成为热门帖子。另一方面，YouTube上自动播放的下一个视频也有许多选择，例如可以根据大部分用户的观看顺序推荐，或者根据用户评分推荐等。总之，如果你将一个用户行为用作模型的标记（label），那么在不同的上下文条件下审视这一行为，可能会得到更丰富的特征（feature），也就更利于模型的训练。需要注意的是这与个性化不同：个性化是确定用户是否在特定的上下文环境中喜欢某一内容，并发现哪些用户喜欢，喜欢的程度如何。Rule #19: Use very specific features when you can.法则 19：尽量使用非常具体的特征在海量数据的支持下，即使学习数百万个简单的特征也比仅仅学习几个复杂的特征要容易实现。由于被检索的文本标识与规范化的查询并不会提供太多的归一化信息，只会调整头部查询中的标记排序。因此你不必担心虽然整体的数据覆盖率高达90%以上，但针对每个特征组里的单一特征却没有多少训练数据可用的情况。另外，你也可以尝试正则化的方法来增加每个特征所对应的样本数。Rule #20: Combine and modify existing features to create new features in human–understandable ways.法则 20: 用人类可理解的方式对已有特征进行组合和修改有很多种方法组合和改良特征。像 TensorFlow 这样的机器学习系统，它允许通过 transformations 预处理数据。其最标准的两种方法分别是“discretization（离散化）”和“crosses（叉积）”。Discretization 会根据一个连续的特征创建许多离散的特征。假定年龄是一个连续的特征。我们可以创建如下特征，当年龄小于 18 时记为 1，或者当年龄在 18 到35 岁之间时为 1，以此类推。不用过多考虑这些数据的边界问题：简单的数字可以给你最直观的冲击。Cross由两个或多个特征栏组成。根据TensorFlow给出的解释， 特征栏是一组同类的特征。（如｛男，女｝、｛美国，加拿大，墨西哥｝等）。而Cross是一个新的特征栏，可以用｛男，女｝×｛美国，加拿大，墨西哥｝等来简单的表示。新的特征栏会包含以下特征，如｛男，加拿大｝。使用TensorFlow时可以让它帮你创建cross。｛男，加拿大｝可以在样本中代表男性加拿大人。注意若模型使用三个以上的特征栏组成的cross，则需要大量的数据来训练模型。Cross 会产生庞大的特征栏，有可能导致过拟合现象。举个例子，假设你要做某种搜索。检索词构成一个特征栏，文档中的词构成另一个特征栏。你可以通过cross 来组合它们，但这样会出现很多特征。处理文本时，有两个替代性方案。最苛刻的方案是 dot product（点积）。点积仅统计检索词和文档词中的公共词汇。得到的特征可以被离散化。另一种方案是取intersection（交集）：因此，我们有一个特征来表示当“pony（色情）”这个词同时出现在文档和检索词中，另一个特征表示“the”同时出现在文档和检索词中。Rule #21: The number of feature weights you can learn in a linear model is roughly proportional to the amount of data you have.法则 21：线性模型中的特征权重的数量应大致和样本数量形成一定的比例关于模型究竟多复杂才合适，统计学习理论有许多有趣的结论。但总的来说，这一条法则就足够了。我曾和一些人交流过，在他们看来，要想学到些东西，一千个样本远远不够，至少需要一百万个样本。这是因为，他们被特定的学习方法束缚了手脚。而诀窍就是，根据数据大小调整学习方法：如果你在开发一个搜索排名系统，并且有数百万不同的词汇存在于文档和检索词中，而你仅有 1000 个带有标记的样本。那么你应该使用文档和检索词的点积特征、 TF－IDF 以及其它六个人工设计的特征。 1000 个样本，对应 12个左右的特征。如果有一百万个的样本，那就通过 regularization 或特征 selection，取文档特征栏和检索词特征栏的交集。这样你能得到数百万个特征，但 regularization会帮你减少些许的特征。一千万个样本，对应大约十万个特征。如果有十亿个乃至几千亿个样本，你可以通过 regularization 和特征选取，取文档特征栏和 query token 的叉积。如果有十亿个样本，那么你会得到一千万个特征。Rule #22: Clean up features you are no longer using.法则22：清理不再使用的特征当决定要清除哪些特征时，需要考虑其覆盖率，即该项特征覆盖了多少样本。举个例子，如果你有一些比较特别的特征，但只有 8% 的用户与之相关，那么这些特征就无足轻重了。同时，有些特征可能超越它们的权重。比如某个特征仅覆盖 1% 的数据，但 90% 的正样本都含有这种特征。那么，也应当将这个特征添加进来。系统的人工分析在进入机器学习第三阶段前，有一些在机器学习课程上学习不到的内容也非常值得关注：如何检测一个模型并改进它。这与其说是门科学，还不如说是一门艺术。这里再介绍几种要避免的反模式（anti-patterns）Rule #23: You are not a typical end user.法则 23: 你并非典型终端用户这可能是让一个团队陷入困境的最简单的方法。虽然fishfooding（只在团队内部使用原型）和dogfooding（只在公司内部使用原型）都有许多优点，但无论哪一种，开发者都应该首先确认这种方式是否符合性能要求。要避免使用一个明显不好的改变，同时，任何看起来合理的产品策略也应该进一步的测试，不管是通过让非专业人士来回答问题，还是通过一个对真实用户的线上实验。Rule #24: Measure the delta between models.法则24：测量模型间的差异在将你的模型发布上线前，一个最简单，有时也是最有效的测试是比较你当前的模型和已经交付的模型生产的结果之间的差异。如果差异很小，那不再需要做实验，你也知道你这个模型不会带来什么改变。如果差异很大，那就要继续确定这种改变是不是好的。检查对等差分很大的查询能帮助理解改变的性质（是变好，还是变坏）。但是，使用不同模型进行比较前，需要确保该模型和它本身比较，这个差异很小（理想情况应该是无任何差异）。Rule #25: When choosing models, utilitarian performance trumps predictive power.法则 25: 选择模型时，性能表现比预测力更重要虽然我们训练模型时 objective 一般都是 logloss，也就是说实在追求模型的预测能力。但是我们在上层应用中却可能有多种用途，例如可能会用来排序，那么这时具体的预测能力就不如排序能力重要；如果用来划定阈值然后跟根据阈值判断垃圾邮件，那么准确率就更重要。当然大多数情况下这几个指标是一致的。Rule #26: Look for patterns in the measured errors, and create new features.法则 26: 在错误中寻找规律，然后创建新特征假设你的模型在某个样本中预测错误。在分类任务中，这可能是误报或漏报。在排名任务中，这可能是一个正向判断弱于逆向判断的组。但更重要的是，在这个样本中机器学习系统知道它错了，需要修正。如果你此时给模型一个允许它修复的特征，那么模型将尝试自行修复这个错误。另一方面，如果你尝试基于未出错的样本创建特征，那么该特征将很可能被系统忽略。例如，假设在 Google Play商店的应用搜索中，有人搜索“免费游戏”，但其中一个排名靠前的搜索结果却是一款其他App，所以你为其他App创建了一个特征。但如果你将其他App的安装数最大化，即人们在搜索免费游戏时安装了其他App，那么这个其他App的特征就不会产生其应有的效果。所以，正确的做法是一旦出现样本错误，那么应该在当前的特征集之外寻找解决方案。例如，如果你的系统降低了内容较长的帖子的排名，那就应该普遍增加帖子的长度。而且也不要拘泥于太具体的细节。例如你要增加帖子的长度，就不要猜测长度的具体含义，而应该直接添加几个相关的特征，交给模型自行处理，这才是最简单有效的方法。Rule #27: Try to quantify observed undesirable behavior.法则 27：尝试量化观察到的异常行为如果在系统中观察到了模型没有优化到的问题，典型的例如推荐系统逼格不够这种问题，这时应该努力将这种不满意转化为具体的数字，具体来讲可以通过人工标注等方法标注出不满意的物品，然后进行统计。如果问题可以被量化，后面就可以将其用作特征、objective或者metric。整体原则就是“先量化，再优化”。Rule #28: Be aware that identical short-term behavior does not imply identical long-term behavior.法则 28：短期行为相同并不代表长期行为也相同假设你有一个新系统，它可以查看每个doc_id和exact_query，然后根据每个文档的每次查询行为计算其点击率。你发现它的行为几乎与当前系统的并行和A/B测试结果完全相同，而且它很简单，于是你启动了这个系统。却没有新的应用显示，为什么？由于你的系统只基于自己的历史查询记录显示文档，所以不知道应该显示一个新的文档。要了解一个系统在长期行为中如何工作的唯一办法，就是让它只基于当前的模型数据展开训练。这一点非常困难。训练偏差（Training－Serving Skew）训练偏差是指训练时的表现和在生产环境中实际运行时的表现的差别。这种偏差可能由以下因素引起：在训练时和在实际工作流中用不同的方式处理数据。训练中的数据和在实际运行中的分布不同。模型和算法之间存在反馈循环。解决这类问题的核心是对系统和数据的变化进行监控，确保一切差异都在监控之内，不会悄悄进入系统。Rule #29: The best way to make sure that you train like you serve is to save the set of features used at serving time, and then pipe those features to a log to use them at training time.法则 29： 要让实际产品和训练时表现一样好，最好的方法是实际运行中保留特征集，并记录到日志中以便训练中使用即使你不能对每个样例都这样做，做一小部分也比什么也不做好，这样你就可以验证服务和训练之间的一致性（见规则37）。在 Google 采取了这项措施的团队有时候会对其效果感到惊讶。比如YouTube主页在服务时会切换到日志记录特征，这不仅大大提高了服务质量，而且减少了代码复杂度。目前有许多团队都已经在其基础设施上采用了这种策略。Rule #30: Importance-weight sampled data, don’t arbitrarily drop it!法则30：给抽样数据按重要性赋权重，不要随意丢弃它们当我们有太多训练数据时，我们会只取其中的一部分。但这是错误的。正确的做法是，如果你给某条样本30%的采样权重，那么在训练时就给它10/3的训练权重。通过这样的重要性赋权（importance weight），整个训练结果的校准性（calibration）就还能够保证。Rule #31: Beware that if you join data from a table at training and serving time, the data in the table may change.法则 31：如果要从表格中组合数据，注意训练时和实际运行时表格可能发生改变假设你要把文档 id 和包含文档特征的表格（比如评论或点击的数量）结合起来。从训练和实际运行，表格中的特征可能会改变（例如用户对物品的评论数），模型对同一文档做的预测也能不同。要避免这这类问题，最简单的办法就是记录所有实际运行时的特征。若表格只是缓慢的变化，你也可以按照每小时或每天的频率对其做出记录，得到足够相近的数据。注意这样不能完美的解决问题。Rule #32: Re-use code between your training pipeline and your serving pipeline whenever possible.法则 32: 尽量在训练流和实际运行流中使用重复代码首先需要明确一点：批处理和在线处理并不一样。在线处理中，你必须及时处理每一个请求（比如，必须为每个查询单独查找），而批处理，你可以合并完成。服务时，你要做的是在线处理，而训练是批处理任务。尽管如此，还是有很多可以重用代码的地方。比如说，你可以创建特定于系统的对象，其中的所有联结和查询结果都以人类可读的方式存储，错误也可以被简单地测试。然后，一旦在服务或训练期间收集了所有信息，你就可以通过一种通用方法在这个特定对象和机器学习系统需要的格式之间形成互通，训练和服务的偏差也得以消除。因此，尽量不要在训练时和服务时使用不同的变成语言，毕竟这样会让你没法重用代码。Rule #33: If you produce a model based on the data until January 5th, test the model on the data from January 6th and after.法则 33: 如果训练数据是1月5日之前的，那么测试数据要从1月6日开始测试模型时应当使用的比训练模型时更加新的数据，因为这更能反映你的系统实际运行表现。如果你用 1 月 5 日前的数据生成了一个模型，那就得用 1月 6 号之后的数据测试它。你会发现，在新的数据下模型表现得没那么好，但也不会差到哪里去。这个结果更加接近真实运行时的表现。Rule #34: In binary classification for filtering (such as spam detection or determining interesting emails), make small short-term sacrifices in performance for very clean data.法则 34：在过滤类的任务中，被标记为负的样本是不会展示给用户的，例如可能会把75%标记为负的样本阻拦住不展现给用户。但如果你只从展示给用户的结果中获取下次训练的样本，显然你的训练样本是有偏的更好的做法是使用一定比例的流量（例如1%）专门收集训练数据，在这部分流量中的用户会看到所有的样本。这样显然会影响线上的真实过滤效果，但是会收集到更好的数据，更有利于系统的长远发展。否则系统会越训练越偏，慢慢就不可用了。同时还能保证至少过滤掉74%的负样本，对系统的影响也不是很大。但是如果你的系统会过滤掉95%或者更多的负样本，这种做法就不那么可行了。即使如此，为了准确衡量模型的效果，你仍然可以通过构造一个更小的数据集（0.1%或者更小）来测试。十万级别的样本足够给出准确的评价指标了。Rule #35: Beware of the inherent skew in ranking problems.法则 35: 注意排序问题存在固有偏差当你对排序算法做出足够多的改动时，一方面会引起完全不同的排序结果，另一方面也可能在很大程度上改变算法未来可能要处理的数据。这会引入一些固有偏差，因此你必须事先充分认识到这一点。以下这些方法可以有效帮你优化训练数据。对涵盖更多查询的特征进行更高的正则化，而不是那些只覆盖单一查询的特征。这种方式使得模型更偏好那些针对个别查询的特征，而不是那些能够泛化到全部查询的特征。这种方式能够帮助阻止非常流行的结果进入不相关查询。这点和更传统的建议不一样，传统建议应该对更独特的特征集进行更高的正则化。只允许特征具有正向权重，这样一来就能保证任何好特征都会比未知特征合适。不要有那些仅仅偏文档（document-only）的特征。这是法则1的极端版本。比如，不管搜索请求是什么，即使一个给定的应用程序是当前的热门下载，你也不会想在所有地方都显示它。没有仅仅偏文档类特征，这会很容易实现。Rule #36: Avoid feedback loops with positional features.法则 36：用位置特征来避免反馈回路大家都知道排序位置本身就会影响用户是否会对物品产生互动，例如点击。所以如果模型中没有位置特征，本来由于位置导致的影响会被算到其他特征头上去，导致模型不够准。可以用加入位置特征的方法来避免这种问题，具体来讲，在训练时加入位置特征，预测时去掉位置特征，或者给所有样本一样的位置特征。这样会让模型更正确地分配特征的权重。需要注意的是，位置特征要保持相对独立，不要与其他特征发生关联。可以将位置相关的特征用一个函数表达，然后将其他特征用另外的函数表达，然后组合起来。具体应用中，可以通过位置特征不与任何其他特征交叉来实现这个目的。Measure Training/Serving Skew.法则 37: 衡量训练和服务之间的差异很多情况会引起偏差。大致上分为一些几种：训练集和测试集之间的差异。这种差异会经常存在，而且不一定是坏事。测试集和“第二天”数据间的差异。这种差异也会一直存在，而这个“第二天”数据上的表现是我们应该努力优化的，例如通过正则化。这两者之间差异如果过大，可能是因为用到了一些时间敏感的特征，导致模型效果变化明显。“第二天”数据和线上数据间的差异。如果同样一条样本，在训练时给出的结果和线上服务时给出的结果不一致，那么这意味着工程实现中出现了bug。机器学习第三阶段：放慢速度、优化细化和复杂的模型一般会有一些明确的信号来标识第二阶段的尾声。首先，每月的提升会逐步降低。你开始在不同指标之间做权衡，有的上升有的下降。这将会变得越来越有趣。增长越来越难实现，必须要考虑更加复杂的机器学习。警告：相对于前面两个阶段，这部分会有很多开放式的法则。第一阶段和第二阶段的机器学习是快乐的。当到了第三阶段，每个团队就不能不去找到他们自己的途径了。Rule #38: Don’t waste time on new features if unaligned objectives have become the issue.法则 38： 如果目标没有达成一致，就不要在新特征上浪费时间当达到评估指标瓶颈，你的团队开始关注机器学习系统目标范围之外的问题。如同之前提到的，如果产品目标没有包括在算法目标之内，你就得修改其中一个。比如说，你也许优化的是点击数、点赞或者下载量，但发布决策还是依赖于人类评估者。Rule #39: Launch decisions are a proxy for long-term product goals.法则 39：模型发布决策是长期产品目标的代理这个法则字面上有点难以理解，其实作者核心就是在讲一件事情：系统、产品甚至公司的长远发展需要通过多个指标来综合衡量，而新模型是否上线要综合考虑这些指标。所谓代理，指的就是优化这些综合指标就是在优化产品、公司的长远目标。决策只有在所有指标都在变好的情况下才会变得简单。但常常事情没那么简单，尤其是当不同指标之间无法换算的时候，例如A系统有一百万日活和四百万日收入，B系统有两百万日活和两百万日收入，你会从A切换到B吗？或者反过来？答案是或许都不会，因为你不知道某个指标的提升是否会cover另外一个指标的下降。关键是，没有任何一个指标能回答：“五年后我的产品在哪里”？而每个个体，尤其是工程师们，显然更喜欢能够直接优化的目标，而这也是机器学习系统常见的场景 。现在也有一些多目标学习系统在试图解决这种问题。但仍然有很多目标无法建模为机器学习问题，比如用户为什么会来访问你的网站等等。作者说这是个AI-complete问题，也常被称为强AI问题，简单来说就是不能用某个单一算法解决的问题。Rule #40: Keep ensembles simple.法则 40: 保持模型集合（ensembles）的简单性接收原始特征、直接对内容排序的统一模型，是最容易理解、最容易修补漏洞的模型。但是，一个集成模型（一个把其他模型得分组合在一起的“模型”）的效果会更好。为保持简洁，每个模型应该要么是一个只接收其他模型的输入的集成模型，要么是一个有多种特征的基础模型，但不能两者皆是。如果你有单独训练、基于其它模型的模型，把它们组合到一起会导致不好的行为。只使用简单模型来集成那些仅仅把你的基础模型输出当做输入。你同样想要给这些集成模型加上属性。比如，基础模型生成得分的提高，不应该降低集成模型的分数。另外，如果连入模型在语义上可解释（比如校准了的）就最好了，这样其下层模型的改变不会影响集成模型。此外，强行让下层分类器预测的概率升高，不会降低集成模型的预测概率。Rule #41: When performance plateaus, look for qualitatively new sources of information to add rather than refining existing signals.法则 41：当效果进入瓶颈期，寻找本质上新的信息源，而不是优化已有的信号。你先是添加了一些用户的人口信息，又添加了一些文档词汇的信息，接着你又浏览了一遍模版，而后又调整了规则，但是最后，关键度量却只提升了不到 1%。现在怎么办？这时候应该用完全不同的特征搭建基础架构，比如用户昨天／上周／去年访问的文档的历史记录。利用 wikidata 或对公司来说比较重要的东西（比如 Google 的知识图）。你或许需要使用深度学习。开始调整你对投资回报的期望，并作出相应努力。如同所有工程项目，你需要平衡新增加的特征与提高的复杂度。Rule #42: Don’t expect diversity, personalization, or relevance to be as correlated with popularity as you think they are.法则 42：不要期望多样性、个性化、相关性和受欢迎程度之间有紧密联系一系列内容的多样性能意味着许多东西，内容来源的多样性最为普遍。个性化意味着每个用户都能获得它自己感兴趣的结果。相关性意味着一个特定的查询对于某个查询总比其他更合适。显然，这三个属性的定义和标准都不相同。问题是标准很难打破。注意：如果你的系统在统计点击量、耗费时间、浏览数、点赞数、分享数等等，你事实上在衡量内容的受欢迎程度。有团队试图学习具备多样性的个性化模型。为个性化，他们加入允许系统进行个性化的特征（有的特征代表用户兴趣），或者加入多样性（表示该文档与其它返回文档有相同特征的特征，比如作者和内容），然后发现这些特征比他们预想的得到更低的权重（有时是不同的信号）。这不意味着多样性、个性化和相关性就不重要。就像之前的规则指出的，你可以通过后处理来增加多样性或者相关性。如果你看到更长远的目标增长了，那至少你可以声称，除了受欢迎度，多样性/相关性是有价值的。你可以继续使用后处理，或者你也可以基于多样性或相关性直接修改你的目标。Rule #43: Your friends tend to be the same across different products. Your interests tend not to be.法则 43: 在不同的产品中，你的朋友可能相同，但兴趣却不尽然Google 经常在不同产品上使用同样的好友关系预测模型，并且取得了很好的效果，这证明不同的产品上好友关系是可以迁移的，毕竟他们是固定的同一批人。但他们尝试将一个产品上的个性化特征使用到另外一个产品上时却常常得不到好结果。可行的做法是使用一个数据源上的原始数据来预测另外数据源上的行为，而不是使用加工后的特征。此外，用户在另一个数据源上的行为历史也会有用。"},"机器学习笔记/【转】神经网络浅讲：从神经元到深度学习":{"slug":"机器学习笔记/【转】神经网络浅讲：从神经元到深度学习","filePath":"机器学习笔记/【转】神经网络浅讲：从神经元到深度学习.md","title":"【转】神经网络浅讲：从神经元到深度学习","links":[],"tags":["机器学习，神经网络"],"content":"这又是我常看的大神写的一篇博客，从最初的从机器学习谈起使我入门机器学习，到现在学到了神经网络，又找到了他的博客，希望对我学习神经网络有所帮助。\n原文地址：www.cnblogs.com/subconscious/p/5058741.html#first\n作者：计算机的潜意识\n\n\t\n\t\n\n\n\n\t\n\t\t\n\t\t\n\t\t\t　　神经网络是一门重要的机器学习技术。它是目前最为火热的研究方向--深度学习的基础。学习神经网络不仅可以让你掌握一门强大的机器学习方法，同时也可以更好地帮助你理解深度学习技术。\n　　本文以一种简单的，循序的方式讲解神经网络。适合对神经网络了解不多的同学。本文对阅读没有一定的前提要求，但是懂一些机器学习基础会更好地帮助理解本文。\n　　神经网络是一种模拟人脑的神经网络以期能够实现类人工智能的机器学习技术。人脑中的神经网络是一个非常复杂的组织。成人的大脑中估计有1000亿个神经元之多。\n\n图1 人脑神经网络\n \n　　那么机器学习中的神经网络是如何实现这种模拟的，并且达到一个惊人的良好效果的？通过本文，你可以了解到这些问题的答案，同时还能知道神经网络的历史，以及如何较好地学习它。\n　　由于本文较长，为方便读者，以下是本文的目录：\n　　一.前言\n　　二.神经元\n　　三.单层神经网络（感知器）\n　　四.两层神经网络（多层感知器）\n　　五.多层神经网络（深度学习）\n　　六.回顾\n　　七.展望\n　　八.总结\n　　九.后记\n　　十.备注\n \n一. 前言\n　　让我们来看一个经典的神经网络。这是一个包含三个层次的神经网络。红色的是输入层，绿色的是输出层，紫色的是中间层（也叫隐藏层）。输入层有3个输入单元，隐藏层有4个单元，输出层有2个单元。后文中，我们统一使用这种颜色来表达神经网络的结构。\n\n图2 神经网络结构图\n \n　　在开始介绍前，有一些知识可以先记在心里：\n\n设计一个神经网络时，输入层与输出层的节点数往往是固定的，中间层则可以自由指定；\n神经网络结构图中的拓扑与箭头代表着预测过程时数据的流向，跟训练时的数据流有一定的区别；\n结构图里的关键不是圆圈（代表“神经元”），而是连接线（代表“神经元”之间的连接）。每个连接线对应一个不同的权重（其值称为权值），这是需要训练得到的。  \n\n　　除了从左到右的形式表达的结构图，还有一种常见的表达形式是从下到上来表示一个神经网络。这时候，输入层在图的最下方。输出层则在图的最上方，如下图：\n\n图3 从下到上的神经网络结构图 \n \n　　从左到右的表达形式以Andrew Ng和LeCun的文献使用较多，Caffe里使用的则是从下到上的表达。在本文中使用Andrew Ng代表的从左到右的表达形式。\n　　下面从简单的神经元开始说起，一步一步介绍神经网络复杂结构的形成。\n \n二. 神经元\n　　1.引子　\n　　对于神经元的研究由来已久，1904年生物学家就已经知晓了神经元的组成结构。\n　　一个神经元通常具有多个树突，主要用来接受传入信息；而轴突只有一条，轴突尾端有许多轴突末梢可以给其他多个神经元传递信息。轴突末梢跟其他神经元的树突产生连接，从而传递信号。这个连接的位置在生物学上叫做“突触”。\n　　人脑中的神经元形状可以用下图做简单的说明：\n\n图4 神经元\n \n 　　1943年，心理学家McCulloch和数学家Pitts参考了生物神经元的结构，发表了抽象的神经元模型MP。在下文中，我们会具体介绍神经元模型。\n   \n图5 Warren McCulloch（左）和 Walter Pitts（右）  \n　　2.结构 \n　　神经元模型是一个包含输入，输出与计算功能的模型。输入可以类比为神经元的树突，而输出可以类比为神经元的轴突，计算则可以类比为细胞核。\n　　下图是一个典型的神经元模型：包含有3个输入，1个输出，以及2个计算功能。\n　　注意中间的箭头线。这些线称为“连接”。每个上有一个“权值”。\n\n图6 神经元模型 \n \n　　连接是神经元中最重要的东西。每一个连接上都有一个权重。\n　　一个神经网络的训练算法就是让权重的值调整到最佳，以使得整个网络的预测效果最好。\n　　我们使用a来表示输入，用w来表示权值。一个表示连接的有向箭头可以这样理解：在初端，传递的信号大小仍然是a，端中间有加权参数w，经过这个加权后的信号会变成a*w，因此在连接的末端，信号的大小就变成了a*w。\n　　在其他绘图模型里，有向箭头可能表示的是值的不变传递。而在神经元模型里，每个有向箭头表示的是值的加权传递。\n\n图7 连接（connection）  \n \n　　如果我们将神经元图中的所有变量用符号表示，并且写出输出的计算公式的话，就是下图。\n\n图8 神经元计算  \n \n　　可见z是在输入和权值的线性加权和叠加了一个函数g的值。在MP模型里，函数g是sgn函数，也就是取符号函数。这个函数当输入大于0时，输出1，否则输出0。\n　　下面对神经元模型的图进行一些扩展。首先将sum函数与sgn函数合并到一个圆圈里，代表神经元的内部计算。其次，把输入a与输出z写到连接线的左上方，便于后面画复杂的网络。最后说明，一个神经元可以引出多个代表输出的有向箭头，但值都是一样的。\n　　神经元可以看作一个计算与存储单元。计算是神经元对其的输入进行计算功能。存储是神经元会暂存计算结果，并传递到下一层。\n\n图9 神经元扩展 \n \n　　当我们用“神经元”组成网络以后，描述网络中的某个“神经元”时，我们更多地会用“单元”（unit）来指代。同时由于神经网络的表现形式是一个有向图，有时也会用“节点”（node）来表达同样的意思。 \n　　3.效果 \n　　神经元模型的使用可以这样理解：\n　　我们有一个数据，称之为样本。样本有四个属性，其中三个属性已知，一个属性未知。我们需要做的就是通过三个已知属性预测未知属性。\n　　具体办法就是使用神经元的公式进行计算。三个已知属性的值是a1，a2，a3，未知属性的值是z。z可以通过公式计算出来。\n　　这里，已知的属性称之为特征，未知的属性称之为目标。假设特征与目标之间确实是线性关系，并且我们已经得到表示这个关系的权值w1，w2，w3。那么，我们就可以通过神经元模型预测新样本的目标。\n　　4.影响\n　　1943年发布的MP模型，虽然简单，但已经建立了神经网络大厦的地基。但是，MP模型中，权重的值都是预先设置的，因此不能学习。\n　　1949年心理学家Hebb提出了Hebb学习率，认为人脑神经细胞的突触（也就是连接）上的强度上可以变化的。于是计算科学家们开始考虑用调整权值的方法来让机器学习。这为后面的学习算法奠定了基础。\n\n图10 Donald Olding Hebb \n \n　　尽管神经元模型与Hebb学习律都已诞生，但限于当时的计算机能力，直到接近10年后，第一个真正意义的神经网络才诞生。\n \n三. 单层神经网络（感知器）\n　　1.引子　　\n　　1958年，计算科学家Rosenblatt提出了由两层神经元组成的神经网络。他给它起了一个名字--“感知器”（Perceptron）（有的文献翻译成“感知机”，下文统一用“感知器”来指代）。\n　　感知器是当时首个可以学习的人工神经网络。Rosenblatt现场演示了其学习识别简单图像的过程，在当时的社会引起了轰动。\n　　人们认为已经发现了智能的奥秘，许多学者和科研机构纷纷投入到神经网络的研究中。美国军方大力资助了神经网络的研究，并认为神经网络比“原子弹工程”更重要。这段时间直到1969年才结束，这个时期可以看作神经网络的第一次高潮。\n\n图11 Rosenblat与感知器 \n　　2.结构\n　　下面来说明感知器模型。\n　　在原来MP模型的“输入”位置添加神经元节点，标志其为“输入单元”。其余不变，于是我们就有了下图：从本图开始，我们将权值w1, w2, w3写到“连接线”的中间。\n\n图12 单层神经网络 \n \n　　在“感知器”中，有两个层次。分别是输入层和输出层。输入层里的“输入单元”只负责传输数据，不做计算。输出层里的“输出单元”则需要对前面一层的输入进行计算。\n　　我们把需要计算的层次称之为“计算层”，并把拥有一个计算层的网络称之为“单层神经网络”。有一些文献会按照网络拥有的层数来命名，例如把“感知器”称为两层神经网络。但在本文里，我们根据计算层的数量来命名。\n　　假如我们要预测的目标不再是一个值，而是一个向量，例如[2,3]。那么可以在输出层再增加一个“输出单元”。\n　　下图显示了带有两个输出单元的单层神经网络，其中输出单元z1的计算公式如下图。\n\n图13 单层神经网络(Z1)\n \n　　可以看到，z1的计算跟原先的z并没有区别。\n　　我们已知一个神经元的输出可以向多个神经元传递，因此z2的计算公式如下图。\n\n图14 单层神经网络(Z2)\n \n　　可以看到，z2的计算中除了三个新的权值：w4，w5，w6以外，其他与z1是一样的。\n　　整个网络的输出如下图。\n\n图15 单层神经网络(Z1和Z2)\n \n　　目前的表达公式有一点不让人满意的就是：w4，w5，w6是后来加的，很难表现出跟原先的w1，w2，w3的关系。\n　　因此我们改用二维的下标，用wx,y来表达一个权值。下标中的x代表后一层神经元的序号，而y代表前一层神经元的序号（序号的顺序从上到下）。\n　　例如，w1,2代表后一层的第1个神经元与前一层的第2个神经元的连接的权值（这种标记方式参照了Andrew Ng的课件）。根据以上方法标记，我们有了下图。\n\n图16 单层神经网络(扩展)\n \n　　如果我们仔细看输出的计算公式，会发现这两个公式就是线性代数方程组。因此可以用矩阵乘法来表达这两个公式。\n　　例如，输入的变量是[a1，a2，a3]T（代表由a1，a2，a3组成的列向量），用向量a来表示。方程的左边是[z1，z2]T，用向量z来表示。\n　　系数则是矩阵W（2行3列的矩阵，排列形式与公式中的一样）。\n　　于是，输出公式可以改写成：\ng(W * a) = z;\n \n　　这个公式就是神经网络中从前一层计算后一层的矩阵运算。\n　　3.效果\n　　与神经元模型不同，感知器中的权值是通过训练得到的。因此，根据以前的知识我们知道，感知器类似一个逻辑回归模型，可以做线性分类任务。\n　　我们可以用决策分界来形象的表达分类的效果。决策分界就是在二维的数据平面中划出一条直线，当数据的维度是3维的时候，就是划出一个平面，当数据的维度是n维时，就是划出一个n-1维的超平面。\n　　下图显示了在二维平面中划出决策分界的效果，也就是感知器的分类效果。\n\n图17 单层神经网络（决策分界）\n　　\n　　4.影响　\n　　感知器只能做简单的线性分类任务。但是当时的人们热情太过于高涨，并没有人清醒的认识到这点。于是，当人工智能领域的巨擘Minsky指出这点时，事态就发生了变化。\n　　Minsky在1969年出版了一本叫《Perceptron》的书，里面用详细的数学证明了感知器的弱点，尤其是感知器对XOR（异或）这样的简单分类任务都无法解决。\n　　Minsky认为，如果将计算层增加到两层，计算量则过大，而且没有有效的学习算法。所以，他认为研究更深层的网络是没有价值的。（本文成文后一个月，即2016年1月，Minsky在美国去世。谨在本文中纪念这位著名的计算机研究专家与大拿。）\n   \n图18 Marvin Minsky\n　　\n　　由于Minsky的巨大影响力以及书中呈现的悲观态度，让很多学者和实验室纷纷放弃了神经网络的研究。神经网络的研究陷入了冰河期。这个时期又被称为“AI winter”。\n　　接近10年以后，对于两层神经网络的研究才带来神经网络的复苏。\n \n四. 两层神经网络（多层感知器）\n　　1.引子\n　　两层神经网络是本文的重点，因为正是在这时候，神经网络开始了大范围的推广与使用。\n　　Minsky说过单层神经网络无法解决异或问题。但是当增加一个计算层以后，两层神经网络不仅可以解决异或问题，而且具有非常好的非线性分类效果。不过两层神经网络的计算是一个问题，没有一个较好的解法。\n　　1986年，Rumelhar和Hinton等人提出了反向传播（Backpropagation，BP）算法，解决了两层神经网络所需要的复杂计算量问题，从而带动了业界使用两层神经网络研究的热潮。目前，大量的教授神经网络的教材，都是重点介绍两层（带一个隐藏层）神经网络的内容。 \n　　这时候的Hinton还很年轻，30年以后，正是他重新定义了神经网络，带来了神经网络复苏的又一春。\n        \n图19 David Rumelhart（左）以及 Geoffery Hinton（右）\n \n　　2.结构\n　　两层神经网络除了包含一个输入层，一个输出层以外，还增加了一个中间层。此时，中间层和输出层都是计算层。我们扩展上节的单层神经网络，在右边新加一个层次（只含有一个节点）。\n　　现在，我们的权值矩阵增加到了两个，我们用上标来区分不同层次之间的变量。\n　　例如ax(y)代表第y层的第x个节点。z1，z2变成了a1(2)，a2(2)。下图给出了a1(2)，a2(2)的计算公式。\n\n图20 两层神经网络（中间层计算）\n \n　　计算最终输出z的方式是利用了中间层的a1(2)，a2(2)和第二个权值矩阵计算得到的，如下图。\n\n图21 两层神经网络（输出层计算）\n \n　　假设我们的预测目标是一个向量，那么与前面类似，只需要在“输出层”再增加节点即可。\n　　我们使用向量和矩阵来表示层次中的变量。a(1)，a(2)，z是网络中传输的向量数据。W(1)和W(2)是网络的矩阵参数。如下图。\n\n图22 两层神经网络（向量形式）\n \n　　使用矩阵运算来表达整个计算公式的话如下：\n  g(W(1) * a(1)) = a(2); \ng(W(2) * a(2)) = z;\n \n　　由此可见，使用矩阵运算来表达是很简洁的，而且也不会受到节点数增多的影响（无论有多少节点参与运算，乘法两端都只有一个变量）。因此神经网络的教程中大量使用矩阵运算来描述。\n　　需要说明的是，至今为止，我们对神经网络的结构图的讨论中都没有提到偏置节点（bias unit）。事实上，这些节点是默认存在的。它本质上是一个只含有存储功能，且存储值永远为1的单元。在神经网络的每个层次中，除了输出层以外，都会含有这样一个偏置单元。正如线性回归模型与逻辑回归模型中的一样。\n　　偏置单元与后一层的所有节点都有连接，我们设这些参数值为向量b，称之为偏置。如下图。\n\n图23 两层神经网络（考虑偏置节点）\n \n　　可以看出，偏置节点很好认，因为其没有输入（前一层中没有箭头指向它）。有些神经网络的结构图中会把偏置节点明显画出来，有些不会。一般情况下，我们都不会明确画出偏置节点。 \n　　在考虑了偏置以后的一个神经网络的矩阵运算如下：\n  g(W(1) * a(1) + b(1)) = a(2); \ng(W(2) * a(2) + b(2)) = z;\n \n　　需要说明的是，在两层神经网络中，我们不再使用sgn函数作为函数g，而是使用平滑函数sigmoid作为函数g。我们把函数g也称作激活函数（active function）。\n　　事实上，神经网络的本质就是通过参数与激活函数来拟合特征与目标之间的真实函数关系。初学者可能认为画神经网络的结构图是为了在程序中实现这些圆圈与线，但在一个神经网络的程序中，既没有“线”这个对象，也没有“单元”这个对象。实现一个神经网络最需要的是线性代数库。\n　　3.效果\n　　与单层神经网络不同。理论证明，两层神经网络可以无限逼近任意连续函数。\n　　这是什么意思呢？也就是说，面对复杂的非线性分类任务，两层（带一个隐藏层）神经网络可以分类的很好。\n　　下面就是一个例子（此两图来自colah的博客），红色的线与蓝色的线代表数据。而红色区域和蓝色区域代表由神经网络划开的区域，两者的分界线就是决策分界。\n\n图24 两层神经网络（决策分界）\n　　\n　　可以看到，这个两层神经网络的决策分界是非常平滑的曲线，而且分类的很好。有趣的是，前面已经学到过，单层网络只能做线性分类任务。而两层神经网络中的后一层也是线性分类层，应该只能做线性分类任务。为什么两个线性分类任务结合就可以做非线性分类任务？\n　　我们可以把输出层的决策分界单独拿出来看一下。就是下图。\n\n图25 两层神经网络（空间变换）\n \n　　可以看到，输出层的决策分界仍然是直线。关键就是，从输入层到隐藏层时，数据发生了空间变换。也就是说，两层神经网络中，隐藏层对原始的数据进行了一个空间变换，使其可以被线性分类，然后输出层的决策分界划出了一个线性分类分界线，对其进行分类。\n　　这样就导出了两层神经网络可以做非线性分类的关键--隐藏层。联想到我们一开始推导出的矩阵公式，我们知道，矩阵和向量相乘，本质上就是对向量的坐标空间进行一个变换。因此，隐藏层的参数矩阵的作用就是使得数据的原始坐标空间从线性不可分，转换成了线性可分。\n　　两层神经网络通过两层的线性模型模拟了数据内真实的非线性函数。因此，多层的神经网络的本质就是复杂函数拟合。\n　　下面来讨论一下隐藏层的节点数设计。在设计一个神经网络时，输入层的节点数需要与特征的维度匹配，输出层的节点数要与目标的维度匹配。而中间层的节点数，却是由设计者指定的。因此，“自由”把握在设计者的手中。但是，节点数设置的多少，却会影响到整个模型的效果。如何决定这个自由层的节点数呢？目前业界没有完善的理论来指导这个决策。一般是根据经验来设置。较好的方法就是预先设定几个可选值，通过切换这几个值来看整个模型的预测效果，选择效果最好的值作为最终选择。这种方法又叫做Grid Search（网格搜索）。\n　　了解了两层神经网络的结构以后，我们就可以看懂其它类似的结构图。例如EasyPR字符识别网络架构（下图）。\n\n图26 EasyPR字符识别网络\n \n　　EasyPR使用了字符的图像去进行字符文字的识别。输入是120维的向量。输出是要预测的文字类别，共有65类。根据实验，我们测试了一些隐藏层数目，发现当值为40时，整个网络在测试集上的效果较好，因此选择网络的最终结构就是120，40，65。\n　　4.训练\n　　下面简单介绍一下两层神经网络的训练。\n　　在Rosenblat提出的感知器模型中，模型中的参数可以被训练，但是使用的方法较为简单，并没有使用目前机器学习中通用的方法，这导致其扩展性与适用性非常有限。从两层神经网络开始，神经网络的研究人员开始使用机器学习相关的技术进行神经网络的训练。例如用大量的数据（1000-10000左右），使用算法进行优化等等，从而使得模型训练可以获得性能与数据利用上的双重优势。\n　　机器学习模型训练的目的，就是使得参数尽可能的与真实的模型逼近。具体做法是这样的。首先给所有参数赋上随机值。我们使用这些随机生成的参数值，来预测训练数据中的样本。样本的预测目标为yp，真实目标为y。那么，定义一个值loss，计算公式如下。\nloss = (yp - y)2\n \n　　这个值称之为损失（loss），我们的目标就是使对所有训练数据的损失和尽可能的小。\n　　如果将先前的神经网络预测的矩阵公式带入到yp中（因为有z=yp），那么我们可以把损失写为关于参数（parameter）的函数，这个函数称之为损失函数（loss function）。下面的问题就是求：如何优化参数，能够让损失函数的值最小。\n　　此时这个问题就被转化为一个优化问题。一个常用方法就是高等数学中的求导，但是这里的问题由于参数不止一个，求导后计算导数等于0的运算量很大，所以一般来说解决这个优化问题使用的是梯度下降算法。梯度下降算法每次计算参数在当前的梯度，然后让参数向着梯度的反方向前进一段距离，不断重复，直到梯度接近零时截止。一般这个时候，所有的参数恰好达到使损失函数达到一个最低值的状态。\n　　在神经网络模型中，由于结构复杂，每次计算梯度的代价很大。因此还需要使用反向传播算法。反向传播算法是利用了神经网络的结构进行的计算。不一次计算所有参数的梯度，而是从后往前。首先计算输出层的梯度，然后是第二个参数矩阵的梯度，接着是中间层的梯度，再然后是第一个参数矩阵的梯度，最后是输入层的梯度。计算结束以后，所要的两个参数矩阵的梯度就都有了。\n　　反向传播算法可以直观的理解为下图。梯度的计算从后往前，一层层反向传播。前缀E代表着相对导数的意思。\n\n图27 反向传播算法\n \n　　反向传播算法的启示是数学中的链式法则。在此需要说明的是，尽管早期神经网络的研究人员努力从生物学中得到启发，但从BP算法开始，研究者们更多地从数学上寻求问题的最优解。不再盲目模拟人脑网络是神经网络研究走向成熟的标志。正如科学家们可以从鸟类的飞行中得到启发，但没有必要一定要完全模拟鸟类的飞行方式，也能制造可以飞天的飞机。\n　　优化问题只是训练中的一个部分。机器学习问题之所以称为学习问题，而不是优化问题，就是因为它不仅要求数据在训练集上求得一个较小的误差，在测试集上也要表现好。因为模型最终是要部署到没有见过训练数据的真实场景。提升模型在测试集上的预测效果的主题叫做泛化（generalization），相关方法被称作正则化（regularization）。神经网络中常用的泛化技术有权重衰减等。\n　　5.影响\n　　两层神经网络在多个地方的应用说明了其效用与价值。10年前困扰神经网络界的异或问题被轻松解决。神经网络在这个时候，已经可以发力于语音识别，图像识别，自动驾驶等多个领域。\n　　历史总是惊人的相似，神经网络的学者们再次登上了《纽约时报》的专访。人们认为神经网络可以解决许多问题。就连娱乐界都开始受到了影响，当年的《终结者》电影中的阿诺都赶时髦地说一句：我的CPU是一个神经网络处理器，一个会学习的计算机。\n　　但是神经网络仍然存在若干的问题：尽管使用了BP算法，一次神经网络的训练仍然耗时太久，而且困扰训练优化的一个问题就是局部最优解问题，这使得神经网络的优化较为困难。同时，隐藏层的节点数需要调参，这使得使用不太方便，工程和研究人员对此多有抱怨。\n　　90年代中期，由Vapnik等人发明的SVM（Support Vector Machines，支持向量机）算法诞生，很快就在若干个方面体现出了对比神经网络的优势：无需调参；高效；全局最优解。基于以上种种理由，SVM迅速打败了神经网络算法成为主流。\n\n图28 Vladimir Vapnik\n \n　　神经网络的研究再次陷入了冰河期。当时，只要你的论文中包含神经网络相关的字眼，非常容易被会议和期刊拒收，研究界那时对神经网络的不待见可想而知。\n \n五. 多层神经网络（深度学习）\n　　1.引子　　\n　　在被人摒弃的10年中，有几个学者仍然在坚持研究。这其中的棋手就是加拿大多伦多大学的Geoffery Hinton教授。\n　　2006年，Hinton在《Science》和相关期刊上发表了论文，首次提出了“深度信念网络”的概念。与传统的训练方式不同，“深度信念网络”有一个“预训练”（pre-training）的过程，这可以方便的让神经网络中的权值找到一个接近最优解的值，之后再使用“微调”(fine-tuning)技术来对整个网络进行优化训练。这两个技术的运用大幅度减少了训练多层神经网络的时间。他给多层神经网络相关的学习方法赋予了一个新名词--“深度学习”。\n 　　很快，深度学习在语音识别领域暂露头角。接着，2012年，深度学习技术又在图像识别领域大展拳脚。Hinton与他的学生在ImageNet竞赛中，用多层的卷积神经网络成功地对包含一千类别的一百万张图片进行了训练，取得了分类错误率15%的好成绩，这个成绩比第二名高了近11个百分点，充分证明了多层神经网络识别效果的优越性。\n　　在这之后，关于深度神经网络的研究与应用不断涌现。\n\n图29 Geoffery Hinton \n \n　　由于篇幅原因，本文不介绍CNN（Conventional Neural Network，卷积神经网络）与RNN（Recurrent Neural Network，递归神经网络）的架构，下面我们只讨论普通的多层神经网络。\n　　2.结构\n　　我们延续两层神经网络的方式来设计一个多层神经网络。\n　　在两层神经网络的输出层后面，继续添加层次。原来的输出层变成中间层，新加的层次成为新的输出层。所以可以得到下图。\n\n图30 多层神经网络\n \n　　依照这样的方式不断添加，我们可以得到更多层的多层神经网络。公式推导的话其实跟两层神经网络类似，使用矩阵运算的话就仅仅是加一个公式而已。\n　　在已知输入a(1)，参数W(1)，W(2)，W(3)的情况下，输出z的推导公式如下：\n     g(W(1) * a(1)) = a(2); \n    g(W(2) * a(2)) = a(3);\ng(W(3) * a(3)) = z;\n \n　　多层神经网络中，输出也是按照一层一层的方式来计算。从最外面的层开始，算出所有单元的值以后，再继续计算更深一层。只有当前层所有单元的值都计算完毕以后，才会算下一层。有点像计算向前不断推进的感觉。所以这个过程叫做“正向传播”。\n　　下面讨论一下多层神经网络中的参数。\n　　首先我们看第一张图，可以看出W(1)中有6个参数，W(2)中有4个参数，W(3)中有6个参数，所以整个神经网络中的参数有16个（这里我们不考虑偏置节点，下同）。\n \n图31 多层神经网络（较少参数）\n \n　　假设我们将中间层的节点数做一下调整。第一个中间层改为3个单元，第二个中间层改为4个单元。\n　　经过调整以后，整个网络的参数变成了33个。\n \n图32 多层神经网络（较多参数）\n \n　　虽然层数保持不变，但是第二个神经网络的参数数量却是第一个神经网络的接近两倍之多，从而带来了更好的表示（represention）能力。表示能力是多层神经网络的一个重要性质，下面会做介绍。\n　　在参数一致的情况下，我们也可以获得一个“更深”的网络。\n \n图33 多层神经网络（更深的层次）\n \n　　上图的网络中，虽然参数数量仍然是33，但却有4个中间层，是原来层数的接近两倍。这意味着一样的参数数量，可以用更深的层次去表达。\n　　3.效果\n　　与两层层神经网络不同。多层神经网络中的层数增加了很多。\n　　增加更多的层次有什么好处？更深入的表示特征，以及更强的函数模拟能力。\n　　更深入的表示特征可以这样理解，随着网络的层数增加，每一层对于前一层次的抽象表示更深入。在神经网络中，每一层神经元学习到的是前一层神经元值的更抽象的表示。例如第一个隐藏层学习到的是“边缘”的特征，第二个隐藏层学习到的是由“边缘”组成的“形状”的特征，第三个隐藏层学习到的是由“形状”组成的“图案”的特征，最后的隐藏层学习到的是由“图案”组成的“目标”的特征。通过抽取更抽象的特征来对事物进行区分，从而获得更好的区分与分类能力。\n　　关于逐层特征学习的例子，可以参考下图。\n \n图34 多层神经网络（特征学习）\n \n　　更强的函数模拟能力是由于随着层数的增加，整个网络的参数就越多。而神经网络其实本质就是模拟特征与目标之间的真实关系函数的方法，更多的参数意味着其模拟的函数可以更加的复杂，可以有更多的容量（capcity）去拟合真正的关系。\n　　通过研究发现，在参数数量一样的情况下，更深的网络往往具有比浅层的网络更好的识别效率。这点也在ImageNet的多次大赛中得到了证实。从2012年起，每年获得ImageNet冠军的深度神经网络的层数逐年增加，2015年最好的方法GoogleNet是一个多达22层的神经网络。\n　　在最新一届的ImageNet大赛上，目前拿到最好成绩的MSRA团队的方法使用的更是一个深达152层的网络！关于这个方法更多的信息有兴趣的可以查阅ImageNet网站。\n　　4.训练\n　　在单层神经网络时，我们使用的激活函数是sgn函数。到了两层神经网络时，我们使用的最多的是sigmoid函数。而到了多层神经网络时，通过一系列的研究发现，ReLU函数在训练多层神经网络时，更容易收敛，并且预测性能更好。因此，目前在深度学习中，最流行的非线性函数是ReLU函数。ReLU函数不是传统的非线性函数，而是分段线性函数。其表达式非常简单，就是y=max(x,0)。简而言之，在x大于0，输出就是输入，而在x小于0时，输出就保持为0。这种函数的设计启发来自于生物神经元对于激励的线性响应，以及当低于某个阈值后就不再响应的模拟。\n　　在多层神经网络中，训练的主题仍然是优化和泛化。当使用足够强的计算芯片（例如GPU图形加速卡）时，梯度下降算法以及反向传播算法在多层神经网络中的训练中仍然工作的很好。目前学术界主要的研究既在于开发新的算法，也在于对这两个算法进行不断的优化，例如，增加了一种带动量因子（momentum）的梯度下降算法。　\n　　在深度学习中，泛化技术变的比以往更加的重要。这主要是因为神经网络的层数增加了，参数也增加了，表示能力大幅度增强，很容易出现过拟合现象。因此正则化技术就显得十分重要。目前，Dropout技术，以及数据扩容（Data-Augmentation）技术是目前使用的最多的正则化技术。\n　　5.影响\n　　目前，深度神经网络在人工智能界占据统治地位。但凡有关人工智能的产业报道，必然离不开深度学习。神经网络界当下的四位引领者除了前文所说的Ng，Hinton以外，还有CNN的发明人Yann Lecun，以及《Deep Learning》的作者Bengio。\n　　前段时间一直对人工智能持谨慎态度的马斯克，搞了一个OpenAI项目，邀请Bengio作为高级顾问。马斯克认为，人工智能技术不应该掌握在大公司如Google，Facebook的手里，更应该作为一种开放技术，让所有人都可以参与研究。马斯克的这种精神值得让人敬佩。\n   \n图35 Yann LeCun（左）和 Yoshua Bengio（右）\n \n　　多层神经网络的研究仍在进行中。现在最为火热的研究技术包括RNN，LSTM等，研究方向则是图像理解方面。图像理解技术是给计算机一幅图片，让它用语言来表达这幅图片的意思。ImageNet竞赛也在不断召开，有更多的方法涌现出来，刷新以往的正确率。\n \n六. 回顾\n　　1.影响　　\n　　我们回顾一下神经网络发展的历程。神经网络的发展历史曲折荡漾，既有被人捧上天的时刻，也有摔落在街头无人问津的时段，中间经历了数次大起大落。\n　　从单层神经网络（感知器）开始，到包含一个隐藏层的两层神经网络，再到多层的深度神经网络，一共有三次兴起过程。详见下图。\n \n图36 三起三落的神经网络\n \n　　上图中的顶点与谷底可以看作神经网络发展的高峰与低谷。图中的横轴是时间，以年为单位。纵轴是一个神经网络影响力的示意表示。如果把1949年Hebb模型提出到1958年的感知机诞生这个10年视为落下（没有兴起）的话，那么神经网络算是经历了“三起三落”这样一个过程，跟“小平”同志类似。俗话说，天将降大任于斯人也，必先苦其心志，劳其筋骨。经历过如此多波折的神经网络能够在现阶段取得成功也可以被看做是磨砺的积累吧。\n　　历史最大的好处是可以给现在做参考。科学的研究呈现螺旋形上升的过程，不可能一帆风顺。同时，这也给现在过分热衷深度学习与人工智能的人敲响警钟，因为这不是第一次人们因为神经网络而疯狂了。1958年到1969年，以及1985年到1995，这两个十年间人们对于神经网络以及人工智能的期待并不现在低，可结果如何大家也能看的很清楚。\n　　因此，冷静才是对待目前深度学习热潮的最好办法。如果因为深度学习火热，或者可以有“钱景”就一窝蜂的涌入，那么最终的受害人只能是自己。神经网络界已经两次有被人们捧上天了的境况，相信也对于捧得越高，摔得越惨这句话深有体会。因此，神经网络界的学者也必须给这股热潮浇上一盆水，不要让媒体以及投资家们过分的高看这门技术。很有可能，三十年河东，三十年河西，在几年后，神经网络就再次陷入谷底。根据上图的历史曲线图，这是很有可能的。\n　　2.效果　　\n　　下面说一下神经网络为什么能这么火热？简而言之，就是其学习效果的强大。随着神经网络的发展，其表示性能越来越强。\n　　从单层神经网络，到两层神经网络，再到多层神经网络，下图说明了，随着网络层数的增加，以及激活函数的调整，神经网络所能拟合的决策分界平面的能力。\n \n图37 表示能力不断增强\n \n　　可以看出，随着层数增加，其非线性分界拟合能力不断增强。图中的分界线并不代表真实训练出的效果，更多的是示意效果。\n　　神经网络的研究与应用之所以能够不断地火热发展下去，与其强大的函数拟合能力是分不开关系的。\n　　3.外因　　\n　　当然，光有强大的内在能力，并不一定能成功。一个成功的技术与方法，不仅需要内因的作用，还需要时势与环境的配合。神经网络的发展背后的外在原因可以被总结为：更强的计算性能，更多的数据，以及更好的训练方法。只有满足这些条件时，神经网络的函数拟合能力才能得已体现，见下图。\n \n图38 发展的外在原因\n \n　　之所以在单层神经网络年代，Rosenblat无法制作一个双层分类器，就在于当时的计算性能不足，Minsky也以此来打压神经网络。但是Minsky没有料到，仅仅10年以后，计算机CPU的快速发展已经使得我们可以做两层神经网络的训练，并且还有快速的学习算法BP。\n　　但是在两层神经网络快速流行的年代。更高层的神经网络由于计算性能的问题，以及一些计算方法的问题，其优势无法得到体现。直到2012年，研究人员发现，用于高性能计算的图形加速卡（GPU）可以极佳地匹配神经网络训练所需要的要求：高并行性，高存储，没有太多的控制需求，配合预训练等算法，神经网络才得以大放光彩。\n　　互联网时代，大量的数据被收集整理，更好的训练方法不断被发现。所有这一切都满足了多层神经网络发挥能力的条件。\n　　“时势造英雄”，正如Hinton在2006年的论文里说道的\n　　“... provided that computers were fast enough, data sets were big enough, and the initial weights were close enough to a good solution. All three conditions are now satisfied.”，\n \n　　外在条件的满足也是神经网络从神经元得以发展到目前的深度神经网络的重要因素。\n　　除此以外，一门技术的发扬没有“伯乐”也是不行的。在神经网络漫长的历史中，正是由于许多研究人员的锲而不舍，不断钻研，才能有了现在的成就。前期的Rosenblat，Rumelhart没有见证到神经网络如今的流行与地位。但是在那个时代，他们为神经网络的发展所打下的基础，却会永远流传下去，不会退色。\n \n七. 展望\n　　1.量子计算\n　　回到我们对神经网络历史的讨论，根据历史趋势图来看，神经网络以及深度学习会不会像以往一样再次陷入谷底？作者认为，这个过程可能取决于量子计算机的发展。\n　　根据一些最近的研究发现，人脑内部进行的计算可能是类似于量子计算形态的东西。而且目前已知的最大神经网络跟人脑的神经元数量相比，仍然显得非常小，仅不及1%左右。所以未来真正想实现人脑神经网络的模拟，可能需要借助量子计算的强大计算能力。\n　　各大研究组也已经认识到了量子计算的重要性。谷歌就在开展量子计算机D-wave的研究，希望用量子计算来进行机器学习，并且在前段时间有了突破性的进展。国内方面，阿里和中科院合作成立了量子计算实验室，意图进行量子计算的研究。\n　　如果量子计算发展不力，仍然需要数十年才能使我们的计算能力得以突飞猛进的发展，那么缺少了强大计算能力的神经网络可能会无法一帆风顺的发展下去。这种情况可以类比为80-90年时期神经网络因为计算能力的限制而被低估与忽视。假设量子计算机真的能够与神经网络结合，并且助力真正的人工智能技术的诞生，而且量子计算机发展需要10年的话，那么神经网络可能还有10年的发展期。直到那时期以后，神经网络才能真正接近实现AI这一目标。\n \n图39 量子计算\n \n　　2.人工智能\n　　最后，作者想简单地谈谈对目前人工智能的看法。虽然现在人工智能非常火热，但是距离真正的人工智能还有很大的距离。就拿计算机视觉方向来说，面对稍微复杂一些的场景，以及易于混淆的图像，计算机就可能难以识别。因此，这个方向还有很多的工作要做。\n　　就普通人看来，这么辛苦的做各种实验，以及投入大量的人力就是为了实现一些不及孩童能力的视觉能力，未免有些不值。但是这只是第一步。虽然计算机需要很大的运算量才能完成一个普通人简单能完成的识图工作，但计算机最大的优势在于并行化与批量推广能力。使用计算机以后，我们可以很轻易地将以前需要人眼去判断的工作交给计算机做，而且几乎没有任何的推广成本。这就具有很大的价值。正如火车刚诞生的时候，有人嘲笑它又笨又重，速度还没有马快。但是很快规模化推广的火车就替代了马车的使用。人工智能也是如此。这也是为什么目前世界上各著名公司以及政府都对此热衷的原因。\n　　目前看来，神经网络要想实现人工智能还有很多的路要走，但方向至少是正确的，下面就要看后来者的不断努力了。\n\n图40 人工智能\n \n八 总结\n　　本文回顾了神经网络的发展历史，从神经元开始，历经单层神经网络，两层神经网络，直到多层神经网络。在历史介绍中穿插讲解神经网络的结构，分类效果以及训练方法等。本文说明了神经网络内部实际上就是矩阵计算，在程序中的实现没有“点”和“线”的对象。本文说明了神经网络强大预测能力的根本，就是多层的神经网络可以无限逼近真实的对应函数，从而模拟数据之间的真实关系。除此之外，本文回顾了神经网络发展的历程，分析了神经网络发展的外在原因，包括计算能力的增强，数据的增多，以及方法的创新等。最后，本文对神经网络的未来进行了展望，包括量子计算与神经网络结合的可能性，以及探讨未来人工智能发展的前景与价值。\n \n九. 后记\n　　本篇文章可以视为作者一年来对神经网络的理解与总结，包括实验的体会，书籍的阅读，以及思考的火花等。神经网络虽然重要，但学习并不容易。这主要是由于其结构图较为难懂，以及历史发展的原因，导致概念容易混淆，一些介绍的博客与网站内容新旧不齐。本篇文章着眼于这些问题，没有太多的数学推导，意图以一种简单的，直观的方式对神经网络进行讲解。在2015年最后一天终于写完。希望本文可以对各位有所帮助。\n \n \n　　作者很感谢能够阅读到这里的读者。如果看完觉得好的话，还请轻轻点一下赞，你们的鼓励就是作者继续行文的动力。本文的备注部分是一些对神经网络学习的建议，供补充阅读与参考。\n　　\n　　目前为止，EasyPR的1.4版已经将神经网络（ANN）训练的模块加以开放，开发者们可以使用这个模块来进行自己的字符模型的训练。有兴趣的可以下载。\n \n十. 备注\n　　神经网络虽然很重要，但是对于神经网络的学习，却并不容易。这些学习困难主要来自以下三个方面：概念，类别，教程。下面简单说明这三点。\n　　1.概念\n　　对于一门技术的学习而言，首先最重要的是弄清概念。只有将概念理解清楚，才能顺畅的进行后面的学习。由于神经网络漫长的发展历史，经常会有一些概念容易混淆，让人学习中产生困惑。这里面包括历史的术语，不一致的说法，以及被遗忘的研究等。　\n　　历史的术语\n　　这个的代表就是多层感知器（MLP）这个术语。起初看文献时很难理解的一个问题就是，为什么神经网络又有另一个名称：MLP。其实MLP（Multi-Layer Perceptron）的名称起源于50-60年代的感知器（Perceptron）。由于我们在感知器之上又增加了一个计算层，因此称为多层感知器。值得注意的是，虽然叫“多层”，MLP一般都指的是两层（带一个隐藏层的）神经网络。\n　　MLP这个术语属于历史遗留的产物。现在我们一般就说神经网络，以及深度神经网络。前者代表带一个隐藏层的两层神经网络，也是EasyPR目前使用的识别网络，后者指深度学习的网络。\n　　不一致的说法\n　　这个最明显的代表就是损失函数loss function，这个还有两个说法是跟它完全一致的意思，分别是残差函数error function，以及代价函数cost function。loss function是目前深度学习里用的较多的一种说法，caffe里也是这么叫的。cost function则是Ng在coursera教学视频里用到的统一说法。这三者都是同一个意思，都是优化问题所需要求解的方程。虽然在使用的时候不做规定，但是在听到各种讲解时要心里明白。\n　　再来就是权重weight和参数parameter的说法，神经网络界由于以前的惯例，一般会将训练得到的参数称之为权重，而不像其他机器学习方法就称之为参数。这个需要记住就好。不过在目前的使用惯例中，也有这样一种规定。那就是非偏置节点连接上的值称之为权重，而偏置节点上的值称之为偏置，两者统一起来称之为参数。\n　　另外一个同义词就是激活函数active function和转移函数transfer function了。同样，他们代表一个意思，都是叠加的非线性函数的说法。\n　　被遗忘的研究\n　　由于神经网络发展历史已经有70年的漫长历史，因此在研究过程中，必然有一些研究分支属于被遗忘阶段。这里面包括各种不同的网络，例如SOM（Self-Organizing Map，自组织特征映射网络），SNN（Synergetic Neural Network，协同神经网络），ART（Adaptive Resonance Theory，自适应共振理论网络）等等。所以看历史文献时会看到许多没见过的概念与名词。\n　　有些历史网络甚至会重新成为新的研究热点，例如RNN与LSTM就是80年代左右开始的研究，目前已经是深度学习研究中的重要一门技术，在语音与文字识别中有很好的效果。　\n　　对于这些易于混淆以及弄错的概念，务必需要多方参考文献，理清上下文，这样才不会在学习与阅读过程中迷糊。\n　　2.类别\n　　下面谈一下关于神经网络中的不同类别。\n　　其实本文的名字“神经网络浅讲”并不合适，因为本文并不是讲的是“神经网络”的内容，而是其中的一个子类，也是目前最常说的前馈神经网络。根据下图的分类可以看出。\n \n图41 神经网络的类别\n \n　　神经网络其实是一个非常宽泛的称呼，它包括两类，一类是用计算机的方式去模拟人脑，这就是我们常说的ANN（人工神经网络），另一类是研究生物学上的神经网络，又叫生物神经网络。对于我们计算机人士而言，肯定是研究前者。\n　　在人工神经网络之中，又分为前馈神经网络和反馈神经网络这两种。那么它们两者的区别是什么呢？这个其实在于它们的结构图。我们可以把结构图看作是一个有向图。其中神经元代表顶点，连接代表有向边。对于前馈神经网络中，这个有向图是没有回路的。你可以仔细观察本文中出现的所有神经网络的结构图，确认一下。而对于反馈神经网络中，结构图的有向图是有回路的。反馈神经网络也是一类重要的神经网络。其中Hopfield网络就是反馈神经网络。深度学习中的RNN也属于一种反馈神经网络。\n　　具体到前馈神经网络中，就有了本文中所分别描述的三个网络：单层神经网络，双层神经网络，以及多层神经网络。深度学习中的CNN属于一种特殊的多层神经网络。另外，在一些Blog中和文献中看到的BP神经网络是什么？其实它们就是使用了反向传播BP算法的两层前馈神经网络。也是最普遍的一种两层神经网络。\n　　通过以上分析可以看出，神经网络这种说法其实是非常广义的，具体在文章中说的是什么网络，需要根据文中的内容加以区分。\n　　3.教程\n　　如何更好的学习神经网络，认真的学习一门课程或者看一本著作都是很有必要的。\n　　说到网络教程的话，这里必须说一下Ng的机器学习课程。对于一个初学者而言，Ng的课程视频是非常有帮助的。Ng一共开设过两门机器学习公开课程：一个是2003年在Standford开设的，面向全球的学生，这个视频现在可以在网易公开课上找到；另一个是2010年专门为Coursera上的用户开设的，需要登陆Coursera上才能学习。\n　　但是，需要注意点是，这两个课程对待神经网络的态度有点不同。早些的课程一共有20节课，Ng花了若干节课去专门讲SVM以及SVM的推导，而当时的神经网络，仅仅放了几段视频，花了大概不到20分钟（一节课60分钟左右）。而到了后来的课程时，总共10节的课程中，Ng给了完整的两节给神经网络，详细介绍了神经网络的反向传播算法。同时给SVM只有一节课，并且没有再讲SVM的推导过程。下面两张图分别是Ng介绍神经网络的开篇，可以大致看出一些端倪。\n \n图42 Ng与神经网络\n \n　　为什么Ng对待神经网络的反应前后相差那么大？事实上就是深度学习的原因。Ng实践了深度学习的效果，认识到深度学习的基础--神经网络的重要性。这就是他在后面重点介绍神经网络的原因。总之，对于神经网络的学习而言，我更推荐Coursera上的。因为在那个时候，Ng才是真正的把神经网络作为一门重要的机器学习方法去传授。你可以从他上课的态度中感受到他的重视，以及他希望你能学好的期望。\n \n版权说明：\n　　本文中的所有文字，图片，代码的版权都是属于作者和博客园共同所有。欢迎转载，但是务必注明作者与出处。任何未经允许的剽窃以及爬虫抓取都属于侵权，作者和博客园保留所有权利。\n  \n参考文献：\n　　1.Neural Networks\n　　2.Andrew Ng Neural Networks \n　　3.神经网络简史\n　　4.中科院 史忠植 神经网络 讲义\n　　5.深度学习 胡晓林\n \n\n\n\n"},"机器学习笔记/从机器学习谈起":{"slug":"机器学习笔记/从机器学习谈起","filePath":"机器学习笔记/从机器学习谈起.md","title":"【转】从机器学习谈起","links":[],"tags":["机器学习"],"content":"从决定向AI方向转已经过了半年了，书买了好多，还没有拆封，四月了，这次是必须下决心了，从博客搭建开始，记录学习过程，首先从这里开始，是最初看了这篇博客，开始了解机器学习，感谢博主的帮助\n博客原地址：\r\nwww.cnblogs.com/subconscious/p/4107357.html#first\n作者：计算机的潜意识\n从机器学习谈起\r\n\r\n\r\n\r\n　　在本篇文章中，我将对机器学习做个概要的介绍。本文的目的是能让即便完全不了解机器学习的人也能了解机器学习，并且上手相关的实践。这篇文档也算是EasyPR开发的番外篇，从这里开始，必须对机器学习了解才能进一步介绍EasyPR的内核。当然，本文也面对一般读者，不会对阅读有相关的前提要求。\n　　在进入正题前，我想读者心中可能会有一个疑惑：机器学习有什么重要性，以至于要阅读完这篇非常长的文章呢？\n　　我并不直接回答这个问题前。相反，我想请大家看两张图，下图是图一：\n 图1 机器学习界的执牛耳者与互联网界的大鳄的联姻　　\n　　这幅图上上的三人是当今机器学习界的执牛耳者。中间的是Geoffrey Hinton, 加拿大多伦多大学的教授，如今被聘为“Google大脑”的负责人。右边的是Yann LeCun, 纽约大学教授，如今是Facebook人工智能实验室的主任。而左边的大家都很熟悉，Andrew Ng，中文名吴恩达，斯坦福大学副教授，如今也是“百度大脑”的负责人与百度首席科学家。这三位都是目前业界炙手可热的大牛，被互联网界大鳄求贤若渴的聘请，足见他们的重要性。而他们的研究方向，则全部都是机器学习的子类--深度学习。　　下图是图二：\n图2 语音助手产品\n　　这幅图上描述的是什么？Windows Phone上的语音助手Cortana，名字来源于《光环》中士官长的助手。相比其他竞争对手，微软很迟才推出这个服务。Cortana背后的核心技术是什么，为什么它能够听懂人的语音？事实上，这个技术正是机器学习。机器学习是所有语音助手产品(包括Apple的siri与Google的Now)能够跟人交互的关键技术。\n　　通过上面两图，我相信大家可以看出机器学习似乎是一个很重要的，有很多未知特性的技术。学习它似乎是一件有趣的任务。实际上，学习机器学习不仅可以帮助我们了解互联网界最新的趋势，同时也可以知道伴随我们的便利服务的实现技术。\n　　机器学习是什么，为什么它能有这么大的魔力，这些问题正是本文要回答的。同时，本文叫做“从机器学习谈起”，因此会以漫谈的形式介绍跟机器学习相关的所有内容，包括学科(如数据挖掘、计算机视觉等)，算法(神经网络，svm)等等。本文的主要目录如下：　　1.一个故事说明什么是机器学习　　2.机器学习的定义　　3.机器学习的范围　　4.机器学习的方法　　5.机器学习的应用--大数据　　6.机器学习的子类--深度学习　　7.机器学习的父类--人工智能　　8.机器学习的思考--计算机的潜意识　　9.总结　　10.后记\n1.一个故事说明什么是机器学习\n　　机器学习这个词是让人疑惑的，首先它是英文名称Machine Learning(简称ML)的直译，在计算界Machine一般指计算机。这个名字使用了拟人的手法，说明了这门技术是让机器“学习”的技术。但是计算机是死的，怎么可能像人类一样“学习”呢？　　传统上如果我们想让计算机工作，我们给它一串指令，然后它遵照这个指令一步步执行下去。有因有果，非常明确。但这样的方式在机器学习中行不通。机器学习根本不接受你输入的指令，相反，它接受你输入的数据! 也就是说，机器学习是一种让计算机利用数据而不是指令来进行各种工作的方法。这听起来非常不可思议，但结果上却是非常可行的。“统计”思想将在你学习“机器学习”相关理念时无时无刻不伴随，相关而不是因果的概念将是支撑机器学习能够工作的核心概念。你会颠覆对你以前所有程序中建立的因果无处不在的根本理念。　　下面我通过一个故事来简单地阐明什么是机器学习。这个故事比较适合用在知乎上作为一个概念的阐明。在这里，这个故事没有展开，但相关内容与核心是存在的。如果你想简单的了解一下什么是机器学习，那么看完这个故事就足够了。如果你想了解机器学习的更多知识以及与它关联紧密的当代技术，那么请你继续往下看，后面有更多的丰富的内容。　　这个例子来源于我真实的生活经验，我在思考这个问题的时候突然发现它的过程可以被扩充化为一个完整的机器学习的过程，因此我决定使用这个例子作为所有介绍的开始。这个故事称为“等人问题”。　　我相信大家都有跟别人相约，然后等人的经历。现实中不是每个人都那么守时的，于是当你碰到一些爱迟到的人，你的时间不可避免的要浪费。我就碰到过这样的一个例子。　　对我的一个朋友小Y而言，他就不是那么守时，最常见的表现是他经常迟到。当有一次我跟他约好3点钟在某个麦当劳见面时，在我出门的那一刻我突然想到一个问题：我现在出发合适么？我会不会又到了地点后，花上30分钟去等他？我决定采取一个策略解决这个问题。　　要想解决这个问题，有好几种方法。第一种方法是采用知识：我搜寻能够解决这个问题的知识。但很遗憾，没有人会把如何等人这个问题作为知识传授，因此我不可能找到已有的知识能够解决这个问题。第二种方法是问他人：我去询问他人获得解决这个问题的能力。但是同样的，这个问题没有人能够解答，因为可能没人碰上跟我一样的情况。第三种方法是准则法：我问自己的内心，我有否设立过什么准则去面对这个问题？例如，无论别人如何，我都会守时到达。但我不是个死板的人，我没有设立过这样的规则。　　事实上，我相信有种方法比以上三种都合适。我把过往跟小Y相约的经历在脑海中重现一下，看看跟他相约的次数中，迟到占了多大的比例。而我利用这来预测他这次迟到的可能性。如果这个值超出了我心里的某个界限，那我选择等一会再出发。假设我跟小Y约过5次，他迟到的次数是1次，那么他按时到的比例为80%，我心中的阈值为70%，我认为这次小Y应该不会迟到，因此我按时出门。如果小Y在5次迟到的次数中占了4次，也就是他按时到达的比例为20%，由于这个值低于我的阈值，因此我选择推迟出门的时间。这个方法从它的利用层面来看，又称为经验法。在经验法的思考过程中，我事实上利用了以往所有相约的数据。因此也可以称之为依据数据做的判断。　　依据数据所做的判断跟机器学习的思想根本上是一致的。　　刚才的思考过程我只考虑“频次”这种属性。在真实的机器学习中，这可能都不算是一个应用。一般的机器学习模型至少考虑两个量：一个是因变量，也就是我们希望预测的结果，在这个例子里就是小Y迟到与否的判断。另一个是自变量，也就是用来预测小Y是否迟到的量。假设我把时间作为自变量，譬如我发现小Y所有迟到的日子基本都是星期五，而在非星期五情况下他基本不迟到。于是我可以建立一个模型，来模拟小Y迟到与否跟日子是否是星期五的概率。见下图：\n \n\n图3 决策树模型\n　　这样的图就是一个最简单的机器学习模型，称之为决策树。　　当我们考虑的自变量只有一个时，情况较为简单。如果把我们的自变量再增加一个。例如小Y迟到的部分情况时是在他开车过来的时候(你可以理解为他开车水平较臭，或者路较堵)。于是我可以关联考虑这些信息。建立一个更复杂的模型，这个模型包含两个自变量与一个因变量。　　再更复杂一点，小Y的迟到跟天气也有一定的原因，例如下雨的时候，这时候我需要考虑三个自变量。　　如果我希望能够预测小Y迟到的具体时间，我可以把他每次迟到的时间跟雨量的大小以及前面考虑的自变量统一建立一个模型。于是我的模型可以预测值，例如他大概会迟到几分钟。这样可以帮助我更好的规划我出门的时间。在这样的情况下，决策树就无法很好地支撑了，因为决策树只能预测离散值。我们可以用节2所介绍的线型回归方法建立这个模型。　　如果我把这些建立模型的过程交给电脑。比如把所有的自变量和因变量输入，然后让计算机帮我生成一个模型，同时让计算机根据我当前的情况，给出我是否需要迟出门，需要迟几分钟的建议。那么计算机执行这些辅助决策的过程就是机器学习的过程。　　机器学习方法是计算机利用已有的数据(经验)，得出了某种模型(迟到的规律)，并利用此模型预测未来(是否迟到)的一种方法。　　通过上面的分析，可以看出机器学习与人类思考的经验过程是类似的，不过它能考虑更多的情况，执行更加复杂的计算。事实上，机器学习的一个主要目的就是把人类思考归纳经验的过程转化为计算机通过对数据的处理计算得出模型的过程。经过计算机得出的模型能够以近似于人的方式解决很多灵活复杂的问题。　　下面，我会开始对机器学习的正式介绍，包括定义、范围，方法、应用等等，都有所包含。\n \n2.机器学习的定义　　从广义上来说，机器学习是一种能够赋予机器学习的能力以此让它完成直接编程无法完成的功能的方法。但从实践的意义上来说，机器学习是一种通过利用数据，训练出模型，然后使用模型预测的一种方法。\n　　让我们具体看一个例子。\n图4 房价的例子\n　　拿国民话题的房子来说。现在我手里有一栋房子需要售卖，我应该给它标上多大的价格？房子的面积是100平方米，价格是100万，120万，还是140万？\n　　很显然，我希望获得房价与面积的某种规律。那么我该如何获得这个规律？用报纸上的房价平均数据么？还是参考别人面积相似的？无论哪种，似乎都并不是太靠谱。\n　　我现在希望获得一个合理的，并且能够最大程度的反映面积与房价关系的规律。于是我调查了周边与我房型类似的一些房子，获得一组数据。这组数据中包含了大大小小房子的面积与价格，如果我能从这组数据中找出面积与价格的规律，那么我就可以得出房子的价格。\n　　对规律的寻找很简单，拟合出一条直线，让它“穿过”所有的点，并且与各个点的距离尽可能的小。\n　　通过这条直线，我获得了一个能够最佳反映房价与面积规律的规律。这条直线同时也是一个下式所表明的函数：\n\n　　房价 = 面积 * a + b\n\n　　上述中的a、b都是直线的参数。获得这些参数以后，我就可以计算出房子的价格。\n　　假设a = 0.75,b = 50，则房价 = 100 * 0.75 + 50 = 125万。这个结果与我前面所列的100万，120万，140万都不一样。由于这条直线综合考虑了大部分的情况，因此从“统计”意义上来说，这是一个最合理的预测。　　在求解过程中透露出了两个信息：　　1.房价模型是根据拟合的函数类型决定的。如果是直线，那么拟合出的就是直线方程。如果是其他类型的线，例如抛物线，那么拟合出的就是抛物线方程。机器学习有众多算法，一些强力算法可以拟合出复杂的非线性模型，用来反映一些不是直线所能表达的情况。　　2.如果我的数据越多，我的模型就越能够考虑到越多的情况，由此对于新情况的预测效果可能就越好。这是机器学习界“数据为王”思想的一个体现。一般来说(不是绝对)，数据越多，最后机器学习生成的模型预测的效果越好。\n　　通过我拟合直线的过程，我们可以对机器学习过程做一个完整的回顾。首先，我们需要在计算机中存储历史的数据。接着，我们将这些 数据通过机器学习算法进行处理，这个过程在机器学习中叫做“训练”，处理的结果可以被我们用来对新的数据进行预测，这个结果一般称之为“模型”。对新数据 的预测过程在机器学习中叫做“预测”。“训练”与“预测”是机器学习的两个过程，“模型”则是过程的中间输出结果，“训练”产生“模型”，“模型”指导 “预测”。\n　　让我们把机器学习的过程与人类对历史经验归纳的过程做个比对。\n\n图5 机器学习与人类思考的类比\n　　人类在成长、生活过程中积累了很多的历史与经验。人类定期地对这些经验进行“归纳”，获得了生活的“规律”。当人类遇到未知的问题或者需要对未来进行“推测”的时候，人类使用这些“规律”，对未知问题与未来进行“推测”，从而指导自己的生活和工作。\n　　机器学习中的“训练”与“预测”过程可以对应到人类的“归纳”和“推测”过程。通过这样的对应，我们可以发现，机器学习的思想并不复杂，仅仅是对人类在生活中学习成长的一个模拟。由于机器学习不是基于编程形成的结果，因此它的处理过程不是因果的逻辑，而是通过归纳思想得出的相关性结论。\n 　　这也可以联想到人类为什么要学习历史，历史实际上是人类过往经验的总结。有句话说得很好，“历史往往不一样，但历史总是惊人的相似”。通过学习历史，我们从历史中归纳出人生与国家的规律，从而指导我们的下一步工作，这是具有莫大价值的。当代一些人忽视了历史的本来价值，而是把其作为一种宣扬功绩的手段，这其实是对历史真实价值的一种误用。\n　　\n3.机器学习的范围\n　　上文虽然说明了机器学习是什么，但是并没有给出机器学习的范围。\n　　其实，机器学习跟模式识别，统计学习，数据挖掘，计算机视觉，语音识别，自然语言处理等领域有着很深的联系。\n　　从范围上来说，机器学习跟模式识别，统计学习，数据挖掘是类似的，同时，机器学习与其他领域的处理技术的结合，形成了计算机视觉、语音识别、自然语言处理等交叉学科。因此，一般说数据挖掘时，可以等同于说机器学习。同时，我们平常所说的机器学习应用，应该是通用的，不仅仅局限在结构化数据，还有图像，音频等应用。　　在这节对机器学习这些相关领域的介绍有助于我们理清机器学习的应用场景与研究范围，更好的理解后面的算法与应用层次。\n　　下图是机器学习所牵扯的一些相关范围的学科与研究领域。\n图6 机器学习与相关学科\n　　模式识别　　模式识别=机器学习。两者的主要区别在于前者是从工业界发展起来的概念，后者则主要源自计算机学科。在著名的《Pattern Recognition And Machine Learning》这本书中，Christopher M. Bishop在开头是这样说的“模式识别源自工业界，而机器学习来自于计算机学科。不过，它们中的活动可以被视为同一个领域的两个方面，同时在过去的10年间，它们都有了长足的发展”。　　　　数据挖掘　　数据挖掘=机器学习+数据库。这几年数据挖掘的概念实在是太耳熟能详。几乎等同于炒作。但凡说数据挖掘都会吹嘘数据挖掘如何如何，例如从数据中挖出金子，以及将废弃的数据转化为价值等等。但是，我尽管可能会挖出金子，但我也可能挖的是“石头”啊。这个说法的意思是，数据挖掘仅仅是一种思考方式，告诉我们应该尝试从数据中挖掘出知识，但不是每个数据都能挖掘出金子的，所以不要神话它。一个系统绝对不会因为上了一个数据挖掘模块就变得无所不能(这是IBM最喜欢吹嘘的)，恰恰相反，一个拥有数据挖掘思维的人员才是关键，而且他还必须对数据有深刻的认识，这样才可能从数据中导出模式指引业务的改善。大部分数据挖掘中的算法是机器学习的算法在数据库中的优化。　　统计学习　　统计学习近似等于机器学习。统计学习是个与机器学习高度重叠的学科。因为机器学习中的大多数方法来自统计学，甚至可以认为，统计学的发展促进机器学习的繁荣昌盛。例如著名的支持向量机算法，就是源自统计学科。但是在某种程度上两者是有分别的，这个分别在于：统计学习者重点关注的是统计模型的发展与优化，偏数学，而机器学习者更关注的是能够解决问题，偏实践，因此机器学习研究者会重点研究学习算法在计算机上执行的效率与准确性的提升。　　　　　　计算机视觉　　计算机视觉=图像处理+机器学习。图像处理技术用于将图像处理为适合进入机器学习模型中的输入，机器学习则负责从图像中识别出相关的模式。计算机视觉相关的应用非常的多，例如百度识图、手写字符识别、车牌识别等等应用。这个领域是应用前景非常火热的，同时也是研究的热门方向。随着机器学习的新领域深度学习的发展，大大促进了计算机图像识别的效果，因此未来计算机视觉界的发展前景不可估量。　　　　语音识别　　语音识别=语音处理+机器学习。语音识别就是音频处理技术与机器学习的结合。语音识别技术一般不会单独使用，一般会结合自然语言处理的相关技术。目前的相关应用有苹果的语音助手siri等。　　自然语言处理　　自然语言处理=文本处理+机器学习。自然语言处理技术主要是让机器理解人类的语言的一门领域。在自然语言处理技术中，大量使用了编译原理相关的技术，例如词法分析，语法分析等等，除此之外，在理解这个层面，则使用了语义理解，机器学习等技术。作为唯一由人类自身创造的符号，自然语言处理一直是机器学习界不断研究的方向。按照百度机器学习专家余凯的说法“听与看，说白了就是阿猫和阿狗都会的，而只有语言才是人类独有的”。如何利用机器学习技术进行自然语言的的深度理解，一直是工业和学术界关注的焦点。　　可以看出机器学习在众多领域的外延和应用。机器学习技术的发展促使了很多智能领域的进步，改善着我们的生活。\n \n4.机器学习的方法\n　　通过上节的介绍我们知晓了机器学习的大致范围，那么机器学习里面究竟有多少经典的算法呢？在这个部分我会简要介绍一下机器学习中的经典代表方法。这部分介绍的重点是这些方法内涵的思想，数学与实践细节不会在这讨论。　　1、回归算法\n　　在大部分机器学习课程中，回归算法都是介绍的第一个算法。原因有两个：一.回归算法比较简单，介绍它可以让人平滑地从统计学迁移到机器学习中。二.回归算法是后面若干强大算法的基石，如果不理解回归算法，无法学习那些强大的算法。回归算法有两个重要的子类：即线性回归和逻辑回归。\n　　线性回归就是我们前面说过的房价求解问题。如何拟合出一条直线最佳匹配我所有的数据？一般使用“最小二乘法”来求解。“最小二乘法”的思想是这样的，假设我们拟合出的直线代表数据的真实值，而观测到的数据代表拥有误差的值。为了尽可能减小误差的影响，需要求解一条直线使所有误差的平方和最小。最小二乘法将最优问题转化为求函数极值问题。函数极值在数学上我们一般会采用求导数为0的方法。但这种做法并不适合计算机，可能求解不出来，也可能计算量太大。\n　　计算机科学界专门有一个学科叫“数值计算”，专门用来提升计算机进行各类计算时的准确性和效率问题。例如，著名的“梯度下降”以及“牛顿法”就是数值计算中的经典算法，也非常适合来处理求解函数极值的问题。梯度下降法是解决回归模型中最简单且有效的方法之一。从严格意义上来说，由于后文中的神经网络和推荐算法中都有线性回归的因子，因此梯度下降法在后面的算法实现中也有应用。　　逻辑回归是一种与线性回归非常类似的算法，但是，从本质上讲，线型回归处理的问题类型与逻辑回归不一致。线性回归处理的是数值问题，也就是最后预测出的结果是数字，例如房价。而逻辑回归属于分类算法，也就是说，逻辑回归预测结果是离散的分类，例如判断这封邮件是否是垃圾邮件，以及用户是否会点击此广告等等。\n　　实现方面的话，逻辑回归只是对对线性回归的计算结果加上了一个Sigmoid函数，将数值结果转化为了0到1之间的概率(Sigmoid函数的图像一般来说并不直观，你只需要理解对数值越大，函数越逼近1，数值越小，函数越逼近0)，接着我们根据这个概率可以做预测，例如概率大于0.5，则这封邮件就是垃圾邮件，或者肿瘤是否是恶性的等等。从直观上来说，逻辑回归是画出了一条分类线，见下图。\n　　图7 逻辑回归的直观解释\n　　假设我们有一组肿瘤患者的数据，这些患者的肿瘤中有些是良性的(图中的蓝色点)，有些是恶性的(图中的红色点)。这里肿瘤的红蓝色可以被称作数据的“标签”。同时每个数据包括两个“特征”：患者的年龄与肿瘤的大小。我们将这两个特征与标签映射到这个二维空间上，形成了我上图的数据。　　当我有一个绿色的点时，我该判断这个肿瘤是恶性的还是良性的呢？根据红蓝点我们训练出了一个逻辑回归模型，也就是图中的分类线。这时，根据绿点出现在分类线的左侧，因此我们判断它的标签应该是红色，也就是说属于恶性肿瘤。　　逻辑回归算法划出的分类线基本都是线性的(也有划出非线性分类线的逻辑回归，不过那样的模型在处理数据量较大的时候效率会很低)，这意味着当两类之间的界线不是线性时，逻辑回归的表达能力就不足。下面的两个算法是机器学习界最强大且重要的算法，都可以拟合出非线性的分类线。　　2、神经网络\n　　神经网络(也称之为人工神经网络，ANN)算法是80年代机器学习界非常流行的算法，不过在90年代中途衰落。现在，携着“深度学习”之势，神经网络重装归来，重新成为最强大的机器学习算法之一。　　神经网络的诞生起源于对大脑工作机理的研究。早期生物界学者们使用神经网络来模拟大脑。机器学习的学者们使用神经网络进行机器学习的实验，发现在视觉与语音的识别上效果都相当好。在BP算法(加速神经网络训练过程的数值算法)诞生以后，神经网络的发展进入了一个热潮。BP算法的发明人之一是前面介绍的机器学习大牛Geoffrey Hinton(图1中的中间者)。　　具体说来，神经网络的学习机理是什么？简单来说，就是分解与整合。在著名的Hubel-Wiesel试验中，学者们研究猫的视觉分析机理是这样的。\n　　 图8 Hubel-Wiesel试验与大脑视觉机理\n　　比方说，一个正方形，分解为四个折线进入视觉处理的下一层中。四个神经元分别处理一个折线。每个折线再继续被分解为两条直线，每条直线再被分解为黑白两个面。于是，一个复杂的图像变成了大量的细节进入神经元，神经元处理以后再进行整合，最后得出了看到的是正方形的结论。这就是大脑视觉识别的机理，也是神经网络工作的机理。　　让我们看一个简单的神经网络的逻辑架构。在这个网络中，分成输入层，隐藏层，和输出层。输入层负责接收信号，隐藏层负责对数据的分解与处理，最后的结果被整合到输出层。每层中的一个圆代表一个处理单元，可以认为是模拟了一个神经元，若干个处理单元组成了一个层，若干个层再组成了一个网络，也就是&quot;神经网络&quot;。\n图9 神经网络的逻辑架构\n　　在神经网络中，每个处理单元事实上就是一个逻辑回归模型，逻辑回归模型接收上层的输入，把模型的预测结果作为输出传输到下一个层次。通过这样的过程，神经网络可以完成非常复杂的非线性分类。\n　　下图会演示神经网络在图像识别领域的一个著名应用，这个程序叫做LeNet，是一个基于多个隐层构建的神经网络。通过LeNet可以识别多种手写数字，并且达到很高的识别精度与拥有较好的鲁棒性。\n\n图10 LeNet的效果展示\n　　右下方的方形中显示的是输入计算机的图像，方形上方的红色字样“answer”后面显示的是计算机的输出。左边的三条竖直的图像列显示的是神经网络中三个隐藏层的输出，可以看出，随着层次的不断深入，越深的层次处理的细节越低，例如层3基本处理的都已经是线的细节了。LeNet的发明人就是前文介绍过的机器学习的大牛Yann LeCun(图1右者)。\n　　进入90年代，神经网络的发展进入了一个瓶颈期。其主要原因是尽管有BP算法的加速，神经网络的训练过程仍然很困难。因此90年代后期支持向量机(SVM)算法取代了神经网络的地位。　　3、SVM（支持向量机）\n　　支持向量机算法是诞生于统计学习界，同时在机器学习界大放光彩的经典算法。\n　　支持向量机算法从某种意义上来说是逻辑回归算法的强化：通过给予逻辑回归算法更严格的优化条件，支持向量机算法可以获得比逻辑回归更好的分类界线。但是如果没有某类函数技术，则支持向量机算法最多算是一种更好的线性分类技术。\n　　但是，通过跟高斯“核”的结合，支持向量机可以表达出非常复杂的分类界线，从而达成很好的的分类效果。“核”事实上就是一种特殊的函数，最典型的特征就是可以将低维的空间映射到高维的空间。\n　　例如下图所示：\n          \n 图11 支持向量机图例\n　　我们如何在二维平面划分出一个圆形的分类界线？在二维平面可能会很困难，但是通过“核”可以将二维空间映射到三维空间，然后使用一个线性平面就可以达成类似效果。也就是说，二维平面划分出的非线性分类界线可以等价于三维平面的线性分类界线。于是，我们可以通过在三维空间中进行简单的线性划分就可以达到在二维平面中的非线性划分效果。\n 图12 三维空间的切割\n　　支持向量机是一种数学成分很浓的机器学习算法（相对的，神经网络则有生物科学成分）。在算法的核心步骤中，有一步证明，即将数据从低维映射到高维不会带来最后计算复杂性的提升。于是，通过支持向量机算法，既可以保持计算效率，又可以获得非常好的分类效果。因此支持向量机在90年代后期一直占据着机器学习中最核心的地位，基本取代了神经网络算法。直到现在神经网络借着深度学习重新兴起，两者之间才又发生了微妙的平衡转变。　　4、聚类算法\n　　前面的算法中的一个显著特征就是我的训练数据中包含了标签，训练出的模型可以对其他未知数据预测标签。在下面的算法中，训练数据都是不含标签的，而算法的目的则是通过训练，推测出这些数据的标签。这类算法有一个统称，即无监督算法(前面有标签的数据的算法则是有监督算法)。无监督算法中最典型的代表就是聚类算法。　　让我们还是拿一个二维的数据来说，某一个数据包含两个特征。我希望通过聚类算法，给他们中不同的种类打上标签，我该怎么做呢？简单来说，聚类算法就是计算种群中的距离，根据距离的远近将数据划分为多个族群。　　聚类算法中最典型的代表就是K-Means算法。\n　　5、降维算法\n　　降维算法也是一种无监督学习算法，其主要特征是将数据从高维降低到低维层次。在这里，维度其实表示的是数据的特征量的大小，例如，房价包含房子的长、宽、面积与房间数量四个特征，也就是维度为4维的数据。可以看出来，长与宽事实上与面积表示的信息重叠了，例如面积=长 × 宽。通过降维算法我们就可以去除冗余信息，将特征减少为面积与房间数量两个特征，即从4维的数据压缩到2维。于是我们将数据从高维降低到低维，不仅利于表示，同时在计算上也能带来加速。　　刚才说的降维过程中减少的维度属于肉眼可视的层次，同时压缩也不会带来信息的损失(因为信息冗余了)。如果肉眼不可视，或者没有冗余的特征，降维算法也能工作，不过这样会带来一些信息的损失。但是，降维算法可以从数学上证明，从高维压缩到的低维中最大程度地保留了数据的信息。因此，使用降维算法仍然有很多的好处。　　降维算法的主要作用是压缩数据与提升机器学习其他算法的效率。通过降维算法，可以将具有几千个特征的数据压缩至若干个特征。另外，降维算法的另一个好处是数据的可视化，例如将5维的数据压缩至2维，然后可以用二维平面来可视。降维算法的主要代表是PCA算法(即主成分分析算法)。　　6、推荐算法\n　　推荐算法是目前业界非常火的一种算法，在电商界，如亚马逊，天猫，京东等得到了广泛的运用。推荐算法的主要特征就是可以自动向用户推荐他们最感兴趣的东西，从而增加购买率，提升效益。推荐算法有两个主要的类别：\n　　一类是基于物品内容的推荐，是将与用户购买的内容近似的物品推荐给用户，这样的前提是每个物品都得有若干个标签，因此才可以找出与用户购买物品类似的物品，这样推荐的好处是关联程度较大，但是由于每个物品都需要贴标签，因此工作量较大。\n　　另一类是基于用户相似度的推荐，则是将与目标用户兴趣相同的其他用户购买的东西推荐给目标用户，例如小A历史上买了物品B和C，经过算法分析，发现另一个与小A近似的用户小D购买了物品E，于是将物品E推荐给小A。\n　　两类推荐都有各自的优缺点，在一般的电商应用中，一般是两类混合使用。推荐算法中最有名的算法就是协同过滤算法。　　7、其他\n　　除了以上算法之外，机器学习界还有其他的如高斯判别，朴素贝叶斯，决策树等等算法。但是上面列的六个算法是使用最多，影响最广，种类最全的典型。机器学习界的一个特色就是算法众多，发展百花齐放。　　下面做一个总结，按照训练的数据有无标签，可以将上面算法分为监督学习算法和无监督学习算法，但推荐算法较为特殊，既不属于监督学习，也不属于非监督学习，是单独的一类。\n　　监督学习算法：　　线性回归，逻辑回归，神经网络，SVM　　无监督学习算法：　　聚类算法，降维算法　　特殊算法：　　推荐算法\n　　除了这些算法以外，有一些算法的名字在机器学习领域中也经常出现。但他们本身并不算是一个机器学习算法，而是为了解决某个子问题而诞生的。你可以理解他们为以上算法的子算法，用于大幅度提高训练过程。其中的代表有：梯度下降法，主要运用在线型回归，逻辑回归，神经网络，推荐算法中；牛顿法，主要运用在线型回归中；BP算法，主要运用在神经网络中；SMO算法，主要运用在SVM中。\n5.机器学习的应用--大数据　　说完机器学习的方法，下面要谈一谈机器学习的应用了。无疑，在2010年以前，机器学习的应用在某些特定领域发挥了巨大的作用，如车牌识别，网络攻击防范，手写字符识别等等。但是，从2010年以后，随着大数据概念的兴起，机器学习大量的应用都与大数据高度耦合，几乎可以认为大数据是机器学习应用的最佳场景。　　譬如，但凡你能找到的介绍大数据魔力的文章，都会说大数据如何准确准确预测到了某些事。例如经典的Google利用大数据预测了H1N1在美国某小镇的爆发。\n　\n图13 Google成功预测H1N1\n　　百度预测2014年世界杯，从淘汰赛到决赛全部预测正确。\n图14 百度世界杯成功预测了所有比赛结果\n　　这些实在太神奇了，那么究竟是什么原因导致大数据具有这些魔力的呢？简单来说，就是机器学习技术。正是基于机器学习技术的应用，数据才能发挥其魔力。\n　　大数据的核心是利用数据的价值，机器学习是利用数据价值的关键技术，对于大数据而言，机器学习是不可或缺的。相反，对于机器学习而言，越多的数据会越 可能提升模型的精确性，同时，复杂的机器学习算法的计算时间也迫切需要分布式计算与内存计算这样的关键技术。因此，机器学习的兴盛也离不开大数据的帮助。 大数据与机器学习两者是互相促进，相依相存的关系。\n　　机器学习与大数据紧密联系。但是，必须清醒的认识到，大数据并不等同于机器学习，同理，机器学习也不等同于大数据。大数据中包含有分布式计算，内存数据库，多维分析等等多种技术。单从分析方法来看，大数据也包含以下四种分析方法：　　1.大数据，小分析：即数据仓库领域的OLAP分析思路，也就是多维分析思想。　　2.大数据，大分析：这个代表的就是数据挖掘与机器学习分析法。　　3.流式分析：这个主要指的是事件驱动架构。　　4.查询分析：经典代表是NoSQL数据库。　　也就是说，机器学习仅仅是大数据分析中的一种而已。尽管机器学习的一些结果具有很大的魔力，在某种场合下是大数据价值最好的说明。但这并不代表机器学习是大数据下的唯一的分析方法。　　机器学习与大数据的结合产生了巨大的价值。基于机器学习技术的发展，数据能够“预测”。对人类而言，积累的经验越丰富，阅历也广泛，对未来的判断越准确。例如常说的“经验丰富”的人比“初出茅庐”的小伙子更有工作上的优势，就在于经验丰富的人获得的规律比他人更准确。而在机器学习领域，根据著名的一个实验，有效的证实了机器学习界一个理论：即机器学习模型的数据越多，机器学习的预测的效率就越好。见下图：\n\n\n图15 机器学习准确率与数据的关系\n　　通过这张图可以看出，各种不同算法在输入的数据量达到一定级数后，都有相近的高准确度。于是诞生了机器学习界的名言：成功的机器学习应用不是拥有最好的算法，而是拥有最多的数据！　　在大数据的时代，有好多优势促使机器学习能够应用更广泛。例如随着物联网和移动设备的发展，我们拥有的数据越来越多，种类也包括图片、文本、视频等非结构化数据，这使得机器学习模型可以获得越来越多的数据。同时大数据技术中的分布式计算Map-Reduce使得机器学习的速度越来越快，可以更方便的使用。种种优势使得在大数据时代，机器学习的优势可以得到最佳的发挥。\n6.机器学习的子类--深度学习　　近来，机器学习的发展产生了一个新的方向，即“深度学习”。　　虽然深度学习这四字听起来颇为高大上，但其理念却非常简单，就是传统的神经网络发展到了多隐藏层的情况。　　在上文介绍过，自从90年代以后，神经网络已经消寂了一段时间。但是BP算法的发明人Geoffrey Hinton一直没有放弃对神经网络的研究。由于神经网络在隐藏层扩大到两个以上，其训练速度就会非常慢，因此实用性一直低于支持向量机。2006年，Geoffrey Hinton在科学杂志《Science》上发表了一篇文章，论证了两个观点：\n　　1.多隐层的神经网络具有优异的特征学习能力，学习得到的特征对数据有更本质的刻画，从而有利于可视化或分类；\n　　2.深度神经网络在训练上的难度，可以通过“逐层初始化” 来有效克服。\n图16 Geoffrey Hinton与他的学生在Science上发表文章\n　　通过这样的发现，不仅解决了神经网络在计算上的难度，同时也说明了深层神经网络在学习上的优异性。从此，神经网络重新成为了机器学习界中的主流强大学习技术。同时，具有多个隐藏层的神经网络被称为深度神经网络，基于深度神经网络的学习研究称之为深度学习。　　由于深度学习的重要性质，在各方面都取得极大的关注，按照时间轴排序，有以下四个标志性事件值得一说：　　2012年6月，《纽约时报》披露了Google Brain项目，这个项目是由Andrew Ng和Map-Reduce发明人Jeff Dean共同主导，用16000个CPU Core的并行计算平台训练一种称为“深层神经网络”的机器学习模型，在语音识别和图像识别等领域获得了巨大的成功。Andrew Ng就是文章开始所介绍的机器学习的大牛(图1中左者)。\n　　2012年11月，微软在中国天津的一次活动上公开演示了一个全自动的同声传译系统，讲演者用英文演讲，后台的计算机一气呵成自动完成语音识别、英中机器翻译，以及中文语音合成，效果非常流畅，其中支撑的关键技术是深度学习；\n　　2013年1月，在百度的年会上，创始人兼CEO李彦宏高调宣布要成立百度研究院，其中第一个重点方向就是深度学习，并为此而成立深度学习研究院(IDL)。\n　　2013年4月，《麻省理工学院技术评论》杂志将深度学习列为2013年十大突破性技术(Breakthrough Technology)之首。\n\n图17 深度学习的发展热潮\n　　文章开头所列的三位机器学习的大牛，不仅都是机器学习界的专家，更是深度学习研究领域的先驱。因此，使他们担任各个大型互联网公司技术掌舵者的原因不仅在于他们的技术实力，更在于他们研究的领域是前景无限的深度学习技术。　　目前业界许多的图像识别技术与语音识别技术的进步都源于深度学习的发展，除了本文开头所提的Cortana等语音助手，还包括一些图像识别应用，其中典型的代表就是下图的百度识图功能。\n\n图18 百度识图\n　　深度学习属于机器学习的子类。基于深度学习的发展极大的促进了机器学习的地位提高，更进一步地，推动了业界对机器学习父类人工智能梦想的再次重视。\n \n7.机器学习的父类--人工智能\n　　人工智能是机器学习的父类。深度学习则是机器学习的子类。如果把三者的关系用图来表明的话，则是下图：\n图19 深度学习、机器学习、人工智能三者关系\n　　毫无疑问，人工智能(AI)是人类所能想象的科技界最突破性的发明了，某种意义上来说，人工智能就像游戏最终幻想的名字一样，是人类对于科技界的最终梦想。从50年代提出人工智能的理念以后，科技界，产业界不断在探索，研究。这段时间各种小说、电影都在以各种方式展现对于人工智能的想象。人类可以发明类似于人类的机器，这是多么伟大的一种理念！但事实上，自从50年代以后，人工智能的发展就磕磕碰碰，未有见到足够震撼的科学技术的进步。　　总结起来，人工智能的发展经历了如下若干阶段，从早期的逻辑推理，到中期的专家系统，这些科研进步确实使我们离机器的智能有点接近了，但还有一大段距离。直到机器学习诞生以后，人工智能界感觉终于找对了方向。基于机器学习的图像识别和语音识别在某些垂直领域达到了跟人相媲美的程度。机器学习使人类第一次如此接近人工智能的梦想。\n　　事实上，如果我们把人工智能相关的技术以及其他业界的技术做一个类比，就可以发现机器学习在人工智能中的重要地位不是没有理由的。\n　　人类区别于其他物体，植物，动物的最主要区别，作者认为是“智慧”。而智慧的最佳体现是什么？\n　　是计算能力么，应该不是，心算速度快的人我们一般称之为天才。　　是反应能力么，也不是，反应快的人我们称之为灵敏。　　是记忆能力么，也不是，记忆好的人我们一般称之为过目不忘。　　是推理能力么，这样的人我也许会称他智力很高，类似“福尔摩斯”，但不会称他拥有智慧。　　是知识能力么，这样的人我们称之为博闻广，也不会称他拥有智慧。\n　　想想看我们一般形容谁有大智慧？圣人，诸如庄子，老子等。智慧是对生活的感悟，是对人生的积淀与思考，这与我们机器学习的思想何其相似？通过经验获取规律，指导人生与未来。没有经验就没有智慧。\n \n\n图20 机器学习与智慧\n　　\n　　那么，从计算机来看，以上的种种能力都有种种技术去应对。\n　　例如计算能力我们有分布式计算，反应能力我们有事件驱动架构，检索能力我们有搜索引擎，知识存储能力我们有数据仓库，逻辑推理能力我们有专家系统，但是，唯有对应智慧中最显著特征的归纳与感悟能力，只有机器学习与之对应。这也是机器学习能力最能表征智慧的根本原因。　　让我们再看一下机器人的制造，在我们具有了强大的计算，海量的存储，快速的检索，迅速的反应，优秀的逻辑推理后我们如果再配合上一个强大的智慧大脑，一个真正意义上的人工智能也许就会诞生，这也是为什么说在机器学习快速发展的现在，人工智能可能不再是梦想的原因。　　人工智能的发展可能不仅取决于机器学习，更取决于前面所介绍的深度学习，深度学习技术由于深度模拟了人类大脑的构成，在视觉识别与语音识别上显著性的突破了原有机器学习技术的界限，因此极有可能是真正实现人工智能梦想的关键技术。无论是谷歌大脑还是百度大脑，都是通过海量层次的深度学习网络所构成的。也许借助于深度学习技术，在不远的将来，一个具有人类智能的计算机真的有可能实现。\n　　最后再说一下题外话，由于人工智能借助于深度学习技术的快速发展，已经在某些地方引起了传统技术界达人的担忧。真实世界的“钢铁侠”，特斯拉CEO马斯克就是其中之一。最近马斯克在参加MIT讨论会时，就表达了对于人工智能的担忧。“人工智能的研究就类似于召唤恶魔，我们必须在某些地方加强注意。”\n \n图21 马斯克与人工智能\n　　尽管马斯克的担心有些危言耸听，但是马斯克的推理不无道理。“如果人工智能想要消除垃圾邮件的话，可能它最后的决定就是消灭人类。”马斯克认为预防此类现象的方法是引入政府的监管。在这里作者的观点与马斯克类似，在人工智能诞生之初就给其加上若干规则限制可能有效，也就是不应该使用单纯的机器学习，而应该是机器学习与规则引擎等系统的综合能够较好的解决这类问题。因为如果学习没有限制，极有可能进入某个误区，必须要加上某些引导。正如人类社会中，法律就是一个最好的规则，杀人者死就是对于人类在探索提高生产力时不可逾越的界限。\n　　在这里，必须提一下这里的规则与机器学习引出的规律的不同，规律不是一个严格意义的准则，其代表的更多是概率上的指导，而规则则是神圣不可侵犯，不可修改的。规律可以调整，但规则是不能改变的。有效的结合规律与规则的特点，可以引导出一个合理的，可控的学习型人工智能。\n \n8.机器学习的思考--计算机的潜意识　　最后，作者想谈一谈关于机器学习的一些思考。主要是作者在日常生活总结出来的一些感悟。\n　　回想一下我在节1里所说的故事，我把小Y过往跟我相约的经历做了一个罗列。但是这种罗列以往所有经历的方法只有少数人会这么做，大部分的人采用的是更直接的方法，即利用直觉。那么，直觉是什么？其实直觉也是你在潜意识状态下思考经验后得出的规律。就像你通过机器学习算法，得到了一个模型，那么你下次只要直接使用就行了。那么这个规律你是什么时候思考的？可能是在你无意识的情况下，例如睡觉，走路等情况。这种时候，大脑其实也在默默地做一些你察觉不到的工作。\n　　这种直觉与潜意识，我把它与另一种人类思考经验的方式做了区分。如果一个人勤于思考，例如他会每天做一个小结，譬如“吾日三省吾身”，或者他经常与同伴讨论最近工作的得失，那么他这种训练模型的方式是直接的，明意识的思考与归纳。这样的效果很好，记忆性强，并且更能得出有效反应现实的规律。但是大部分的人可能很少做这样的总结，那么他们得出生活中规律的方法使用的就是潜意识法。\n　　举一个作者本人关于潜意识的例子。作者本人以前没开过车，最近一段时间买了车后，天天开车上班。我每天都走固定的路线。有趣的是，在一开始的几天，我非常紧张的注意着前方的路况，而现在我已经在无意识中就把车开到了目标。这个过程中我的眼睛是注视着前方的，我的大脑是没有思考，但是我手握着的方向盘会自动的调整方向。也就是说。随着我开车次数的增多，我已经把我开车的动作交给了潜意识。这是非常有趣的一件事。在这段过程中，我的大脑将前方路况的图像记录了下来，同时大脑也记忆了我转动方向盘的动作。经过大脑自己的潜意识思考，最后生成的潜意识可以直接根据前方的图像调整我手的动作。假设我们将前方的录像交给计算机，然后让计算机记录与图像对应的驾驶员的动作。经过一段时间的学习，计算机生成的机器学习模型就可以进行自动驾驶了。这很神奇，不是么。其实包括Google、特斯拉在内的自动驾驶汽车技术的原理就是这样。　　除了自动驾驶汽车以外，潜意识的思想还可以扩展到人的交际。譬如说服别人，一个最佳的方法就是给他展示一些信息，然后让他自己去归纳得出我们想要的结论。这就好比在阐述一个观点时，用一个事实，或者一个故事，比大段的道理要好很多。古往今来，但凡优秀的说客，无不采用的是这种方法。春秋战国时期，各国合纵连横，经常有各种说客去跟一国之君交流，直接告诉君主该做什么，无异于自寻死路，但是跟君主讲故事，通过这些故事让君主恍然大悟，就是一种正确的过程。这里面有许多杰出的代表，如墨子，苏秦等等。　　基本上所有的交流过程，使用故事说明的效果都要远胜于阐述道义之类的效果好很多。为什么用故事的方法比道理或者其他的方法好很多，这是因为在人成长的过程，经过自己的思考，已经形成了很多规律与潜意识。如果你告诉的规律与对方的不相符，很有可能出于保护，他们会本能的拒绝你的新规律，但是如果你跟他讲一个故事，传递一些信息，输送一些数据给他，他会思考并自我改变。他的思考过程实际上就是机器学习的过程，他把新的数据纳入到他的旧有的记忆与数据中，经过重新训练。如果你给出的数据的信息量非常大，大到调整了他的模型，那么他就会按照你希望的规律去做事。有的时候，他会本能的拒绝执行这个思考过程，但是数据一旦输入，无论他希望与否，他的大脑都会在潜意识状态下思考，并且可能改变他的看法。　　如果计算机也拥有潜意识(正如本博客的名称一样)，那么会怎么样？譬如让计算机在工作的过程中，逐渐产生了自身的潜意识，于是甚至可以在你不需要告诉它做什么时它就会完成那件事。这是个非常有意思的设想，这里留给各位读者去发散思考吧。\n9.总结\n　　本文首先介绍了互联网界与机器学习大牛结合的趋势，以及使用机器学习的相关应用，接着以一个“等人故事”展开对机器学习的介绍。介绍中首先是机器学习的概念与定义，然后是机器学习的相关学科，机器学习中包含的各类学习算法，接着介绍机器学习与大数据的关系，机器学习的新子类深度学习，最后探讨了一下机器学习与人工智能发展的联系以及机器学习与潜意识的关联。经过本文的介绍，相信大家对机器学习技术有一定的了解，例如机器学习是什么，它的内核思想是什么(即统计和归纳)，通过了解机器学习与人类思考的近似联系可以知晓机器学习为什么具有智慧能力的原因等等。其次，本文漫谈了机器学习与外延学科的关系，机器学习与大数据相互促进相得益彰的联系，机器学习界最新的深度学习的迅猛发展，以及对于人类基于机器学习开发智能机器人的一种展望与思考，最后作者简单谈了一点关于让计算机拥有潜意识的设想。　　机器学习是目前业界最为Amazing与火热的一项技术，从网上的每一次淘宝的购买东西，到自动驾驶汽车技术，以及网络攻击抵御系统等等，都有机器学习的因子在内，同时机器学习也是最有可能使人类完成AI dream的一项技术，各种人工智能目前的应用，如微软小冰聊天机器人，到计算机视觉技术的进步，都有机器学习努力的成分。作为一名当代的计算机领域的开发或管理人员，以及身处这个世界，使用者IT技术带来便利的人们，最好都应该了解一些机器学习的相关知识与概念，因为这可以帮你更好的理解为你带来莫大便利技术的背后原理，以及让你更好的理解当代科技的进程。10.后记　　这篇文档花了作者两个月的时间，终于在2014年的最后一天的前一天基本完成。通过这篇文章，作者希望对机器学习在国内的普及做一点贡献，同时也是作者本人自己对于所学机器学习知识的一个融汇贯通，整体归纳的提高过程。作者把这么多的知识经过自己的大脑思考，训练出了一个模型，形成了这篇文档，可以说这也是一种机器学习的过程吧(笑)。\n　　作者所在的行业会接触到大量的数据，因此对于数据的处理和分析是平常非常重要的工作，机器学习课程的思想和理念对于作者日常的工作指引作用极大，几乎导致了作者对于数据价值的重新认识。想想半年前，作者还对机器学习似懂非懂，如今也可以算是一个机器学习的Expert了(笑)。但作者始终认为，机器学习的真正应用不是通过概念或者思想的方式，而是通过实践。只有当把机器学习技术真正应用时，才可算是对机器学习的理解进入了一个层次。正所谓再“阳春白雪”的技术，也必须落到“下里巴人”的场景下运用。目前有一种风气，国内外研究机器学习的某些学者，有一种高贵的逼格，认为自己的研究是普通人无法理解的，但是这样的理念是根本错误的，没有在真正实际的地方发挥作用，凭什么证明你的研究有所价值呢？作者认为必须将高大上的技术用在改变普通人的生活上，才能发挥其根本的价值。一些简单的场景，恰恰是实践机器学习技术的最好地方。"},"机器学习笔记/文本相似度":{"slug":"机器学习笔记/文本相似度","filePath":"机器学习笔记/文本相似度.md","title":"文本相似度","links":[],"tags":["python"],"content":"如何对比两个文本的相似度，这里记录几个简单计算文本距离的方法。\n规定，结果为0-1的浮点数，0为完全不相关，1为完全正相关\n分词\n使用 jieba.cut 等工具对文本进行分词处理，获得到两个字符串序列。\nimport jieba\nseq1 = [i for i in jieba.cut(text1, cut_all=True) if i != &#039;&#039;]\nseq2 = [i for i in jieba.cut(text2, cut_all=True) if i != &#039;&#039;]\n\n共有词比率\n比较简单的算法就是看两句话中相同的汉字数，如果有较多的相同汉字，则认为它们是比较相似的。\ndef similartiy_rate(seq1, seq2):\n    set1, set2 = set(seq1), set(seq2)\n    return len(set1 &amp; set2) / len(set2)\n编辑距离\n编辑距离（Edit Distance），又称Levenshtein距离，是指两个字串之间，由一个转成另一个所需的最少编辑操作次数。编辑操作包括将一个字符替换成另一个字符，插入一个字符，删除一个字符。一般来说，编辑距离越小，两个串的相似度越大。由俄罗斯科学家Vladimir Levenshtein 在1965年提出这个概念。\n例如将abcdef转换为abcdgh,需要改变两个字符(ef-&gt;gh)，所以编辑距离就是2\n然后拿编辑距离去除以两者之间的最大长度，2/6≈0.333，意味着只要变动这么多就可以从A变成B，所以不用变动的字符便代表了相似度，1-0.333＝0.667。\n参数 method：\n1:序列间对齐最短长度的序列\n2:序列间对齐最长长度的序列\n\nfrom array import array\n\ndef levenshtein(seq1, seq2, method=1):\n\tif seq1 == seq2:\n\t\treturn 0.0\n\tlen1, len2 = len(seq1), len(seq2)\n\tif len1 == 0 or len2 == 0:\n\t\treturn 1.0\n\tif len1 &lt; len2: # minimize the arrays size\n\t\tlen1, len2 = len2, len1\n\t\tseq1, seq2 = seq2, seq1\n\t\n\tif method == 1:\n\t\treturn 1 - levenshtein(seq1, seq2) / float(len1)\n\tif method != 2:\n\t\traise ValueError(&quot;expected either 1 or 2 for `method` parameter&quot;)\n\t\n\tcolumn = array(&#039;L&#039;, range(len2 + 1))\n\tlength = array(&#039;L&#039;, range(len2 + 1))\n\t\n\tfor x in range(1, len1 + 1):\n\t\n\t\tcolumn[0] = length[0] = x\n\t\tlast = llast = x - 1\n\t\t\n\t\tfor y in range(1, len2 + 1):\n\t\t\n\t\t\t# dist\n\t\t\told = column[y]\n\t\t\tic = column[y - 1] + 1\n\t\t\tdc = column[y] + 1\n\t\t\trc = last + (seq1[x - 1] != seq2[y - 1])\n\t\t\tcolumn[y] = min(ic, dc, rc)\n\t\t\tlast = old\n\t\t\t\n\t\t\t# length\n\t\t\tlold = length[y]\n\t\t\tlic = length[y - 1] + 1 if ic == column[y] else 0\n\t\t\tldc = length[y] + 1 if dc == column[y] else 0\n\t\t\tlrc = llast + 1 if rc == column[y] else 0\n\t\t\tlength[y] = max(ldc, lic, lrc)\n\t\t\tllast = lold\n\t\n\treturn 1 - column[y] / float(length[y])\n\n余弦相似度\n余弦相似度度是常用的计算距离的公式，通常用来比较文本的相似性\n首先看一下公式\n\n基本步骤：\n\n1.通过中文分词，把完整的句子根据分词算法分为独立的词集合\n2.求出两个词集合的并集(词包)\n3.计算各自词集的词频并把词频向量化\n4.带入向量计算模型就可以求出文本相似度\n5.套用余弦函数计量两个句子的相似度。\n\n# 统计每个元素出现的次数\ndef all_list(arr):\n    result = {}\n    for i in set(arr):\n        result[i] = arr.count(i) - 1\n    return result\n\n# 词频编码\ndef word_frequency(word_list1, word_list2):\n    word_list = word_list1 + word_list2\n    dict = all_list(word_list)\n    word_vector1 = map(lambda x: dict[x], word_list1)\n    word_vector2 = map(lambda x: dict[x], word_list2)\n    return list(word_vector1), list(word_vector2)\n\n# 计算余弦\ndef calculate_cosine(a, b):\n    sum_list = map(lambda x, y: x*y, a, b)\n    p = reduce(lambda x, y: x+y, sum_list)\n\n    q1 = map(lambda x: x*x, a)\n    q1_sum = reduce(lambda x, y: x+y, q1)\n\n    q2 = map(lambda x: x*x, b)\n    q2_sum = reduce(lambda x, y: x+y, q2)\n\n    q = math.sqrt(q1_sum) * math.sqrt(q2_sum)\n    if q != 0:\n        return p/q\n    return 0\n\n# 余弦相似度\ndef cosine_similartiy(seq1, seq2):\n    p1, p2 = word_frequency(seq1, seq2)\n    return calculate_cosine(p1, p2)\n\n汉明距离\n汉明距离是使用在数据传输差错控制编码里的，它表示两个（相同长度）字对应位不同的数量，我们以d(x,y)表示两个字x,y之间的汉明距离。对两个字符串进行异或运算，并统计结果为1的个数，那么这个数就是汉明距离。\n# 汉明距离\ndef hamming(seq1, seq2):\n    L = len(seq1)\n    if L != len(seq2):\n        seq2.extend([0] * (len(seq1) - len(seq2)))\n    if L == 0:\n        return 0 # equal\n    dist = sum(c1 != c2 for c1, c2 in zip(seq1, seq2))\n    return 1 - dist / float(L)\n\nJaccard系数\n定义\n给定两个集合A,B，Jaccard 系数定义为A与B交集的大小与A与B并集的大小的比值，定义如下：\n\n当集合A，B都为空时，J(A,B)定义为1。\n与Jaccard 系数相关的指标叫做Jaccard 距离，用于描述集合之间的不相似度。Jaccard 距离越大，样本相似度越低。公式定义如下：\n\n其中对参差（symmetric difference）  \n性质\n\n# 杰卡德系数\ndef jaccard(seq1, seq2):\n    set1, set2 = set(seq1), set(seq2)\n    return len(set1 &amp; set2) / float(len(set1 | set2))\n\nDice系数\nDice 系数可以计算两个字符串的相似度：Dice（s1,s2）=2*comm(s1,s2)/(leng(s1)+leng(s2))。\n其中，comm (s1,s2)是s1、s2 中相同字符的个数leng(s1)，leng(s2)是字符串s1、s2 的长度。\nDice 系数是一种集合相似度度量函数，与相似度指数相同，也被称为系数，形式上也和杰卡德系数没有多大区别，但还是有些不同的性质。\n\n# Dice系数\ndef sorensen(seq1, seq2):\n    set1, set2 = set(seq1), set(seq2)\n    return 2 * len(set1 &amp; set2) / float(len(set1) + len(set2))\n\n总结\n简单的几个计算文本相似度的方法就是这些了，常用的为余弦相似度，稍复杂的可以尝试多个算法的叠加，例如还有欧几里得距离，曼哈顿距离，SimHash等方法可以计算文本之间的相似度。7"},"机器学习笔记/机器学习习题/ex1-linear-regression":{"slug":"机器学习笔记/机器学习习题/ex1-linear-regression","filePath":"机器学习笔记/机器学习习题/ex1-linear regression.md","title":"ex1-linear regression","links":[],"tags":["机器学习"],"content":"AndrewNg 机器学习习题ex1-linear regression\n练习用数据\n练习数据ex1data1.txt和ex2data2.txt都是以逗号为分割符的文本文件，所以我们也可以把它们看作csv文件处理。\nex1data1中的第一列是一个城市的人口，第二列是这个城市中卡车司机的利润。\nex2data2三列分别是，一个房子的大小，房间数，售价。\n浏览数据\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n \n# 加载数据\npath = &#039;./data/ex1data1.txt&#039;\ndata = pd.read_csv(path, header=None, names=[&#039;Population&#039;, &#039;Profit&#039;])\n# 看一下数据的内容\nprint(data.head())\nprint(data.describe())\n# 画出散点图\ndata.plot(kind=&#039;scatter&#039;, x=&#039;Population&#039;, y=&#039;Profit&#039;, figsize=(12, 8))\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n-PopulationProfit06.110117.592015.52779.130228.518613.662037.003211.854045.85986.8233\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n-PopulationProfitcount97.00000097.000000mean8.1598005.839135std3.8698845.510262min5.026900-2.68070025%5.7077001.98690050%6.5894004.56230075%8.5781007.046700max22.20300024.147000\n\n代价函数\n我们将创建一个以参数θ为特征函数的代价函数\nJ\\left( \\theta \\right)=\\frac{1}{2m}\\sum\\limits_{i=1}^{m}{{{\\left( {{h}_{\\theta }}\\left( {{x}^{(i)}} \\right)-{{y}^{(i)}} \\right)}^{2}}}\n其中：\n{{h}{\\theta }}\\left( x \\right)={{\\theta }^{T}}X={{\\theta }{0}}{{x}{0}}+{{\\theta }{1}}{{x}{1}}+{{\\theta }{2}}{{x}{2}}+...+{{\\theta }{n}}{{x}_{n}}\ndef compute_cost(X, y, theta):\n    inner = np.power(((X * theta.T) - y), 2)\n    return np.sum(inner) / (2 * len(X))\n\n预处理\n# 预处理\ndata.insert(0, &#039;Ones&#039;, 1)  # 添加一列1\ncols = data.shape[1]\nX = data.iloc[:, :cols - 1]  # 去掉最后一列\nY = data.iloc[:, cols - 1: cols]  # 最后一列\n \n# 检查X和Y 是否正确\nprint(X.head())\nprint(Y.head())\n \n# 把X和Y转换为numpy的矩阵\nX = np.matrix(X.values)\nY = np.matrix(Y.values)\n# 初始化theta\ntheta = np.matrix(np.array([0, 0]))\n# 检查维度\nprint(X.shape, Y.shape, theta.shape)  # (97, 2) (97, 1) (1, 2)\n批量梯度下降\n我们要这个公式来更新θ。\n{\\theta_{j}}:={\\theta_{j}}-\\alpha \\frac{\\partial }{\\partial {\\theta_{j}}}J\\left(\\theta \\right)\n# 梯度下降\n# X矩阵，Y矩阵，初始的θ，学习速率，迭代次数\ndef gradient_descent(X, Y, theta, alpha, iters):\n    temp = np.matrix(np.zeros(theta.shape))\n    parameters = int(theta.ravel().shape[1])\n    cost =  np.zeros(iters)\n \n    for i in range(iters):\n        error = (X * theta.T) - Y\n        for j in range(parameters):\n            term = np.multiply(error, X[:, j])\n            temp[0, j] = theta[0, j] - ((alpha / len(X)) * np.sum(term))\n        theta = temp\n        cost[i] = compute_cost(X, Y, theta)\n \n    return theta, cost\n \n \n# 初始化迭代次数和学习速率\nalpha = 0.01\niters = 1000\n \ng, cost = gradient_descent(X, Y, theta, alpha, iters)\n# 用我们得到的参数g计算代价函数，查看误差\nprint(g, compute_cost(X, Y, theta))\n可视化\n# 绘制线性模型以及数据，查看拟合效果\ndef data_visual(data, g):\n    x = np.linspace(data.Population.min(), data.Population.max(), 100)\n    f = g[0, 0] + (g[0, 1] * x)\n \n    fig, ax = plt.subplots(figsize=(12, 8))\n    ax.plot(x, f, &#039;g&#039;, label=&#039;Prediction&#039;)\n    ax.scatter(data.Population, data.Profit, label=&#039;Traning Data&#039;)\n    ax.legend(loc=2)\n    ax.set_xlabel(&#039;Population&#039;)\n    ax.set_ylabel(&#039;Profit&#039;)\n    plt.show()\n \ndata_visual(data, g)\n\n# 绘制代价向量\ndef cost_visual(cost):\n    fig, ax = plt.subplots(figsize=(12, 8))\n    ax.plot(np.arange(iters), cost, &#039;r&#039;)\n    ax.set_xlabel(&#039;Iterations&#039;)\n    ax.set_ylabel(&#039;Cost&#039;)\n    plt.show()\n\ncost_visual(cost)\n\n\n多变量的线性回归\n练习1还包括一个房屋价格数据集，其中有2个变量（房子的大小，卧室的数量）和目标（房子的价格）。 我们使用我们已经应用的技术来分析数据集。\npath = &#039;./data/ex1data2.txt&#039;\ndata2 = pd.read_csv(path, header=None, names =[&#039;Size&#039;, &#039;Bedrooms&#039;, &#039;Price&#039;])\nprint(data2.head())\n \n# 特征归一化\ndata2 = (data2 - data2.mean()) / data2.std()\n \n# 预处理\n# add ones column\ndata2.insert(0, &#039;Ones&#039;, 1)\n \n# set X (training data) and y (target variable)\ncols = data2.shape[1]\nX2 = data2.iloc[:, : cols - 1]\nY2 = data2.iloc[:, cols - 1: cols]\n \n# convert to matrices and initialize theta\nX2 = np.matrix(X2.values)\nY2 = np.matrix(Y2.values)\ntheta2 = np.matrix(np.array([0, 0, 0]))\n \ng2, cost2 = gradient_descent(X2, Y2, theta2, alpha, iters)\ncost_visual(cost2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n-SizeBedroomsPrice021043399900116003329900224003369000314162232000430004539900\n\n正规方程\n\\theta ={{\\left( {X^T}X \\right)}^{-1}}{X^{T}}y\n# 正规方程\ndef normal_func(X ,Y):\n    theta = np.linalg.inv(X.T@X)@X.T@Y\n    return theta\n\ng = normal_func(X, Y)\ndata_visual(data, g.T)\n"},"机器学习笔记/机器学习习题/ex2-logistic-regression":{"slug":"机器学习笔记/机器学习习题/ex2-logistic-regression","filePath":"机器学习笔记/机器学习习题/ex2-logistic regression.md","title":"ex2-logistic regression","links":[],"tags":["机器学习"],"content":"AndrewNg 机器学习习题ex2-logistic regression\n练习用数据\n练习数据ex2data1.txt和ex2data2.txt都是由三列数字组成的文本文件，前两列是特征，第三列是结果，结果只有0和1两种。\n浏览数据\n画出散点图，观察两个不同结果的分类情况，有明显的决策边界。\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\npath = &#039;./data/ex2data1.txt&#039;\nnames=[&#039;exam 1&#039;, &#039;exam 2&#039;, &#039;admitted&#039;]\ndata = pd.read_csv(path, header=None, names=names)\nprint(data.head())\n\n\n# 可视化\ndef data_visual(data, names, theta=None):\n    positive = data[data[names[2]].isin([1])]\n    negative = data[data[names[2]].isin([0])]\n    fig, ax = plt.subplots(figsize=(12, 8))\n    ax.scatter(positive[names[0]], positive[names[1]], s=50, c=&#039;b&#039;, marker=&#039;o&#039;, label=&#039;1&#039;)\n    ax.scatter(negative[names[0]], negative[names[1]], s=50, c=&#039;r&#039;, marker=&#039;x&#039;, label=&#039;0&#039;)\n    ax.legend()\n\n    if theta is not None:\n        x1 = np.arange(20, 100, 5)\n        x2 = (- theta[0] - theta[1] * x1) / theta[2]\n        plt.plot(x1, x2, color=&#039;black&#039;)\n\n    plt.show()\n\n\ndata_visual(data, names)\n\n\n激活函数\n逻辑回归模型的假设是：h_\\theta \\left( x \\right)=g\\left(\\theta^{T}X \\right)其中： X 代表特征向量 g 代表逻辑函数（logistic function)是一个常用的逻辑函数为S形函数（Sigmoid function），公式为： g\\left( z \\right)=\\frac{1}{1+{{e}^{-z}}}。\n# sigmoid函数\ndef sigmoid(z):\n    return 1 / (1 + np.exp(-z))\n\n\n# 检查激活函数\ndef sigmoid_visual():\n    nums = np.arange(-10, 10, step=1)\n    plt.plot(nums, sigmoid(nums))\n    plt.show()\n\n\nsigmoid_visual()\n\n\n代价函数与预处理\n代价函数：\nJ\\left( \\theta \\right)=\\frac{1}{m}\\sum\\limits_{i=1}^{m}{[-{{y}^{(i)}}\\log \\left( {{h}_{\\theta }}\\left( {{x}^{(i)}} \\right) \\right)-\\left( 1-{{y}^{(i)}} \\right)\\log \\left( 1-{{h}_{\\theta }}\\left( {{x}^{(i)}} \\right) \\right)]}\n# 代价函数\ndef cost(theta, X, Y):\n    theta = np.matrix(theta)\n    X = np.matrix(X)\n    Y = np.matrix(Y)\n    first = np.multiply(-Y, np.log(sigmoid(X * theta.T)))\n    second = np.multiply((1 - Y), np.log(1 - sigmoid(X * theta.T)))\n    return np.sum(first - second) / len(X)\n \n \n# 数据预处理\n# add a ones column - this makes the matrix multiplication work out easier\ndata.insert(0, &#039;Ones&#039;, 1)\n \n# set X (training data) and y (target variable)\ncols = data.shape[1]\nX = data.iloc[:, 0: cols - 1]\nY = data.iloc[:, cols - 1: cols]\n \n# convert to numpy arrays and initalize the parameter array theta\nX = np.array(X.values)\nY = np.array(Y.values)\ntheta = np.zeros(3)\n \n# 检查维度\nprint(X.shape, theta.shape, Y.shape)  # (100, 3) (3,) (100, 1)\nprint(cost(theta, X, Y))  # 初始值的代价\n初始化的代价函数值为：0.6931471805599453\n梯度下降\n\\frac{\\partial J\\left( \\theta \\right)}{\\partial {{\\theta }_{j}}}=\\frac{1}{m}\\sum\\limits_{i=1}^{m}{({{h}_{\\theta }}\\left( {{x}^{(i)}} \\right)-{{y}^{(i)}})x_{_{j}}^{(i)}}\n# 梯度下降\ndef gradient(theta, X, Y):\n    theta = np.matrix(theta)\n    X = np.matrix(X)\n    y = np.matrix(Y)\n \n    parameters = int(theta.ravel().shape[1])\n    grad = np.zeros(parameters)\n \n    error = sigmoid(X * theta.T) - Y\n \n    for i in range(parameters):\n        term = np.multiply(error, X[:, i])\n        grad[i] = np.sum(term) / len(X)\n \n \n    return grad\n训练数据与决策边界\n# 用SciPy&#039;s truncated newton（TNC）实现寻找最优参数\nresult = opt.fmin_tnc(func=cost, x0=theta, fprime=gradient, args=(X, Y))\nprint(result)\nprint(cost(result[0], X, Y))\n \ntheta = result[0]\n# 画出决策边界\ndata_visual(data, names, theta)\n \n \n# 计算预测效果\ndef predict(theta, X):\n    probability = sigmoid(X * theta.T)\n    return [1 if x &gt;= 0.5 else 0 for x in probability]\n \n \ntheta_min = np.matrix(result[0])\npredictions = predict(theta_min, X)\ncorrect = [1 if ((a == 1 and b == 1) or (a == 0 and b == 0)) else 0 for (a, b) in zip(predictions, Y)]\naccuracy = (sum(map(int, correct)) % len(correct))\nprint(&#039;accuracy = {}%&#039;.format(accuracy))\naccuracy = 89%\n\n逻辑回归正则化\npath2 = &#039;./data/ex2data2.txt&#039;\nnames = [&#039;test1&#039;, &#039;test2&#039;, &#039;accepted&#039;]\ndata2 = pd.read_csv(path2, header=None, names=names)\nprint(data2.head())\n \n#data_visual(data2, names)\n \ndegree = 5\nx1 = data2[&#039;test1&#039;]\nx2 = data2[&#039;test2&#039;]\n \ndata2.insert(3, &#039;Ones&#039;, 1)\n \nfor i in range(1, degree):\n    for j in range(0, i):\n        data2[&#039;F&#039; + str(i) + str(j)] = np.power(x1, i-j) * np.power(x2, j)\n \ndata2.drop(&#039;test1&#039;, axis=1, inplace=True)\ndata2.drop(&#039;test2&#039;, axis=1, inplace=True)\n \nprint(data2.head())\n \n# 正则化代价函数 learng_rate = λ lambda\ndef cost_reg(theta, X, Y, learng_rate):\n    theta = np.matrix(theta)\n    X = np.matrix(X)\n    Y = np.matrix(Y)\n \n    first = np.multiply(-Y, np.log(sigmoid(X * theta.T)))\n    second = np.multiply((1 - Y), np.log(1 - sigmoid(X * theta.T)))\n    reg = (learng_rate / (2 * len(X))) * np.sum(np.power(theta[:, 1: theta.shape[1]], 2))\n \n    return np.sum(first - second) / len(X) + reg\n \n \ndef gradient_reg(theta, X, Y, learng_rate):\n    theta = np.matrix(theta)\n    X = np.matrix(X)\n    Y = np.matrix(Y)\n \n    parameters = int(theta.ravel().shape[1])\n    grad = np.zeros(parameters)\n \n    error = sigmoid(X * theta.T) - Y\n \n    for i in range(parameters):\n        term = np.multiply(error, X[:, i])\n \n        if(i == 0):\n            grad[i] = np.sum(term) / len(X)\n        else:\n            grad[i] = (np.sum(term) / len(X)) + ((learng_rate / len(X)) * theta[:, i])\n \n    return grad\n \n# set X and y (remember from above that we moved the label to column 0)\ncols = data2.shape[1]\nX2 = data2.iloc[:,1:cols]\nY2 = data2.iloc[:, :1]\n \n# convert to numpy arrays and initalize the parameter array theta\nX2 = np.array(X2.values)\nY2 = np.array(Y2.values)\ntheta2 = np.zeros(11)\n \nlearng_rate = 1\n \nprint(cost_reg(theta2, X2, Y2, learng_rate))\nprint(gradient_reg(theta2, X2, Y2, learng_rate))\n \nresult2 = opt.fmin_tnc(func=cost_reg, x0=theta2, fprime=gradient_reg, args=(X2, Y2, learng_rate))\nprint(result2)\n \ntheta_min = np.matrix(result2[0])\npredictions = predict(theta_min, X2)\ncorrect = [1 if ((a == 1 and b == 1) or (a == 0 and b == 0)) else 0 for (a, b) in zip(predictions, Y2)]\naccuracy = (sum(map(int, correct)) % len(correct))\nprint(&#039;accuracy = {0}%&#039;.format(accuracy))\naccuracy = 78%\n正则化画出决策边界\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport scipy.optimize as opt\nimport seaborn as sns\n \n \ndef get_y(df):  # 读取标签\n#     &#039;&#039;&#039;assume the last column is the target&#039;&#039;&#039;\n    return np.array(df.iloc[:, -1])  # df.iloc[:, -1]是指df的最后一列\n \n \ndef sigmoid(z):\n    return 1 / (1 + np.exp(-z))\n \n \ndef gradient(theta, X, y):\n#     &#039;&#039;&#039;just 1 batch gradient&#039;&#039;&#039;\n    return (1 / len(X)) * X.T @ (sigmoid(X @ theta) - y)\n \n \ndef cost(theta, X, y):\n    &#039;&#039;&#039; cost fn is -l(theta) for you to minimize&#039;&#039;&#039;\n    return np.mean(-y * np.log(sigmoid(X @ theta)) - (1 - y) * np.log(1 - sigmoid(X @ theta)))\n \n \ndf = pd.read_csv(&#039;./data/ex2data2.txt&#039;, names=[&#039;test1&#039;, &#039;test2&#039;, &#039;accepted&#039;])\ndf.head()\n \n \ndef feature_mapping(x, y, power, as_ndarray=False):\n#     &quot;&quot;&quot;return mapped features as ndarray or dataframe&quot;&quot;&quot;\n    # data = {}\n    # # inclusive\n    # for i in np.arange(power + 1):\n    #     for p in np.arange(i + 1):\n    #         data[&quot;f{}{}&quot;.format(i - p, p)] = np.power(x, i - p) * np.power(y, p)\n \n    data = {&quot;f{}{}&quot;.format(i - p, p): np.power(x, i - p) * np.power(y, p)\n                for i in np.arange(power + 1)\n                for p in np.arange(i + 1)\n            }\n \n    if as_ndarray:\n        return pd.DataFrame(data).as_matrix()\n    else:\n        return pd.DataFrame(data)\n \n \nx1 = np.array(df.test1)\nx2 = np.array(df.test2)\ndata = feature_mapping(x1, x2, power=6)\nprint(data.shape)\nprint(data.head())\n \n \ntheta = np.zeros(data.shape[1])\nX = feature_mapping(x1, x2, power=6, as_ndarray=True)\nprint(X.shape)\n \ny = get_y(df)\nprint(y.shape)\n \n \ndef regularized_cost(theta, X, y, l=1):\n#     &#039;&#039;&#039;you don&#039;t penalize theta_0&#039;&#039;&#039;\n    theta_j1_to_n = theta[1:]\n    regularized_term = (l / (2 * len(X))) * np.power(theta_j1_to_n, 2).sum()\n \n    return cost(theta, X, y) + regularized_term\n# 正则化代价函数\n \n \nregularized_cost(theta, X, y, l=1)\n \n \ndef regularized_gradient(theta, X, y, l=1):\n#     &#039;&#039;&#039;still, leave theta_0 alone&#039;&#039;&#039;\n    theta_j1_to_n = theta[1:]\n    regularized_theta = (l / len(X)) * theta_j1_to_n\n \n    # by doing this, no offset is on theta_0\n    regularized_term = np.concatenate([np.array([0]), regularized_theta])\n \n    return gradient(theta, X, y) + regularized_term\n \n \nprint(&#039;init cost = {}&#039;.format(regularized_cost(theta, X, y)))\n \nres = opt.minimize(fun=regularized_cost, x0=theta, args=(X, y), method=&#039;Newton-CG&#039;, jac=regularized_gradient)\n \n \ndef draw_boundary(power, l):\n#     &quot;&quot;&quot;\n#     power: polynomial power for mapped feature\n#     l: lambda constant\n#     &quot;&quot;&quot;\n    density = 1000\n    threshhold = 2 * 10**-3\n \n    final_theta = feature_mapped_logistic_regression(power, l)\n    x, y = find_decision_boundary(density, power, final_theta, threshhold)\n \n    df = pd.read_csv(&#039;./data/ex2data2.txt&#039;, names=[&#039;test1&#039;, &#039;test2&#039;, &#039;accepted&#039;])\n    sns.lmplot(&#039;test1&#039;, &#039;test2&#039;, hue=&#039;accepted&#039;, data=df, size=6, fit_reg=False, scatter_kws={&quot;s&quot;: 100})\n \n    plt.scatter(x, y, c=&#039;R&#039;, s=10)\n    plt.title(&#039;Decision boundary&#039;)\n    plt.show()\n \n \ndef feature_mapped_logistic_regression(power, l):\n#     &quot;&quot;&quot;for drawing purpose only.. not a well generealize logistic regression\n#     power: int\n#         raise x1, x2 to polynomial power\n#     l: int\n#         lambda constant for regularization term\n#     &quot;&quot;&quot;\n    df = pd.read_csv(&#039;./data/ex2data2.txt&#039;, names=[&#039;test1&#039;, &#039;test2&#039;, &#039;accepted&#039;])\n    x1 = np.array(df.test1)\n    x2 = np.array(df.test2)\n    y = get_y(df)\n \n    X = feature_mapping(x1, x2, power, as_ndarray=True)\n    theta = np.zeros(X.shape[1])\n \n    res = opt.minimize(fun=regularized_cost,\n                       x0=theta,\n                       args=(X, y, l),\n                       method=&#039;TNC&#039;,\n                       jac=regularized_gradient)\n    final_theta = res.x\n \n    return final_theta\n \n \ndef find_decision_boundary(density, power, theta, threshhold):\n    t1 = np.linspace(-1, 1.5, density)\n    t2 = np.linspace(-1, 1.5, density)\n \n    cordinates = [(x, y) for x in t1 for y in t2]\n    x_cord, y_cord = zip(*cordinates)\n    mapped_cord = feature_mapping(x_cord, y_cord, power)  # this is a dataframe\n \n    inner_product = mapped_cord.as_matrix() @ theta\n \n    decision = mapped_cord[np.abs(inner_product) &lt; threshhold]\n \n    return decision.f10, decision.f01\n \n \n# 寻找决策边界函数\ndraw_boundary(power=6, l=1)  # lambda=1\ndraw_boundary(power=6, l=0)  # lambda=1 过拟合\ndraw_boundary(power=6, l=100)  # lambda=1 欠拟合\n\n\n"},"机器学习笔记/机器学习习题/ex3-neural-network":{"slug":"机器学习笔记/机器学习习题/ex3-neural-network","filePath":"机器学习笔记/机器学习习题/ex3-neural network.md","title":"ex3-neural network","links":[],"tags":["机器学习"],"content":"AndrewNg 机器学习习题ex3-neural network\n练习用数据\nex3data1.mat是一个matlab文件，储存了5000个图像的数据，每个图像是一个20像素×20像素的灰度图，展开后为一个400维的向量，每一个向量都储存为矩阵X的行，所以X的维度是（5000，400）\ny的每一行代表X所对应的手写数字，y的维度是（5000，1）\n需要的头：\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport scipy.io as sio\nimport matplotlib\nimport scipy.optimize as opt\nfrom sklearn.metrics import classification_report # 这个包是评价报告\nVisualizing the data\n载入数据：\ndef load_data(path, transpose=True):\n    data = sio.loadmat(path)\n    y = data.get(&#039;y&#039;)\n    y = y.reshape(y.shape[0])\n    X = data.get(&#039;X&#039;)\n    if transpose:\n        X = np.array([im.reshape((20, 20)).T for im in X])\n        X = np.array([im.reshape(400) for im in X])\n    return X, y\n \nX, y = load_data(&#039;./data/ex3data1.mat&#039;)\n画一个图\ndef plot_an_image(image):\n    fig, ax = plt.subplots(figsize=(1, 1))\n    ax.matshow(image.reshape((20, 20)), cmap=matplotlib.cm.binary)\n    plt.xticks(np.array([]))\n    plt.yticks(np.array([]))\n    plt.show()\n \npick_one = np.random.randint(0, 5000)\nplot_an_image(X[pick_one, :])\nprint(&#039;this should be {}&#039;.format(y[pick_one]))\n\n画一百个图\ndef plot_100_image(X):\n    size = int(np.sqrt(X.shape[1]))\n    sample_idx = np.random.choice(np.array(X.shape[0]), 100)\n    sample_images = X[sample_idx, :]\n \n    fig, ax_array = plt.subplots(nrows=10, ncols=10, sharey=True, sharex=True, figsize=(8, 8))\n \n    for r in range(10):\n        for c in range(10):\n            ax_array[r, c].matshow(sample_images[10 * r + c].reshape((size, size)), cmap=matplotlib.cm.binary)\n            plt.xticks(np.array([]))\n            plt.yticks(np.array([]))\n    plt.show()\n    \nplot_100_image(X)\n\n准备数据\n加载好ex3data1.mat文件后我们需要处理一下，首先X是一个(5000,400)的矩阵，我们在第一列加上一列全为1的矩阵为偏差量，y是一个(5000,)的矩阵，需要注意的是，为了兼容Oxtave和matlab，y中0的被标记为了10。我们把y分成10类整理y数据为(10,5000)的一个矩阵。\n扩展 5000*1 到 5000*10\n     比如 y=10 -&gt; [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]: ndarray\n     比如 y=1 -&gt; [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]: ndarray\n\nraw_X, raw_y = load_data(&#039;./data/ex3data1.mat&#039;)\nX = np.insert(raw_X, 0, values=np.ones(raw_X.shape[0]), axis = 1) # 插入了第一列 全为1\ny_matrix = []\nfor k in range(1, 11):\n    y_matrix.append((raw_y == k).astype(int))\ny_matrix = [y_matrix[-1]] + y_matrix[:-1]\ny = np.array(y_matrix)\n训练一维模型\n处理好数据后接着写，激活函数和代价函数，代价函数的偏导数就是梯度函数，我们期望这个函数最小。给梯度函数和代价函数加入正则项。\ndef sigmoid(z):\n    return 1 / (1 + np.exp(-z))\n \ndef cost(theta, X, y):\n    return np.mean(-y * np.log(sigmoid(X @ theta)) - (1 - y) * np.log(1 - sigmoid(X @ theta)))\n \n# 梯度就是jθ的在θ偏导\ndef gradient(theta, X, y):\n    # @ 对应元素相乘求和\n    return (1 / len(X)) * X.T @ (sigmoid(X @ theta) - y)\n \ndef regularized_cost(theta, X, y, l=1):\n    theta_j1_to_n = theta[1:]\n    regularized_term = (1 / (2 * len(X))) * np.power(theta_j1_to_n, 2).sum()\n \n    return cost(theta, X, y) + regularized_term\n \ndef regularized_gradient(theta, X, y, l=1):\n    theta_j1_to_n = theta[1:]\n    regularized_theta = (l / len(X)) * theta_j1_to_n\n    regularized_term = np.concatenate([np.array([0]), regularized_theta]) # 在theta矩阵前接一个[0]\n    return gradient(theta, X, y) + regularized_term\n运用minimize()函数开始迭代，计算出theta，然后验证theta的准确性。\ndef logistic_regression(X, y, l=1):\n    theta = np.zeros(X.shape[1])\n    res = opt.minimize(fun=regularized_cost, x0=theta, args=(X, y, l), method=&#039;TNC&#039;, jac=regularized_gradient, options={&#039;disp&#039;: True})\n    final_theta = res.x\n    return final_theta\n \ndef predict(x, theta):\n    prob = sigmoid(x @ theta)\n    return (prob &gt;= 0.5).astype(int)\n \nt0 = logistic_regression(X, y[0])\ny_pred = predict(X, t0)\nprint(&#039;Accuracy={}&#039;.format(np.mean(y[0] == y_pred)))\n最终求得结果为 Accuracy=0.9974\n训练K维模型\nk_theta = np.array([logistic_regression(X, y[k]) for k in range(10)])\nprint(k_theta.shape) # (10, 401)\n \nprob_matrix = sigmoid(X @ k_theta.T)\nnp.set_printoptions(suppress=True) # 科学计数法表示\nprint(prob_matrix.shape) # (5000, 10)\n \ny_pred = np.argmax(prob_matrix, axis=1)\nprint(y_pred.shape) # (5000,)\n \ny_answer = raw_y.copy()\ny_answer[y_answer==10] = 0\n \nprint(classification_report(y_answer, y_pred))\n             precision    recall  f1-score   support\n\n          0       0.97      0.99      0.98       500\n          1       0.95      0.99      0.97       500\n          2       0.95      0.92      0.93       500\n          3       0.95      0.91      0.93       500\n          4       0.95      0.95      0.95       500\n          5       0.92      0.92      0.92       500\n          6       0.97      0.98      0.97       500\n          7       0.95      0.95      0.95       500\n          8       0.93      0.92      0.92       500\n          9       0.92      0.92      0.92       500\n\navg / total       0.94      0.94      0.94      5000\n\n如ex3.pdf中所说，我们成功的分类出94%的例子。\nFeedforward Propagation and Prediction\n\n我们的神经网路如上图所示，它有3层构成（一个输入层，一个隐藏层a，一个输出层。）已经提供了一组训练参数（Θ1，Θ2）储存在ex3weights.mat中\n% Load saved matrices from file\nload(&#039;ex3weights.mat&#039;);\n% The matrices Theta1 and Theta2 will now be in your Octave\n% environment\n% Theta1 has size 25 x 401\n% Theta2 has size 10 x 26\n\ndef load_weight(path):\n    data = sio.loadmat(path)\n    return data[&#039;Theta1&#039;], data[&#039;Theta2&#039;]\n \ntheta1, theta2 = load_weight(&#039;./data/ex3weights.mat&#039;)\n \nX, y = load_data(&#039;./data/ex3data1.mat&#039;,transpose=False)\n \nX = np.insert(X, 0, values=np.ones(X.shape[0]), axis=1)  # intercept\n \na1 = X\n \nz2 = a1 @ theta1.T # (5000, 401) @ (25,401).T = (5000, 25)\nprint(z2.shape)\n \nz2 = np.insert(z2, 0, values=np.ones(z2.shape[0]), axis=1)\n \na2 = sigmoid(z2)\n \nz3 = a2 @ theta2.T\n \na3 = sigmoid(z3)\n \ny_pred = np.argmax(a3, axis=1) + 1  # numpy is 0 base index, +1 for matlab convention，返回沿轴axis最大值的索引，axis=1代表行\n \nprint(classification_report(y, y_pred))\n             precision    recall  f1-score   support\n\n          1       0.97      0.98      0.97       500\n          2       0.98      0.97      0.97       500\n          3       0.98      0.96      0.97       500\n          4       0.97      0.97      0.97       500\n          5       0.98      0.98      0.98       500\n          6       0.97      0.99      0.98       500\n          7       0.98      0.97      0.97       500\n          8       0.98      0.98      0.98       500\n          9       0.97      0.96      0.96       500\n         10       0.98      0.99      0.99       500\n\navg / total       0.98      0.98      0.98      5000\n"},"机器学习笔记/机器学习习题/ex4-NN-back-propagation":{"slug":"机器学习笔记/机器学习习题/ex4-NN-back-propagation","filePath":"机器学习笔记/机器学习习题/ex4-NN back propagation.md","title":"ex4-NN back propagation","links":[],"tags":["机器学习"],"content":"AndrewNg 机器学习习题ex4-NN back propagation\n练习用数据\n需要的头：\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport scipy.io as sio\nimport matplotlib\nimport scipy.optimize as opt\nfrom sklearn.metrics import classification_report # 这个包是评价报告\nVisualizing the data\n载入数据：\ndef load_data(path, transpose=True):\n    data = sio.loadmat(path)\n    y = data.get(&#039;y&#039;)\n    y = y.reshape(y.shape[0])\n    X = data.get(&#039;X&#039;)\n    if transpose:\n        X = np.array([im.reshape((20, 20)).T for im in X])\n        X = np.array([im.reshape(400) for im in X])\n    return X, y\n \n \nX, y = load_data(&#039;./data/ex4data1.mat&#039;)\n \n \ndef plot_100_image(X):\n    size = int(np.sqrt(X.shape[1]))\n    sample_idx = np.random.choice(np.array(X.shape[0]), 100)\n    sample_images = X[sample_idx, :]\n \n    fig, ax_array = plt.subplots(nrows=10, ncols=10, sharey=True, sharex=True, figsize=(8, 8))\n \n    for r in range(10):\n        for c in range(10):\n            ax_array[r, c].matshow(sample_images[10 * r + c].reshape((size, size)), cmap=matplotlib.cm.binary)\n            plt.xticks(np.array([]))\n            plt.yticks(np.array([]))\n    plt.show()\nplot_100_image(X)\n\n准备数据\n特征集合X添加一列全为1的偏差向量，把目标向量y进行OneHot编码。\nX_raw, y_raw = load_data(&#039;./data/ex4data1.mat&#039;, transpose=False) # 这里转置\nX = np.insert(X_raw, 0, np.ones(X_raw.shape[0]), axis=1) # 增加全为1的一列\nprint(y.shape) # (5000,)\ny = np.array([y_raw]).T\nfrom sklearn.preprocessing import OneHotEncoder\nencoder = OneHotEncoder(sparse=False)\ny_onehot = encoder.fit_transform(y)\nprint(y_onehot.shape) # (5000, 10)\n读取权重\n先读取出ex4weights.mat中的theta1和theta2，把theta展开后进行扁平化处理。\ndef load_weight(path):\n    data = sio.loadmat(path)\n    return data[&#039;Theta1&#039;], data[&#039;Theta2&#039;]\n \n \nt1, t2 = load_weight(&#039;./data/ex4weights.mat&#039;)\nprint(t1.shape, t2.shape) # (25, 401) (10, 26)\n \n \ndef serialize(a, b):\n    # np.ravel() 降维\n    # np.concatenate() 拼接\n    return np.concatenate((np.ravel(a), np.ravel(b)))\n \n \ndef deserialize(seq):\n    # 解开为两个theta\n    return seq[:25 * 401].reshape(25, 401), seq[25 * 401:].reshape(10, 26)\n \n \ntheta = serialize(t1, t2)\nprint(theta.shape)  # (25 * 401) + (10 * 26) = 10285\n前向传播 feed forward\n（400 + 1） → (25 + 1) → (1)\n\ndef sigmoid(z):\n    return 1 / (1 + np.exp(-z))\n    \n    \ndef feed_forward(theta, X):\n    t1, t2 = deserialize(theta)\n    m = X.shape[0]\n \n    a1 = X # 5000 * 401\n    z2 = a1 @ t1.T\n    a2 = np.insert(sigmoid(z2), 0, np.ones(m), axis=1)  # 5000*26 第一列加一列一\n    z3 = a2 @ t2.T  # 5000 * 100\n    h = sigmoid(z3)  # 5000 * 10 这是 h_theta(X)\n \n    return a1, z2, a2, z3, h  # 把每一层的计算都返回\n \n#_, _, _, _, h = feed_forward(theta, X)\n \n#print(h.shape) # (5000, 10)\n代价函数与正则化\n![](机器学习习题/images/ex4-NN back propagation_3.Figure_4.png)\n![](机器学习习题/images/ex4-NN back propagation_4.Figure_5.png)\ndef cost(theta, X, y):\n    m = X.shape[0]\n    _, _, _, _, h = feed_forward(theta, X)\n    pair_computation = -np.multiply(y, np.log(h)) - np.multiply((1 - y), np.log(1 - h))\n    return pair_computation.sum() / m\n \n \ncost_res = cost(theta, X, y)\nprint(&quot;cost:&quot;,cost_res)\n \n \ndef regularized_cost(theta, X, y, l=1):\n    t1, t2 = deserialize(theta)  # t1: (25,401) t2: (10,26)\n    m = X.shape[0]\n \n    reg_t1 = np.power(t1[:, 1:], 2).sum()\n    reg_t2 = np.power(t2[:, 1:], 2).sum()\n    reg = (1 / (2 * m)) * (reg_t1 + reg_t2)\n \n    return cost(theta, X, y) + reg\n \n \nregularized_cost_res = regularized_cost(theta, X, y)\nprint(&quot;reg cost:&quot;,regularized_cost_res)\n反向传播\ndef sigmoid_gradient(z):\n    return np.multiply(sigmoid(z), 1 - sigmoid(z))\n \n \nprint(sigmoid_gradient(0))  #0.25 \n \n \ndef gradient(theta, X, y):\n    t1, t2 = deserialize(theta)\n    m = X.shape[0]\n    deltal = np.zeros(t1.shape)\n    delta2 = np.zeros(t2.shape)\n \n    a1, z2, a2, z3, h = feed_forward(theta, X)\n \n    for i in range(m):\n        a1i = a1[i, :]\n        z2i = z2[i, :]\n        a2i = a2[i, :]\n \n        hi = h[i, :]\n        yi = y[i, :]\n \n        d3i = hi - yi\n \n        z2i = np.insert(z2i, 0, np.ones(1))\n        d2i = np.multiply(t2.T @ d3i, sigmoid_gradient(z2i))\n \n        delta2 += np.matrix(d3i).T @ np.matrix(a2i)\n        deltal += np.matrix(d2i[1:]).T @ np.matrix(a1i)\n \n \n    delta1 = deltal / m\n    delta2 = delta2 / m\n \n    return serialize(delta1, delta2)\n \n \nd1, d2 = deserialize(gradient(theta, X, y))\nprint(d1.shape, d2.shape) # (25, 401) (10, 26)\n梯度校验\n\n梯度正则化：\n\ndef regularized_gradient(theta, X, y, l=1):\n    &quot;&quot;&quot;don&#039;t regularize theta of bias terms&quot;&quot;&quot;\n    m = X.shape[0]\n    delta1, delta2 = deserialize(gradient(theta, X, y))\n    t1, t2 = deserialize(theta)\n \n    t1[:, 0] = 0\n    reg_term_d1 = (l / m) * t1\n    delta1 = delta1 + reg_term_d1\n \n    t2[:, 0] = 0\n    reg_term_d2 = (l / m) * t2\n    delta2 = delta2 + reg_term_d2\n \n    return serialize(delta1, delta2)\n \ndef expand_array(arr):\n    &quot;&quot;&quot;replicate array into matrix\n    [1, 2, 3]\n \n    [[1, 2, 3],\n     [1, 2, 3],\n     [1, 2, 3]]\n    &quot;&quot;&quot;\n    # turn matrix back to ndarray\n    return np.array(np.matrix(np.ones(arr.shape[0])).T @ np.matrix(arr))\n \ndef gradient_checking(theta, X, y, epsilon, regularized=False):\n    def a_numeric_grad(plus, minus, regularized=False):\n        &quot;&quot;&quot;calculate a partial gradient with respect to 1 theta&quot;&quot;&quot;\n        if regularized:\n            return (regularized_cost(plus, X, y) - regularized_cost(minus, X, y)) / (epsilon * 2)\n        else:\n            return (cost(plus, X, y) - cost(minus, X, y)) / (epsilon * 2)\n \n    theta_matrix = expand_array(theta)  # expand to (10285, 10285)\n    epsilon_matrix = np.identity(len(theta)) * epsilon\n \n    plus_matrix = theta_matrix + epsilon_matrix\n    minus_matrix = theta_matrix - epsilon_matrix\n \n    # calculate numerical gradient with respect to all theta\n    numeric_grad = np.array([a_numeric_grad(plus_matrix[i], minus_matrix[i], regularized)\n                                    for i in range(len(theta))])\n \n    # analytical grad will depend on if you want it to be regularized or not\n    analytic_grad = regularized_gradient(theta, X, y) if regularized else gradient(theta, X, y)\n \n    # If you have a correct implementation, and assuming you used EPSILON = 0.0001\n    # the diff below should be less than 1e-9\n    # this is how original matlab code do gradient checking\n    diff = np.linalg.norm(numeric_grad - analytic_grad) / np.linalg.norm(numeric_grad + analytic_grad)\n \n    print(&#039;If your backpropagation implementation is correct,\\nthe relative difference will be smaller than 10e-9 (assume epsilon=0.0001).\\nRelative Difference: {}\\n&#039;.format(diff))\n \n \n# gradient_checking(theta, X, y, epsilon= 0.0001)#这个运行很慢，谨慎运行\nIf your backpropagation implementation is correct,\nthe relative difference will be smaller than 10e-9 (assume epsilon=0.0001).\nRelative Difference: 2.1466000818218673e-09\n准备训练模型\ndef random_init(size):\n    return np.random.uniform(-0.12, 0.12, size)\n \ndef nn_training(X, y):\n    init_theta = random_init(10285) # 25 * 401 + 10 * 26\n \n    res = opt.minimize(fun=regularized_cost,\n                       x0=init_theta,\n                       args=(X ,y, 1),\n                       method=&#039;TNC&#039;,\n                       jac=regularized_gradient,\n                       options={&#039;maxiter&#039;: 400})\n    return res\n \nres = nn_training(X, y) # 慢\nprint(res)\nOut put：\n     fun: 0.32211992072588747\n     jac: array([ 2.15004329e-04,  3.88985627e-08, -3.33174201e-08, ...,\n        3.15328424e-05,  2.82831419e-05, -1.68082404e-05])\n message: &#039;Max. number of function evaluations reached&#039;\n    nfev: 400\n     nit: 26\n  status: 3\n success: False\n       x: array([ 0.00000000e+00,  1.94492814e-04, -1.66587101e-04, ...,\n       -7.15493763e-01, -1.36561388e+00, -2.90127262e+00])\n\n显示准确率\n_, y_answer = load_data(&#039;./data/ex4data1.mat&#039;)\n \nfinal_theta = res.x\n \ndef show_accuracy(theta, X, y):\n    _, _, _, _, h = feed_forward(theta, X)\n \n    y_pred = np.argmax(h, axis=1) + 1\n \n    print(classification_report(y, y_pred))\n \nshow_accuracy(final_theta, X, y_answer)\nOut Put:\n             precision    recall  f1-score   support\n\n          1       1.00      0.79      0.88       500\n          2       0.73      1.00      0.85       500\n          3       0.82      0.99      0.89       500\n          4       1.00      0.89      0.94       500\n          5       1.00      0.86      0.92       500\n          6       0.94      0.99      0.97       500\n          7       0.99      0.81      0.89       500\n          8       0.94      0.95      0.95       500\n          9       0.96      0.95      0.95       500\n         10       0.96      0.98      0.97       500\n\navg / total       0.93      0.92      0.92      5000\n\n显示隐藏层\ndef plot_hidden_layer(theta):\n    &quot;&quot;&quot;\n    theta: (10285, )\n    &quot;&quot;&quot;\n    final_theta1, _ = deserialize(theta)\n    hidden_layer = final_theta1[:, 1:]  # ger rid of bias term theta\n \n    fig, ax_array = plt.subplots(nrows=5, ncols=5, sharey=True, sharex=True, figsize=(5, 5))\n \n    for r in range(5):\n        for c in range(5):\n            ax_array[r, c].matshow(hidden_layer[5 * r + c].reshape((20, 20)),\n                                   cmap=matplotlib.cm.binary)\n            plt.xticks(np.array([]))\n            plt.yticks(np.array([]))\n \nplot_hidden_layer(final_theta)\nplt.show()\n"},"机器学习笔记/机器学习习题/ex5-bias-vs-variance":{"slug":"机器学习笔记/机器学习习题/ex5-bias-vs-variance","filePath":"机器学习笔记/机器学习习题/ex5-bias vs variance.md","title":"ex5-bias vs variance","links":[],"tags":["机器学习"],"content":"AndrewNg 机器学习习题ex5-bias vs variance\n练习用数据\nex5data1.mat文件储存了大坝出水量的数据，由三部分组成：\n\n训练集：X，y\n交叉验证集：Xval，yval\n测试集：Xtest，ytest\n\n需要的头：\nimport numpy as np\nimport scipy.io as sio\nimport scipy.optimize as opt\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n数据预处理\n画出训练集的散点图，给特征集加一列1.\ndef load_data():\n    d = sio.loadmat(&#039;./data/ex5data1.mat&#039;)\n    return map(np.ravel, [d[&#039;X&#039;], d[&#039;y&#039;], d[&#039;Xval&#039;], d[&#039;yval&#039;], d[&#039;Xtest&#039;], d[&#039;ytest&#039;]])\n \n \nX, y, Xval, yval, Xtest, ytest = load_data()\ndf = pd.DataFrame({&#039;water_level&#039;: X, &#039;flow&#039;: y})\nprint(df.shape)\nsns.lmplot(&#039;water_level&#039;, &#039;flow&#039;, data=df, fit_reg=False)\nplt.show()\n \nX, Xval, Xtest = [np.insert(x.reshape(x.shape[0], 1), 0, np.ones(x.shape[0]), axis=1) for x in (X, Xval, Xtest)]\n# print(X, Xval, Xtest )\n\n正则化\n代价函数是：\n\n梯度下降：\n{\\theta_{j}}:={\\theta_{j}}-\\alpha \\frac{\\partial }{\\partial {\\theta_{j}}}J\\left(\\theta \\right)\n正则化线性回归的代价函数为：\nJ\\left( \\theta \\right)=\\frac{1}{2m}\\sum\\limits_{i=1}^{m}{[({{({h_\\theta}({{x}^{(i)}})-{{y}^{(i)}})}^{2}}+\\lambda \\sum\\limits_{j=1}^{n}{\\theta _{j}^{2}})]}\n如果我们要使用梯度下降法令这个代价函数最小化，因为我们未对\\theta_0进行正则化，所以梯度下降算法将分两种情形：\nRepeat until convergence{\n{\\theta_0}:={\\theta_0}-a\\frac{1}{m}\\sum\\limits_{i=1}^{m}{(({h_\\theta}({{x}^{(i)}})-{{y}^{(i)}})x_{0}^{(i)}})\n{\\theta_j}:={\\theta_j}-a[\\frac{1}{m}\\sum\\limits_{i=1}^{m}{(({h_\\theta}({{x}^{(i)}})-{{y}^{(i)}})x_{j}^{\\left( i \\right)}}+\\frac{\\lambda }{m}{\\theta_j}]\nfor j=1,2,...n\n}\n# 代价函数\ndef cost(theta, X, y):\n    m = X.shape[0]\n    inner = X @ theta - y  # R(m+1)\n    # 1*m @ m*1 = 1*1 矩阵乘法\n    # 一维矩阵的转置乘以它自己等于每个元素的平方和\n    return inner.T @ inner / (2 * m)\n \n \n \nprint(cost(theta, X, y,))\n# 303.9515255535976\n \n# 梯度\ndef gradient(theta, X, y):\n    m = X.shape[0]\n    return X.T @ (X @ theta - y) / m  # (m, n).T @ (m, 1) -&gt; (n, 1)\n \n \nprint(gradient(theta, X, y,))\n# [-15.30301567 598.16741084]\n \n \n# 正则化\ndef regularized_cost(theta, X, y, l=1):\n    return cost(theta, X, y) + (l / (2 * X.shape[0])) * np.power(theta[1:], 2).sum()\n \n \ndef regularized_gradient(theta, X, y, l=1):\n    m = X.shape[0]\n    regularized_term = theta.copy()\n    regularized_term[0] = 0\n    regularized_term = (l / m) * regularized_term\n    return gradient(theta, X, y) + regularized_term\n \nprint(regularized_gradient(theta, X, y, l=1))\n# [-15.30301567 598.25074417]\n训练数据\n正则化项 \\lambda=0\ndef linear_regression_np(theta, X, y, l=1):\n    res = opt.fmin_tnc(func=regularized_cost, x0=theta, fprime=regularized_gradient, args=(X, y))\n    return res\n \n \nfinal_theta = linear_regression_np(theta, X, y)[0]\nb = final_theta[0]\nm = final_theta[1]\n \nplt.scatter(X[:, 1], y, label=&quot;Training data&quot;)\nplt.plot(X[:, 1], X[:, 1]*m + b, label=&#039;Prediction&#039;)\nplt.legend(loc=2)\nplt.show()\n\n学习曲线\ndef plot_learning_curve(X, y, Xval, yval, l=0):\n    training_cost, cv_cost = [], []  # 计算训练集的代价和交叉验证（cross validation）集的代价\n    m = X.shape[0]\n    for i in range(1, m + 1):\n        res = linear_regression_np(theta, X[:i, :], y[:i], l=0)\n \n        tc = regularized_cost(res[0], X[:i, :], y[:i], l=0)\n        cv = regularized_cost(res[0], Xval, yval, l=0)\n \n        training_cost.append(tc)\n        cv_cost.append(cv)\n \n    plt.plot(np.arange(1, m + 1), training_cost, label=&#039;training cost&#039;)\n    plt.plot(np.arange(1, m + 1), cv_cost, label=&#039;cv cost&#039;)\n    plt.legend(loc=1)\n \n \nplot_learning_curve(X, y, Xval, yval, l=0)\nplt.show()\n\n观察学习曲线发现拟合的不太好，欠拟合。很显然我们的模型不优秀，改为多项式特征尝试。\n多项式特征\n把特征扩展到8阶，然后归一化特征值。\ndef poly_features(x, power, as_ndarray=False):\n    data = {&#039;f{}&#039;.format(i): np.power(x, i) for i in range(1, power + 1)}\n    df = pd.DataFrame(data)\n    return df.as_matrix() if as_ndarray else df\n \n \n# 归一化特征值，减去平均数除以标准差\ndef normalize_feature(df):\n    &quot;&quot;&quot;Applies function along input axis(default 0) of DataFrame.&quot;&quot;&quot;\n    return df.apply(lambda column: (column - column.mean()) / column.std())\n \n \ndef prepare_poly_data(*args, power):\n    &quot;&quot;&quot;\n    args: keep feeding in X, Xval, or Xtest\n        will return in the same order\n    &quot;&quot;&quot;\n    def prepare(x):\n        # expand feature\n        df = poly_features(x, power=power)\n \n        # normalization\n        ndarr = normalize_feature(df).as_matrix()\n \n        # add intercept term\n        return np.insert(ndarr, 0, np.ones(ndarr.shape[0]), axis=1)\n \n    return [prepare(x) for x in args]\n尝试不同的λ来观察学习曲线\nX, y, Xval, yval, Xtest, ytest = load_data()\nX_poly, Xval_poly, Xtest_poly= prepare_poly_data(X, Xval, Xtest, power=8)\n \nplot_learning_curve(X_poly, y, Xval_poly, yval, l=0)\nplt.show()\nplot_learning_curve(X_poly, y, Xval_poly, yval, l=1)\nplt.show()\nplot_learning_curve(X_poly, y, Xval_poly, yval, l=100)\nplt.show()\n当λ取0时，也就是没有正则项时，可以看到训练的代价太低了，不真实. 这是 过拟合了\n\n当训练代价增加了些，不再是0了。 稍减轻了过拟合\n\n当λ取100时，正则化过多，变成了欠拟合。\n\n最优λ\n# 找到最佳拟合\nl_candidate = [0, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10]\ntraining_cost, cv_cost = [], []\nfor l in l_candidate:\n    theta = np.ones(X_poly.shape[1])\n    theta = linear_regression_np(theta, X_poly, y, l)[0]\n \n    tc = cost(theta, X_poly, y)\n    cv = cost(theta, Xval_poly, yval)\n    training_cost.append(tc)\n    cv_cost.append(cv)\n \nplt.plot(l_candidate, training_cost, label=&#039;training&#039;)\nplt.plot(l_candidate, cv_cost, label=&#039;cross validation&#039;)\nplt.legend(loc=2)\nplt.xlabel(&#039;lambda&#039;)\nplt.ylabel(&#039;cost&#039;)\nplt.show()\n \n \n# best cv I got from all those candidates\nl_candidate[np.argmin(cv_cost)]\n \n# use test data to compute the cost\nfor l in l_candidate:\n    theta = np.ones(X_poly.shape[1])\n    theta = linear_regression_np(theta, X_poly, y, l)[0]\n    print(&#039;test cost(l={}) = {}&#039;.format(l, cost(theta, Xtest_poly, ytest)))\n\ntest cost(l=0) = 9.799399498688892\ntest cost(l=0.001) = 11.054987989655938\ntest cost(l=0.003) = 11.249198861537238\ntest cost(l=0.01) = 10.879605199670008\ntest cost(l=0.03) = 10.022734920552129\ntest cost(l=0.1) = 8.632060998872074\ntest cost(l=0.3) = 7.336602384055533\ntest cost(l=1) = 7.46630349664086\ntest cost(l=3) = 11.643928200535115\ntest cost(l=10) = 27.715080216719304\n\n调参后， lambda = 0.3 是最优选择，这个时候测试代价最小"},"机器学习笔记/机器学习习题/ex6-SVM":{"slug":"机器学习笔记/机器学习习题/ex6-SVM","filePath":"机器学习笔记/机器学习习题/ex6-SVM.md","title":"ex6-SVM","links":[],"tags":["机器学习"],"content":"AndrewNg 机器学习习题ex6-SVM\n练习用数据\n线性SVM\n#!/usr/bin/python\n# coding=utf-8\n \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sb\nfrom scipy.io import loadmat\nfrom sklearn import svm\n \n# 在一个简单的二位数据集中 SVM中不同的C处理结果\nraw_data = loadmat(&#039;data/ex6data1.mat&#039;)\nprint(raw_data)\n \ndata = pd.DataFrame(raw_data[&#039;X&#039;], columns=[&#039;X1&#039;, &#039;X2&#039;])\ndata[&#039;y&#039;] = raw_data[&#039;y&#039;]\n \npositive = data[data[&#039;y&#039;].isin([1])]\nnegative = data[data[&#039;y&#039;].isin([0])]\n \nfig, ax = plt.subplots(figsize=(12,8))\n \nax.scatter(positive[&#039;X1&#039;], positive[&#039;X2&#039;], s=50, marker=&#039;x&#039;, label=&#039;Positive&#039;)\nax.scatter(negative[&#039;X1&#039;], negative[&#039;X2&#039;], s=50, marker=&#039;o&#039;, label=&#039;Negative&#039;)\nax.legend()\nplt.show()\n \nsvc = svm.LinearSVC(C=1, loss=&#039;hinge&#039;, max_iter=1000)\nprint(svc)\n \n# 首先看下C=1的结果\nsvc.fit(data[[&#039;X1&#039;, &#039;X2&#039;]], data[&#039;y&#039;])\nscore = svc.score(data[[&#039;X1&#039;, &#039;X2&#039;]], data[&#039;y&#039;])\nprint(score) # 0.9803921568627451\n \n# 当C=100的时候\nsvc2 = svm.LinearSVC(C=100, loss=&#039;hinge&#039;, max_iter=1000)\nsvc2.fit(data[[&#039;X1&#039;, &#039;X2&#039;]], data[&#039;y&#039;])\nscore2 = svc2.score(data[[&#039;X1&#039;, &#039;X2&#039;]], data[&#039;y&#039;])\nprint(score2) # 0.9411764705882353 每次执行的结果可能不同 \n \ndata[&#039;SVM 1 Confidence&#039;] = svc.decision_function(data[[&#039;X1&#039;, &#039;X2&#039;]])\nfig, ax = plt.subplots(figsize=(12, 8))\nax.scatter(data[&#039;X1&#039;], data[&#039;X2&#039;], s=50, c=data[&#039;SVM 1 Confidence&#039;], cmap=&#039;seismic&#039;)\nax.set_title(&#039;SVM (C=1) Decision Confidence&#039;)\nplt.show()\n \ndata[&#039;SVM 2 Confidence&#039;] = svc2.decision_function(data[[&#039;X1&#039;, &#039;X2&#039;]])\n \nfig, ax = plt.subplots(figsize=(12,8))\nax.scatter(data[&#039;X1&#039;], data[&#039;X2&#039;], s=50, c=data[&#039;SVM 2 Confidence&#039;], cmap=&#039;seismic&#039;)\nax.set_title(&#039;SVM (C=100) Decision Confidence&#039;)\nplt.show()\n\n\n\n高斯核函数\n# 核函数\ndef gaussian_kernel(x1, x2, sigma):\n    return np.exp(-(np.sum((x1 - x2) ** 2) / (2 * (sigma ** 2))))\n \n \nx1 = np.array([1.0, 2.0, 1.0])\nx2 = np.array([0.0, 4.0, -1.0])\nsigma = 2\n \ngaussian_kernel(x1, x2, sigma)\n \n# 0.32465246735834974\n非线性决策边界\nraw_data = loadmat(&#039;data/ex6data2.mat&#039;)\ndata = pd.DataFrame(raw_data[&#039;X&#039;], columns=[&#039;X1&#039;, &#039;X2&#039;])\ndata[&#039;y&#039;] = raw_data[&#039;y&#039;]\n \npositive = data[data[&#039;y&#039;].isin([1])]\nnegative = data[data[&#039;y&#039;].isin([0])]\n \nfig, ax = plt.subplots(figsize=(12, 8))\nax.scatter(positive[&#039;X1&#039;], positive[&#039;X2&#039;], s=30, marker=&#039;x&#039;, label=&#039;Positive&#039;)\nax.scatter(negative[&#039;X1&#039;], negative[&#039;X2&#039;], s=30, marker=&#039;o&#039;, label=&#039;Negative&#039;)\nax.legend()\nplt.show()\n\n对于该数据集，我们将使用内置的RBF内核构建支持向量机分类器，并检查其对训练数据的准确性。 为了可视化决策边界，这一次我们将根据实例具有负类标签的预测概率来对点做阴影。 从结果可以看出，它们大部分是正确的。\nsvc = svm.SVC(C=100, gamma=10, probability=True)\nprint(svc)\n \nsvc.fit(data[[&#039;X1&#039;, &#039;X2&#039;]], data[&#039;y&#039;])\nsvc.score(data[[&#039;X1&#039;, &#039;X2&#039;]], data[&#039;y&#039;])\ndata[&#039;Probability&#039;] = svc.predict_proba(data[[&#039;X1&#039;, &#039;X2&#039;]])[:,0]\nfig, ax = plt.subplots(figsize=(12,8))\nax.scatter(data[&#039;X1&#039;], data[&#039;X2&#039;], s=30, c=data[&#039;Probability&#039;], cmap=&#039;Reds&#039;)\nplt.show()\n\n搜索最佳参数\n# 搜索最佳参数\nraw_data = loadmat(&#039;data/ex6data3.mat&#039;)\nX = raw_data[&#039;X&#039;]\nXval = raw_data[&#039;Xval&#039;]\ny = raw_data[&#039;y&#039;].ravel()\nyval = raw_data[&#039;yval&#039;]. ravel()\n \nC_values = [0.001, 0.003, 0.1, 0.3, 1, 3, 10, 30, 100]\ngamma_values = [0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30, 100]\n \nbest_score = 0\nbest_params = {&#039;C&#039;: None, &#039;gamma&#039;:None}\n \nfor C in C_values:\n    for gamma in gamma_values:\n        svc = svm.SVC(C=C, gamma=gamma)\n        svc.fit(X, y)\n        score = svc.score(Xval, yval)\n \n        if score &gt; best_score:\n            best_score = score\n            best_params[&#039;C&#039;] = C\n            best_params[&#039;gamma&#039;] = gamma\n \nprint(best_params, best_score)\n{‘C’: 0.3, ‘gamma’: 100} 0.965\n垃圾邮件过滤\n现在，我们将进行第二部分的练习。 在这一部分中，我们的目标是使用SVM来构建垃圾邮件过滤器。 在练习文本中，有一个任务涉及一些文本预处理，以获得适合SVM处理的格式的数据。 然而，这个任务很简单（将字词映射到为练习提供的字典中的ID），而其余的预处理步骤（如HTML删除，词干，标准化等）已经完成。 我将跳过机器学习任务，而不是重现这些预处理步骤，其中包括从预处理过的训练集构建分类器，以及将垃圾邮件和非垃圾邮件转换为单词出现次数的向量的测试数据集。\n# 垃圾邮件过滤\nmat_tr = loadmat(&#039;data/spamTrain.mat&#039;)\nX, y = mat_tr.get(&#039;X&#039;), mat_tr.get(&#039;y&#039;).ravel()\nprint(X.shape, y.shape)  # ((4000, 1899), (4000,))\n \nmat_test = loadmat(&#039;data/spamTest.mat&#039;)\ntest_X, test_y = mat_test.get(&#039;Xtest&#039;), mat_test.get(&#039;ytest&#039;).ravel()\nprint(test_X.shape, test_y.shape)  # ((1000, 1899), (1000,))\n \nsvc = svm.SVC()\nsvc.fit(X, y)\npred = svc.predict(test_X)\nprint(metrics.classification_report(test_y, pred))\n             precision    recall  f1-score   support\n\n          0       0.94      0.99      0.97       692\n          1       0.98      0.87      0.92       308\n\navg / total       0.95      0.95      0.95      1000\n\n这个结果是使用使用默认参数的。 。\n然后用逻辑回归来计算后精确的达到了99%\n# 如果是逻辑回归呢？\nlogit = LogisticRegression()\nlogit.fit(X, y)\npred = logit.predict(test_X)\nprint(metrics.classification_report(test_y, pred))\n             precision    recall  f1-score   support\n\n          0       1.00      0.99      0.99       692\n          1       0.97      0.99      0.98       308\n\navg / total       0.99      0.99      0.99      1000\n\n调整参数后也可以达到和逻辑回归一样的精确度\nsvc = svm.SVC(C=100)\nsvc.fit(X, y)\npred = svc.predict(test_X)\nprint(metrics.classification_report(test_y, pred))\n             precision    recall  f1-score   support\n\n          0       1.00      0.99      0.99       692\n          1       0.97      0.99      0.98       308\n\navg / total       0.99      0.99      0.99      1000\n"},"机器学习笔记/机器学习习题/ex7-k-means-and-PCA":{"slug":"机器学习笔记/机器学习习题/ex7-k-means-and-PCA","filePath":"机器学习笔记/机器学习习题/ex7-k means and PCA.md","title":"ex7-k means and PCA","links":[],"tags":["机器学习"],"content":"AndrewNg 机器学习习题ex7-k means and PCA\n练习用数据\n在本练习中，我们将实现K-means聚类，并使用它来压缩图像。 我们将从一个简单的2D数据集开始，以了解K-means是如何工作的，然后我们将其应用于图像压缩。 我们还将对主成分分析进行实验，并了解如何使用它来找到面部图像的低维表示。\nImplementing K-means\n我们将实施和应用K-means到一个简单的二维数据集，以获得一些直观的工作原理。 K-means是一个迭代的，无监督的聚类算法，将类似的实例组合成簇。 该算法通过猜测每个簇的初始聚类中心开始，然后重复将实例分配给最近的簇，并重新计算该簇的聚类中心。 我们要实现的第一部分是找到数据中每个实例最接近的聚类中心的函数。\n可视化数据\n#!/usr/bin/python\n# coding=utf-8\n \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sb\nfrom scipy.io import loadmat\n \n# 数据可视化\ndata = loadmat(&#039;data/ex7data2.mat&#039;)\nX = data[&#039;X&#039;]\ndata2 = pd.DataFrame(data.get(&#039;X&#039;), columns=[&#039;X1&#039;, &#039;X2&#039;])\nprint(data2.head())\nsb.set(context=&quot;notebook&quot;, style=&quot;white&quot;)\nsb.lmplot(&#039;X1&#039;, &#039;X2&#039;, data=data2, fit_reg=False)\nplt.show()\n\nFinding closest centroids\nc^{(i)} := j\\ that\\ minimizes\\ ||x^i - u_j||^2 \n计算每一个特征值到所选取的聚类中心的距离，纪录最短距离的聚类中心编号。\ndef find_closest_centroids(X, centroids):\n    m = X.shape[0]\n    k = centroids.shape[0]\n    idx = np.zeros(m)\n \n    for i in range(m):\n        min_dist = 1000000\n        for j in range(k):\n            dist = np.sum((X[i, :] - centroids[j, :]) ** 2)\n \n            if dist &lt; min_dist:\n                min_dist = dist\n \n                idx[i] = j\n \n    return idx\n  \n    \ninitial_centroids = initial_centroids = np.array([[3, 3], [6, 2], [8, 5]])\nidx = find_closest_centroids(X, initial_centroids)\nprint(idx[: 3])\n[0. 2. 1.]\n输出与文本中的预期值匹配（记住我们的数组是从零开始索引的，而不是从一开始索引的，所以值比练习中的值低一个）。\nComputing centroid means\n接下来，我们需要一个函数来计算簇的聚类中心。 聚类中心只是当前分配给簇的所有样本的平均值。\ndef compute_centroids(X, idx, k):\n    m, n = X.shape\n    centroids = np.zeros((k, n))\n \n    for i in range(k):\n        indices = np.where(idx == i)\n        centroids[i, :] = (np.sum(X[indices, :], axis=1) / len(indices[0])).ravel()\n \n    return centroids\n \n \nprint(compute_centroids(X, idx, 3))\n[[2.42830111 3.15792418]\n[5.81350331 2.63365645]\n[7.11938687 3.6166844 ]]\n此输出也符合练习中的预期值。 下一部分涉及实际运行该算法的一些迭代次数和可视化结果。 这个步骤是由于并不复杂，我将从头开始构建它。 为了运行算法，我们只需要在将样本分配给最近的簇并重新计算簇的聚类中心。\nK-means on example dataset\ncentroids_trace = np.empty(shape=[0, 2])\n \n \ndef run_k_means(X, initial_centroids, max_iters):\n    m, n = X.shape\n    k = initial_centroids.shape[0]\n    idx = np.zeros(m)\n    centroids = initial_centroids\n    global centroids_trace\n    for i in range(max_iters):\n        idx = find_closest_centroids(X, centroids)\n        centroids = compute_centroids(X, idx, k)\n        centroids_trace = np.append(centroids_trace, centroids, axis=0)\n \n    return idx, centroids\n \n \ndef init_centroids(X, k):\n    m, n = X.shape\n    centroids = np.zeros((k, n))\n    idx = np.random.randint(0, m, k)\n \n    for i in range(k):\n        centroids[i, :] = X[idx[i], :]\n \n    return centroids\n \n \ninitial_centroids = init_centroids(X, 3)\nidx, centroids = run_k_means(X, initial_centroids, 10)\n \ncluster1 = X[np.where(idx == 0)[0], :]\ncluster2 = X[np.where(idx == 1)[0], :]\ncluster3 = X[np.where(idx == 2)[0], :]\n \nfig, ax = plt.subplots(figsize=(12, 8))\n \nax.scatter(cluster1[:, 0], cluster1[:, 1], s=30, color=&#039;r&#039;, label=&#039;Cluster 1&#039;)\nax.scatter(cluster2[:, 0], cluster2[:, 1], s=30, color=&#039;g&#039;, label=&#039;Cluster 2&#039;)\nax.scatter(cluster3[:, 0], cluster3[:, 1], s=30, color=&#039;b&#039;, label=&#039;Cluster 3&#039;)\nax.legend()\n \nx = centroids_trace[:, 0]\ny = centroids_trace[:, 1]\nax.scatter(x, y, color=&#039;black&#039;, s=50, zorder=2)\nplt.show()\n\nImage compression with K-means\n我们的下一个任务是将K-means应用于图像压缩。 从下面的演示可以看到，我们可以使用聚类来找到最具代表性的少数颜色，并使用聚类分配将原始的24位颜色映射到较低维的颜色空间。\n下面是我们要压缩的图像。\n\nimage_data = loadmat(&#039;data/bird_small.mat&#039;)\nprint(image_data)\nA = image_data[&#039;A&#039;]\nprint(A.shape)\n \n# 数据预处理\n# normalize value ranges\nA = A / 255\n \n# reshape the array\nprint((A.shape[0] * A.shape[1], A.shape[2]))\nX = np.reshape(A, (128*128, 3))\nprint(X.shape)\n \n# randomly initalize the centroids\ninitial_centroids = init_centroids(X, 16)\n \n# run the algorithm\nidx, centroids = run_k_means(X, initial_centroids, 10)\n \n# gor the closet centroids one last time\nidx = find_closest_centroids(X, centroids)\n \n# map each poxel to the centroid value\nX_recovered = centroids[idx.astype(int), :]\nprint(X_recovered.shape)\n \n# reshape to the original dimensions\nX_recovered = np.reshape(X_recovered, (A.shape[0], A.shape[1], A.shape[2]))\n用scikit-learn来实现K-means\nfrom sklearn.cluster import KMeans  # 导入kmeans库\n \nmodel = KMeans(n_clusters=16, n_init=100, n_jobs=1)\nmodel.fit(X)\ncentroids = model.cluster_centers_\nprint(centroids.shape)\nC = model.predict(X)\nprint(C.shape)\nprint(centroids[C].shape)\n \ncompressed_pic = centroids[C].reshape((128, 128, 3))\n \nfig, ax = plt.subplots(1, 3)\nax[0].imshow(A)\nax[1].imshow(X_recovered)\nax[2].imshow(compressed_pic)\nplt.show()\n\n通过K-means算法，我们让图像以更少的色彩来显示实现压缩，但是图像的主要特征仍然存在。\nPrincipal component analysis（主成分分析）\nPCA是在数据集中找到“主成分”或最大方差方向的线性变换。 它可以用于降维。 在本练习中，我们首先负责实现PCA并将其应用于一个简单的二维数据集，以了解它是如何工作的。 我们从加载和可视化数据集开始。\ndata = loadmat(&#039;data/ex7data1.mat&#039;)\nX = data[&#039;X&#039;]\n \n \ndef pca(X):\n    # normalize the feature\n    X = (X - X.mean()) / X.std()\n \n    # compute the covariance matrix\n    X = np.matrix(X)\n    cov = (X.T * X) / X.shape[0]\n \n    # perform SVD\n    U, S, V = np.linalg.svd(cov)\n \n    return U, S, V\n \n \nU, S, V = pca(X)\nprint(U, S, V)\n \n \ndef project_data(X, U, k):\n    U_reduced = U[:, :k]\n    return np.dot(X, U_reduced)\n \n \nZ = project_data(X, U, 1)\n \n \ndef recover_data(Z, U, k):\n    U_reduced = U[:, :k]\n    return np.dot(Z, U_reduced.T)\n \n \nX_recovered = recover_data(Z, U, 1)\n \nfig, ax = plt.subplots(1, 2)\nax[0].scatter(X[:, 0], X[:, 1])\nax[1].scatter(list(X_recovered[:, 0]), list(X_recovered[:, 1]))\nplt.show()\n\n请注意，第一主成分的投影轴基本上是数据集中的对角线。 当我们将数据减少到一个维度时，我们失去了该对角线周围的变化，所以在我们的再现中，一切都沿着该对角线。\n我们在此练习中的最后一个任务是将PCA应用于脸部图像。 通过使用相同的降维技术，我们可以使用比原始图像少得多的数据来捕获图像的“本质”\ndef plot_n_image(X, n):\n    &quot;&quot;&quot; plot first n images\n    n has to be a square number\n    &quot;&quot;&quot;\n    pic_size = int(np.sqrt(X.shape[1]))\n    grid_size = int(np.sqrt(n))\n    first_n_images = X[:n, :]\n \n    fig, ax_array = plt.subplots(nrows=grid_size, ncols=grid_size,\n                                    sharey=True, sharex=True, figsize=(8, 8))\n    for r in range(grid_size):\n        for c in range(grid_size):\n            ax_array[r, c].imshow(first_n_images[grid_size * r + c].reshape((pic_size, pic_size)).T, cmap=plt.cm.gray)\n            plt.xticks(np.array([]))\n            plt.yticks(np.array([]))\n \n \nfaces = loadmat(&#039;data/ex7faces.mat&#039;)\nX = faces[&#039;X&#039;]\nprint(X.shape)\nplot_n_image(X, 100)\nface1 = np.reshape(X[1,:], (32, 32)).T\n \nU, S, V = pca(X)\nZ = project_data(X, U, 100)\nprint(Z.shape)\nX_recovered = recover_data(Z, U, 100)\nface2 = np.reshape(X_recovered[1,:], (32, 32)).T\n \nfig, ax = plt.subplots(1, 2)\nax[0].imshow(face1, cmap=plt.cm.gray)\nax[1].imshow(face2, cmap=plt.cm.gray)\nplt.show()\n# 计算平均均方差误差与训练集方差的比例\nprint(np.sum(S[:100]) / np.sum(S))  # 0.9434273519364477\n\n我们把1024个特征缩减到100个时还保留了94%的差异值。"},"机器学习笔记/机器学习习题/ex8-anomaly-detection-and-recommendation":{"slug":"机器学习笔记/机器学习习题/ex8-anomaly-detection-and-recommendation","filePath":"机器学习笔记/机器学习习题/ex8-anomaly detection and recommendation.md","title":"ex8-anomaly detection and recommendation","links":[],"tags":["机器学习"],"content":"AndrewNg 机器学习习题ex6-anomaly detection and recommendation\n这是最后一个练习了，共有两个算法，第一个是异常检测，第二个是推荐系统。\n异常检测\n之前写过了这里就不再重复了：Python实现异常检测算法\n推荐系统\n推荐系统使用的算法就是协同过滤（collaborative ltering learning algorithm）\n首先来看提供的数据都有些什么，更具PDF可知，有5个文件是我们需要的数据集合。\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n数据集名称内容movie_ids.txt电影的列表ex8data1.mat用于异常检测的第一个示例数据集ex8data2.mat用于异常检测的第二个示例数据集ex8_movies.mat电影评论数据集ex8_movieParams.mat为调试提供的参数\n导入库和检查数据集\nex8_movies.mat中有两个标签的数据，Y是1682个电影的评分，每个电影有943条五个级别的评分，R是一个和Y相同维度的二进制数组，0代表评过分，1代表没评分。\n% Notes: X - num_movies (1682)  x num_features (10) matrix of movie features\n%        Theta - num_users (943)  x num_features (10) matrix of user features\n%        Y - num_movies x num_users matrix of user ratings of movies\n%        R - num_movies x num_users matrix, where R(i, j) = 1 if the\n%            i-th movie was rated by the j-th user\n#!/usr/bin/python\n# coding=utf-8\nimport scipy.io as sio\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport pandas as pd\n \nsns.set(context=&quot;notebook&quot;, style=&quot;white&quot;)\n \n# Y是包含从1到5的等级的（数量的电影x数量的用户）数组.R是包含指示用户是否给电影评分的二进制值的“指示符”数组。\nmovies_mat = sio.loadmat(&#039;./data/ex8_movies.mat&#039;);\nY, R = movies_mat.get(&#039;Y&#039;), movies_mat.get(&#039;R&#039;)\nprint(Y.shape, R.shape)\n# (1682, 943) (1682, 943)\n \nm, u = Y.shape\n# m: how many movies\n# u: how many users\nn = 10\n# how many features for a movie\n \nparam_mat = sio.loadmat(&#039;./data/ex8_movieParams.mat&#039;)\ntheta, X = param_mat.get(&#039;Theta&#039;), param_mat.get(&#039;X&#039;)\nprint(theta.shape, X.shape)\n# (943, 10) (1682, 10)\ncost function\n\n在对feature运算时，我们先把params serialize为只有一个维度的数组，通过deserialize函数来恢复为原状。\ndef serialize(X, theta):\n    # serialize 2 matrix\n    # X(move, feature), (1682, 10): movie features\n    # theta (user, feature), (943, 10): user preference\n    # 1682*10 + 943*10 = (26250,)\n    return np.concatenate((X.ravel(), theta.ravel()))\n \n \ndef deserialize(param, n_movie, n_user, n_featuers):\n    # into ndarray of X(1682, 10), theta(943, 10)\n    return param[:n_movie * n_featuers].reshape(n_movie, n_featuers),\\\n            param[n_movie * n_featuers:].reshape(n_user, n_featuers)\n \n \n# recomendation fn\ndef cost(param, Y, R, n_features):\n    &quot;&quot;&quot;compute cost for every r(i, j) = 1\n        arg:\n            param: serialized X, theta\n            Y (movie, user), (1682, 943): (movie, user) rating\n            R (movie, user), (1682, 943): (movie, user) has rating\n    &quot;&quot;&quot;\n    # theta (user, feat)\n    # X(movie, feature), (1682, 10): movie features\n    n_movie, n_user = Y.shape\n    X, theta = deserialize(param, n_movie, n_user, n_features)\n    inner = np.multiply(X @ theta.T - Y, R)\n    return np.power(inner, 2).sum() / 2\n \n \ndef gradient(param, Y, R, n_features):\n    # theta (user, feature), (943, 10): user preference\n    # X(movie, feature), (1682, 10): movie features\n    n_movies, n_user = Y.shape\n    X, theta = deserialize(param, n_movies, n_user, n_features)\n \n    inner = np.multiply(X @ theta.T - Y, R)  # (1682, 943)\n \n    # X_grad (1682, 10)\n    X_grad = inner @ theta\n \n    # theta_grad (943, 10)\n    theta_grad = inner.T @ X\n \n    # roll them together and return\n    return serialize(X_grad, theta_grad)\n \n \ndef regularized_cost(param, Y, R, n_features, l=1):\n    reg_term = np.power(param, 2).sum() * (1/2)\n    return cost(param, Y, R, n_features) + reg_term\n \n \ndef regularized_gradient(param, Y, R, n_features, l=1):\n    grad = gradient(param, Y, R, n_features)\n    reg_term = l * param\n    return grad + reg_term\n按照练习8中的参数cost输出为22，验证结果“\n# 按照练习中给出计算结果为22\nusers = 4\nmovies = 5\nfeatures = 3\n \nX_sub = X[:movies, :features]\ntheta_sub = theta[:users, :features]\nY_sub = Y[:movies, :users]\nR_sub = R[:movies, :users]\n \nparam_sub = serialize(X_sub, theta_sub)\nc = cost(param_sub, Y_sub, R_sub, features)\nprint(c)  # 22.224603725685675\n计算一下总的cost\n# total readl params\nparam = serialize(X, theta)\n# total cost\ntotal_cost = cost(param, Y, R, 10)\nprint(total_cost)  # 27918.64012454421\ngradient function\n\n\nn_movie, n_user = Y.shape\nX_grad, theta_grad = deserialize(gradient(param, Y, R, 10),\n                                n_movie, n_user, 10)\n \nassert X_grad.shape == X.shape\nassert theta_grad.shape == theta.shape\nregularized cost and gradient\n\n# regularized cost\n# in the ex8_confi.m, lambda = 1.5, and it&#039;s using sub data set\nreg_cost = regularized_cost(param_sub, Y_sub, R_sub, features, l=1.5)\nprint(reg_cost)  # 28.304238738078038\n# total regularized cost\ntotal_cost = regularized_cost(param, Y, R, 10, l=1)\nprint(total_cost)   # 32520.682450229557\n \nn_movie, n_user = Y.shape\n \nX_grad, theta_grad = deserialize(regularized_gradient(param, Y, R, 10),\n                                                      n_movie, n_user, 10)\n \nassert X_grad.shape == X.shape\nassert theta_grad.shape == theta.shape\nparse movie_id.txt\n# parse movie_id.txt\nmovie_list = []\nwith open(&#039;./data/movie_ids.txt&#039;, encoding=&#039;latin-1&#039;) as f:\n    for line in f:\n        tokens = line.strip().split(&#039; &#039;)\n        movie_list.append(&#039; &#039;.join(tokens[1:]))\n\nmovie_list = np.array(movie_list)\n\n给电影打分\n# reproduce my ratings\nratings = np.zeros(1682)\nratings[0] = 4\nratings[6] = 3\nratings[11] = 5\nratings[53] = 4\nratings[63] = 5\nratings[65] = 3\nratings[68] = 5\nratings[97] = 2\nratings[182] = 4\nratings[225] = 5\nratings[354] = 5\n数据预处理\n把我们的评价插入到所有电影的评分中去，把参数theta和X处理为正态分布。\n# prepare data\n# now I become user 0\nY, R = movies_mat.get(&#039;Y&#039;), movies_mat.get(&#039;R&#039;)\nY = np.insert(Y, 0, ratings, axis=1)\nR = np.insert(R, 0, ratings != 0, axis=1)\nprint(Y.shape)  # (1682, 944)\nprint(R.shape)  # (1682, 944)\n \nn_features = 50\nn_movie, n_user = Y.shape\nl = 10\n \n# 转换为正态分布\nX = np.random.standard_normal((n_movie, n_features))\ntheta = np.random.standard_normal((n_user, n_features))\n \nprint(X.shape, theta.shape)  # (1682, 50) (944, 50)\n \nparam = serialize(X, theta)\n \n# normalized ratings\nY_norm = Y - Y.mean()\nprint(Y_norm.mean())  # 4.6862111343939375e-17\n训练\n# training\nimport scipy.optimize as opt\nres = opt.minimize(fun=regularized_cost,\n                   x0=param,\n                   args=(Y_norm, R, n_features, l),\n                   method=&#039;TNC&#039;,\n                   jac=regularized_gradient)\n\nprint(res)\n\n稍等一会儿得到一下结果\n     fun: 24268.448311691616\n     jac: array([-12.49378802,  14.209063  ,  -6.75343791, ...,   0.61519582,\n        -1.32599207,   0.58813019])\n message: &#039;Converged (|f_n-f_(n-1)| ~= 0)&#039;\n    nfev: 219\n     nit: 14\n  status: 1\n success: True\n       x: array([-0.30795529,  0.88620348, -0.10899471, ...,  0.18986581,\n       -0.28537047, -0.11540767])\n\n检查推荐结果\ny=np.argsort(x)将x中的元素从小到大排列，提取其对应的index(索引)，然后输出到y\nX_trained, theta_trained = deserialize(res.x, n_movie, n_user, n_features)\nprint(X_trained.shape, theta_trained.shape)\n\nprediction = X_trained @ theta_trained.T\nmy_preds = prediction[:, 0] + Y.mean()\n\nidx = np.argsort(my_preds)[::-1]  # descending order\nprint(idx.shape)\n\n# top ten idx\nmy_preds[idx][:10]\n\nfor m in movie_list[idx][:10]:\n    print(m)\n\nGodfather, The (1972)\nForrest Gump (1994)\nStar Wars (1977)\nTitanic (1997)\nShawshank Redemption, The (1994)\nRaiders of the Lost Ark (1981)\nReturn of the Jedi (1983)\nUsual Suspects, The (1995)\nBraveheart (1995)\nEmpire Strikes Back, The (1980)\n\n每次得到的结果有个别差别，但是七成时没有变化的。"},"机器学习笔记/机器学习总结":{"slug":"机器学习笔记/机器学习总结","filePath":"机器学习笔记/机器学习总结.md","title":"机器学习总结","links":[],"tags":["机器学习"],"content":"Andrew Ng的机器学习入门课程已经全部看完了，笔记也写了一些，这里总结所有所学的内容，说实话，现在完全忘记了开始所学的内容了。\n什么是机器学习\nArthur Samuel。他定义机器学习为，在进行特定编程的情况下给予计算学习能力的领域。\nTom Mitchell。他定义的的机器学习是，一个程序被认为能从经验E中学习，解决任务T，达到性能度量值P，当且仅当，有了经验E后，经过P评判，程序再处理T时的性能有所提升。\n周志华。他再机器学习一书中的意思是，让机器从数据中学习，进而得到一个更加符合现实规律的模型，通过对模型的使用使得机器比以往表现的更好，这就是机器学习。\n我的愚见。机器学习就是在已有的数据中发现规律再寻找符合这个规律的数据。\n监督学习\n回归（房价预测），分类（肿瘤预测），给出特征值与其对应的结果。\n无监督学习\n聚类（新闻、邮件的分类），只根据特征值寻找其中的规律。\n线性回归\n模型表示\nm：训练集中实例的数量\nx：特征值/输入变量\ny：目标值/输出变量\n（x，y）：训练集中的实例\n第i个实例：(x^i, y^i)\nh：学习算法中的解决方案或函数，也称为假设（hypothesis）\nh_\\theta(x)=\\theta_0+\\theta_1x\n线性回归代价函数\n预测函数h_\\theta(x)是关于x的函数,而代价函数是一个关于(\\theta_0,\\theta_1)的函数\nJ(\\theta_0,\\theta_1) = \\frac{1}{2m} \\sum^m_{i=1}(h_\\theta(x^i)-y^i)^2\n优化目标：minimize J(\\theta_0,\\theta_1)\n梯度下降\n梯度下降是一个用来求函数最小值的算法，我们将使用梯度下降算法来求出代价函数J(\\theta_0,\\theta_1)的最小值。\n梯度下降背后的思想是：开始时我们随机选择一个参数组合(\\theta_0, \\theta_1, ......,\\theta_n)，计算代价函数，然后我们寻找下一个能让代价函数值下降最多的参数组合。我们持续这么做直到到一个局部最小值，因为我们没有尝试完所有的参数组合，所以不能确定我们得到的局部最小值是否是全局最小值，选择不同的初始参数组合，可能回找到不同的局部最小值。\n线性回归问题运用梯度下降法，关键在于求出代价函数的导数，即：\n\\frac{\\partial }{\\partial {{\\theta }{j}}}J({{\\theta }{0}},{{\\theta }{1}})=\\frac{\\partial }{\\partial {{\\theta }{j}}}\\frac{1}{2m}{{\\sum\\limits_{i=1}^{m}{\\left( {{h}_{\\theta }}({{x}^{(i)}})-{{y}^{(i)}} \\right)}}^{2}}\nj=0 时：\\frac{\\partial }{\\partial {{\\theta }{0}}}J({{\\theta }{0}},{{\\theta }{1}})=\\frac{1}{m}{{\\sum\\limits{i=1}^{m}{\\left( {{h}_{\\theta }}({{x}^{(i)}})-{{y}^{(i)}} \\right)}}}\nj=1 时：\\frac{\\partial }{\\partial {{\\theta }{1}}}J({{\\theta }{0}},{{\\theta }{1}})=\\frac{1}{m}\\sum\\limits{i=1}^{m}{\\left( \\left( {{h}_{\\theta }}({{x}^{(i)}})-{{y}^{(i)}} \\right)\\cdot {{x}^{(i)}} \\right)}\n则算法写成：\nRepeat {\n​ {\\theta_{0}}:={\\theta_{0}}-a\\frac{1}{m}\\sum\\limits_{i=1}^{m}{ \\left({{h}_{\\theta }}({{x}^{(i)}})-{{y}^{(i)}} \\right)}\n​ {\\theta_{1}}:={\\theta_{1}}-a\\frac{1}{m}\\sum\\limits_{i=1}^{m}{\\left( \\left({{h}_{\\theta }}({{x}^{(i)}})-{{y}^{(i)}} \\right)\\cdot {{x}^{(i)}} \\right)}\n​ }\n特征缩放\n尝试将所有特征的尺度都尽量缩放到-1到1之间，\n最简单的方法是令：{{x}{n}}=\\frac{{{x}{n}}-{{\\mu}{n}}}{{{s}{n}}}，其中 {\\mu_{n}}是平均值，{s_{n}}是标准差。\n学习速率\n梯度下降算法的每次迭代受到学习率的影响，如果学习率a过小，则达到收敛所需的迭代次数会非常高；如果学习率a过大，每次迭代可能不会减小代价函数，可能会越过局部最小值导致无法收敛。\n通常可以考虑尝试些学习率：\n\\alpha=0.01，0.03，0.1，0.3，1，3，10\n正规方程\n正规方程是通过求解下面的方程来找出使得代价函数最小的参数的：\\frac{\\partial}{\\partial{\\theta_{j}}}J\\left( {\\theta_{j}} \\right)=0 。 假设我们的训练集特征矩阵为 X（包含了 {{x}_{0}}=1）并且我们的训练集结果为向量 y，则利用正规方程解出向量 \\theta ={{\\left( {X^T}X \\right)}^{-1}}{X^{T}}y 。\n梯度下降与正规方程的比较\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n梯度下降正规方程需要选择学习速率不需要需要多次迭代需要计算{{\\left( {X^T}X \\right)}^{-1}}{X^{T}}如果特征数量n较大则运算代价大，因为矩阵逆的计算时间复杂读为O(n^3)，通常来说n小于一万时还可以接受适用于各种类型的模型只适用于线性模型，不适合逻辑回归等其他模型\n总结一下，只要特征变量的数目并不大，标准方程是一个很好的计算参数\\theta 的替代方法。具体地说，只要特征变量数量小于一万，通常使用标准方程法，而不使用梯度下降法。\n逻辑回归\n逻辑回归(Logistic Regression)一般用在分类问题中。\n假设函数\nh_\\theta(x) = g(\\theta^TX)\ng\\left( z \\right)=\\frac{1}{1+{{e}^{-z}}}\nX代表特征向量，g代表逻辑函数(Logistic function)，常用的逻辑函数为S形函数(Sigmoid function)\n\n判定边界\n在逻辑回归中，我们预测：\n当{h_\\theta}\\left( x \\right)&gt;=0.5时，预测 y=1。\n当{h_\\theta}\\left( x \\right)&lt;0.5时，预测 y=0。\n根据 S 形函数图像，我们知道当\nz=0 时 g(z)=0.5\nz&gt;0 时 g(z)&gt;0.5\nz&lt;0 时 g(z)&lt;0.5\n又 z={\\theta^{T}}x，即：\n{\\theta^{T}}x&gt;=0 时，预测 y=1.\n{\\theta^{T}}x&lt;0 时，预测 y=0\n接下来看价函数\n逻辑回归代价函数\n逻辑回归的代价函数为：\nJ\\left( \\theta \\right)=\\frac{1}{m}\\sum\\limits_{i=1}^{m}{{Cost}\\left( {h_\\theta}\\left( {x}^{\\left( i \\right)} \\right),{y}^{\\left( i \\right)} \\right)}.\n{h_\\theta}\\left( x \\right)与 Cost\\left( {h_\\theta}\\left( x \\right),y \\right)之间的关系如下图所示：.\n\n这样构建的Cost\\left( {h_\\theta}\\left( x \\right),y \\right)函数的特点是：当实际的 y=1 且{h_\\theta}\\left( x \\right)也为 1 时误差为 0，当 y=1 但{h_\\theta}\\left( x \\right)不为1时误差随着{h_\\theta}\\left( x \\right)变小而变大；当实际的 y=0 且{h_\\theta}\\left( x \\right)也为 0 时代价为 0，当y=0 但{h_\\theta}\\left( x \\right)不为 0时误差随着 {h_\\theta}\\left( x \\right)的变大而变大。 将构建的 Cost\\left( {h_\\theta}\\left( x \\right),y \\right)简化如下： Cost\\left( {h_\\theta}\\left( x \\right),y \\right)=-y\\times log\\left( {h_\\theta}\\left( x \\right) \\right)-(1-y)\\times log\\left( 1-{h_\\theta}\\left( x \\right) \\right) 带入代价函数得到：\nJ\\left( \\theta \\right)=\\frac{1}{m}\\sum\\limits_{i=1}^{m}{[-{{y}^{(i)}}\\log \\left( {h_\\theta}\\left( {{x}^{(i)}} \\right) \\right)-\\left( 1-{{y}^{(i)}} \\right)\\log \\left( 1-{h_\\theta}\\left( {{x}^{(i)}} \\right) \\right)]}.\n即：\nJ\\left( \\theta \\right)=-\\frac{1}{m}\\sum\\limits_{i=1}^{m}{[{{y}^{(i)}}\\log \\left( {h_\\theta}\\left( {{x}^{(i)}} \\right) \\right)+\\left( 1-{{y}^{(i)}} \\right)\\log \\left( 1-{h_\\theta}\\left( {{x}^{(i)}} \\right) \\right)]}.\n在得到这样一个代价函数以后，我们便可以用梯度下降算法来求得能使代价函数最小的参数了。算法为：\nRepeat { \\theta_j := \\theta_j - \\alpha \\frac{\\partial}{\\partial\\theta_j} J(\\theta) (simultaneously update all ) }\n求导后得到：\nRepeat { \\theta_j := \\theta_j - \\alpha \\frac{1}{m}\\sum\\limits_{i=1}^{m}{{\\left( {h_\\theta}\\left( \\mathop{x}^{\\left( i \\right)} \\right)-\\mathop{y}^{\\left( i \\right)} \\right)}}\\mathop{x}_{j}^{(i)} (simultaneously update all ) }\n高级优化\n共轭梯度法 BFGS (变尺度法)\nL-BFGS (限制变尺度法)\n线性搜索(line search)\n正则化\n正则化可以改善或者减少过拟合问题。\n...+\\frac{\\lambda}{2m}\\sum\\limits_{j=1}^{n}\\theta_j^2\n神经网络\n当特征他多时，需要神经网络。\n标记方法\n训练样本数：m\n输入信号：x\n输出信号：y\n神经网络层数：L\n每层的neuron个数：S_1 - S_L\n神经网络的分类\n二类分类：S_L = 0, y = 0 or 1\nK类分类：S_L = k, y_i = 1 (k &gt; 2)\n\n代价函数\n\\newcommand{\\subk}[1]{ #1_k } h_\\theta\\left(x\\right)\\in \\mathbb{R}^{K} {\\left({h_\\theta}\\left(x\\right)\\right)}_{i}={i}^{th} \\text{output}\nJ(\\Theta) = -\\frac{1}{m} \\left[ \\sum\\limits_{i=1}^{m} \\sum\\limits_{k=1}^{k} {y_k}^{(i)} \\log \\subk{(h_\\Theta(x^{(i)}))} + \\left( 1 - y_k^{(i)} \\right) \\log \\left( 1- \\subk{\\left( h_\\Theta \\left( x^{(i)} \\right) \\right)} \\right) \\right] + \\frac{\\lambda}{2m} \\sum\\limits_{l=1}^{L-1} \\sum\\limits_{i=1}^{s_l} \\sum\\limits_{j=1}^{s_l+1} \\left( \\Theta_{ji}^{(l)} \\right)^2\n反向传播\n向前传播的算法是:\n\n\n反向传播的算法就是先正向传播计算出每一层的激活单元，然后利用训练集的结果与神经网络预测的结果求出最后一层的误差，然后利用该误差运用反向传播计算出直至第二层的所有误差。\n在求出了\\Delta_{ij}^{(l)}之后，我们便可以计算代价函数的偏导数了，计算方法如下：\n\n{if}; j \\neq 0\nD_{ij}^{(l)} :=\\frac{1}{m}\\Delta_{ij}^{(l)}\n{if}; j = 0\n神经网络的总结\n网络结构：第一件要做的事是选择网络结构，即决定选择多少层以及决定每层分别有多少个单元。\n第一层的单元数即我们训练集的特征数量。\n最后一层的单元数是我们训练集的结果的类的数量。\n如果隐藏层数大于1，确保每个隐藏层的单元个数相同，通常情况下隐藏层单元的个数越多越好。\n我们真正要决定的是隐藏层的层数和每个中间层的单元数。\n训练神经网络：\n参数的随机初始化\n利用正向传播方法计算所有的h_{\\theta}(x)\n编写计算代价函数 J 的代码\n利用反向传播方法计算所有偏导数\n利用数值检验方法检验这些偏导数\n使用优化算法来最小化代价函数"},"机器学习笔记/机器学习策略":{"slug":"机器学习笔记/机器学习策略","filePath":"机器学习笔记/机器学习策略.md","title":"机器学习策略","links":[],"tags":["深度学习"],"content":"当我们的系统达到了90%的准确率时，觉得还是不够好，我们有很多想法去改善我们的系统，比如，去收集更多的训练数据，收集更多不同姿势图片丰富样本的多样性，或者更多的反例集。或者再用梯度下降训练算法，训练久一点。或者尝试用一个完全不同的优化算法，比如Adam优化算法。或者尝试使用规模更大或者更小的神经网络。或者试试dropout或者L2正则化。或者想要修改网络的架构，比如修改激活函数，改变隐藏单元的数目之类的方法。\n如何选择更好的方法而不是浪费时间，这里记录下一些策略。\n单一评估指标\npercision：查准率\n被你的分类器中标记为真的例子中，有多少真的为真\n\nrecall：查全率\n对于所有为真的例子，有多少被识别出来\n\n查准率和查全率之间往往需要折衷，两个指标都要顾及到，用F1分数可以更好的衡量系统的优劣\nF_1 = \\frac{2}{\\frac{1}{P} + \\frac{1}{R}}\n在数学中，这个函数叫做查准率P和查全率R的调和平均数\n满足和优化指标\n除了F1分数或者其它衡量准确度的指标外，我们还要考虑运行时间，就是需要多长时间来分类一张图。A分类器需要80毫秒，B需要95毫秒，C需要1500毫秒，就是说需要1.5秒来分类图像。\n\n如何选取上图中的分类器，可以这样判断一下代价\ncost = accuracy - 0.5 × running time\n\n这个方法可能太过刻意，当然在具体情况具体考虑。\n训练/开发/测试集合\n设立训练集，开发集和测试集的方式大大影响了建立机器学习应用方面取得进展的速度。所以，我们希望最终应用的目标数据是和训练集合来自同一处。比如，我们想要做手机摄像头识别猫，训练集最好来自手机拍摄的图片而不是在网上爬取的很清晰的图片或者卡通猫之类。\n开发集和测试集的大小\n如果你总共有100个样本，这样70/30或者60/20/20分的经验法则是相当合理的。如果你有几千个样本或者有一万个样本，这些做法也还是合理的。\n但在现代机器学习中，我们更习惯操作规模大得多的数据集，比如说你有1百万个训练样本，这样分可能更合理，98%作为训练集，1%开发集，1%测试集。\n错误率指标\nError = \\frac{1}{\\sum_{}^{}w^{(i)}}\\sum_{i = 1}^{m_{{dev}}}{w^{(i)}I\\{ y_{{pred}}^{(i)} \\neq y^{(i)}\\}}\n可避免偏差\n我们使用猫分类器来做例子，比如人类具有近乎完美的准确度，所以人类水平的错误是1%。在这种情况下，如果我们的学习算法达到8%的训练错误率和10%的开发错误率，你的算法在训练集上的表现和人类水平的表现有很大差距的话，说明你的算法对训练集的拟合并不好。所以从减少偏差和方差的工具这个角度看，在这种情况下，我会把重点放在减少偏差上。需要做的是，比如说训练更大的神经网络，或者跑久一点梯度下降，就试试能不能在训练集上做得更好。\n进行误差分析\n进行错误分析，应该找一组错误样本，可能在你的开发集里或者测试集里，观察错误标记的样本，看看假阳性（false positives）和假阴性（false negatives），统计属于不同错误类型的错误数量。在这个过程中，可能会得到启发，归纳出新的错误类型。如果过了一遍错误样本，然后说，天，有这么多Instagram滤镜或Snapchat滤镜，这些滤镜干扰了我的分类器，你就可以在途中新建一个错误类型。总之，通过统计不同错误标记类型占总数的百分比，可以帮发现哪些问题需要优先解决，或者构思新优化方向的灵感。\n检查是否有错误的标记，当我们的数据量较少时，应该检查下是否有错误的标记。\n迁移学习\n迁移学习最有用的场合是，如果你尝试优化任务B的性能，通常这个任务数据相对较少，例如，在放射科中你知道很难收集很多X射线扫描图来搭建一个性能良好的放射科诊断系统，所以在这种情况下，你可能会找一个相关但不同的任务，如图像识别，其中你可能用1百万张图片训练过了，并从中学到很多低层次特征，所以那也许能帮助网络在任务B在放射科任务上做得更好，尽管任务B没有这么多数据。迁移学习什么时候是有意义的？它确实可以显著提高你的学习任务的性能，但任务A实际上数据量比任务B要少，这种情况下增益可能不多。\n多任务学习\n在多任务学习中，是同时开始学习的，试图让单个神经网络同时做几件事情，然后希望这里每个任务都能帮到其他所有任务。\n多任务学习能让你训练一个神经网络来执行许多任务，这可以给你更高的性能，比单独完成各个任务更高的性能。但要注意，实际上迁移学习比多任务学习使用频率更高。我看到很多任务都是，如果你想解决一个机器学习问题，但你的数据集相对较小，那么迁移学习真的能帮到你，就是如果你找到一个相关问题，其中数据量要大得多，你就能以它为基础训练你的神经网络，然后迁移到这个数据量很少的任务上来。\n端到端的深度学习\n简而言之，以前有一些数据处理系统或者学习系统，它们需要多个阶段的处理。那么端到端深度学习就是忽略所有这些不同的阶段，用单个神经网络代替它。\n我们来看一些例子，以语音识别为例，你的目标是输入，比如说一段音频，然后把它映射到一个输出，就是这段音频的听写文本。所以传统上，语音识别需要很多阶段的处理。首先你会提取一些特征，一些手工设计的音频特征，也许你听过MFCC，这种算法是用来从音频中提取一组特定的人工设计的特征。在提取出一些低层次特征之后，你可以应用机器学习算法在音频片段中找到音位，所以音位是声音的基本单位，比如说“Cat”这个词是三个音节构成的，Cu-、Ah-和Tu-，算法就把这三个音位提取出来，然后你将音位串在一起构成独立的词，然后你将词串起来构成音频片段的听写文本。\n所以和这种有很多阶段的流水线相比，端到端深度学习做的是，你训练一个巨大的神经网络，输入就是一段音频，输出直接是听写文本。AI的其中一个有趣的社会学效应是，随着端到端深度学习系统表现开始更好，有一些花了大量时间或者整个事业生涯设计出流水线各个步骤的研究员，还有其他领域的研究员，不只是语言识别领域的，也许是计算机视觉，还有其他领域，他们花了大量的时间，写了很多论文，有些甚至整个职业生涯的一大部分都投入到开发这个流水线的功能或者其他构件上去了。而端到端深度学习就只需要把训练集拿过来，直接学到了和之间的函数映射，直接绕过了其中很多步骤。对一些学科里的人来说，这点相当难以接受，他们无法接受这样构建AI系统，因为有些情况，端到端方法完全取代了旧系统，某些投入了多年研究的中间组件也许已经过时了。\n事实证明，端到端深度学习的挑战之一是，你可能需要大量数据才能让系统表现良好，比如，你只有3000小时数据去训练你的语音识别系统，那么传统的流水线效果真的很好。但当你拥有非常大的数据集时，比如10,000小时数据或者100,000小时数据，这样端到端方法突然开始很厉害了。所以当你的数据集较小的时候，传统流水线方法其实效果也不错，通常做得更好。你需要大数据集才能让端到端方法真正发出耀眼光芒。如果你的数据量适中，那么也可以用中间件方法，你可能输入还是音频，然后绕过特征提取，直接尝试从神经网络输出音位，然后也可以在其他阶段用，所以这是往端到端学习迈出的一小步，但还没有到那里。\n另一个例子，比如说你希望观察一个孩子手部的X光照片，并估计一个孩子的年龄。\n处理这个例子的一个非端到端方法，就是照一张图，然后分割出每一块骨头，所以就是分辨出那段骨头应该在哪里，那段骨头在哪里，那段骨头在哪里，等等。然后，知道不同骨骼的长度，你可以去查表，查到儿童手中骨头的平均长度，然后用它来估计孩子的年龄，所以这种方法实际上很好。\n相比之下，如果你直接从图像去判断孩子的年龄，那么你需要大量的数据去直接训练。据我所知，这种做法今天还是不行的，因为没有足够的数据来用端到端的方式来训练这个任务。\n你可以想象一下如何将这个问题分解成两个步骤，第一步是一个比较简单的问题，也许你不需要那么多数据，也许你不需要许多X射线图像来切分骨骼。而任务二，收集儿童手部的骨头长度的统计数据，你不需要太多数据也能做出相当准确的估计，所以这个多步方法看起来很有希望，也许比端对端方法更有希望，至少直到你能获得更多端到端学习的数据之前。\n所以端到端深度学习系统是可行的，它表现可以很好，也可以简化系统架构，让你不需要搭建那么多手工设计的单独组件。\n是否要使用端到端的深度学习\n应用端到端学习的一些好处，首先端到端学习真的只是让数据说话。所以如果你有足够多的数据，那么不管从x到y最适合的函数映射是什么，如果你训练一个足够大的神经网络，希望这个神经网络能自己搞清楚，而使用纯机器学习方法，直接从到输入去训练的神经网络，可能更能够捕获数据中的任何统计信息，而不是被迫引入人类的成见。\n例如，在语音识别领域，早期的识别系统有这个音位概念，就是基本的声音单元，如cat单词的“cat”的Cu-、Ah-和Tu-，我觉得这个音位是人类语言学家生造出来的，我实际上认为音位其实是语音学家的幻想，用音位描述语言也还算合理。但是不要强迫你的学习算法以音位为单位思考，这点有时没那么明显。如果你让你的学习算法学习它想学习的任意表示方式，而不是强迫你的学习算法使用音位作为表示方式，那么其整体表现可能会更好。\n端到端深度学习的第二个好处就是这样，所需手工设计的组件更少，所以这也许能够简化你的设计工作流程，你不需要花太多时间去手工设计功能，手工设计这些中间表示方式。\n这里有一些缺点，首先，它可能需要大量的数据。要直接学到这个到的映射。\n另一个缺点是，它排除了可能有用的手工设计组件。如果你没有很多数据，你的学习算法就没办法从很小的训练集数据中获得洞察力。当你有大量数据时，手工设计的东西就不太重要了，但是当你没有太多的数据时，构造一个精心设计的系统，实际上可以将人类对这个问题的很多认识直接注入到问题里，进入算法里应该挺有帮助的。\n所以端到端深度学习的弊端之一是它把可能有用的人工设计的组件排除在外了，精心设计的人工组件可能非常有用，但它们也有可能真的伤害到你的算法表现。例如，强制你的算法以音位为单位思考，也许让算法自己找到更好的表示方法更好。所以这是一把双刃剑，可能有坏处，可能有好处，但往往好处更多，手工设计的组件往往在训练集更小的时候帮助更大。"},"机器学习笔记/正则化":{"slug":"机器学习笔记/正则化","filePath":"机器学习笔记/正则化.md","title":"正则化","links":[],"tags":["机器学习","正则化"],"content":"在机器学习中遇到了难题，就是对正则化的理解，通过查阅资料，记录下什么是正则化。\n正则化\n模型选择的典型方法是正则化（regularization）。正则化是结构风险最小化策略的实现，是在经验风险上加一个正则化项（regularizer）或罚项（penalty term）。正则化项一般是模型复杂度的单调递增函数，模型越复杂，正则化值越大。比如，正则化项可以是模型参数向量的范数。\n在这里我又遇到了一个问题，什么是范数，哎，高数没学好，啥也不知道了。\n\n范数(norm)是数学中的一种基本概念。在泛函分析中，它定义在赋范线性空间中，并满足一定的条件，即①非负性；②齐次性；③三角不等式。它常常被用来度量某个向量空间（或矩阵）中的每个向量的长度或大小。\n\n看完定义又是头大，完全不理解啊！\n定义和性质什么的都不重要了，这里我只需要知道范数所代表的函数意义：\n1-范数：║x║1=│x1│+│x2│+…+│xn│\n2-范数：║x║2=（│x1│2+│x2│2+…+│xn│2）1/2\n∞-范数：║x║∞=max（│x1│，│x2│，…，│xn│）\n其中2-范数就是通常意义下的距离。\n\n矩阵范数：\n1-范数：\n║A║1 = max{ ∑|ai1|，∑|ai2|，……，∑|ain| } （列和范数，A每一列元素绝对值之和的最大值）（其中∑|ai1|第一列元素绝对值的和∑|ai1|=|a11|+|a21|+...+|an1|，其余类似）；\n2-范数：\n║A║2 = A的最大奇异值 = (max{ λi(AH*A) }) 1/2 （谱范数，即A^H*A特征值λi中最大者λ1的平方根，其中AH为A的转置共轭矩阵）；\n∞-范数：\n║A║∞ = max{ ∑|a1j|，∑|a2j|,...，∑|amj| } （行和范数，A每一行元素绝对值之和的最大值）（其中∑|a1j| 为第一行元素绝对值的和，其余类似）；\n\n看了这几个例子大概理解了，若\n，那么\n\n继续正则化的话题，正则化主要解决的问题：\n1.正则化就是对最小化经验误差函数上加约束，这样的约束可以解释为先验知识(正则化参数等价于对参数引入先验分布)。约束有引导作用，在优化误差函数的时候倾向于选择满足约束的梯度减少的方向，使最终的解倾向于符合先验知识(如一般的l-norm先验，表示原问题更可能是比较简单的，这样的优化倾向于产生参数值量级小的解，一般对应于稀疏参数的平滑解)。\n2.同时，正则化解决了逆问题的不适定性，产生的解是存在，唯一同时也依赖于数据的，噪声对不适定的影响就弱，解就不会过拟合，而且如果先验(正则化)合适，则解就倾向于是符合真解(更不会过拟合了)，即使训练集中彼此间不相关的样本数很少。\n正则化一般具有如下形式：\n\n其中第一项是经验风险，第二项是正则化项，λ&gt;=0为调整两者关系的系数。正则化项可以取不同的形式，例如，回归问题，损失函数是平方损失，正则化项可以使参数向量的2范类。\n范类的记录大概就是这么多了。\n泰勒公式\n顺便说下泰勒公式，是在学习吴恩达的机器学习正则化时才想到的，我们的预测模型是一个多项式的和，当多项式过少会欠拟合，过多会过拟合，当多项式足够多的时候就会区分出所有的种类，这不就是泰勒公式展开式吗？\n\n数学中，泰勒公式是一个用函数在某点的信息描述其附近取值的公式。如果函数足够平滑的话，在已知函数在某一点的各阶导数值的情况之下，泰勒公式可以用这些导数值做系数构建一个多项式来近似函数在这一点的邻域中的值。\n\n泰勒公式是将一个在x=x0处具有n阶导数的函数f(x)利用关于(x-x0)的n次多项式来逼近函数的方法。\n若函数f(x)在包含x0的某个闭区间[a,b]上具有n阶导数，且在开区间(a,b)上具有(n+1)阶导数，则对闭区间[a,b]上任意一点x，成立下式：\n其中,fn^(x)表示fn(x)的n阶导数，等号后的多项式称为函数f(x)在x0处的泰勒展开式，剩余的Rn(x)是泰勒公式的余项，是(x-x0)n的高阶无穷小。\n参考：\n李航 统计学习方法"},"机器学习笔记/深度学习的参数调试":{"slug":"机器学习笔记/深度学习的参数调试","filePath":"机器学习笔记/深度学习的参数调试.md","title":"深度学习的参数调试","links":[],"tags":["深度学习"],"content":"在构建神经网络模型时，有许多的参数需要去调试，比如learning rate （学习率）、iterations(梯度下降法循环的数量)、L（隐藏层数目）、（隐藏层单元数目）、choice of activation function（激活函数的选择）除了这些基本的还有一些其他的参数，如momentum、mini batch size、regularization parameters等等。\n在这里记录下常用的几个参数的选择。\n隐藏单元 hidden units\n这是首先要考虑的问题，我们要构建多少个隐藏单元才会更适合？\n首先要明白一点，hidden units不是越多越好，具体多少要通过具体的情况尝试，虽然有许多的论文写了各种数学公式来验证样本集合与神经元的关系，但是我觉得还是在具体情况下，根据直觉来设置一个，然后再删减找到最优。\n通常的建议是设置小于输入的75%，但具体的情况具体考虑，例如我在做数字识别时，我们有0-9十个数字，设置十个隐藏单元要好于设置大于十和小于十。\n学习速率 learning rate\n这是非常重要的参数，因为学习速率的选择对梯度下降的影响最大，它可能会在0到1之间，\n所以在Python中，你可以这样做，使\npython\nr = -4*np.random.rand()\na = 10**r\n因为r \\in \\lbrack - 4,0\\rbrack所以a \\in \\lbrack 10^{-4}, 10^{0}\\rbrack，这样我们就可以在0.0001和1之间随机取出一个值。\nbatch size\n在样本非常多的情况下会使用mini batch梯度下降，我们每次迭代选择多少size来计算呢？\n首先，如果训练集较小，小于2000个样本，直接使用batch梯度下降法，也就是size=m（样本总数），样本集较小就没必要使用mini-batch梯度下降法，我们可以快速处理整个训练集，所以使用batch梯度下降法也很好。\n样本数目较大的话，一般的mini batch大小为64到512，考虑到电脑内存设置和使用的方式，如果mini batch大小是2的次方，代码会运行地快一些，64就是2的6次方，以此类推，128是2的7次方，256是2的8次方，512是2的9次方。\n最后需要注意的是在mini batch中，要确保和要符合CPU/GPU内存，取决于我们的应用方向以及训练集的大小。如果处理的mini batch和CPU/GPU内存不相符，不管用什么方法处理数据，算法的表现会急转直下变得惨不忍睹，我们需要做一些尝试，才能找到能够最有效地减少成本函数的那个，一般会尝试几个不同的值，几个不同的2次方，然后看能否找到一个让梯度下降优化算法最高效的大小。\n正则化参数\n正则话可以防止过拟合，除了之前在逻辑回归中使用的的L2正则化，还可以使用随机失活（dropout）正则化，实施dropout，在计算机视觉领域很成功。计算视觉中的输入量非常大，输入太多像素，以至于没有足够的数据，所以dropout在计算机视觉中应用得比较频繁，有些计算机视觉研究人员非常喜欢用它，几乎成了默认的选择，但要牢记一点，dropout是一种正则化方法，它有助于预防过拟合，因此除非算法过拟合，不然我是不会使用dropout的，所以它在其它领域应用得比较少，主要存在于计算机视觉领域，因为我们通常没有足够的数据，所以一直存在过拟合，这就是有些计算机视觉研究人员如此钟情于dropout函数的原因。\ndropout一大缺点就是代价函数不再被明确定义，每次迭代，都会随机移除一些节点，如果再三检查梯度下降的性能，实际上是很难进行复查的。\n扩充我们的数据可以防止过拟合，但扩增数据代价很高，而且有时候我们无法扩增数据，但我们可以通过添加这类图片来增加训练集。例如，水平翻转图片，扭曲图片，随意裁剪图片，并把它添加到训练集。所以现在训练集中有原图，还有变换后的这张图片，这虽然不如我们额外收集一组新图片那么好，但这样做节省了获取更多图片的花费。\n还有另外一种常用的方法叫作early stopping，运行梯度下降时，我们可以绘制训练误差，或只绘制代价函数的优化过程，还可以绘制验证集误差，它可以是验证集上的分类误差，或验证集上的代价函数，逻辑损失和对数损失等，你会发现，验证集误差通常会先呈下降趋势，然后在某个节点处开始上升，我们在此停止训练吧，这并不是一个很好的建议。\noptimizer\n优化器的具体选择就比较复杂了，有许多的论文，具体可以看看这篇论文：\nAn overview of gradient descent optimization algorithms\n看两张动图直观上感受下算法的优化过程。第一张图为不同算法在损失平面等高线上随时间的变化情况，第二张图为不同算法在鞍点处的行为比较。\noptimization on loss surface contours\n\noptimization on saddle point\n\ntensorflow 提供了多个优化器的api，使用起来非常简单。 tf.train.Optimizer"},"机器学习笔记/激活函数（Activation-functions）":{"slug":"机器学习笔记/激活函数（Activation-functions）","filePath":"机器学习笔记/激活函数（Activation functions）.md","title":"激活函数（Activation functions）","links":[],"tags":["激活函数"],"content":"在使用神经网络时，需要决定使用哪种激活函数作用在隐藏层上，哪种用在输出节点上。普通的激活函数有sigmoid, tanh, Relu和Relu的优化版Leaky Relu。\n为什么需要非线性激活函数\n为什么神经网络需要非线性激活函数？事实证明：要让你的神经网络能够计算出有趣的函数，你必须使用非线性激活函数，如果是用线性激活函数或者叫恒等激励函数，那么神经网络只是把输入线性组合再输出。\n事实证明，如果你使用线性激活函数或者没有使用一个激活函数，那么无论你的神经网络有多少层一直在做的只是计算线性函数，所以不如直接去掉全部隐藏层。事实证明如果你在隐藏层用线性激活函数，在输出层用sigmoid函数，那么这个模型的复杂度和没有任何隐藏层的标准Logistic回归是一样的。\n只有一个地方可以使用线性激活函数，就是你在做机器学习中的回归问题。举个例子，比如你想预测房地产价格，目标结果就不是二分类任务0或1，而是一个实数，从0到正无穷。如果目标是个实数，那么在输出层用线性激活函数也许可行，你的输出也是一个实数，从负无穷到正无穷。\n总而言之，不能在隐藏层用线性激活函数，可以用ReLU或者tanh或者leaky ReLU或者其他的非线性激活函数，唯一可以用线性激活函数的通常就是输出层；除了这种情况，会在隐层用线性函数的，除了一些特殊情况，比如与压缩有关的，那方面在这里将不深入讨论。在这之外，在隐层使用线性激活函数非常少见。因为房价都是非负数，所以我们也可以在输出层使用ReLU函数这样你的输出结果都大于等于0。\nsigmoid\n\na=\\frac{1}{1+e^x}\nsigmoid激活函数是之前在学习逻辑回归的时候使用的激活函数，但它是一个基本不使用的激活函数，tanh函数（者双曲正切函数）是总体上都优于sigmoid函数的激活函数。我们只有在二分类任务中会使用，因为在二分类的问题中，对于输出层，目标的值是0或1，所以想让预测的数值介于0和1之间，而不是在-1和+1之间。所以需要使用sigmoid激活函数。\n导数\n其具体的求导如下：\n\\frac{d}{dz}g(z) = {\\frac{1}{1 + e^{-z}} (1-\\frac{1}{1 + e^{-z}})}=g(z)(1-g(z))\n注：\n当z=10或z=-10 \\frac{d}{dz}g(z)\\approx0\n当z=0,\\frac{d}{dz}g(z)\\text{=g(z)(1-g(z))=}{1}/{4}\n在神经网络中a= g(z);g{{(z)}^{&#039;}}=\\frac{d}{dz}g(z)=a(1-a)\ntanh\n\na=\\frac{e^x-e^{-x}}{e^x+e^{-x}}\n事实上，tanh函数是sigmoid的向下平移和伸缩后的结果。对它进行了变形后，穿过了点(0, 0)，并且值域介于+1和-1之间。\n结果表明，如果在隐藏层上使用tanh函数效果总是优于sigmoid函数。因为函数值域在-1和+1的激活函数，其均值是更接近零均值的。在训练一个算法模型时，如果使用tanh函数代替sigmoid函数中心化数据，使得数据的平均值更接近0而不是0.5.\nsigmoid函数和tanh函数两者共同的缺点是，在z特别大或者特别小的情况下，导数的梯度或者函数的斜率会变得特别小，最后就会接近于0，导致降低梯度下降的速度。\n导数\n其具体的求导如下：\ng(z)=\\frac{e^x-e^{-x}}{e^x+e^{-x}}\n\\frac{d}{{d}z}g(z) = 1 - (tanh(z))^{2}\n注：\n当z=10或z=-10 \\frac{d}{dz}g(z)\\approx0\n当z=0,\\frac{d}{dz}g(z)\\text{=1-(0)=}1\nRelu\n\na=\\max\\left(0,\\ x\\right)\n在机器学习另一个很流行的函数是：修正线性单元的函数（ReLu）。只要是正值的情况下，导数恒等于1，当是负值的时候，导数恒等于0。从实际上来说，当使用的导数时，z=0的导数是没有定义的。但是当编程实现的时候，z的取值刚好等于0.00000001，这个值相当小，所以，在实践中，不需要担心这个值，是等于0的时候，假设一个导数是1或者0效果都可以。\n这有一些选择激活函数的经验法则：\n如果输出是0、1值（二分类问题），则输出层选择sigmoid函数，然后其它的所有单元都选择Relu函数。\n这是很多激活函数的默认选择，如果在隐藏层上不确定使用哪个激活函数，那么通常会使用Relu激活函数。有时，也会使用tanh激活函数，但Relu的一个优点是：当z是负值的时候，导数等于0。\n导数\ng(z)^{&#039;}=\n  \\begin{cases}\n  0&amp;\t\\text{if z &lt; 0}\\\\\n  1&amp;\t\\text{if z &gt; 0}\\\\\nundefined&amp;\t\\text{if z = 0}\n\\end{cases}\n注：通常在z=0的时候给定其导数1,0；当然z=0的情况很少\nLeaky Relu\n\na=\\max\\left(0.01x,\\ x\\right)\n这里也有另一个版本的Relu被称为Leaky Relu。当是负值时，这个函数的值不是等于0，而是轻微的倾斜，这个函数通常比Relu激活函数效果要好，尽管在实际中Leaky ReLu使用的并不多。\n两者的优点是：\n第一，在的区间变动很大的情况下，激活函数的导数或者激活函数的斜率都会远大于0，在程序实现就是一个if-else语句，而sigmoid函数需要进行浮点四则运算，在实践中，使用ReLu激活函数神经网络通常会比使用sigmoid或者tanh激活函数学习的更快。\n第二，sigmoid和tanh函数的导数在正负饱和区的梯度都会接近于0，这会造成梯度弥散，而Relu和Leaky ReLu函数大于0部分都为常数，不会产生梯度弥散现象。(同时应该注意到的是，Relu进入负半区的时候，梯度为0，神经元此时不会训练，产生所谓的稀疏性，而Leaky ReLu不会有这问题)\n在ReLu的梯度一半都是0，但是，有足够的隐藏层使得z值大于0，所以对大多数的训练数据来说学习过程仍然可以很快。\n导数\ng(z)=\\max(0.01z,z)\ng(z)^{&#039;}=\n    \\begin{cases}\n    0.01&amp; \t\\text{if z &lt; 0}\\\\\n    1&amp;\t\\text{if z &gt; 0}\\\\\n    undefined&amp;\t\\text{if z = 0}\n    \\end{cases}\n注：通常在z=0的时候给定其导数1,0.01；当然z=0的情况很少。\n总结\n概括一下不同激活函数的过程和结论。\nsigmoid激活函数：除了输出层是一个二分类问题基本不会用它。\ntanh激活函数：tanh是非常优秀的，几乎适合所有场合。\nReLu激活函数：最常用的默认函数，如果不确定用哪个激活函数，就使用ReLu或者Leaky ReLu。g(z)=\\max(0.01z,z)为什么常数是0.01？当然，可以为学习算法选择不同的参数。\n在选择自己神经网络的激活函数时，有一定的直观感受，在深度学习中的经常遇到一个问题：在编写神经网络的时候，会有很多选择：隐藏层单元的个数、激活函数的选择、初始化权值……这些选择想得到一个对比较好的指导原则是挺困难的。\n鉴于以上三个原因，以及在工业界的见闻，提供一种直观的感受，哪一种工业界用的多，哪一种用的少。但是，自己的神经网络的应用，以及其特殊性，是很难提前知道选择哪些效果更好。所以通常的建议是：如果不确定哪一个激活函数效果更好，可以把它们都试试，然后在验证集或者发展集上进行评价。然后看哪一种表现的更好，就去使用它。\n为自己的神经网络的应用测试这些不同的选择，会在以后检验自己的神经网络或者评估算法的时候，看到不同的效果。如果仅仅遵守使用默认的ReLu激活函数，而不要用其他的激励函数，那就可能在近期或者往后，每次解决问题的时候都使用相同的办法。"},"机器学习笔记/独热编码One-Hot-Encoder":{"slug":"机器学习笔记/独热编码One-Hot-Encoder","filePath":"机器学习笔记/独热编码One Hot Encoder.md","title":"独热编码One Hot Encoder","links":[],"tags":["机器学习","OneHotEncoder"],"content":"在机器学习时我们通常要进行归一化数据后再进行训练，还有一些其他处理方法比如使用独热编码。\n什么是独热码\n独热编码即 One-Hot 编码，又称一位有效编码，其方法是使用N位状态寄存器来对N个状态进行编码，每个状态都有它独立的寄存器位，并且在任意时候，其中只有一位有效。\n可以这样理解，对于每一个特征，如果它有m个可能值，那么经过独热编码后，就变成了m个二元特征。并且，这些特征互斥，每次只有一个激活。因此，数据会变成稀疏的。\n例如对六个状态进行编码：\n自然顺序码为 000,001,010,011,100,101\n独热编码则是 000001,000010,000100,001000,010000,100000\n独热码的优点\n有一些特征无法直接应用在需要数值计算的算法中，例如，用户的性别，爱好，住址等，一般简单粗暴的处理方式时直接将不同的类别映射为一个整数，比如男性为0，女性为1，其他为2，这种简单的实现最大的问题就在于各种类别的特征都被看成是有序的，这显然不符合实际场景。\n使用独热码可以处理非连续型数值特征，并且在一定程度上扩充了特征。\n1.使用one-hot编码，将离散特征的取值扩展到了欧式空间，离散特征的某个取值就对应欧式空间的某个点。\n2.将离散特征通过one-hot编码映射到欧式空间，是因为，在回归，分类，聚类等机器学习算法中，特征之间距离的计算或相似度的计算是非常重要的，而我们常用的距离或相似度的计算都是在欧式空间的相似度计算，计算余弦相似性，基于的就是欧式空间。\n3.将离散型特征使用one-hot编码，可以会让特征之间的距离计算更加合理。比如，有一个离散型特征，代表工作类型，该离散型特征，共有三个取值，不使用one-hot编码，其表示分别是x_1 = (1), x_2 = (2), x_3 = (3)。两个工作之间的距离是，(x_1, x_2) = 1, d(x_2, x_3) = 1, d(x_1, x_3) = 2。那么x_1和x_3工作之间就越不相似吗？显然这样的表示，计算出来的特征的距离是不合理。那如果使用one-hot编码，则得到x_1 = (1, 0, 0), x_2 = (0, 1, 0), x_3 = (0, 0, 1)，那么两个工作之间的距离就都是sqrt(2).即每两个工作之间的距离是一样的，显得更合理。\n编码过程\n　　假如只有一个特征是离散值：\n　　　　{sex：{male， female，other}}\n　　该特征总共有3个不同的分类值，此时需要3个bit位表示该特征是什么值，对应bit位为1的位置对应原来的特征的值（一般情况下可以将原始的特征的取值进行排序，以便于后期使用），此时得到独热码为{100}男性 ，{010}女性，{001}其他\n　　假如多个特征需要独热码编码，那么久按照上面的方法依次将每个特征的独热码拼接起来：\n　　　　{sex：{male， female，other}}\n　　　　{grade：{一年级， 二年级，三年级， 四年级}}\n　　此时对于输入为{sex：male； grade： 四年级}进行独热编码，可以首先将sex按照上面的进行编码得到{100}，然后按照grade进行编码为{0001}，那么两者连接起来得到最后的独热码{1000001}；\r\n　　\nsklearn中的One Hot Encoder\n官方文档\nfrom sklearn.preprocessing import OneHotEncoder\ny = np.array([[1, 2, 3]]).T\nencoder = OneHotEncoder(sparse=False)\ny_onehot = encoder.fit_transform(y)\nprint(y_onehot)\n[[1. 0. 0.]\r\n [0. 1. 0.]\r\n [0. 0. 1.]]\n"},"算法练习题/CodeForces978C-Letters":{"slug":"算法练习题/CodeForces978C-Letters","filePath":"算法练习题/CodeForces978C Letters.md","title":"CodeForces978C Letters","links":[],"tags":["算法","c","LeetCode"],"content":"There are  dormitories in Berland State University, they are numbered with integers from  to . Each dormitory consists of rooms, there are  rooms in -th dormitory. The rooms in -th dormitory are numbered from  to .\nA postman delivers letters. Sometimes there is no specific dormitory and room number in it on an envelope. Instead of it only a room number among all rooms of all dormitories is written on an envelope. In this case, assume that all the rooms are numbered from  to  and the rooms of the first dormitory go first, the rooms of the second dormitory go after them and so on.\nFor example, in case ,  and  an envelope can have any integer from to  written on it. If the number  is written on an envelope, it means that the letter should be delivered to the room number  of the second dormitory.\nFor each of  letters by the room number among all  dormitories, determine the particular dormitory and the room number in a dormitory where this letter should be delivered.\nInput\nThe first line contains two integers  and   — the number of dormitories and the number of letters.\nThe second line contains a sequence  , where  equals to the number of rooms in the -th dormitory. The third line contains a sequence  , where  equals to the room number (among all rooms of all dormitories) for the -th letter. All  are given in increasing order.\nOutput\nPrint  lines. For each letter print two integers  and  — the dormitory number  and the room number  in this dormitory  to deliver the letter.\nExamples\nInput\n3 6\n10 15 12\n1 9 12 23 26 37\nOutput\n1 1\n1 9\n2 2\n2 13\n3 1\n3 12\nInput\n2 3\n5 10000000000\n5 6 9999999999\nOutput\n1 5\n2 1\n2 9999999994\nNote\nIn the first example letters should be delivered in the following order:\n\nthe first letter in room  of the first dormitory\nthe second letter in room  of the first dormitory\nthe third letter in room  of the second dormitory\nthe fourth letter in room  of the second dormitory\nthe fifth letter in room  of the third dormitory\nthe sixth letter in room  of the third dormitory\n\n解析：\n题意: 查询m个数分别在一个有n个元素的递增序列中的相对位置。\n\n两种解法：\n\t暴力解法为两层嵌套循环，时间复杂度为O(m×n)；\n\t二分法查找，时间复杂度为O(m×log2n)\n\n#include &lt;stdio.h&gt;\n#define MAXN 2048 //定义一个数组的大小\n \n// 暴力求解\nint letters(int* a, int* b, int n, int m)\n{\n    int i, j, t, sum=0;\n    \n    for(j = 0; j &lt; m; j++)\n    {\n        sum = 0;\n        for(i = 0; i &lt; n; i++)\n        {\n            sum += a[i];\n            if(b[j] &lt;= sum)\n            {\n                t = sum - a[i];\n                printf(&quot;%d %d\\n&quot;, i+1, b[j] - t);\n                break;\n            }\n        }\n    }\n    \n    return 0;\n}\n \n// 二分法\nint binary_letters(int* a, int* b, int n, int m)\n{\n    int sum[MAXN] = {0};\n    int i, j, r, l, mid;\n    \n    sum[0] = a[0];\n \n    // 构建ai序列\n    for(i = 1; i &lt; n; i++)\n        sum[i] = sum[i-1] + a[i];\n \n    for(j = 0; j &lt; m; j++)\n    {\n        if(b[j] &gt; sum[i-1])\n        {\n            printf(&quot;%d index out of bounds.\\n&quot;, b[j]);\n            continue;\n        }\n        \n        for(r = n - 1, l = 0; r - l &gt; 1; )\n        {\n            mid = (r + l) / 2;\n            if(b[j] &gt; sum[mid])\n                l = mid;\n            else\n                r = mid;\n        }\n        \n        if(b[j] &gt; sum[l])\n            printf(&quot;%d %d\\n&quot;, l+2, b[j]-sum[l]);\n        else\n            printf(&quot;%d %d\\n&quot;, l+1, b[j]);\n    }\n    \n    return 0;\n}\n \nint main()\n{\n    int a[] = {10, 15, 12, 12, 12};\n    int b[] = {1, 9, 12, 23, 26, 37, 50, 62};\n \n    int n = sizeof(a) / sizeof(int);\n    int m = sizeof(b) / sizeof(int);\n \n    letters(a, b, n, m);\n    printf(&quot;================\\n&quot;);\n    binary_letters(a, b, n, m);\n    \n    return 0;\n}\nout put\n1 1\n1 9\n2 2\n2 13\n3 1\n3 12\n5 1\n================\n1 1\n1 9\n2 2\n2 13\n3 1\n3 12\n5 1\n62 index out of bounds.\n"},"算法练习题/LeetCode-16.-3Sum-Closest":{"slug":"算法练习题/LeetCode-16.-3Sum-Closest","filePath":"算法练习题/LeetCode 16. 3Sum Closest.md","title":"LeetCode 16. 3Sum Closest","links":[],"tags":["算法","c","LeetCode"],"content":"好久没写C的，写个C复习一下，写个简单的leetcode吧\n\n3Sum Closest\nGiven an array nums of n integers and an integer target, find three integers in nums such that the sum is closest to target. Return the sum of the three integers. You may assume that each input would have exactly one solution.\n\nExample:\nGiven array nums = [-1, 2, 1, -4], and target = 1.\nThe sum that is closest to the target is 2. (-1 + 2 + 1 = 2).\n这个问题比较简单，我先写了个暴力解法\nint threeSumClosest(int* nums, int numsSize, int target) {\n    int result=nums[0] + nums[1] + nums[2];\n    int i, j, k;\n    int sum;\n    \n    for(i=0; i&lt;numsSize-2; i++)\n    {\n        for(j=i+1; j&lt;numsSize-1; j++)\n        {\n            for(k=j+1; k&lt;numsSize; k++)\n            {\n                sum = nums[i] + nums[j] + nums[k];\n                if(abs(sum-target) &lt;= abs(result-target))\n                    result = sum;\n            }\n        }\n    }\n    \n    return result;\n}\nruntime为172 ms可想而知是很低的。\n看了下别人的solution，先排序再算要快好多，于是我先尝试了\n冒泡排序\nvoid bubble_sort(int arr[], int len) {\n    int i, j, temp;\n    for (i = 0; i &lt; len - 1; i++)\n        for (j = 0; j &lt; len - 1 - i; j++)\n            if (arr[j] &gt; arr[j + 1]) {\n                temp = arr[j];\n                arr[j] = arr[j + 1];\n                arr[j + 1] = temp;\n            }\n}\nRuntime 为8 ms，还是差点，仔细看了下，因该是我的排序算法慢了，看到别人的solution用c中的快排qsort()，可惜我之前没学过这个函数,所以没想到，赶紧恶补下。\n原文地址\nvoid qsort(\n    void *base,\n    size_t nmemb,\n    size_t size,\n    int (*compar)(const void *, const void *)\n    );\n    \n函数功能：qsort()函数的功能是对数组进行排序，数组有nmemb个元素，每个元素大小为size。\n\n参数base       - base指向数组的起始地址，通常该位置传入的是一个数组名\n参数nmemb  - nmemb表示该数组的元素个数\n参数size        - size表示该数组中每个元素的大小（字节数）\n参数(*compar)(const void *, const void *) - 此为指向比较函数的函数指针，决定了排序的顺序。\n\n函数返回值：无\n\n注意：如果两个元素的值是相同的，那么它们的前后顺序是不确定的。也就是说qsort()是一个不稳定的排序算法。\n\ncompar参数\ncompar参数指向一个比较两个元素的函数。比较函数的原型应该像下面这样。注意两个形参必须是const void *型，同时在调用compar 函数（compar实质为函数指针，这里称它所指向的函数也为compar）时，传入的实参也必须转换成const void *型。在compar函数内部会将const void *型转换成实际类型，见下文。\n\nint compar(const void *p1, const void *p2);\n如果compar返回值小于0（&lt; 0），那么p1所指向元素会被排在p2所指向元素的前面\n如果compar返回值等于0（= 0），那么p1所指向元素与p2所指向元素的顺序不确定\n如果compar返回值大于0（&gt; 0），那么p1所指向元素会被排在p2所指向元素的后面\n因此，如果想让qsort()进行从小到大（升序）排序，那么一个通用的compar函数可以写成这样：\n\nint compareMyType (const void * a, const void * b)\n{\n   if ( *(MyType*)a &lt;  *(MyType*)b ) return -1;\n   if ( *(MyType*)a == *(MyType*)b ) return 0;\n   if ( *(MyType*)a &gt;  *(MyType*)b ) return 1;\n}\n\n注意：你要将MyType换成实际数组元素的类型。\n\n使用快排后\nRuntime: 4 ms, faster than 100.00% of C online submissions for 3Sum Closest.\nint comp(const void*a,const void*b)\n{\n    return *(int*)a-*(int*)b;\n}\n \nint threeSumClosest(int* nums, int numsSize, int target) \n{\n    int result=nums[0] + nums[1] + nums[2];\n    \n    if(numsSize &lt;= 3)\n        return result;\n    \n    int i, j, k;\n    int sum;\n    \n    qsort(nums, numsSize, sizeof(int), comp);\n        \n    for(i=0; i&lt;numsSize-2; i++)\n    {\n        j = i + 1;\n        k = numsSize - 1;\n        while(j &lt; k)\n        {\n            sum = nums[i] + nums[j] + nums[k];\n            if(abs(sum-target) &lt;= abs(result-target))\n            {\n                if(sum == target)\n                    return sum;\n                result = sum;   \n            }\n            (sum &gt; target)? k--: j++;\n        }\n    }\n    \n    return result;\n}"},"算法练习题/LeetCode-28.Implement-strStr().":{"slug":"算法练习题/LeetCode-28.Implement-strStr().","filePath":"算法练习题/LeetCode 28.Implement strStr()..md","title":"LeetCode 28.Implement strStr().","links":[],"tags":["算法","c","LeetCode"],"content":"28.Implement strStr().\nReturn the index of the first occurrence of needle in haystack, or -1 if needle is not part of haystack.\nExample 1:\nInput: haystack = “hello”, needle = “ll”\nOutput: 2\nExample 2:\nInput: haystack = “aaaaa”, needle = “bba”\nOutput: -1\nClarification:\nWhat should we return when needle is an empty string? This is a great question to ask during an interview.\nFor the purpose of this problem, we will return 0 when needle is an empty string. This is consistent to C’s strstr() and Java’s indexOf().\n三种解法，首先是暴力解法，本来我先写了这个解法试试看，结果leetcode没通过，在倒数第二个case时超时了，在处理aaaa…ab, aa…ab(非常多个a)这种情况时，时间复杂度时n*m，显然不是一个很好的算法，使用KMP算法只有O(n+m)。\n感谢UP主 [KMP算法]NEXT数列手算演示\n暴力破解法：\nint strStr(char* haystack, char* needle) {\n    int i = 0, j = 0, n = 0;\n    if(strlen(haystack) &lt; strlen(needle))\n        return -1;\n    if(strlen(needle) == 0)\n        return 0;\n    while(i &lt;= strlen(haystack))\n    {\n        if(haystack[i] == needle[j])\n            for(j = 0, n = i; j &lt; strlen(needle) &amp;&amp; haystack[n] == needle[j]; j++, n++)\n                ;\n        if(j == strlen(needle))\n            return i;\n        i++;\n        j = 0;\n    }\n    return -1;\n}\n \nKMP算法：\nint* next_arr(char* ptr)\n{\n    int ptr_num = strlen(ptr);\n    int* next;\n    int i = 0, j = 1;\n    next = calloc(ptr_num, sizeof(int));\n    for(i = 1, j = 0; i &lt; ptr_num;)\n    {\n        if(ptr[i] == ptr[j])\n            next[i++] = ++j;\n        else if(j)\n            j = next[j - 1];\n        else\n            next[i++] = 0;\n    }\n    return next;\n}\n \nint strStr_kmp(char* haystack, char* needle) \n{\n    int m = strlen(haystack), n = strlen(needle);\n    int i, j;\n    int* next = next_arr(needle);\n    if (!n)\n        return 0;\n    for (i = 0, j = 0; i &lt; m;)\n    {\n        if (haystack[i] == needle[j])\n        {\n            i++;\n            j++;\n        }\n        if(j == n)\n            return i - j;\n        if (i &lt; m &amp;&amp; haystack[i] != needle[j])\n        {\n            if (j)\n                j = next[j - 1];\n            else\n                 i++;\n        }\n    }\n    free(next);\n    return -1;  \n}\n使用KMP算法运算速率成功的打败了100%的人，然后点了\nsample 0 ms submission，看到了别人的算法。。。\nint strStr(char* haystack, char* needle) {\n    int a = strlen(needle);\n    int b = strlen(haystack);\n    if(a==0)\n        return 0;\n    for(int i=0;i&lt;b-a+1;i++){\n        for(int j = 0; j&lt;b; j++){\n            if(haystack[i+j] == needle[j]){\n                if(j == a-1)\n                    return i;\n            }\n            else\n                break;\n        }\n    }\n    return -1;\n}"},"算法练习题/埃及分数":{"slug":"算法练习题/埃及分数","filePath":"算法练习题/埃及分数.md","title":"埃及分数","links":[],"tags":["python"],"content":"在看科普视频的时候学到了埃及分数和贪婪算法，这里用code实现一下。\n埃及分数\n古埃人使用的是象形文字，他们用这样的符号表示分数\n\n\n他们在圈圈下面画几个竖线代表几分之一，还有几个规定好的分数有特殊的符号，分数有无数多个，不可能给所有分数都画上符号，所以古埃及人把任意分数都表示为不同的单位分数的和，就是分子为1，分母为各不相同的正整数。任何正有理数都能表达成这一个形式。\n例如：\n1=\\frac12+\\frac13+\\frac16\n1还可以表示为：\n1=\\frac12+\\frac13+\\frac19+\\frac1{18}\n接下使用贪婪算法来生成任意一个正有理数的埃及分数形式。\n贪婪算法\n贪婪算法：将一项分数分解成若干项单分子分数后的项数最少，称为第一种好算法；最大的分母数值最小，称为第二种好算法。 例如：\n{\\displaystyle {\\frac {2}{7}}={\\frac {1}{4}}+{\\frac {1}{28}}}。共2项，是第一种好算法，比{\\displaystyle {\\frac {2}{7}}={\\frac {1}{5}}+{\\frac {1}{20}}+{\\frac {1}{28}}}的项数要少。\n又例如，{\\displaystyle {\\frac {5}{121}}={\\frac {1}{33}}+{\\frac {1}{121}}+{\\frac {1}{363}}}比 {\\displaystyle {\\frac {5}{121}}={\\frac {1}{25}}+{\\frac {1}{759}}+{\\frac {1}{208725}}} 的最大分母要小，所以是第二种好算法。\n找出仅小于{\\displaystyle r={\\frac {a}{b}}}的最大单位分数。这个分数的分母的计算方法是：即用{\\displaystyle b}除以{\\displaystyle a}，舍去余数，再加1。（如果没有余数，则{\\displaystyle r}已是单位分数。）\n把{\\displaystyle r}减去单位分数，以这个新的、更小的{\\displaystyle r}重复步骤1。\n例子：把{\\displaystyle {\\frac {19}{20}}}转成单位分数。\n{\\displaystyle \\lfloor 20\\div 19\\rfloor =1}，所以第1个单位分数是{\\frac {1}{2}}；\n{\\displaystyle {\\frac {19}{20}}-{\\frac {1}{2}}={\\frac {9}{20}}}；\n{\\displaystyle \\lfloor 20\\div 9\\rfloor =2}，所以第2个单位分数是{\\displaystyle {\\frac {1}{3}}}；\n{\\displaystyle {\\frac {9}{20}}-{\\frac {1}{3}}={\\frac {7}{60}}}；\n{\\displaystyle \\lfloor 60\\div 7\\rfloor =8}，所以第3个单位分数是{\\displaystyle {\\frac {1}{9}}}；\n{\\displaystyle {\\frac {7}{60}}-{\\frac {1}{9}}={\\frac {1}{180}}}已是单位分数。\n所以结果是：\n{\\displaystyle {\\frac {19}{20}}={\\frac {1}{2}}+{\\frac {1}{3}}+{\\frac {1}{9}}+{\\frac {1}{180}}}。\n代码实现\n# 寻找最大公约数\ndef _gcd(a, b):\n    # Supports non-integers for backward compatibility.\n    while b:\n        a, b = b, a%b\n    return a\n \n# 约分\ndef ReduceFraction(numerator, denominator):\n    if type(numerator) is int is type(denominator):\n        g = _gcd(numerator, denominator)\n        if denominator &lt; 0:\n            g = -g\n        else:\n            g = _gcd(numerator, denominator)\n        numerator //= g\n        denominator //= g\n    return (numerator, denominator)\n \n# 分数减法\ndef SubFraction(a, b):\n    an, ad = a\n    bn, bd = b\n    numerator = an*bd - ad*bn\n    denominator = ad * bd\n    return ReduceFraction(numerator, denominator)\n \n# 埃及分数生成器返回分母的list\ndef EgpytFraction(numerator=0, denominator=None, ret=[]):\n    a = (denominator // numerator) + 1\n    ret.append(a)\n    t = SubFraction((numerator, denominator), (1, a))\n    if t[0] == 1:\n        ret.append(t[1])\n        return ret\n    return EgpytFraction(t[0], t[1], ret=ret)\nEgpytFraction(5, 121, ret)\n[25, 757, 763309, 873960180913, 1527612795642093418846225]\n\n总结\n埃及分数的表示不是唯一的，但应该有一个项数最少的表达式，我们把这个叫做最优的，但目前还没有一个算法可以求出最优的埃及分数。"},"算法练习题/微软笔试算法（一）":{"slug":"算法练习题/微软笔试算法（一）","filePath":"算法练习题/微软笔试算法（一）.md","title":"微软笔试算法（一）","links":[],"tags":["算法","python"],"content":"最近求职中接到的好多外包公司的电话，一般我都是拒绝的，其中告知外包到微软的就有三家，有一家说是微软小冰项目，给了我八道算法题，都不是太难，有的在LeetCode上还做过，在这里记录一下，正好也整理下LeetCode上刷过的题。\n这里记录的算法是只用python实现，也是我目前能想到的最优解，一共八个问题。\n\n完成一个函数：def Add(num1, num2):其中，两个输入都是数字，都是字符串（如：“12345678986543210123456789”），要求计算两个数字的和，返回一个字符串，不能使用内置函数，如int，long等。例如，输入两个数字是：“1000000000000000”和“-1”，返回“999999999999999”。\n\n这个问题比较简单了，主要就是三个小问题：\n\n字符串和数字的互转\n加法的进位\n减法的借位\n\n字符串和数字的互转不让用int函数，这个简单，python只要写个字典就解决了，C的话就要写switch case来一个一个比较了。这里就解决了字符转数字的问题了。\n到的数字开始计算，因为要设计借位和进位，答案会有四种情况\n\n正数 + 正数 = 正数   2 + 3 = 5\n负数 + 负数 = 负数   -2 + -3 = -5\n大正数 + 负数 = 正数 -2 + 3 = 1\n正数 + 大负数 = 负数 2 + -3 = -1\n\n貌似就是这样，但是观察结果除了负号就是两种，所以程序一开始应该先分析负号，进位的情况有1,2，借位情况是3,4，正好这两个结果除了负号答案一样，所以分两类就OK\n# 问题 1\ndef question1_add(num1, num2):\n    # 创建个字典用来把字符转为int便于计算\n    def strtoint(str):\n        numdict = {&quot;0&quot;: 0, &quot;1&quot;: 1, &quot;2&quot;: 2, &quot;3&quot;: 3, &quot;4&quot;: 4, &quot;5&quot;: 5, &quot;6&quot;: 6, &quot;7&quot;: 7, &quot;8&quot;: 8, &quot;9&quot;: 9}\n        return numdict.get(str)\n    \n    # 把列表中的数字转会字符串 \n    def inttostr(lt):\n        ls2 = [str(i) for i in lt]\n        return &#039;&#039;.join(ls2)\n \n    lt, x, y = [], [], []\n    \n    # 两个数是否为负数的标记\n    minusflag1, minusflag2 = 0, 0\n    \n    # 首先先提取出负号，记录负号的情况 \n    if num1[0] is &quot;-&quot;:\n        x = list(map(strtoint, num1[1:]))\n        minusflag1 = 1\n    else:\n        x = list(map(strtoint, num1))\n \n    if num2[0] is &quot;-&quot;:\n        y = list(map(strtoint, num2[1:]))\n        minusflag2 = 1\n    else:\n        y = list(map(strtoint, num2))\n \n    # 把 x作为最长的那列，y是短的那列\n    if len(x) &lt; len(y):\n        x, y = y, x\n \n    maxlen, minlen = len(x), len(y)\n    \n    # 在短的那列前面插上0让两列一样齐以便于计算\n    for i in range(maxlen-minlen):\n        y.insert(0, 0)\n \n    # 计算通常是从最低位开始，所以i从最低位开始\n    # carry记录借位和进位情况\n    i, carry = 0, 0\n \n    # 两个数都是正数或负数\n    if (minusflag1 == 0 and minusflag2 == 0) or (minusflag1 == 1 and minusflag2 == 1):\n        # +1是为了防止最高位有进位的情况，给最高位加一个0\n        for j in range(maxlen+1):\n            i -= 1\n            if j &lt; maxlen:\n                sum = x[i] + y[i] + carry\n            else:\n                sum = carry\n            # 两个数相加大于十代表有进位， 把结果和10的商给新列表，余给进位标志   \n            if sum &gt;= 10:\n                lt.insert(0, sum%10)\n                carry = sum//10\n            else:\n                lt.insert(0, sum)\n                carry = 0\n        # 如果最高位没有进位 把0删除       \n        if lt[0] == 0:\n            lt.pop(0)\n \n        # 如果都为负数 加 -\n        if minusflag1 == 1 and minusflag2 == 1:\n            lt.insert(0, &quot;-&quot;)\n            return inttostr(lt)\n        else:\n            return inttostr(lt)\n \n    # 一正一负\n    if (minusflag1 == 1 and minusflag2 == 0) or (minusflag1 == 0 and minusflag2 == 1):\n        for j in range(maxlen):\n            i -= 1\n            sum = x[i] - y[i] + carry\n            if sum &lt; 0:\n                lt.insert(0, sum%10)\n                carry = sum//10\n            else:\n                lt.insert(0, sum)\n                carry = 0\n \n        while lt[0] == 0:\n            lt.pop(0)\n            if len(lt) == 1:\n                break\n \n        # 判断是否应该加 - 号\n        if (num1[0] is &quot;-&quot; and len(num1[1:]) &gt; len(num2)) or num2[0] is &quot;-&quot; and len(num2[1:]) &gt; len(num1) :\n            lt.insert(0, &quot;-&quot;)\n            return inttostr(lt)\n        return inttostr(lt)\n\n给定一个数组nums，然后对其排序，使得排序结果满足nums[0] &lt; nums[1] &gt; nums[2] &lt; nums[3]…。 例如给定数组nums=[1,2,3,4,5,6,7,8,9],其中一个满足条件的结果是1&lt;6&gt;2&lt;7&gt;3&lt;8&gt;4&lt;9&gt;5.给出一个结果即可（可能无解）。最优解法是O(n)时间复杂度和O(1)空间复杂度。\n\n原题位置：leetcode.com/problems/wiggle-sort-ii/\n这道题就比较有趣了，LeetCode上大神给出的算法挺多，但是最优解不是O(n)，用python实现的话还能比O(n)小。\n先说我看到题第一眼的想法和看了LeetCode上大神的方法吧，很简单就三行\n    nums.sort()\n    median = len(nums[::2]) - 1\n    nums[::2], nums[1::2] = nums[median::-1], nums[:median:-1]\n首先排序，找到中位数，然后把中位数左右两边的交叉相排就OK了。\n好吧，详细一点吧，S代表比中位数小的，L代表比中位数大的，M代表中位数\nSmall half:  M . S . S . S      Small half:  M . S . S . S .\nLarge half:  . L . L . M .      Large half:  . L . L . L . M\n--------------------------      --------------------------\nTogether:    M L S L S M S      Together:    M L S L S L S M\n\n根据这个原理还可以写成其他形式\n    nums.sort()\n    median = len(nums[::2])\n    nums[1::2], nums[::2] = nums[median:],nums[:median]\n这个方式简单但是不满足O(n)时间复杂度的要求，这个方法是LeetCode上大神想出的叫virtual indexing，地址leetcode.com/problems/wiggle-sort-ii/discuss/77677/O(n)+O(1)-after-median-Virtual-Indexing,我就是照着用python抄袭了一遍o(￣︶￣)o，顺便说下我写的这个稍有区别，时间复杂度比O(n)小一点。\n解释一下site()函数的作用\n为了防止median的元素挨在一起，也就是说奇数位置上的值是median，同时与他相邻的某个偶数位置上的值也是median导致排序失败\n为了避免这个问题，可以采用一种方法，即另j 每次移动两步，也就是说先另j直线奇数位置，再另j指向偶数位置，所以对于大小为10的序列，j的变化可能像是这样\n1 -&gt; 3 -&gt; 5 -&gt; 7 -&gt; 9 -&gt; 0 -&gt; 2 -&gt; 4 -&gt; 6 -&gt; 8\n\n暂且先不考虑怎么实现这样的改变，先说一下这样做带来的好处\n由上面j的变化可知，j的改变是每次移动两步，所以，根据算法描述。所有和median相等的元素一定是最后才固定位置，又因为当j指向的值等于median时，是不与i和k指向的任何一个元素交换的。所以，每次移动两步带来的结果是这些median永远不可能相邻。换句话说就是永远不会出现两个median挨着的情况\ndef question2_sort2(nums):\n    def site(n):\n        return (1 + 2 * n) % (len(nums) | 1)\n \n    nums.sort()\n    if len(nums) % 2 == 0:\n        median = (nums[len(nums) // 2] + nums[len(nums) // 2 - 1]) / 2\n    else:\n        median = nums[len(nums) // 2]\n    i, j, k = 0, 0, len(nums) - 1\n    while j &lt;= k:\n        if nums[site(j)] &gt; median:\n            nums[site(i)], nums[site(j)] = nums[site(j)], nums[site(i)]\n            i += 1\n            j += 1\n        elif nums[site(j)] &lt; median:\n            nums[site(j)], nums[site(k)] = nums[site(k)], nums[site(j)]\n            k -= 1\n            j += 1 # 没错就是这里，我认为执行过交换后j就在正确的位置上了，所以我让j+1结果就是循环次数减少了，如果进行j+1则会循环N次，两种的结果不同但都复合条件\n        else:\n            j += 1\n    return nums\n\n写一个函数，输入是两个int数组A和B。要求从A和B中分别取出一个数，使他们的和为20。打印出所有的组合。要求数字在数组中的位置和数字本身。比如输入为 A = [18, 2, 7, 8, 3], B = [17, 1, 19]，输出为 3 (A4) + 17 (B0) = 20，表示A的第4个元素是3，B的第0个元素是17\n\n这就是道送分题了，嵌套循环就OK了，没什么好说的。\ndef question3_take(a, b):\n    for i in range(len(a)):\n        for j in range(len(b)):\n            if a[i] + b[j] == 20:\n                print(&quot;%d (A%d) + %d (B%d) = 20&quot; % (a[i], i, b[j], j))\n                return\n    print(&quot; No solution!&quot;)\n\n写一个函数，输入一个随机的01序列，打印出这个序列中最长的01交替出现的序列的起始位置和结束位置。例如：输入“000101010101101”，输出起始位置2, 结束位置10\n\n这题让我纠结了下，因为需要记录的位置有点多\n因为我们要遍历这个序列中所有的01所以要两个指针同时移动才能确保01的出现，当出现01我们计数器n +1，然后判断是否到结尾或者下一个不是01的情况，我们就结束这段，记录这段的长度和结束开始的位置，如果下一段出现的01长我们就替换掉上次的记录。\n值得注意的是01可能出现在序列的奇数或偶数位上，所以要遍历两次，再比较奇偶上哪个长，返回长的那段。\ndef func(i, j, num):\n    start, end, n, count = 0, 0, 0, 0\n    while j &lt; len(num):\n        if num[i] is &#039;0&#039; and num[j] is &#039;1&#039;:\n            n += 1\n        if j+1 &gt;= len(num) - 1 or num[i+2] is &#039;1&#039; or num[j+2] is &#039;0&#039;:\n            if n &gt; count:\n                count = n\n                n = 0\n                end = j + 1\n                start = end - count*2\n        i += 2\n        j += 2\n    return start, end\n \n \ndef question4(num):\n    i, j = 0, 1\n    start1, end1 = func(i, j, num)\n    i, j = 1, 2\n    start2, end2 = func(i, j, num)\n    if end1 - start1 &gt; end2- start2:\n        return start1, end1\n    else:\n        return start2, end2"}}